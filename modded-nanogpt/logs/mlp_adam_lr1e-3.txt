import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        # self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "mlp_adam_lr1e-3"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
# embed_params = [p for n, p in model.named_parameters() if "embed" in n]
embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=1e-1, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizer2 = torch.optim.Adam(
    hidden_matrix_params + gate_params,
    lr=1e-3,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 04:36:32 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   33C    P0             112W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   31C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   28C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   31C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   32C    P0             116W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:70ms step_avg:69.61ms
step:2/2330 train_time:132ms step_avg:66.06ms
step:3/2330 train_time:144ms step_avg:47.95ms
step:4/2330 train_time:159ms step_avg:39.73ms
step:5/2330 train_time:172ms step_avg:34.33ms
step:6/2330 train_time:183ms step_avg:30.54ms
step:7/2330 train_time:207ms step_avg:29.59ms
step:8/2330 train_time:247ms step_avg:30.93ms
step:9/2330 train_time:282ms step_avg:31.29ms
step:10/2330 train_time:322ms step_avg:32.19ms
step:11/2330 train_time:356ms step_avg:32.35ms
step:12/2330 train_time:396ms step_avg:33.00ms
step:13/2330 train_time:430ms step_avg:33.09ms
step:14/2330 train_time:470ms step_avg:33.59ms
step:15/2330 train_time:504ms step_avg:33.63ms
step:16/2330 train_time:544ms step_avg:34.02ms
step:17/2330 train_time:578ms step_avg:34.01ms
step:18/2330 train_time:618ms step_avg:34.36ms
step:19/2330 train_time:653ms step_avg:34.35ms
step:20/2330 train_time:693ms step_avg:34.65ms
step:21/2330 train_time:727ms step_avg:34.63ms
step:22/2330 train_time:767ms step_avg:34.88ms
step:23/2330 train_time:801ms step_avg:34.84ms
step:24/2330 train_time:842ms step_avg:35.06ms
step:25/2330 train_time:876ms step_avg:35.03ms
step:26/2330 train_time:916ms step_avg:35.24ms
step:27/2330 train_time:951ms step_avg:35.21ms
step:28/2330 train_time:992ms step_avg:35.44ms
step:29/2330 train_time:1031ms step_avg:35.56ms
step:30/2330 train_time:1074ms step_avg:35.79ms
step:31/2330 train_time:1111ms step_avg:35.84ms
step:32/2330 train_time:1153ms step_avg:36.02ms
step:33/2330 train_time:1189ms step_avg:36.02ms
step:34/2330 train_time:1230ms step_avg:36.17ms
step:35/2330 train_time:1264ms step_avg:36.12ms
step:36/2330 train_time:1305ms step_avg:36.24ms
step:37/2330 train_time:1340ms step_avg:36.22ms
step:38/2330 train_time:1381ms step_avg:36.33ms
step:39/2330 train_time:1416ms step_avg:36.30ms
step:40/2330 train_time:1457ms step_avg:36.42ms
step:41/2330 train_time:1491ms step_avg:36.36ms
step:42/2330 train_time:1532ms step_avg:36.47ms
step:43/2330 train_time:1566ms step_avg:36.41ms
step:44/2330 train_time:1606ms step_avg:36.50ms
step:45/2330 train_time:1640ms step_avg:36.45ms
step:46/2330 train_time:1681ms step_avg:36.55ms
step:47/2330 train_time:1715ms step_avg:36.49ms
step:48/2330 train_time:1756ms step_avg:36.57ms
step:49/2330 train_time:1790ms step_avg:36.53ms
step:50/2330 train_time:1831ms step_avg:36.61ms
step:51/2330 train_time:1865ms step_avg:36.57ms
step:52/2330 train_time:1905ms step_avg:36.64ms
step:53/2330 train_time:1941ms step_avg:36.62ms
step:54/2330 train_time:1982ms step_avg:36.70ms
step:55/2330 train_time:2018ms step_avg:36.69ms
step:56/2330 train_time:2059ms step_avg:36.77ms
step:57/2330 train_time:2096ms step_avg:36.77ms
step:58/2330 train_time:2137ms step_avg:36.85ms
step:59/2330 train_time:2173ms step_avg:36.83ms
step:60/2330 train_time:2215ms step_avg:36.91ms
step:61/2330 train_time:2250ms step_avg:36.88ms
step:62/2330 train_time:2291ms step_avg:36.94ms
step:63/2330 train_time:2326ms step_avg:36.91ms
step:64/2330 train_time:2366ms step_avg:36.97ms
step:65/2330 train_time:2402ms step_avg:36.95ms
step:66/2330 train_time:2442ms step_avg:37.01ms
step:67/2330 train_time:2478ms step_avg:36.98ms
step:68/2330 train_time:2518ms step_avg:37.03ms
step:69/2330 train_time:2553ms step_avg:37.01ms
step:70/2330 train_time:2594ms step_avg:37.05ms
step:71/2330 train_time:2628ms step_avg:37.02ms
step:72/2330 train_time:2669ms step_avg:37.07ms
step:73/2330 train_time:2703ms step_avg:37.03ms
step:74/2330 train_time:2744ms step_avg:37.07ms
step:75/2330 train_time:2778ms step_avg:37.04ms
step:76/2330 train_time:2819ms step_avg:37.09ms
step:77/2330 train_time:2854ms step_avg:37.06ms
step:78/2330 train_time:2894ms step_avg:37.11ms
step:79/2330 train_time:2930ms step_avg:37.09ms
step:80/2330 train_time:2971ms step_avg:37.13ms
step:81/2330 train_time:3007ms step_avg:37.12ms
step:82/2330 train_time:3047ms step_avg:37.16ms
step:83/2330 train_time:3084ms step_avg:37.15ms
step:84/2330 train_time:3124ms step_avg:37.19ms
step:85/2330 train_time:3160ms step_avg:37.18ms
step:86/2330 train_time:3201ms step_avg:37.23ms
step:87/2330 train_time:3237ms step_avg:37.21ms
step:88/2330 train_time:3278ms step_avg:37.25ms
step:89/2330 train_time:3314ms step_avg:37.24ms
step:90/2330 train_time:3355ms step_avg:37.28ms
step:91/2330 train_time:3390ms step_avg:37.26ms
step:92/2330 train_time:3432ms step_avg:37.30ms
step:93/2330 train_time:3467ms step_avg:37.28ms
step:94/2330 train_time:3508ms step_avg:37.31ms
step:95/2330 train_time:3543ms step_avg:37.29ms
step:96/2330 train_time:3583ms step_avg:37.32ms
step:97/2330 train_time:3617ms step_avg:37.29ms
step:98/2330 train_time:3658ms step_avg:37.33ms
step:99/2330 train_time:3693ms step_avg:37.30ms
step:100/2330 train_time:3733ms step_avg:37.33ms
step:101/2330 train_time:3768ms step_avg:37.31ms
step:102/2330 train_time:3809ms step_avg:37.34ms
step:103/2330 train_time:3844ms step_avg:37.32ms
step:104/2330 train_time:3884ms step_avg:37.35ms
step:105/2330 train_time:3920ms step_avg:37.33ms
step:106/2330 train_time:3960ms step_avg:37.36ms
step:107/2330 train_time:3996ms step_avg:37.34ms
step:108/2330 train_time:4036ms step_avg:37.37ms
step:109/2330 train_time:4072ms step_avg:37.36ms
step:110/2330 train_time:4113ms step_avg:37.39ms
step:111/2330 train_time:4149ms step_avg:37.38ms
step:112/2330 train_time:4190ms step_avg:37.41ms
step:113/2330 train_time:4225ms step_avg:37.39ms
step:114/2330 train_time:4266ms step_avg:37.42ms
step:115/2330 train_time:4302ms step_avg:37.41ms
step:116/2330 train_time:4343ms step_avg:37.44ms
step:117/2330 train_time:4378ms step_avg:37.42ms
step:118/2330 train_time:4419ms step_avg:37.45ms
step:119/2330 train_time:4455ms step_avg:37.44ms
step:120/2330 train_time:4496ms step_avg:37.46ms
step:121/2330 train_time:4531ms step_avg:37.44ms
step:122/2330 train_time:4571ms step_avg:37.47ms
step:123/2330 train_time:4606ms step_avg:37.45ms
step:124/2330 train_time:4646ms step_avg:37.47ms
step:125/2330 train_time:4681ms step_avg:37.45ms
step:126/2330 train_time:4722ms step_avg:37.47ms
step:127/2330 train_time:4757ms step_avg:37.46ms
step:128/2330 train_time:4798ms step_avg:37.48ms
step:129/2330 train_time:4833ms step_avg:37.46ms
step:130/2330 train_time:4873ms step_avg:37.49ms
step:131/2330 train_time:4909ms step_avg:37.48ms
step:132/2330 train_time:4951ms step_avg:37.50ms
step:133/2330 train_time:4985ms step_avg:37.48ms
step:134/2330 train_time:5026ms step_avg:37.51ms
step:135/2330 train_time:5062ms step_avg:37.49ms
step:136/2330 train_time:5103ms step_avg:37.52ms
step:137/2330 train_time:5137ms step_avg:37.50ms
step:138/2330 train_time:5178ms step_avg:37.52ms
step:139/2330 train_time:5213ms step_avg:37.51ms
step:140/2330 train_time:5255ms step_avg:37.54ms
step:141/2330 train_time:5291ms step_avg:37.52ms
step:142/2330 train_time:5331ms step_avg:37.54ms
step:143/2330 train_time:5367ms step_avg:37.53ms
step:144/2330 train_time:5408ms step_avg:37.55ms
step:145/2330 train_time:5444ms step_avg:37.54ms
step:146/2330 train_time:5484ms step_avg:37.56ms
step:147/2330 train_time:5520ms step_avg:37.55ms
step:148/2330 train_time:5561ms step_avg:37.58ms
step:149/2330 train_time:5596ms step_avg:37.56ms
step:150/2330 train_time:5637ms step_avg:37.58ms
step:151/2330 train_time:5672ms step_avg:37.56ms
step:152/2330 train_time:5712ms step_avg:37.58ms
step:153/2330 train_time:5747ms step_avg:37.56ms
step:154/2330 train_time:5788ms step_avg:37.58ms
step:155/2330 train_time:5823ms step_avg:37.57ms
step:156/2330 train_time:5863ms step_avg:37.59ms
step:157/2330 train_time:5899ms step_avg:37.57ms
step:158/2330 train_time:5939ms step_avg:37.59ms
step:159/2330 train_time:5974ms step_avg:37.57ms
step:160/2330 train_time:6015ms step_avg:37.59ms
step:161/2330 train_time:6050ms step_avg:37.58ms
step:162/2330 train_time:6091ms step_avg:37.60ms
step:163/2330 train_time:6126ms step_avg:37.59ms
step:164/2330 train_time:6167ms step_avg:37.60ms
step:165/2330 train_time:6203ms step_avg:37.60ms
step:166/2330 train_time:6244ms step_avg:37.61ms
step:167/2330 train_time:6280ms step_avg:37.60ms
step:168/2330 train_time:6321ms step_avg:37.62ms
step:169/2330 train_time:6356ms step_avg:37.61ms
step:170/2330 train_time:6397ms step_avg:37.63ms
step:171/2330 train_time:6432ms step_avg:37.62ms
step:172/2330 train_time:6473ms step_avg:37.64ms
step:173/2330 train_time:6508ms step_avg:37.62ms
step:174/2330 train_time:6549ms step_avg:37.64ms
step:175/2330 train_time:6585ms step_avg:37.63ms
step:176/2330 train_time:6626ms step_avg:37.65ms
step:177/2330 train_time:6661ms step_avg:37.63ms
step:178/2330 train_time:6702ms step_avg:37.65ms
step:179/2330 train_time:6737ms step_avg:37.63ms
step:180/2330 train_time:6778ms step_avg:37.65ms
step:181/2330 train_time:6812ms step_avg:37.64ms
step:182/2330 train_time:6854ms step_avg:37.66ms
step:183/2330 train_time:6888ms step_avg:37.64ms
step:184/2330 train_time:6929ms step_avg:37.66ms
step:185/2330 train_time:6963ms step_avg:37.64ms
step:186/2330 train_time:7004ms step_avg:37.66ms
step:187/2330 train_time:7039ms step_avg:37.64ms
step:188/2330 train_time:7080ms step_avg:37.66ms
step:189/2330 train_time:7115ms step_avg:37.65ms
step:190/2330 train_time:7156ms step_avg:37.66ms
step:191/2330 train_time:7192ms step_avg:37.65ms
step:192/2330 train_time:7233ms step_avg:37.67ms
step:193/2330 train_time:7268ms step_avg:37.66ms
step:194/2330 train_time:7309ms step_avg:37.68ms
step:195/2330 train_time:7345ms step_avg:37.67ms
step:196/2330 train_time:7385ms step_avg:37.68ms
step:197/2330 train_time:7421ms step_avg:37.67ms
step:198/2330 train_time:7462ms step_avg:37.69ms
step:199/2330 train_time:7498ms step_avg:37.68ms
step:200/2330 train_time:7538ms step_avg:37.69ms
step:201/2330 train_time:7574ms step_avg:37.68ms
step:202/2330 train_time:7614ms step_avg:37.69ms
step:203/2330 train_time:7650ms step_avg:37.68ms
step:204/2330 train_time:7691ms step_avg:37.70ms
step:205/2330 train_time:7727ms step_avg:37.69ms
step:206/2330 train_time:7767ms step_avg:37.70ms
step:207/2330 train_time:7803ms step_avg:37.70ms
step:208/2330 train_time:7844ms step_avg:37.71ms
step:209/2330 train_time:7879ms step_avg:37.70ms
step:210/2330 train_time:7920ms step_avg:37.71ms
step:211/2330 train_time:7956ms step_avg:37.70ms
step:212/2330 train_time:7996ms step_avg:37.72ms
step:213/2330 train_time:8031ms step_avg:37.71ms
step:214/2330 train_time:8072ms step_avg:37.72ms
step:215/2330 train_time:8108ms step_avg:37.71ms
step:216/2330 train_time:8148ms step_avg:37.72ms
step:217/2330 train_time:8185ms step_avg:37.72ms
step:218/2330 train_time:8225ms step_avg:37.73ms
step:219/2330 train_time:8260ms step_avg:37.72ms
step:220/2330 train_time:8302ms step_avg:37.74ms
step:221/2330 train_time:8337ms step_avg:37.72ms
step:222/2330 train_time:8377ms step_avg:37.74ms
step:223/2330 train_time:8414ms step_avg:37.73ms
step:224/2330 train_time:8455ms step_avg:37.74ms
step:225/2330 train_time:8490ms step_avg:37.74ms
step:226/2330 train_time:8532ms step_avg:37.75ms
step:227/2330 train_time:8566ms step_avg:37.74ms
step:228/2330 train_time:8607ms step_avg:37.75ms
step:229/2330 train_time:8643ms step_avg:37.74ms
step:230/2330 train_time:8683ms step_avg:37.75ms
step:231/2330 train_time:8719ms step_avg:37.74ms
step:232/2330 train_time:8759ms step_avg:37.76ms
step:233/2330 train_time:8795ms step_avg:37.75ms
step:234/2330 train_time:8836ms step_avg:37.76ms
step:235/2330 train_time:8871ms step_avg:37.75ms
step:236/2330 train_time:8912ms step_avg:37.76ms
step:237/2330 train_time:8947ms step_avg:37.75ms
step:238/2330 train_time:8988ms step_avg:37.77ms
step:239/2330 train_time:9024ms step_avg:37.76ms
step:240/2330 train_time:9065ms step_avg:37.77ms
step:241/2330 train_time:9100ms step_avg:37.76ms
step:242/2330 train_time:9142ms step_avg:37.78ms
step:243/2330 train_time:9177ms step_avg:37.76ms
step:244/2330 train_time:9217ms step_avg:37.78ms
step:245/2330 train_time:9253ms step_avg:37.77ms
step:246/2330 train_time:9295ms step_avg:37.78ms
step:247/2330 train_time:9330ms step_avg:37.77ms
step:248/2330 train_time:9371ms step_avg:37.79ms
step:249/2330 train_time:9407ms step_avg:37.78ms
step:250/2330 train_time:9448ms step_avg:37.79ms
step:250/2330 val_loss:5.5519 train_time:9560ms step_avg:38.24ms
step:251/2330 train_time:9571ms step_avg:38.13ms
step:252/2330 train_time:9584ms step_avg:38.03ms
step:253/2330 train_time:9594ms step_avg:37.92ms
step:254/2330 train_time:9605ms step_avg:37.81ms
step:255/2330 train_time:9636ms step_avg:37.79ms
step:256/2330 train_time:9676ms step_avg:37.80ms
step:257/2330 train_time:9711ms step_avg:37.78ms
step:258/2330 train_time:9751ms step_avg:37.79ms
step:259/2330 train_time:9786ms step_avg:37.78ms
step:260/2330 train_time:9827ms step_avg:37.79ms
step:261/2330 train_time:9862ms step_avg:37.79ms
step:262/2330 train_time:9903ms step_avg:37.80ms
step:263/2330 train_time:9945ms step_avg:37.81ms
step:264/2330 train_time:9985ms step_avg:37.82ms
step:265/2330 train_time:10022ms step_avg:37.82ms
step:266/2330 train_time:10062ms step_avg:37.83ms
step:267/2330 train_time:10099ms step_avg:37.82ms
step:268/2330 train_time:10139ms step_avg:37.83ms
step:269/2330 train_time:10175ms step_avg:37.82ms
step:270/2330 train_time:10215ms step_avg:37.83ms
step:271/2330 train_time:10250ms step_avg:37.82ms
step:272/2330 train_time:10291ms step_avg:37.84ms
step:273/2330 train_time:10326ms step_avg:37.83ms
step:274/2330 train_time:10367ms step_avg:37.84ms
step:275/2330 train_time:10403ms step_avg:37.83ms
step:276/2330 train_time:10443ms step_avg:37.84ms
step:277/2330 train_time:10478ms step_avg:37.83ms
step:278/2330 train_time:10520ms step_avg:37.84ms
step:279/2330 train_time:10555ms step_avg:37.83ms
step:280/2330 train_time:10597ms step_avg:37.85ms
step:281/2330 train_time:10632ms step_avg:37.84ms
step:282/2330 train_time:10673ms step_avg:37.85ms
step:283/2330 train_time:10707ms step_avg:37.83ms
step:284/2330 train_time:10748ms step_avg:37.84ms
step:285/2330 train_time:10782ms step_avg:37.83ms
step:286/2330 train_time:10823ms step_avg:37.84ms
step:287/2330 train_time:10859ms step_avg:37.84ms
step:288/2330 train_time:10900ms step_avg:37.85ms
step:289/2330 train_time:10936ms step_avg:37.84ms
step:290/2330 train_time:10977ms step_avg:37.85ms
step:291/2330 train_time:11014ms step_avg:37.85ms
step:292/2330 train_time:11055ms step_avg:37.86ms
step:293/2330 train_time:11092ms step_avg:37.86ms
step:294/2330 train_time:11133ms step_avg:37.87ms
step:295/2330 train_time:11169ms step_avg:37.86ms
step:296/2330 train_time:11210ms step_avg:37.87ms
step:297/2330 train_time:11245ms step_avg:37.86ms
step:298/2330 train_time:11285ms step_avg:37.87ms
step:299/2330 train_time:11320ms step_avg:37.86ms
step:300/2330 train_time:11361ms step_avg:37.87ms
step:301/2330 train_time:11396ms step_avg:37.86ms
step:302/2330 train_time:11437ms step_avg:37.87ms
step:303/2330 train_time:11473ms step_avg:37.86ms
step:304/2330 train_time:11514ms step_avg:37.87ms
step:305/2330 train_time:11549ms step_avg:37.86ms
step:306/2330 train_time:11590ms step_avg:37.87ms
step:307/2330 train_time:11624ms step_avg:37.86ms
step:308/2330 train_time:11665ms step_avg:37.87ms
step:309/2330 train_time:11700ms step_avg:37.86ms
step:310/2330 train_time:11741ms step_avg:37.87ms
step:311/2330 train_time:11776ms step_avg:37.86ms
step:312/2330 train_time:11817ms step_avg:37.88ms
step:313/2330 train_time:11853ms step_avg:37.87ms
step:314/2330 train_time:11894ms step_avg:37.88ms
step:315/2330 train_time:11931ms step_avg:37.87ms
step:316/2330 train_time:11972ms step_avg:37.89ms
step:317/2330 train_time:12008ms step_avg:37.88ms
step:318/2330 train_time:12050ms step_avg:37.89ms
step:319/2330 train_time:12087ms step_avg:37.89ms
step:320/2330 train_time:12126ms step_avg:37.89ms
step:321/2330 train_time:12162ms step_avg:37.89ms
step:322/2330 train_time:12203ms step_avg:37.90ms
step:323/2330 train_time:12238ms step_avg:37.89ms
step:324/2330 train_time:12279ms step_avg:37.90ms
step:325/2330 train_time:12313ms step_avg:37.89ms
step:326/2330 train_time:12354ms step_avg:37.90ms
step:327/2330 train_time:12390ms step_avg:37.89ms
step:328/2330 train_time:12432ms step_avg:37.90ms
step:329/2330 train_time:12466ms step_avg:37.89ms
step:330/2330 train_time:12507ms step_avg:37.90ms
step:331/2330 train_time:12543ms step_avg:37.89ms
step:332/2330 train_time:12583ms step_avg:37.90ms
step:333/2330 train_time:12618ms step_avg:37.89ms
step:334/2330 train_time:12659ms step_avg:37.90ms
step:335/2330 train_time:12694ms step_avg:37.89ms
step:336/2330 train_time:12735ms step_avg:37.90ms
step:337/2330 train_time:12769ms step_avg:37.89ms
step:338/2330 train_time:12811ms step_avg:37.90ms
step:339/2330 train_time:12847ms step_avg:37.90ms
step:340/2330 train_time:12888ms step_avg:37.91ms
step:341/2330 train_time:12923ms step_avg:37.90ms
step:342/2330 train_time:12963ms step_avg:37.90ms
step:343/2330 train_time:13000ms step_avg:37.90ms
step:344/2330 train_time:13041ms step_avg:37.91ms
step:345/2330 train_time:13077ms step_avg:37.90ms
step:346/2330 train_time:13117ms step_avg:37.91ms
step:347/2330 train_time:13153ms step_avg:37.91ms
step:348/2330 train_time:13194ms step_avg:37.91ms
step:349/2330 train_time:13230ms step_avg:37.91ms
step:350/2330 train_time:13271ms step_avg:37.92ms
step:351/2330 train_time:13307ms step_avg:37.91ms
step:352/2330 train_time:13348ms step_avg:37.92ms
step:353/2330 train_time:13383ms step_avg:37.91ms
step:354/2330 train_time:13424ms step_avg:37.92ms
step:355/2330 train_time:13459ms step_avg:37.91ms
step:356/2330 train_time:13500ms step_avg:37.92ms
step:357/2330 train_time:13535ms step_avg:37.91ms
step:358/2330 train_time:13575ms step_avg:37.92ms
step:359/2330 train_time:13610ms step_avg:37.91ms
step:360/2330 train_time:13652ms step_avg:37.92ms
step:361/2330 train_time:13687ms step_avg:37.91ms
step:362/2330 train_time:13728ms step_avg:37.92ms
step:363/2330 train_time:13763ms step_avg:37.91ms
step:364/2330 train_time:13804ms step_avg:37.92ms
step:365/2330 train_time:13839ms step_avg:37.91ms
step:366/2330 train_time:13880ms step_avg:37.92ms
step:367/2330 train_time:13915ms step_avg:37.92ms
step:368/2330 train_time:13956ms step_avg:37.92ms
step:369/2330 train_time:13992ms step_avg:37.92ms
step:370/2330 train_time:14033ms step_avg:37.93ms
step:371/2330 train_time:14069ms step_avg:37.92ms
step:372/2330 train_time:14110ms step_avg:37.93ms
step:373/2330 train_time:14146ms step_avg:37.92ms
step:374/2330 train_time:14187ms step_avg:37.93ms
step:375/2330 train_time:14223ms step_avg:37.93ms
step:376/2330 train_time:14263ms step_avg:37.93ms
step:377/2330 train_time:14299ms step_avg:37.93ms
step:378/2330 train_time:14340ms step_avg:37.94ms
step:379/2330 train_time:14375ms step_avg:37.93ms
step:380/2330 train_time:14415ms step_avg:37.94ms
step:381/2330 train_time:14451ms step_avg:37.93ms
step:382/2330 train_time:14492ms step_avg:37.94ms
step:383/2330 train_time:14528ms step_avg:37.93ms
step:384/2330 train_time:14568ms step_avg:37.94ms
step:385/2330 train_time:14604ms step_avg:37.93ms
step:386/2330 train_time:14645ms step_avg:37.94ms
step:387/2330 train_time:14681ms step_avg:37.93ms
step:388/2330 train_time:14722ms step_avg:37.94ms
step:389/2330 train_time:14756ms step_avg:37.93ms
step:390/2330 train_time:14797ms step_avg:37.94ms
step:391/2330 train_time:14832ms step_avg:37.93ms
step:392/2330 train_time:14873ms step_avg:37.94ms
step:393/2330 train_time:14910ms step_avg:37.94ms
step:394/2330 train_time:14951ms step_avg:37.95ms
step:395/2330 train_time:14987ms step_avg:37.94ms
step:396/2330 train_time:15028ms step_avg:37.95ms
step:397/2330 train_time:15064ms step_avg:37.94ms
step:398/2330 train_time:15105ms step_avg:37.95ms
step:399/2330 train_time:15140ms step_avg:37.95ms
step:400/2330 train_time:15181ms step_avg:37.95ms
step:401/2330 train_time:15217ms step_avg:37.95ms
step:402/2330 train_time:15259ms step_avg:37.96ms
step:403/2330 train_time:15294ms step_avg:37.95ms
step:404/2330 train_time:15335ms step_avg:37.96ms
step:405/2330 train_time:15371ms step_avg:37.95ms
step:406/2330 train_time:15412ms step_avg:37.96ms
step:407/2330 train_time:15447ms step_avg:37.95ms
step:408/2330 train_time:15488ms step_avg:37.96ms
step:409/2330 train_time:15524ms step_avg:37.96ms
step:410/2330 train_time:15565ms step_avg:37.96ms
step:411/2330 train_time:15600ms step_avg:37.96ms
step:412/2330 train_time:15641ms step_avg:37.96ms
step:413/2330 train_time:15676ms step_avg:37.96ms
step:414/2330 train_time:15718ms step_avg:37.97ms
step:415/2330 train_time:15753ms step_avg:37.96ms
step:416/2330 train_time:15794ms step_avg:37.97ms
step:417/2330 train_time:15829ms step_avg:37.96ms
step:418/2330 train_time:15870ms step_avg:37.97ms
step:419/2330 train_time:15906ms step_avg:37.96ms
step:420/2330 train_time:15947ms step_avg:37.97ms
step:421/2330 train_time:15984ms step_avg:37.97ms
step:422/2330 train_time:16024ms step_avg:37.97ms
step:423/2330 train_time:16060ms step_avg:37.97ms
step:424/2330 train_time:16101ms step_avg:37.97ms
step:425/2330 train_time:16137ms step_avg:37.97ms
step:426/2330 train_time:16178ms step_avg:37.98ms
step:427/2330 train_time:16213ms step_avg:37.97ms
step:428/2330 train_time:16254ms step_avg:37.98ms
step:429/2330 train_time:16291ms step_avg:37.97ms
step:430/2330 train_time:16331ms step_avg:37.98ms
step:431/2330 train_time:16366ms step_avg:37.97ms
step:432/2330 train_time:16407ms step_avg:37.98ms
step:433/2330 train_time:16444ms step_avg:37.98ms
step:434/2330 train_time:16484ms step_avg:37.98ms
step:435/2330 train_time:16520ms step_avg:37.98ms
step:436/2330 train_time:16561ms step_avg:37.98ms
step:437/2330 train_time:16596ms step_avg:37.98ms
step:438/2330 train_time:16637ms step_avg:37.99ms
step:439/2330 train_time:16674ms step_avg:37.98ms
step:440/2330 train_time:16715ms step_avg:37.99ms
step:441/2330 train_time:16750ms step_avg:37.98ms
step:442/2330 train_time:16791ms step_avg:37.99ms
step:443/2330 train_time:16825ms step_avg:37.98ms
step:444/2330 train_time:16866ms step_avg:37.99ms
step:445/2330 train_time:16902ms step_avg:37.98ms
step:446/2330 train_time:16943ms step_avg:37.99ms
step:447/2330 train_time:16979ms step_avg:37.98ms
step:448/2330 train_time:17020ms step_avg:37.99ms
step:449/2330 train_time:17055ms step_avg:37.99ms
step:450/2330 train_time:17097ms step_avg:37.99ms
step:451/2330 train_time:17132ms step_avg:37.99ms
step:452/2330 train_time:17173ms step_avg:37.99ms
step:453/2330 train_time:17210ms step_avg:37.99ms
step:454/2330 train_time:17251ms step_avg:38.00ms
step:455/2330 train_time:17287ms step_avg:37.99ms
step:456/2330 train_time:17328ms step_avg:38.00ms
step:457/2330 train_time:17364ms step_avg:38.00ms
step:458/2330 train_time:17404ms step_avg:38.00ms
step:459/2330 train_time:17441ms step_avg:38.00ms
step:460/2330 train_time:17482ms step_avg:38.00ms
step:461/2330 train_time:17517ms step_avg:38.00ms
step:462/2330 train_time:17558ms step_avg:38.00ms
step:463/2330 train_time:17594ms step_avg:38.00ms
step:464/2330 train_time:17634ms step_avg:38.00ms
step:465/2330 train_time:17670ms step_avg:38.00ms
step:466/2330 train_time:17711ms step_avg:38.01ms
step:467/2330 train_time:17747ms step_avg:38.00ms
step:468/2330 train_time:17788ms step_avg:38.01ms
step:469/2330 train_time:17824ms step_avg:38.00ms
step:470/2330 train_time:17865ms step_avg:38.01ms
step:471/2330 train_time:17900ms step_avg:38.01ms
step:472/2330 train_time:17941ms step_avg:38.01ms
step:473/2330 train_time:17977ms step_avg:38.01ms
step:474/2330 train_time:18018ms step_avg:38.01ms
step:475/2330 train_time:18054ms step_avg:38.01ms
step:476/2330 train_time:18095ms step_avg:38.01ms
step:477/2330 train_time:18131ms step_avg:38.01ms
step:478/2330 train_time:18173ms step_avg:38.02ms
step:479/2330 train_time:18207ms step_avg:38.01ms
step:480/2330 train_time:18249ms step_avg:38.02ms
step:481/2330 train_time:18284ms step_avg:38.01ms
step:482/2330 train_time:18325ms step_avg:38.02ms
step:483/2330 train_time:18361ms step_avg:38.01ms
step:484/2330 train_time:18402ms step_avg:38.02ms
step:485/2330 train_time:18438ms step_avg:38.02ms
step:486/2330 train_time:18479ms step_avg:38.02ms
step:487/2330 train_time:18514ms step_avg:38.02ms
step:488/2330 train_time:18555ms step_avg:38.02ms
step:489/2330 train_time:18592ms step_avg:38.02ms
step:490/2330 train_time:18632ms step_avg:38.03ms
step:491/2330 train_time:18668ms step_avg:38.02ms
step:492/2330 train_time:18709ms step_avg:38.03ms
step:493/2330 train_time:18745ms step_avg:38.02ms
step:494/2330 train_time:18786ms step_avg:38.03ms
step:495/2330 train_time:18823ms step_avg:38.03ms
step:496/2330 train_time:18863ms step_avg:38.03ms
step:497/2330 train_time:18900ms step_avg:38.03ms
step:498/2330 train_time:18941ms step_avg:38.03ms
step:499/2330 train_time:18976ms step_avg:38.03ms
step:500/2330 train_time:19017ms step_avg:38.03ms
step:500/2330 val_loss:5.4197 train_time:19129ms step_avg:38.26ms
step:501/2330 train_time:19141ms step_avg:38.21ms
step:502/2330 train_time:19154ms step_avg:38.16ms
step:503/2330 train_time:19164ms step_avg:38.10ms
step:504/2330 train_time:19176ms step_avg:38.05ms
step:505/2330 train_time:19206ms step_avg:38.03ms
step:506/2330 train_time:19247ms step_avg:38.04ms
step:507/2330 train_time:19281ms step_avg:38.03ms
step:508/2330 train_time:19322ms step_avg:38.04ms
step:509/2330 train_time:19357ms step_avg:38.03ms
step:510/2330 train_time:19398ms step_avg:38.03ms
step:511/2330 train_time:19434ms step_avg:38.03ms
step:512/2330 train_time:19477ms step_avg:38.04ms
step:513/2330 train_time:19517ms step_avg:38.05ms
step:514/2330 train_time:19559ms step_avg:38.05ms
step:515/2330 train_time:19595ms step_avg:38.05ms
step:516/2330 train_time:19636ms step_avg:38.05ms
step:517/2330 train_time:19672ms step_avg:38.05ms
step:518/2330 train_time:19713ms step_avg:38.06ms
step:519/2330 train_time:19749ms step_avg:38.05ms
step:520/2330 train_time:19790ms step_avg:38.06ms
step:521/2330 train_time:19827ms step_avg:38.06ms
step:522/2330 train_time:19867ms step_avg:38.06ms
step:523/2330 train_time:19903ms step_avg:38.06ms
step:524/2330 train_time:19943ms step_avg:38.06ms
step:525/2330 train_time:19979ms step_avg:38.05ms
step:526/2330 train_time:20020ms step_avg:38.06ms
step:527/2330 train_time:20055ms step_avg:38.06ms
step:528/2330 train_time:20096ms step_avg:38.06ms
step:529/2330 train_time:20132ms step_avg:38.06ms
step:530/2330 train_time:20173ms step_avg:38.06ms
step:531/2330 train_time:20208ms step_avg:38.06ms
step:532/2330 train_time:20249ms step_avg:38.06ms
step:533/2330 train_time:20284ms step_avg:38.06ms
step:534/2330 train_time:20325ms step_avg:38.06ms
step:535/2330 train_time:20360ms step_avg:38.06ms
step:536/2330 train_time:20402ms step_avg:38.06ms
step:537/2330 train_time:20438ms step_avg:38.06ms
step:538/2330 train_time:20480ms step_avg:38.07ms
step:539/2330 train_time:20516ms step_avg:38.06ms
step:540/2330 train_time:20558ms step_avg:38.07ms
step:541/2330 train_time:20594ms step_avg:38.07ms
step:542/2330 train_time:20635ms step_avg:38.07ms
step:543/2330 train_time:20671ms step_avg:38.07ms
step:544/2330 train_time:20712ms step_avg:38.07ms
step:545/2330 train_time:20748ms step_avg:38.07ms
step:546/2330 train_time:20788ms step_avg:38.07ms
step:547/2330 train_time:20825ms step_avg:38.07ms
step:548/2330 train_time:20866ms step_avg:38.08ms
step:549/2330 train_time:20900ms step_avg:38.07ms
step:550/2330 train_time:20941ms step_avg:38.07ms
step:551/2330 train_time:20977ms step_avg:38.07ms
step:552/2330 train_time:21018ms step_avg:38.08ms
step:553/2330 train_time:21054ms step_avg:38.07ms
step:554/2330 train_time:21095ms step_avg:38.08ms
step:555/2330 train_time:21130ms step_avg:38.07ms
step:556/2330 train_time:21171ms step_avg:38.08ms
step:557/2330 train_time:21207ms step_avg:38.07ms
step:558/2330 train_time:21248ms step_avg:38.08ms
step:559/2330 train_time:21283ms step_avg:38.07ms
step:560/2330 train_time:21324ms step_avg:38.08ms
step:561/2330 train_time:21359ms step_avg:38.07ms
step:562/2330 train_time:21400ms step_avg:38.08ms
step:563/2330 train_time:21436ms step_avg:38.07ms
step:564/2330 train_time:21477ms step_avg:38.08ms
step:565/2330 train_time:21514ms step_avg:38.08ms
step:566/2330 train_time:21555ms step_avg:38.08ms
step:567/2330 train_time:21592ms step_avg:38.08ms
step:568/2330 train_time:21632ms step_avg:38.09ms
step:569/2330 train_time:21669ms step_avg:38.08ms
step:570/2330 train_time:21710ms step_avg:38.09ms
step:571/2330 train_time:21746ms step_avg:38.08ms
step:572/2330 train_time:21787ms step_avg:38.09ms
step:573/2330 train_time:21821ms step_avg:38.08ms
step:574/2330 train_time:21862ms step_avg:38.09ms
step:575/2330 train_time:21897ms step_avg:38.08ms
step:576/2330 train_time:21938ms step_avg:38.09ms
step:577/2330 train_time:21974ms step_avg:38.08ms
step:578/2330 train_time:22015ms step_avg:38.09ms
step:579/2330 train_time:22051ms step_avg:38.08ms
step:580/2330 train_time:22092ms step_avg:38.09ms
step:581/2330 train_time:22128ms step_avg:38.09ms
step:582/2330 train_time:22169ms step_avg:38.09ms
step:583/2330 train_time:22204ms step_avg:38.09ms
step:584/2330 train_time:22245ms step_avg:38.09ms
step:585/2330 train_time:22280ms step_avg:38.09ms
step:586/2330 train_time:22321ms step_avg:38.09ms
step:587/2330 train_time:22357ms step_avg:38.09ms
step:588/2330 train_time:22398ms step_avg:38.09ms
step:589/2330 train_time:22434ms step_avg:38.09ms
step:590/2330 train_time:22475ms step_avg:38.09ms
step:591/2330 train_time:22511ms step_avg:38.09ms
step:592/2330 train_time:22552ms step_avg:38.09ms
step:593/2330 train_time:22589ms step_avg:38.09ms
step:594/2330 train_time:22630ms step_avg:38.10ms
step:595/2330 train_time:22666ms step_avg:38.09ms
step:596/2330 train_time:22707ms step_avg:38.10ms
step:597/2330 train_time:22743ms step_avg:38.10ms
step:598/2330 train_time:22784ms step_avg:38.10ms
step:599/2330 train_time:22820ms step_avg:38.10ms
step:600/2330 train_time:22860ms step_avg:38.10ms
step:601/2330 train_time:22896ms step_avg:38.10ms
step:602/2330 train_time:22937ms step_avg:38.10ms
step:603/2330 train_time:22974ms step_avg:38.10ms
step:604/2330 train_time:23015ms step_avg:38.10ms
step:605/2330 train_time:23051ms step_avg:38.10ms
step:606/2330 train_time:23091ms step_avg:38.10ms
step:607/2330 train_time:23127ms step_avg:38.10ms
step:608/2330 train_time:23168ms step_avg:38.11ms
step:609/2330 train_time:23204ms step_avg:38.10ms
step:610/2330 train_time:23244ms step_avg:38.11ms
step:611/2330 train_time:23279ms step_avg:38.10ms
step:612/2330 train_time:23321ms step_avg:38.11ms
step:613/2330 train_time:23356ms step_avg:38.10ms
step:614/2330 train_time:23397ms step_avg:38.11ms
step:615/2330 train_time:23433ms step_avg:38.10ms
step:616/2330 train_time:23474ms step_avg:38.11ms
step:617/2330 train_time:23510ms step_avg:38.10ms
step:618/2330 train_time:23551ms step_avg:38.11ms
step:619/2330 train_time:23587ms step_avg:38.10ms
step:620/2330 train_time:23628ms step_avg:38.11ms
step:621/2330 train_time:23664ms step_avg:38.11ms
step:622/2330 train_time:23705ms step_avg:38.11ms
step:623/2330 train_time:23740ms step_avg:38.11ms
step:624/2330 train_time:23781ms step_avg:38.11ms
step:625/2330 train_time:23817ms step_avg:38.11ms
step:626/2330 train_time:23859ms step_avg:38.11ms
step:627/2330 train_time:23894ms step_avg:38.11ms
step:628/2330 train_time:23934ms step_avg:38.11ms
step:629/2330 train_time:23971ms step_avg:38.11ms
step:630/2330 train_time:24011ms step_avg:38.11ms
step:631/2330 train_time:24047ms step_avg:38.11ms
step:632/2330 train_time:24088ms step_avg:38.11ms
step:633/2330 train_time:24124ms step_avg:38.11ms
step:634/2330 train_time:24165ms step_avg:38.11ms
step:635/2330 train_time:24200ms step_avg:38.11ms
step:636/2330 train_time:24241ms step_avg:38.11ms
step:637/2330 train_time:24276ms step_avg:38.11ms
step:638/2330 train_time:24318ms step_avg:38.12ms
step:639/2330 train_time:24353ms step_avg:38.11ms
step:640/2330 train_time:24394ms step_avg:38.12ms
step:641/2330 train_time:24430ms step_avg:38.11ms
step:642/2330 train_time:24471ms step_avg:38.12ms
step:643/2330 train_time:24507ms step_avg:38.11ms
step:644/2330 train_time:24547ms step_avg:38.12ms
step:645/2330 train_time:24583ms step_avg:38.11ms
step:646/2330 train_time:24624ms step_avg:38.12ms
step:647/2330 train_time:24661ms step_avg:38.12ms
step:648/2330 train_time:24702ms step_avg:38.12ms
step:649/2330 train_time:24737ms step_avg:38.11ms
step:650/2330 train_time:24778ms step_avg:38.12ms
step:651/2330 train_time:24813ms step_avg:38.12ms
step:652/2330 train_time:24854ms step_avg:38.12ms
step:653/2330 train_time:24891ms step_avg:38.12ms
step:654/2330 train_time:24931ms step_avg:38.12ms
step:655/2330 train_time:24968ms step_avg:38.12ms
step:656/2330 train_time:25009ms step_avg:38.12ms
step:657/2330 train_time:25045ms step_avg:38.12ms
step:658/2330 train_time:25086ms step_avg:38.12ms
step:659/2330 train_time:25121ms step_avg:38.12ms
step:660/2330 train_time:25162ms step_avg:38.12ms
step:661/2330 train_time:25198ms step_avg:38.12ms
step:662/2330 train_time:25239ms step_avg:38.13ms
step:663/2330 train_time:25274ms step_avg:38.12ms
step:664/2330 train_time:25316ms step_avg:38.13ms
step:665/2330 train_time:25352ms step_avg:38.12ms
step:666/2330 train_time:25393ms step_avg:38.13ms
step:667/2330 train_time:25428ms step_avg:38.12ms
step:668/2330 train_time:25469ms step_avg:38.13ms
step:669/2330 train_time:25505ms step_avg:38.12ms
step:670/2330 train_time:25546ms step_avg:38.13ms
step:671/2330 train_time:25582ms step_avg:38.13ms
step:672/2330 train_time:25624ms step_avg:38.13ms
step:673/2330 train_time:25660ms step_avg:38.13ms
step:674/2330 train_time:25701ms step_avg:38.13ms
step:675/2330 train_time:25737ms step_avg:38.13ms
step:676/2330 train_time:25779ms step_avg:38.13ms
step:677/2330 train_time:25814ms step_avg:38.13ms
step:678/2330 train_time:25856ms step_avg:38.14ms
step:679/2330 train_time:25891ms step_avg:38.13ms
step:680/2330 train_time:25933ms step_avg:38.14ms
step:681/2330 train_time:25969ms step_avg:38.13ms
step:682/2330 train_time:26009ms step_avg:38.14ms
step:683/2330 train_time:26045ms step_avg:38.13ms
step:684/2330 train_time:26086ms step_avg:38.14ms
step:685/2330 train_time:26122ms step_avg:38.13ms
step:686/2330 train_time:26163ms step_avg:38.14ms
step:687/2330 train_time:26199ms step_avg:38.14ms
step:688/2330 train_time:26240ms step_avg:38.14ms
step:689/2330 train_time:26276ms step_avg:38.14ms
step:690/2330 train_time:26318ms step_avg:38.14ms
step:691/2330 train_time:26353ms step_avg:38.14ms
step:692/2330 train_time:26394ms step_avg:38.14ms
step:693/2330 train_time:26430ms step_avg:38.14ms
step:694/2330 train_time:26471ms step_avg:38.14ms
step:695/2330 train_time:26507ms step_avg:38.14ms
step:696/2330 train_time:26548ms step_avg:38.14ms
step:697/2330 train_time:26583ms step_avg:38.14ms
step:698/2330 train_time:26624ms step_avg:38.14ms
step:699/2330 train_time:26660ms step_avg:38.14ms
step:700/2330 train_time:26701ms step_avg:38.14ms
step:701/2330 train_time:26737ms step_avg:38.14ms
step:702/2330 train_time:26778ms step_avg:38.15ms
step:703/2330 train_time:26814ms step_avg:38.14ms
step:704/2330 train_time:26855ms step_avg:38.15ms
step:705/2330 train_time:26891ms step_avg:38.14ms
step:706/2330 train_time:26932ms step_avg:38.15ms
step:707/2330 train_time:26969ms step_avg:38.15ms
step:708/2330 train_time:27009ms step_avg:38.15ms
step:709/2330 train_time:27046ms step_avg:38.15ms
step:710/2330 train_time:27087ms step_avg:38.15ms
step:711/2330 train_time:27123ms step_avg:38.15ms
step:712/2330 train_time:27165ms step_avg:38.15ms
step:713/2330 train_time:27199ms step_avg:38.15ms
step:714/2330 train_time:27241ms step_avg:38.15ms
step:715/2330 train_time:27276ms step_avg:38.15ms
step:716/2330 train_time:27317ms step_avg:38.15ms
step:717/2330 train_time:27353ms step_avg:38.15ms
step:718/2330 train_time:27395ms step_avg:38.15ms
step:719/2330 train_time:27431ms step_avg:38.15ms
step:720/2330 train_time:27472ms step_avg:38.16ms
step:721/2330 train_time:27508ms step_avg:38.15ms
step:722/2330 train_time:27549ms step_avg:38.16ms
step:723/2330 train_time:27586ms step_avg:38.15ms
step:724/2330 train_time:27627ms step_avg:38.16ms
step:725/2330 train_time:27662ms step_avg:38.15ms
step:726/2330 train_time:27703ms step_avg:38.16ms
step:727/2330 train_time:27739ms step_avg:38.15ms
step:728/2330 train_time:27780ms step_avg:38.16ms
step:729/2330 train_time:27815ms step_avg:38.16ms
step:730/2330 train_time:27857ms step_avg:38.16ms
step:731/2330 train_time:27893ms step_avg:38.16ms
step:732/2330 train_time:27935ms step_avg:38.16ms
step:733/2330 train_time:27971ms step_avg:38.16ms
step:734/2330 train_time:28012ms step_avg:38.16ms
step:735/2330 train_time:28047ms step_avg:38.16ms
step:736/2330 train_time:28088ms step_avg:38.16ms
step:737/2330 train_time:28123ms step_avg:38.16ms
step:738/2330 train_time:28164ms step_avg:38.16ms
step:739/2330 train_time:28200ms step_avg:38.16ms
step:740/2330 train_time:28240ms step_avg:38.16ms
step:741/2330 train_time:28276ms step_avg:38.16ms
step:742/2330 train_time:28318ms step_avg:38.16ms
step:743/2330 train_time:28354ms step_avg:38.16ms
step:744/2330 train_time:28396ms step_avg:38.17ms
step:745/2330 train_time:28431ms step_avg:38.16ms
step:746/2330 train_time:28473ms step_avg:38.17ms
step:747/2330 train_time:28508ms step_avg:38.16ms
step:748/2330 train_time:28549ms step_avg:38.17ms
step:749/2330 train_time:28585ms step_avg:38.16ms
step:750/2330 train_time:28626ms step_avg:38.17ms
step:750/2330 val_loss:5.3541 train_time:28739ms step_avg:38.32ms
step:751/2330 train_time:28750ms step_avg:38.28ms
step:752/2330 train_time:28761ms step_avg:38.25ms
step:753/2330 train_time:28771ms step_avg:38.21ms
step:754/2330 train_time:28782ms step_avg:38.17ms
step:755/2330 train_time:28817ms step_avg:38.17ms
step:756/2330 train_time:28857ms step_avg:38.17ms
step:757/2330 train_time:28892ms step_avg:38.17ms
step:758/2330 train_time:28933ms step_avg:38.17ms
step:759/2330 train_time:28968ms step_avg:38.17ms
step:760/2330 train_time:29008ms step_avg:38.17ms
step:761/2330 train_time:29046ms step_avg:38.17ms
step:762/2330 train_time:29089ms step_avg:38.17ms
step:763/2330 train_time:29126ms step_avg:38.17ms
step:764/2330 train_time:29167ms step_avg:38.18ms
step:765/2330 train_time:29205ms step_avg:38.18ms
step:766/2330 train_time:29247ms step_avg:38.18ms
step:767/2330 train_time:29281ms step_avg:38.18ms
step:768/2330 train_time:29322ms step_avg:38.18ms
step:769/2330 train_time:29358ms step_avg:38.18ms
step:770/2330 train_time:29399ms step_avg:38.18ms
step:771/2330 train_time:29434ms step_avg:38.18ms
step:772/2330 train_time:29475ms step_avg:38.18ms
step:773/2330 train_time:29510ms step_avg:38.18ms
step:774/2330 train_time:29550ms step_avg:38.18ms
step:775/2330 train_time:29585ms step_avg:38.17ms
step:776/2330 train_time:29626ms step_avg:38.18ms
step:777/2330 train_time:29663ms step_avg:38.18ms
step:778/2330 train_time:29704ms step_avg:38.18ms
step:779/2330 train_time:29742ms step_avg:38.18ms
step:780/2330 train_time:29782ms step_avg:38.18ms
step:781/2330 train_time:29819ms step_avg:38.18ms
step:782/2330 train_time:29859ms step_avg:38.18ms
step:783/2330 train_time:29895ms step_avg:38.18ms
step:784/2330 train_time:29936ms step_avg:38.18ms
step:785/2330 train_time:29971ms step_avg:38.18ms
step:786/2330 train_time:30013ms step_avg:38.18ms
step:787/2330 train_time:30050ms step_avg:38.18ms
step:788/2330 train_time:30091ms step_avg:38.19ms
step:789/2330 train_time:30128ms step_avg:38.19ms
step:790/2330 train_time:30169ms step_avg:38.19ms
step:791/2330 train_time:30206ms step_avg:38.19ms
step:792/2330 train_time:30247ms step_avg:38.19ms
step:793/2330 train_time:30282ms step_avg:38.19ms
step:794/2330 train_time:30324ms step_avg:38.19ms
step:795/2330 train_time:30358ms step_avg:38.19ms
step:796/2330 train_time:30399ms step_avg:38.19ms
step:797/2330 train_time:30435ms step_avg:38.19ms
step:798/2330 train_time:30476ms step_avg:38.19ms
step:799/2330 train_time:30512ms step_avg:38.19ms
step:800/2330 train_time:30553ms step_avg:38.19ms
step:801/2330 train_time:30589ms step_avg:38.19ms
step:802/2330 train_time:30629ms step_avg:38.19ms
step:803/2330 train_time:30666ms step_avg:38.19ms
step:804/2330 train_time:30707ms step_avg:38.19ms
step:805/2330 train_time:30743ms step_avg:38.19ms
step:806/2330 train_time:30784ms step_avg:38.19ms
step:807/2330 train_time:30821ms step_avg:38.19ms
step:808/2330 train_time:30861ms step_avg:38.19ms
step:809/2330 train_time:30897ms step_avg:38.19ms
step:810/2330 train_time:30938ms step_avg:38.20ms
step:811/2330 train_time:30974ms step_avg:38.19ms
step:812/2330 train_time:31015ms step_avg:38.20ms
step:813/2330 train_time:31052ms step_avg:38.19ms
step:814/2330 train_time:31093ms step_avg:38.20ms
step:815/2330 train_time:31131ms step_avg:38.20ms
step:816/2330 train_time:31171ms step_avg:38.20ms
step:817/2330 train_time:31208ms step_avg:38.20ms
step:818/2330 train_time:31248ms step_avg:38.20ms
step:819/2330 train_time:31284ms step_avg:38.20ms
step:820/2330 train_time:31325ms step_avg:38.20ms
step:821/2330 train_time:31360ms step_avg:38.20ms
step:822/2330 train_time:31401ms step_avg:38.20ms
step:823/2330 train_time:31436ms step_avg:38.20ms
step:824/2330 train_time:31478ms step_avg:38.20ms
step:825/2330 train_time:31513ms step_avg:38.20ms
step:826/2330 train_time:31555ms step_avg:38.20ms
step:827/2330 train_time:31590ms step_avg:38.20ms
step:828/2330 train_time:31631ms step_avg:38.20ms
step:829/2330 train_time:31667ms step_avg:38.20ms
step:830/2330 train_time:31708ms step_avg:38.20ms
step:831/2330 train_time:31743ms step_avg:38.20ms
step:832/2330 train_time:31784ms step_avg:38.20ms
step:833/2330 train_time:31819ms step_avg:38.20ms
step:834/2330 train_time:31860ms step_avg:38.20ms
step:835/2330 train_time:31896ms step_avg:38.20ms
step:836/2330 train_time:31937ms step_avg:38.20ms
step:837/2330 train_time:31973ms step_avg:38.20ms
step:838/2330 train_time:32014ms step_avg:38.20ms
step:839/2330 train_time:32051ms step_avg:38.20ms
step:840/2330 train_time:32092ms step_avg:38.20ms
step:841/2330 train_time:32128ms step_avg:38.20ms
step:842/2330 train_time:32169ms step_avg:38.21ms
step:843/2330 train_time:32206ms step_avg:38.20ms
step:844/2330 train_time:32247ms step_avg:38.21ms
step:845/2330 train_time:32282ms step_avg:38.20ms
step:846/2330 train_time:32324ms step_avg:38.21ms
step:847/2330 train_time:32359ms step_avg:38.20ms
step:848/2330 train_time:32400ms step_avg:38.21ms
step:849/2330 train_time:32436ms step_avg:38.20ms
step:850/2330 train_time:32477ms step_avg:38.21ms
step:851/2330 train_time:32513ms step_avg:38.21ms
step:852/2330 train_time:32555ms step_avg:38.21ms
step:853/2330 train_time:32590ms step_avg:38.21ms
step:854/2330 train_time:32631ms step_avg:38.21ms
step:855/2330 train_time:32666ms step_avg:38.21ms
step:856/2330 train_time:32707ms step_avg:38.21ms
step:857/2330 train_time:32742ms step_avg:38.21ms
step:858/2330 train_time:32784ms step_avg:38.21ms
step:859/2330 train_time:32820ms step_avg:38.21ms
step:860/2330 train_time:32860ms step_avg:38.21ms
step:861/2330 train_time:32896ms step_avg:38.21ms
step:862/2330 train_time:32937ms step_avg:38.21ms
step:863/2330 train_time:32973ms step_avg:38.21ms
step:864/2330 train_time:33015ms step_avg:38.21ms
step:865/2330 train_time:33050ms step_avg:38.21ms
step:866/2330 train_time:33092ms step_avg:38.21ms
step:867/2330 train_time:33128ms step_avg:38.21ms
step:868/2330 train_time:33169ms step_avg:38.21ms
step:869/2330 train_time:33206ms step_avg:38.21ms
step:870/2330 train_time:33247ms step_avg:38.21ms
step:871/2330 train_time:33282ms step_avg:38.21ms
step:872/2330 train_time:33323ms step_avg:38.21ms
step:873/2330 train_time:33359ms step_avg:38.21ms
step:874/2330 train_time:33400ms step_avg:38.21ms
step:875/2330 train_time:33436ms step_avg:38.21ms
step:876/2330 train_time:33477ms step_avg:38.22ms
step:877/2330 train_time:33514ms step_avg:38.21ms
step:878/2330 train_time:33555ms step_avg:38.22ms
step:879/2330 train_time:33591ms step_avg:38.22ms
step:880/2330 train_time:33632ms step_avg:38.22ms
step:881/2330 train_time:33668ms step_avg:38.22ms
step:882/2330 train_time:33709ms step_avg:38.22ms
step:883/2330 train_time:33744ms step_avg:38.21ms
step:884/2330 train_time:33785ms step_avg:38.22ms
step:885/2330 train_time:33820ms step_avg:38.21ms
step:886/2330 train_time:33860ms step_avg:38.22ms
step:887/2330 train_time:33897ms step_avg:38.22ms
step:888/2330 train_time:33938ms step_avg:38.22ms
step:889/2330 train_time:33974ms step_avg:38.22ms
step:890/2330 train_time:34016ms step_avg:38.22ms
step:891/2330 train_time:34052ms step_avg:38.22ms
step:892/2330 train_time:34093ms step_avg:38.22ms
step:893/2330 train_time:34129ms step_avg:38.22ms
step:894/2330 train_time:34170ms step_avg:38.22ms
step:895/2330 train_time:34207ms step_avg:38.22ms
step:896/2330 train_time:34248ms step_avg:38.22ms
step:897/2330 train_time:34283ms step_avg:38.22ms
step:898/2330 train_time:34324ms step_avg:38.22ms
step:899/2330 train_time:34359ms step_avg:38.22ms
step:900/2330 train_time:34400ms step_avg:38.22ms
step:901/2330 train_time:34437ms step_avg:38.22ms
step:902/2330 train_time:34478ms step_avg:38.22ms
step:903/2330 train_time:34514ms step_avg:38.22ms
step:904/2330 train_time:34555ms step_avg:38.22ms
step:905/2330 train_time:34591ms step_avg:38.22ms
step:906/2330 train_time:34632ms step_avg:38.23ms
step:907/2330 train_time:34668ms step_avg:38.22ms
step:908/2330 train_time:34708ms step_avg:38.23ms
step:909/2330 train_time:34744ms step_avg:38.22ms
step:910/2330 train_time:34785ms step_avg:38.23ms
step:911/2330 train_time:34819ms step_avg:38.22ms
step:912/2330 train_time:34860ms step_avg:38.22ms
step:913/2330 train_time:34898ms step_avg:38.22ms
step:914/2330 train_time:34938ms step_avg:38.23ms
step:915/2330 train_time:34975ms step_avg:38.22ms
step:916/2330 train_time:35016ms step_avg:38.23ms
step:917/2330 train_time:35053ms step_avg:38.23ms
step:918/2330 train_time:35094ms step_avg:38.23ms
step:919/2330 train_time:35130ms step_avg:38.23ms
step:920/2330 train_time:35171ms step_avg:38.23ms
step:921/2330 train_time:35207ms step_avg:38.23ms
step:922/2330 train_time:35247ms step_avg:38.23ms
step:923/2330 train_time:35283ms step_avg:38.23ms
step:924/2330 train_time:35325ms step_avg:38.23ms
step:925/2330 train_time:35359ms step_avg:38.23ms
step:926/2330 train_time:35400ms step_avg:38.23ms
step:927/2330 train_time:35436ms step_avg:38.23ms
step:928/2330 train_time:35477ms step_avg:38.23ms
step:929/2330 train_time:35513ms step_avg:38.23ms
step:930/2330 train_time:35554ms step_avg:38.23ms
step:931/2330 train_time:35590ms step_avg:38.23ms
step:932/2330 train_time:35631ms step_avg:38.23ms
step:933/2330 train_time:35666ms step_avg:38.23ms
step:934/2330 train_time:35707ms step_avg:38.23ms
step:935/2330 train_time:35743ms step_avg:38.23ms
step:936/2330 train_time:35784ms step_avg:38.23ms
step:937/2330 train_time:35820ms step_avg:38.23ms
step:938/2330 train_time:35860ms step_avg:38.23ms
step:939/2330 train_time:35897ms step_avg:38.23ms
step:940/2330 train_time:35938ms step_avg:38.23ms
step:941/2330 train_time:35973ms step_avg:38.23ms
step:942/2330 train_time:36015ms step_avg:38.23ms
step:943/2330 train_time:36050ms step_avg:38.23ms
step:944/2330 train_time:36091ms step_avg:38.23ms
step:945/2330 train_time:36127ms step_avg:38.23ms
step:946/2330 train_time:36168ms step_avg:38.23ms
step:947/2330 train_time:36204ms step_avg:38.23ms
step:948/2330 train_time:36245ms step_avg:38.23ms
step:949/2330 train_time:36281ms step_avg:38.23ms
step:950/2330 train_time:36322ms step_avg:38.23ms
step:951/2330 train_time:36357ms step_avg:38.23ms
step:952/2330 train_time:36399ms step_avg:38.23ms
step:953/2330 train_time:36435ms step_avg:38.23ms
step:954/2330 train_time:36476ms step_avg:38.23ms
step:955/2330 train_time:36511ms step_avg:38.23ms
step:956/2330 train_time:36552ms step_avg:38.23ms
step:957/2330 train_time:36589ms step_avg:38.23ms
step:958/2330 train_time:36630ms step_avg:38.24ms
step:959/2330 train_time:36665ms step_avg:38.23ms
step:960/2330 train_time:36706ms step_avg:38.24ms
step:961/2330 train_time:36742ms step_avg:38.23ms
step:962/2330 train_time:36784ms step_avg:38.24ms
step:963/2330 train_time:36819ms step_avg:38.23ms
step:964/2330 train_time:36860ms step_avg:38.24ms
step:965/2330 train_time:36896ms step_avg:38.23ms
step:966/2330 train_time:36937ms step_avg:38.24ms
step:967/2330 train_time:36972ms step_avg:38.23ms
step:968/2330 train_time:37014ms step_avg:38.24ms
step:969/2330 train_time:37049ms step_avg:38.23ms
step:970/2330 train_time:37090ms step_avg:38.24ms
step:971/2330 train_time:37126ms step_avg:38.24ms
step:972/2330 train_time:37167ms step_avg:38.24ms
step:973/2330 train_time:37204ms step_avg:38.24ms
step:974/2330 train_time:37245ms step_avg:38.24ms
step:975/2330 train_time:37279ms step_avg:38.24ms
step:976/2330 train_time:37320ms step_avg:38.24ms
step:977/2330 train_time:37357ms step_avg:38.24ms
step:978/2330 train_time:37398ms step_avg:38.24ms
step:979/2330 train_time:37433ms step_avg:38.24ms
step:980/2330 train_time:37475ms step_avg:38.24ms
step:981/2330 train_time:37511ms step_avg:38.24ms
step:982/2330 train_time:37552ms step_avg:38.24ms
step:983/2330 train_time:37588ms step_avg:38.24ms
step:984/2330 train_time:37629ms step_avg:38.24ms
step:985/2330 train_time:37665ms step_avg:38.24ms
step:986/2330 train_time:37706ms step_avg:38.24ms
step:987/2330 train_time:37741ms step_avg:38.24ms
step:988/2330 train_time:37782ms step_avg:38.24ms
step:989/2330 train_time:37818ms step_avg:38.24ms
step:990/2330 train_time:37859ms step_avg:38.24ms
step:991/2330 train_time:37895ms step_avg:38.24ms
step:992/2330 train_time:37936ms step_avg:38.24ms
step:993/2330 train_time:37972ms step_avg:38.24ms
step:994/2330 train_time:38013ms step_avg:38.24ms
step:995/2330 train_time:38049ms step_avg:38.24ms
step:996/2330 train_time:38090ms step_avg:38.24ms
step:997/2330 train_time:38127ms step_avg:38.24ms
step:998/2330 train_time:38168ms step_avg:38.24ms
step:999/2330 train_time:38203ms step_avg:38.24ms
step:1000/2330 train_time:38244ms step_avg:38.24ms
step:1000/2330 val_loss:5.3182 train_time:38358ms step_avg:38.36ms
step:1001/2330 train_time:38369ms step_avg:38.33ms
step:1002/2330 train_time:38380ms step_avg:38.30ms
step:1003/2330 train_time:38389ms step_avg:38.27ms
step:1004/2330 train_time:38400ms step_avg:38.25ms
step:1005/2330 train_time:38435ms step_avg:38.24ms
step:1006/2330 train_time:38476ms step_avg:38.25ms
step:1007/2330 train_time:38511ms step_avg:38.24ms
step:1008/2330 train_time:38551ms step_avg:38.25ms
step:1009/2330 train_time:38586ms step_avg:38.24ms
step:1010/2330 train_time:38627ms step_avg:38.24ms
step:1011/2330 train_time:38662ms step_avg:38.24ms
step:1012/2330 train_time:38704ms step_avg:38.25ms
step:1013/2330 train_time:38750ms step_avg:38.25ms
step:1014/2330 train_time:38791ms step_avg:38.26ms
step:1015/2330 train_time:38828ms step_avg:38.25ms
step:1016/2330 train_time:38869ms step_avg:38.26ms
step:1017/2330 train_time:38904ms step_avg:38.25ms
step:1018/2330 train_time:38944ms step_avg:38.26ms
step:1019/2330 train_time:38980ms step_avg:38.25ms
step:1020/2330 train_time:39021ms step_avg:38.26ms
step:1021/2330 train_time:39056ms step_avg:38.25ms
step:1022/2330 train_time:39097ms step_avg:38.26ms
step:1023/2330 train_time:39132ms step_avg:38.25ms
step:1024/2330 train_time:39173ms step_avg:38.25ms
step:1025/2330 train_time:39207ms step_avg:38.25ms
step:1026/2330 train_time:39248ms step_avg:38.25ms
step:1027/2330 train_time:39287ms step_avg:38.25ms
step:1028/2330 train_time:39328ms step_avg:38.26ms
step:1029/2330 train_time:39366ms step_avg:38.26ms
step:1030/2330 train_time:39407ms step_avg:38.26ms
step:1031/2330 train_time:39442ms step_avg:38.26ms
step:1032/2330 train_time:39483ms step_avg:38.26ms
step:1033/2330 train_time:39519ms step_avg:38.26ms
step:1034/2330 train_time:39560ms step_avg:38.26ms
step:1035/2330 train_time:39595ms step_avg:38.26ms
step:1036/2330 train_time:39636ms step_avg:38.26ms
step:1037/2330 train_time:39673ms step_avg:38.26ms
step:1038/2330 train_time:39714ms step_avg:38.26ms
step:1039/2330 train_time:39752ms step_avg:38.26ms
step:1040/2330 train_time:39793ms step_avg:38.26ms
step:1041/2330 train_time:39829ms step_avg:38.26ms
step:1042/2330 train_time:39871ms step_avg:38.26ms
step:1043/2330 train_time:39906ms step_avg:38.26ms
step:1044/2330 train_time:39947ms step_avg:38.26ms
step:1045/2330 train_time:39982ms step_avg:38.26ms
step:1046/2330 train_time:40023ms step_avg:38.26ms
step:1047/2330 train_time:40059ms step_avg:38.26ms
step:1048/2330 train_time:40100ms step_avg:38.26ms
step:1049/2330 train_time:40135ms step_avg:38.26ms
step:1050/2330 train_time:40176ms step_avg:38.26ms
step:1051/2330 train_time:40212ms step_avg:38.26ms
step:1052/2330 train_time:40252ms step_avg:38.26ms
step:1053/2330 train_time:40288ms step_avg:38.26ms
step:1054/2330 train_time:40329ms step_avg:38.26ms
step:1055/2330 train_time:40365ms step_avg:38.26ms
step:1056/2330 train_time:40406ms step_avg:38.26ms
step:1057/2330 train_time:40441ms step_avg:38.26ms
step:1058/2330 train_time:40483ms step_avg:38.26ms
step:1059/2330 train_time:40518ms step_avg:38.26ms
step:1060/2330 train_time:40560ms step_avg:38.26ms
step:1061/2330 train_time:40596ms step_avg:38.26ms
step:1062/2330 train_time:40637ms step_avg:38.26ms
step:1063/2330 train_time:40674ms step_avg:38.26ms
step:1064/2330 train_time:40714ms step_avg:38.27ms
step:1065/2330 train_time:40751ms step_avg:38.26ms
step:1066/2330 train_time:40792ms step_avg:38.27ms
step:1067/2330 train_time:40828ms step_avg:38.26ms
step:1068/2330 train_time:40870ms step_avg:38.27ms
step:1069/2330 train_time:40905ms step_avg:38.26ms
step:1070/2330 train_time:40946ms step_avg:38.27ms
step:1071/2330 train_time:40982ms step_avg:38.26ms
step:1072/2330 train_time:41023ms step_avg:38.27ms
step:1073/2330 train_time:41058ms step_avg:38.26ms
step:1074/2330 train_time:41100ms step_avg:38.27ms
step:1075/2330 train_time:41135ms step_avg:38.26ms
step:1076/2330 train_time:41176ms step_avg:38.27ms
step:1077/2330 train_time:41212ms step_avg:38.27ms
step:1078/2330 train_time:41252ms step_avg:38.27ms
step:1079/2330 train_time:41288ms step_avg:38.26ms
step:1080/2330 train_time:41329ms step_avg:38.27ms
step:1081/2330 train_time:41365ms step_avg:38.27ms
step:1082/2330 train_time:41406ms step_avg:38.27ms
step:1083/2330 train_time:41442ms step_avg:38.27ms
step:1084/2330 train_time:41483ms step_avg:38.27ms
step:1085/2330 train_time:41519ms step_avg:38.27ms
step:1086/2330 train_time:41561ms step_avg:38.27ms
step:1087/2330 train_time:41597ms step_avg:38.27ms
step:1088/2330 train_time:41639ms step_avg:38.27ms
step:1089/2330 train_time:41675ms step_avg:38.27ms
step:1090/2330 train_time:41715ms step_avg:38.27ms
step:1091/2330 train_time:41753ms step_avg:38.27ms
step:1092/2330 train_time:41794ms step_avg:38.27ms
step:1093/2330 train_time:41830ms step_avg:38.27ms
step:1094/2330 train_time:41871ms step_avg:38.27ms
step:1095/2330 train_time:41906ms step_avg:38.27ms
step:1096/2330 train_time:41947ms step_avg:38.27ms
step:1097/2330 train_time:41983ms step_avg:38.27ms
step:1098/2330 train_time:42024ms step_avg:38.27ms
step:1099/2330 train_time:42059ms step_avg:38.27ms
step:1100/2330 train_time:42100ms step_avg:38.27ms
step:1101/2330 train_time:42136ms step_avg:38.27ms
step:1102/2330 train_time:42178ms step_avg:38.27ms
step:1103/2330 train_time:42214ms step_avg:38.27ms
step:1104/2330 train_time:42255ms step_avg:38.27ms
step:1105/2330 train_time:42290ms step_avg:38.27ms
step:1106/2330 train_time:42331ms step_avg:38.27ms
step:1107/2330 train_time:42366ms step_avg:38.27ms
step:1108/2330 train_time:42407ms step_avg:38.27ms
step:1109/2330 train_time:42443ms step_avg:38.27ms
step:1110/2330 train_time:42484ms step_avg:38.27ms
step:1111/2330 train_time:42520ms step_avg:38.27ms
step:1112/2330 train_time:42561ms step_avg:38.27ms
step:1113/2330 train_time:42597ms step_avg:38.27ms
step:1114/2330 train_time:42638ms step_avg:38.28ms
step:1115/2330 train_time:42675ms step_avg:38.27ms
step:1116/2330 train_time:42716ms step_avg:38.28ms
step:1117/2330 train_time:42754ms step_avg:38.28ms
step:1118/2330 train_time:42794ms step_avg:38.28ms
step:1119/2330 train_time:42830ms step_avg:38.28ms
step:1120/2330 train_time:42872ms step_avg:38.28ms
step:1121/2330 train_time:42907ms step_avg:38.28ms
step:1122/2330 train_time:42948ms step_avg:38.28ms
step:1123/2330 train_time:42983ms step_avg:38.28ms
step:1124/2330 train_time:43025ms step_avg:38.28ms
step:1125/2330 train_time:43061ms step_avg:38.28ms
step:1126/2330 train_time:43102ms step_avg:38.28ms
step:1127/2330 train_time:43137ms step_avg:38.28ms
step:1128/2330 train_time:43178ms step_avg:38.28ms
step:1129/2330 train_time:43214ms step_avg:38.28ms
step:1130/2330 train_time:43255ms step_avg:38.28ms
step:1131/2330 train_time:43291ms step_avg:38.28ms
step:1132/2330 train_time:43332ms step_avg:38.28ms
step:1133/2330 train_time:43367ms step_avg:38.28ms
step:1134/2330 train_time:43408ms step_avg:38.28ms
step:1135/2330 train_time:43443ms step_avg:38.28ms
step:1136/2330 train_time:43484ms step_avg:38.28ms
step:1137/2330 train_time:43520ms step_avg:38.28ms
step:1138/2330 train_time:43562ms step_avg:38.28ms
step:1139/2330 train_time:43598ms step_avg:38.28ms
step:1140/2330 train_time:43640ms step_avg:38.28ms
step:1141/2330 train_time:43676ms step_avg:38.28ms
step:1142/2330 train_time:43717ms step_avg:38.28ms
step:1143/2330 train_time:43754ms step_avg:38.28ms
step:1144/2330 train_time:43795ms step_avg:38.28ms
step:1145/2330 train_time:43832ms step_avg:38.28ms
step:1146/2330 train_time:43873ms step_avg:38.28ms
step:1147/2330 train_time:43909ms step_avg:38.28ms
step:1148/2330 train_time:43950ms step_avg:38.28ms
step:1149/2330 train_time:43985ms step_avg:38.28ms
step:1150/2330 train_time:44026ms step_avg:38.28ms
step:1151/2330 train_time:44063ms step_avg:38.28ms
step:1152/2330 train_time:44104ms step_avg:38.28ms
step:1153/2330 train_time:44140ms step_avg:38.28ms
step:1154/2330 train_time:44181ms step_avg:38.29ms
step:1155/2330 train_time:44217ms step_avg:38.28ms
step:1156/2330 train_time:44258ms step_avg:38.29ms
step:1157/2330 train_time:44293ms step_avg:38.28ms
step:1158/2330 train_time:44334ms step_avg:38.28ms
step:1159/2330 train_time:44370ms step_avg:38.28ms
step:1160/2330 train_time:44411ms step_avg:38.29ms
step:1161/2330 train_time:44445ms step_avg:38.28ms
step:1162/2330 train_time:44486ms step_avg:38.28ms
step:1163/2330 train_time:44522ms step_avg:38.28ms
step:1164/2330 train_time:44563ms step_avg:38.28ms
step:1165/2330 train_time:44599ms step_avg:38.28ms
step:1166/2330 train_time:44641ms step_avg:38.29ms
step:1167/2330 train_time:44676ms step_avg:38.28ms
step:1168/2330 train_time:44719ms step_avg:38.29ms
step:1169/2330 train_time:44754ms step_avg:38.28ms
step:1170/2330 train_time:44796ms step_avg:38.29ms
step:1171/2330 train_time:44832ms step_avg:38.29ms
step:1172/2330 train_time:44873ms step_avg:38.29ms
step:1173/2330 train_time:44909ms step_avg:38.29ms
step:1174/2330 train_time:44950ms step_avg:38.29ms
step:1175/2330 train_time:44986ms step_avg:38.29ms
step:1176/2330 train_time:45026ms step_avg:38.29ms
step:1177/2330 train_time:45063ms step_avg:38.29ms
step:1178/2330 train_time:45104ms step_avg:38.29ms
step:1179/2330 train_time:45140ms step_avg:38.29ms
step:1180/2330 train_time:45181ms step_avg:38.29ms
step:1181/2330 train_time:45217ms step_avg:38.29ms
step:1182/2330 train_time:45258ms step_avg:38.29ms
step:1183/2330 train_time:45293ms step_avg:38.29ms
step:1184/2330 train_time:45334ms step_avg:38.29ms
step:1185/2330 train_time:45369ms step_avg:38.29ms
step:1186/2330 train_time:45410ms step_avg:38.29ms
step:1187/2330 train_time:45445ms step_avg:38.29ms
step:1188/2330 train_time:45486ms step_avg:38.29ms
step:1189/2330 train_time:45522ms step_avg:38.29ms
step:1190/2330 train_time:45563ms step_avg:38.29ms
step:1191/2330 train_time:45599ms step_avg:38.29ms
step:1192/2330 train_time:45640ms step_avg:38.29ms
step:1193/2330 train_time:45676ms step_avg:38.29ms
step:1194/2330 train_time:45717ms step_avg:38.29ms
step:1195/2330 train_time:45754ms step_avg:38.29ms
step:1196/2330 train_time:45795ms step_avg:38.29ms
step:1197/2330 train_time:45830ms step_avg:38.29ms
step:1198/2330 train_time:45872ms step_avg:38.29ms
step:1199/2330 train_time:45907ms step_avg:38.29ms
step:1200/2330 train_time:45948ms step_avg:38.29ms
step:1201/2330 train_time:45984ms step_avg:38.29ms
step:1202/2330 train_time:46025ms step_avg:38.29ms
step:1203/2330 train_time:46062ms step_avg:38.29ms
step:1204/2330 train_time:46103ms step_avg:38.29ms
step:1205/2330 train_time:46138ms step_avg:38.29ms
step:1206/2330 train_time:46180ms step_avg:38.29ms
step:1207/2330 train_time:46215ms step_avg:38.29ms
step:1208/2330 train_time:46256ms step_avg:38.29ms
step:1209/2330 train_time:46292ms step_avg:38.29ms
step:1210/2330 train_time:46333ms step_avg:38.29ms
step:1211/2330 train_time:46369ms step_avg:38.29ms
step:1212/2330 train_time:46410ms step_avg:38.29ms
step:1213/2330 train_time:46445ms step_avg:38.29ms
step:1214/2330 train_time:46486ms step_avg:38.29ms
step:1215/2330 train_time:46522ms step_avg:38.29ms
step:1216/2330 train_time:46563ms step_avg:38.29ms
step:1217/2330 train_time:46599ms step_avg:38.29ms
step:1218/2330 train_time:46640ms step_avg:38.29ms
step:1219/2330 train_time:46676ms step_avg:38.29ms
step:1220/2330 train_time:46718ms step_avg:38.29ms
step:1221/2330 train_time:46754ms step_avg:38.29ms
step:1222/2330 train_time:46795ms step_avg:38.29ms
step:1223/2330 train_time:46831ms step_avg:38.29ms
step:1224/2330 train_time:46872ms step_avg:38.29ms
step:1225/2330 train_time:46908ms step_avg:38.29ms
step:1226/2330 train_time:46949ms step_avg:38.29ms
step:1227/2330 train_time:46985ms step_avg:38.29ms
step:1228/2330 train_time:47026ms step_avg:38.29ms
step:1229/2330 train_time:47061ms step_avg:38.29ms
step:1230/2330 train_time:47103ms step_avg:38.29ms
step:1231/2330 train_time:47138ms step_avg:38.29ms
step:1232/2330 train_time:47180ms step_avg:38.30ms
step:1233/2330 train_time:47215ms step_avg:38.29ms
step:1234/2330 train_time:47256ms step_avg:38.30ms
step:1235/2330 train_time:47292ms step_avg:38.29ms
step:1236/2330 train_time:47333ms step_avg:38.29ms
step:1237/2330 train_time:47368ms step_avg:38.29ms
step:1238/2330 train_time:47409ms step_avg:38.30ms
step:1239/2330 train_time:47444ms step_avg:38.29ms
step:1240/2330 train_time:47485ms step_avg:38.29ms
step:1241/2330 train_time:47521ms step_avg:38.29ms
step:1242/2330 train_time:47563ms step_avg:38.30ms
step:1243/2330 train_time:47599ms step_avg:38.29ms
step:1244/2330 train_time:47640ms step_avg:38.30ms
step:1245/2330 train_time:47676ms step_avg:38.29ms
step:1246/2330 train_time:47718ms step_avg:38.30ms
step:1247/2330 train_time:47754ms step_avg:38.29ms
step:1248/2330 train_time:47795ms step_avg:38.30ms
step:1249/2330 train_time:47831ms step_avg:38.30ms
step:1250/2330 train_time:47872ms step_avg:38.30ms
step:1250/2330 val_loss:5.2853 train_time:47985ms step_avg:38.39ms
step:1251/2330 train_time:47995ms step_avg:38.37ms
step:1252/2330 train_time:48006ms step_avg:38.34ms
step:1253/2330 train_time:48016ms step_avg:38.32ms
step:1254/2330 train_time:48028ms step_avg:38.30ms
step:1255/2330 train_time:48062ms step_avg:38.30ms
step:1256/2330 train_time:48102ms step_avg:38.30ms
step:1257/2330 train_time:48137ms step_avg:38.30ms
step:1258/2330 train_time:48178ms step_avg:38.30ms
step:1259/2330 train_time:48213ms step_avg:38.29ms
step:1260/2330 train_time:48253ms step_avg:38.30ms
step:1261/2330 train_time:48293ms step_avg:38.30ms
step:1262/2330 train_time:48335ms step_avg:38.30ms
step:1263/2330 train_time:48375ms step_avg:38.30ms
step:1264/2330 train_time:48417ms step_avg:38.30ms
step:1265/2330 train_time:48453ms step_avg:38.30ms
step:1266/2330 train_time:48494ms step_avg:38.31ms
step:1267/2330 train_time:48530ms step_avg:38.30ms
step:1268/2330 train_time:48572ms step_avg:38.31ms
step:1269/2330 train_time:48608ms step_avg:38.30ms
step:1270/2330 train_time:48649ms step_avg:38.31ms
step:1271/2330 train_time:48685ms step_avg:38.30ms
step:1272/2330 train_time:48726ms step_avg:38.31ms
step:1273/2330 train_time:48761ms step_avg:38.30ms
step:1274/2330 train_time:48802ms step_avg:38.31ms
step:1275/2330 train_time:48837ms step_avg:38.30ms
step:1276/2330 train_time:48877ms step_avg:38.31ms
step:1277/2330 train_time:48913ms step_avg:38.30ms
step:1278/2330 train_time:48954ms step_avg:38.31ms
step:1279/2330 train_time:48991ms step_avg:38.30ms
step:1280/2330 train_time:49033ms step_avg:38.31ms
step:1281/2330 train_time:49068ms step_avg:38.30ms
step:1282/2330 train_time:49109ms step_avg:38.31ms
step:1283/2330 train_time:49144ms step_avg:38.30ms
step:1284/2330 train_time:49185ms step_avg:38.31ms
step:1285/2330 train_time:49222ms step_avg:38.30ms
step:1286/2330 train_time:49263ms step_avg:38.31ms
step:1287/2330 train_time:49301ms step_avg:38.31ms
step:1288/2330 train_time:49343ms step_avg:38.31ms
step:1289/2330 train_time:49380ms step_avg:38.31ms
step:1290/2330 train_time:49421ms step_avg:38.31ms
step:1291/2330 train_time:49457ms step_avg:38.31ms
step:1292/2330 train_time:49498ms step_avg:38.31ms
step:1293/2330 train_time:49534ms step_avg:38.31ms
step:1294/2330 train_time:49575ms step_avg:38.31ms
step:1295/2330 train_time:49611ms step_avg:38.31ms
step:1296/2330 train_time:49652ms step_avg:38.31ms
step:1297/2330 train_time:49687ms step_avg:38.31ms
step:1298/2330 train_time:49729ms step_avg:38.31ms
step:1299/2330 train_time:49763ms step_avg:38.31ms
step:1300/2330 train_time:49805ms step_avg:38.31ms
step:1301/2330 train_time:49840ms step_avg:38.31ms
step:1302/2330 train_time:49881ms step_avg:38.31ms
step:1303/2330 train_time:49917ms step_avg:38.31ms
step:1304/2330 train_time:49958ms step_avg:38.31ms
step:1305/2330 train_time:49993ms step_avg:38.31ms
step:1306/2330 train_time:50034ms step_avg:38.31ms
step:1307/2330 train_time:50070ms step_avg:38.31ms
step:1308/2330 train_time:50111ms step_avg:38.31ms
step:1309/2330 train_time:50146ms step_avg:38.31ms
step:1310/2330 train_time:50188ms step_avg:38.31ms
step:1311/2330 train_time:50223ms step_avg:38.31ms
step:1312/2330 train_time:50264ms step_avg:38.31ms
step:1313/2330 train_time:50300ms step_avg:38.31ms
step:1314/2330 train_time:50342ms step_avg:38.31ms
step:1315/2330 train_time:50379ms step_avg:38.31ms
step:1316/2330 train_time:50420ms step_avg:38.31ms
step:1317/2330 train_time:50456ms step_avg:38.31ms
step:1318/2330 train_time:50497ms step_avg:38.31ms
step:1319/2330 train_time:50532ms step_avg:38.31ms
step:1320/2330 train_time:50573ms step_avg:38.31ms
step:1321/2330 train_time:50608ms step_avg:38.31ms
step:1322/2330 train_time:50650ms step_avg:38.31ms
step:1323/2330 train_time:50685ms step_avg:38.31ms
step:1324/2330 train_time:50727ms step_avg:38.31ms
step:1325/2330 train_time:50762ms step_avg:38.31ms
step:1326/2330 train_time:50803ms step_avg:38.31ms
step:1327/2330 train_time:50839ms step_avg:38.31ms
step:1328/2330 train_time:50880ms step_avg:38.31ms
step:1329/2330 train_time:50915ms step_avg:38.31ms
step:1330/2330 train_time:50956ms step_avg:38.31ms
step:1331/2330 train_time:50992ms step_avg:38.31ms
step:1332/2330 train_time:51033ms step_avg:38.31ms
step:1333/2330 train_time:51069ms step_avg:38.31ms
step:1334/2330 train_time:51110ms step_avg:38.31ms
step:1335/2330 train_time:51146ms step_avg:38.31ms
step:1336/2330 train_time:51187ms step_avg:38.31ms
step:1337/2330 train_time:51224ms step_avg:38.31ms
step:1338/2330 train_time:51265ms step_avg:38.31ms
step:1339/2330 train_time:51301ms step_avg:38.31ms
step:1340/2330 train_time:51342ms step_avg:38.32ms
step:1341/2330 train_time:51379ms step_avg:38.31ms
step:1342/2330 train_time:51420ms step_avg:38.32ms
step:1343/2330 train_time:51455ms step_avg:38.31ms
step:1344/2330 train_time:51497ms step_avg:38.32ms
step:1345/2330 train_time:51532ms step_avg:38.31ms
step:1346/2330 train_time:51573ms step_avg:38.32ms
step:1347/2330 train_time:51609ms step_avg:38.31ms
step:1348/2330 train_time:51650ms step_avg:38.32ms
step:1349/2330 train_time:51686ms step_avg:38.31ms
step:1350/2330 train_time:51727ms step_avg:38.32ms
step:1351/2330 train_time:51764ms step_avg:38.32ms
step:1352/2330 train_time:51805ms step_avg:38.32ms
step:1353/2330 train_time:51841ms step_avg:38.32ms
step:1354/2330 train_time:51882ms step_avg:38.32ms
step:1355/2330 train_time:51917ms step_avg:38.32ms
step:1356/2330 train_time:51958ms step_avg:38.32ms
step:1357/2330 train_time:51993ms step_avg:38.31ms
step:1358/2330 train_time:52034ms step_avg:38.32ms
step:1359/2330 train_time:52069ms step_avg:38.31ms
step:1360/2330 train_time:52111ms step_avg:38.32ms
step:1361/2330 train_time:52146ms step_avg:38.31ms
step:1362/2330 train_time:52188ms step_avg:38.32ms
step:1363/2330 train_time:52224ms step_avg:38.32ms
step:1364/2330 train_time:52266ms step_avg:38.32ms
step:1365/2330 train_time:52302ms step_avg:38.32ms
step:1366/2330 train_time:52343ms step_avg:38.32ms
step:1367/2330 train_time:52380ms step_avg:38.32ms
step:1368/2330 train_time:52421ms step_avg:38.32ms
step:1369/2330 train_time:52458ms step_avg:38.32ms
step:1370/2330 train_time:52499ms step_avg:38.32ms
step:1371/2330 train_time:52534ms step_avg:38.32ms
step:1372/2330 train_time:52576ms step_avg:38.32ms
step:1373/2330 train_time:52611ms step_avg:38.32ms
step:1374/2330 train_time:52653ms step_avg:38.32ms
step:1375/2330 train_time:52688ms step_avg:38.32ms
step:1376/2330 train_time:52730ms step_avg:38.32ms
step:1377/2330 train_time:52765ms step_avg:38.32ms
step:1378/2330 train_time:52807ms step_avg:38.32ms
step:1379/2330 train_time:52842ms step_avg:38.32ms
step:1380/2330 train_time:52882ms step_avg:38.32ms
step:1381/2330 train_time:52918ms step_avg:38.32ms
step:1382/2330 train_time:52960ms step_avg:38.32ms
step:1383/2330 train_time:52995ms step_avg:38.32ms
step:1384/2330 train_time:53036ms step_avg:38.32ms
step:1385/2330 train_time:53072ms step_avg:38.32ms
step:1386/2330 train_time:53113ms step_avg:38.32ms
step:1387/2330 train_time:53150ms step_avg:38.32ms
step:1388/2330 train_time:53191ms step_avg:38.32ms
step:1389/2330 train_time:53227ms step_avg:38.32ms
step:1390/2330 train_time:53269ms step_avg:38.32ms
step:1391/2330 train_time:53304ms step_avg:38.32ms
step:1392/2330 train_time:53345ms step_avg:38.32ms
step:1393/2330 train_time:53382ms step_avg:38.32ms
step:1394/2330 train_time:53422ms step_avg:38.32ms
step:1395/2330 train_time:53458ms step_avg:38.32ms
step:1396/2330 train_time:53499ms step_avg:38.32ms
step:1397/2330 train_time:53534ms step_avg:38.32ms
step:1398/2330 train_time:53575ms step_avg:38.32ms
step:1399/2330 train_time:53610ms step_avg:38.32ms
step:1400/2330 train_time:53652ms step_avg:38.32ms
step:1401/2330 train_time:53688ms step_avg:38.32ms
step:1402/2330 train_time:53729ms step_avg:38.32ms
step:1403/2330 train_time:53765ms step_avg:38.32ms
step:1404/2330 train_time:53806ms step_avg:38.32ms
step:1405/2330 train_time:53842ms step_avg:38.32ms
step:1406/2330 train_time:53883ms step_avg:38.32ms
step:1407/2330 train_time:53919ms step_avg:38.32ms
step:1408/2330 train_time:53961ms step_avg:38.32ms
step:1409/2330 train_time:53995ms step_avg:38.32ms
step:1410/2330 train_time:54036ms step_avg:38.32ms
step:1411/2330 train_time:54071ms step_avg:38.32ms
step:1412/2330 train_time:54112ms step_avg:38.32ms
step:1413/2330 train_time:54149ms step_avg:38.32ms
step:1414/2330 train_time:54190ms step_avg:38.32ms
step:1415/2330 train_time:54226ms step_avg:38.32ms
step:1416/2330 train_time:54267ms step_avg:38.32ms
step:1417/2330 train_time:54302ms step_avg:38.32ms
step:1418/2330 train_time:54343ms step_avg:38.32ms
step:1419/2330 train_time:54380ms step_avg:38.32ms
step:1420/2330 train_time:54421ms step_avg:38.32ms
step:1421/2330 train_time:54457ms step_avg:38.32ms
step:1422/2330 train_time:54498ms step_avg:38.32ms
step:1423/2330 train_time:54533ms step_avg:38.32ms
step:1424/2330 train_time:54575ms step_avg:38.33ms
step:1425/2330 train_time:54610ms step_avg:38.32ms
step:1426/2330 train_time:54652ms step_avg:38.33ms
step:1427/2330 train_time:54688ms step_avg:38.32ms
step:1428/2330 train_time:54729ms step_avg:38.33ms
step:1429/2330 train_time:54765ms step_avg:38.32ms
step:1430/2330 train_time:54805ms step_avg:38.33ms
step:1431/2330 train_time:54842ms step_avg:38.32ms
step:1432/2330 train_time:54883ms step_avg:38.33ms
step:1433/2330 train_time:54919ms step_avg:38.32ms
step:1434/2330 train_time:54960ms step_avg:38.33ms
step:1435/2330 train_time:54995ms step_avg:38.32ms
step:1436/2330 train_time:55036ms step_avg:38.33ms
step:1437/2330 train_time:55071ms step_avg:38.32ms
step:1438/2330 train_time:55113ms step_avg:38.33ms
step:1439/2330 train_time:55148ms step_avg:38.32ms
step:1440/2330 train_time:55190ms step_avg:38.33ms
step:1441/2330 train_time:55225ms step_avg:38.32ms
step:1442/2330 train_time:55266ms step_avg:38.33ms
step:1443/2330 train_time:55302ms step_avg:38.32ms
step:1444/2330 train_time:55343ms step_avg:38.33ms
step:1445/2330 train_time:55379ms step_avg:38.32ms
step:1446/2330 train_time:55420ms step_avg:38.33ms
step:1447/2330 train_time:55456ms step_avg:38.32ms
step:1448/2330 train_time:55497ms step_avg:38.33ms
step:1449/2330 train_time:55533ms step_avg:38.33ms
step:1450/2330 train_time:55574ms step_avg:38.33ms
step:1451/2330 train_time:55610ms step_avg:38.33ms
step:1452/2330 train_time:55652ms step_avg:38.33ms
step:1453/2330 train_time:55687ms step_avg:38.33ms
step:1454/2330 train_time:55729ms step_avg:38.33ms
step:1455/2330 train_time:55765ms step_avg:38.33ms
step:1456/2330 train_time:55807ms step_avg:38.33ms
step:1457/2330 train_time:55843ms step_avg:38.33ms
step:1458/2330 train_time:55883ms step_avg:38.33ms
step:1459/2330 train_time:55920ms step_avg:38.33ms
step:1460/2330 train_time:55961ms step_avg:38.33ms
step:1461/2330 train_time:55996ms step_avg:38.33ms
step:1462/2330 train_time:56037ms step_avg:38.33ms
step:1463/2330 train_time:56072ms step_avg:38.33ms
step:1464/2330 train_time:56113ms step_avg:38.33ms
step:1465/2330 train_time:56149ms step_avg:38.33ms
step:1466/2330 train_time:56191ms step_avg:38.33ms
step:1467/2330 train_time:56227ms step_avg:38.33ms
step:1468/2330 train_time:56268ms step_avg:38.33ms
step:1469/2330 train_time:56304ms step_avg:38.33ms
step:1470/2330 train_time:56345ms step_avg:38.33ms
step:1471/2330 train_time:56381ms step_avg:38.33ms
step:1472/2330 train_time:56421ms step_avg:38.33ms
step:1473/2330 train_time:56457ms step_avg:38.33ms
step:1474/2330 train_time:56498ms step_avg:38.33ms
step:1475/2330 train_time:56534ms step_avg:38.33ms
step:1476/2330 train_time:56575ms step_avg:38.33ms
step:1477/2330 train_time:56611ms step_avg:38.33ms
step:1478/2330 train_time:56652ms step_avg:38.33ms
step:1479/2330 train_time:56688ms step_avg:38.33ms
step:1480/2330 train_time:56730ms step_avg:38.33ms
step:1481/2330 train_time:56766ms step_avg:38.33ms
step:1482/2330 train_time:56807ms step_avg:38.33ms
step:1483/2330 train_time:56843ms step_avg:38.33ms
step:1484/2330 train_time:56884ms step_avg:38.33ms
step:1485/2330 train_time:56921ms step_avg:38.33ms
step:1486/2330 train_time:56962ms step_avg:38.33ms
step:1487/2330 train_time:56997ms step_avg:38.33ms
step:1488/2330 train_time:57038ms step_avg:38.33ms
step:1489/2330 train_time:57073ms step_avg:38.33ms
step:1490/2330 train_time:57114ms step_avg:38.33ms
step:1491/2330 train_time:57150ms step_avg:38.33ms
step:1492/2330 train_time:57192ms step_avg:38.33ms
step:1493/2330 train_time:57228ms step_avg:38.33ms
step:1494/2330 train_time:57270ms step_avg:38.33ms
step:1495/2330 train_time:57305ms step_avg:38.33ms
step:1496/2330 train_time:57346ms step_avg:38.33ms
step:1497/2330 train_time:57382ms step_avg:38.33ms
step:1498/2330 train_time:57423ms step_avg:38.33ms
step:1499/2330 train_time:57459ms step_avg:38.33ms
step:1500/2330 train_time:57500ms step_avg:38.33ms
step:1500/2330 val_loss:5.2474 train_time:57612ms step_avg:38.41ms
step:1501/2330 train_time:57624ms step_avg:38.39ms
step:1502/2330 train_time:57634ms step_avg:38.37ms
step:1503/2330 train_time:57644ms step_avg:38.35ms
step:1504/2330 train_time:57655ms step_avg:38.33ms
step:1505/2330 train_time:57690ms step_avg:38.33ms
step:1506/2330 train_time:57730ms step_avg:38.33ms
step:1507/2330 train_time:57765ms step_avg:38.33ms
step:1508/2330 train_time:57806ms step_avg:38.33ms
step:1509/2330 train_time:57841ms step_avg:38.33ms
step:1510/2330 train_time:57882ms step_avg:38.33ms
step:1511/2330 train_time:57920ms step_avg:38.33ms
step:1512/2330 train_time:57963ms step_avg:38.34ms
step:1513/2330 train_time:58002ms step_avg:38.34ms
step:1514/2330 train_time:58044ms step_avg:38.34ms
step:1515/2330 train_time:58080ms step_avg:38.34ms
step:1516/2330 train_time:58120ms step_avg:38.34ms
step:1517/2330 train_time:58157ms step_avg:38.34ms
step:1518/2330 train_time:58198ms step_avg:38.34ms
step:1519/2330 train_time:58233ms step_avg:38.34ms
step:1520/2330 train_time:58273ms step_avg:38.34ms
step:1521/2330 train_time:58308ms step_avg:38.34ms
step:1522/2330 train_time:58349ms step_avg:38.34ms
step:1523/2330 train_time:58385ms step_avg:38.34ms
step:1524/2330 train_time:58425ms step_avg:38.34ms
step:1525/2330 train_time:58461ms step_avg:38.34ms
step:1526/2330 train_time:58501ms step_avg:38.34ms
step:1527/2330 train_time:58538ms step_avg:38.34ms
step:1528/2330 train_time:58579ms step_avg:38.34ms
step:1529/2330 train_time:58615ms step_avg:38.34ms
step:1530/2330 train_time:58657ms step_avg:38.34ms
step:1531/2330 train_time:58691ms step_avg:38.34ms
step:1532/2330 train_time:58732ms step_avg:38.34ms
step:1533/2330 train_time:58768ms step_avg:38.34ms
step:1534/2330 train_time:58809ms step_avg:38.34ms
step:1535/2330 train_time:58844ms step_avg:38.34ms
step:1536/2330 train_time:58886ms step_avg:38.34ms
step:1537/2330 train_time:58923ms step_avg:38.34ms
step:1538/2330 train_time:58965ms step_avg:38.34ms
step:1539/2330 train_time:59002ms step_avg:38.34ms
step:1540/2330 train_time:59044ms step_avg:38.34ms
step:1541/2330 train_time:59080ms step_avg:38.34ms
step:1542/2330 train_time:59121ms step_avg:38.34ms
step:1543/2330 train_time:59158ms step_avg:38.34ms
step:1544/2330 train_time:59198ms step_avg:38.34ms
step:1545/2330 train_time:59234ms step_avg:38.34ms
step:1546/2330 train_time:59275ms step_avg:38.34ms
step:1547/2330 train_time:59310ms step_avg:38.34ms
step:1548/2330 train_time:59351ms step_avg:38.34ms
step:1549/2330 train_time:59387ms step_avg:38.34ms
step:1550/2330 train_time:59428ms step_avg:38.34ms
step:1551/2330 train_time:59463ms step_avg:38.34ms
step:1552/2330 train_time:59504ms step_avg:38.34ms
step:1553/2330 train_time:59539ms step_avg:38.34ms
step:1554/2330 train_time:59580ms step_avg:38.34ms
step:1555/2330 train_time:59615ms step_avg:38.34ms
step:1556/2330 train_time:59656ms step_avg:38.34ms
step:1557/2330 train_time:59691ms step_avg:38.34ms
step:1558/2330 train_time:59732ms step_avg:38.34ms
step:1559/2330 train_time:59768ms step_avg:38.34ms
step:1560/2330 train_time:59809ms step_avg:38.34ms
step:1561/2330 train_time:59846ms step_avg:38.34ms
step:1562/2330 train_time:59887ms step_avg:38.34ms
step:1563/2330 train_time:59924ms step_avg:38.34ms
step:1564/2330 train_time:59966ms step_avg:38.34ms
step:1565/2330 train_time:60004ms step_avg:38.34ms
step:1566/2330 train_time:60046ms step_avg:38.34ms
step:1567/2330 train_time:60082ms step_avg:38.34ms
step:1568/2330 train_time:60124ms step_avg:38.34ms
step:1569/2330 train_time:60160ms step_avg:38.34ms
step:1570/2330 train_time:60201ms step_avg:38.34ms
step:1571/2330 train_time:60238ms step_avg:38.34ms
step:1572/2330 train_time:60279ms step_avg:38.35ms
step:1573/2330 train_time:60314ms step_avg:38.34ms
step:1574/2330 train_time:60354ms step_avg:38.34ms
step:1575/2330 train_time:60390ms step_avg:38.34ms
step:1576/2330 train_time:60431ms step_avg:38.34ms
step:1577/2330 train_time:60466ms step_avg:38.34ms
step:1578/2330 train_time:60508ms step_avg:38.34ms
step:1579/2330 train_time:60544ms step_avg:38.34ms
step:1580/2330 train_time:60585ms step_avg:38.35ms
step:1581/2330 train_time:60621ms step_avg:38.34ms
step:1582/2330 train_time:60661ms step_avg:38.34ms
step:1583/2330 train_time:60697ms step_avg:38.34ms
step:1584/2330 train_time:60738ms step_avg:38.34ms
step:1585/2330 train_time:60774ms step_avg:38.34ms
step:1586/2330 train_time:60816ms step_avg:38.35ms
step:1587/2330 train_time:60851ms step_avg:38.34ms
step:1588/2330 train_time:60892ms step_avg:38.35ms
step:1589/2330 train_time:60929ms step_avg:38.34ms
step:1590/2330 train_time:60970ms step_avg:38.35ms
step:1591/2330 train_time:61007ms step_avg:38.35ms
step:1592/2330 train_time:61049ms step_avg:38.35ms
step:1593/2330 train_time:61084ms step_avg:38.35ms
step:1594/2330 train_time:61127ms step_avg:38.35ms
step:1595/2330 train_time:61162ms step_avg:38.35ms
step:1596/2330 train_time:61203ms step_avg:38.35ms
step:1597/2330 train_time:61240ms step_avg:38.35ms
step:1598/2330 train_time:61281ms step_avg:38.35ms
step:1599/2330 train_time:61318ms step_avg:38.35ms
step:1600/2330 train_time:61358ms step_avg:38.35ms
step:1601/2330 train_time:61393ms step_avg:38.35ms
step:1602/2330 train_time:61434ms step_avg:38.35ms
step:1603/2330 train_time:61470ms step_avg:38.35ms
step:1604/2330 train_time:61510ms step_avg:38.35ms
step:1605/2330 train_time:61546ms step_avg:38.35ms
step:1606/2330 train_time:61587ms step_avg:38.35ms
step:1607/2330 train_time:61623ms step_avg:38.35ms
step:1608/2330 train_time:61664ms step_avg:38.35ms
step:1609/2330 train_time:61700ms step_avg:38.35ms
step:1610/2330 train_time:61741ms step_avg:38.35ms
step:1611/2330 train_time:61777ms step_avg:38.35ms
step:1612/2330 train_time:61818ms step_avg:38.35ms
step:1613/2330 train_time:61854ms step_avg:38.35ms
step:1614/2330 train_time:61895ms step_avg:38.35ms
step:1615/2330 train_time:61931ms step_avg:38.35ms
step:1616/2330 train_time:61972ms step_avg:38.35ms
step:1617/2330 train_time:62009ms step_avg:38.35ms
step:1618/2330 train_time:62050ms step_avg:38.35ms
step:1619/2330 train_time:62087ms step_avg:38.35ms
step:1620/2330 train_time:62128ms step_avg:38.35ms
step:1621/2330 train_time:62165ms step_avg:38.35ms
step:1622/2330 train_time:62207ms step_avg:38.35ms
step:1623/2330 train_time:62243ms step_avg:38.35ms
step:1624/2330 train_time:62284ms step_avg:38.35ms
step:1625/2330 train_time:62321ms step_avg:38.35ms
step:1626/2330 train_time:62362ms step_avg:38.35ms
step:1627/2330 train_time:62399ms step_avg:38.35ms
step:1628/2330 train_time:62439ms step_avg:38.35ms
step:1629/2330 train_time:62476ms step_avg:38.35ms
step:1630/2330 train_time:62517ms step_avg:38.35ms
step:1631/2330 train_time:62551ms step_avg:38.35ms
step:1632/2330 train_time:62592ms step_avg:38.35ms
step:1633/2330 train_time:62627ms step_avg:38.35ms
step:1634/2330 train_time:62669ms step_avg:38.35ms
step:1635/2330 train_time:62704ms step_avg:38.35ms
step:1636/2330 train_time:62746ms step_avg:38.35ms
step:1637/2330 train_time:62782ms step_avg:38.35ms
step:1638/2330 train_time:62822ms step_avg:38.35ms
step:1639/2330 train_time:62859ms step_avg:38.35ms
step:1640/2330 train_time:62900ms step_avg:38.35ms
step:1641/2330 train_time:62936ms step_avg:38.35ms
step:1642/2330 train_time:62978ms step_avg:38.35ms
step:1643/2330 train_time:63014ms step_avg:38.35ms
step:1644/2330 train_time:63055ms step_avg:38.35ms
step:1645/2330 train_time:63090ms step_avg:38.35ms
step:1646/2330 train_time:63131ms step_avg:38.35ms
step:1647/2330 train_time:63168ms step_avg:38.35ms
step:1648/2330 train_time:63209ms step_avg:38.36ms
step:1649/2330 train_time:63246ms step_avg:38.35ms
step:1650/2330 train_time:63287ms step_avg:38.36ms
step:1651/2330 train_time:63323ms step_avg:38.35ms
step:1652/2330 train_time:63365ms step_avg:38.36ms
step:1653/2330 train_time:63401ms step_avg:38.35ms
step:1654/2330 train_time:63442ms step_avg:38.36ms
step:1655/2330 train_time:63477ms step_avg:38.35ms
step:1656/2330 train_time:63518ms step_avg:38.36ms
step:1657/2330 train_time:63553ms step_avg:38.35ms
step:1658/2330 train_time:63594ms step_avg:38.36ms
step:1659/2330 train_time:63629ms step_avg:38.35ms
step:1660/2330 train_time:63670ms step_avg:38.36ms
step:1661/2330 train_time:63706ms step_avg:38.35ms
step:1662/2330 train_time:63748ms step_avg:38.36ms
step:1663/2330 train_time:63783ms step_avg:38.35ms
step:1664/2330 train_time:63825ms step_avg:38.36ms
step:1665/2330 train_time:63861ms step_avg:38.35ms
step:1666/2330 train_time:63902ms step_avg:38.36ms
step:1667/2330 train_time:63938ms step_avg:38.36ms
step:1668/2330 train_time:63979ms step_avg:38.36ms
step:1669/2330 train_time:64015ms step_avg:38.36ms
step:1670/2330 train_time:64056ms step_avg:38.36ms
step:1671/2330 train_time:64092ms step_avg:38.36ms
step:1672/2330 train_time:64133ms step_avg:38.36ms
step:1673/2330 train_time:64169ms step_avg:38.36ms
step:1674/2330 train_time:64211ms step_avg:38.36ms
step:1675/2330 train_time:64246ms step_avg:38.36ms
step:1676/2330 train_time:64287ms step_avg:38.36ms
step:1677/2330 train_time:64323ms step_avg:38.36ms
step:1678/2330 train_time:64365ms step_avg:38.36ms
step:1679/2330 train_time:64401ms step_avg:38.36ms
step:1680/2330 train_time:64441ms step_avg:38.36ms
step:1681/2330 train_time:64478ms step_avg:38.36ms
step:1682/2330 train_time:64519ms step_avg:38.36ms
step:1683/2330 train_time:64554ms step_avg:38.36ms
step:1684/2330 train_time:64595ms step_avg:38.36ms
step:1685/2330 train_time:64630ms step_avg:38.36ms
step:1686/2330 train_time:64671ms step_avg:38.36ms
step:1687/2330 train_time:64707ms step_avg:38.36ms
step:1688/2330 train_time:64748ms step_avg:38.36ms
step:1689/2330 train_time:64784ms step_avg:38.36ms
step:1690/2330 train_time:64825ms step_avg:38.36ms
step:1691/2330 train_time:64861ms step_avg:38.36ms
step:1692/2330 train_time:64902ms step_avg:38.36ms
step:1693/2330 train_time:64937ms step_avg:38.36ms
step:1694/2330 train_time:64978ms step_avg:38.36ms
step:1695/2330 train_time:65014ms step_avg:38.36ms
step:1696/2330 train_time:65055ms step_avg:38.36ms
step:1697/2330 train_time:65091ms step_avg:38.36ms
step:1698/2330 train_time:65132ms step_avg:38.36ms
step:1699/2330 train_time:65168ms step_avg:38.36ms
step:1700/2330 train_time:65210ms step_avg:38.36ms
step:1701/2330 train_time:65245ms step_avg:38.36ms
step:1702/2330 train_time:65287ms step_avg:38.36ms
step:1703/2330 train_time:65323ms step_avg:38.36ms
step:1704/2330 train_time:65364ms step_avg:38.36ms
step:1705/2330 train_time:65400ms step_avg:38.36ms
step:1706/2330 train_time:65441ms step_avg:38.36ms
step:1707/2330 train_time:65476ms step_avg:38.36ms
step:1708/2330 train_time:65518ms step_avg:38.36ms
step:1709/2330 train_time:65553ms step_avg:38.36ms
step:1710/2330 train_time:65594ms step_avg:38.36ms
step:1711/2330 train_time:65630ms step_avg:38.36ms
step:1712/2330 train_time:65671ms step_avg:38.36ms
step:1713/2330 train_time:65707ms step_avg:38.36ms
step:1714/2330 train_time:65749ms step_avg:38.36ms
step:1715/2330 train_time:65784ms step_avg:38.36ms
step:1716/2330 train_time:65825ms step_avg:38.36ms
step:1717/2330 train_time:65861ms step_avg:38.36ms
step:1718/2330 train_time:65901ms step_avg:38.36ms
step:1719/2330 train_time:65938ms step_avg:38.36ms
step:1720/2330 train_time:65979ms step_avg:38.36ms
step:1721/2330 train_time:66015ms step_avg:38.36ms
step:1722/2330 train_time:66056ms step_avg:38.36ms
step:1723/2330 train_time:66091ms step_avg:38.36ms
step:1724/2330 train_time:66132ms step_avg:38.36ms
step:1725/2330 train_time:66168ms step_avg:38.36ms
step:1726/2330 train_time:66209ms step_avg:38.36ms
step:1727/2330 train_time:66246ms step_avg:38.36ms
step:1728/2330 train_time:66287ms step_avg:38.36ms
step:1729/2330 train_time:66323ms step_avg:38.36ms
step:1730/2330 train_time:66364ms step_avg:38.36ms
step:1731/2330 train_time:66400ms step_avg:38.36ms
step:1732/2330 train_time:66440ms step_avg:38.36ms
step:1733/2330 train_time:66477ms step_avg:38.36ms
step:1734/2330 train_time:66518ms step_avg:38.36ms
step:1735/2330 train_time:66553ms step_avg:38.36ms
step:1736/2330 train_time:66594ms step_avg:38.36ms
step:1737/2330 train_time:66629ms step_avg:38.36ms
step:1738/2330 train_time:66670ms step_avg:38.36ms
step:1739/2330 train_time:66706ms step_avg:38.36ms
step:1740/2330 train_time:66748ms step_avg:38.36ms
step:1741/2330 train_time:66783ms step_avg:38.36ms
step:1742/2330 train_time:66824ms step_avg:38.36ms
step:1743/2330 train_time:66860ms step_avg:38.36ms
step:1744/2330 train_time:66901ms step_avg:38.36ms
step:1745/2330 train_time:66937ms step_avg:38.36ms
step:1746/2330 train_time:66979ms step_avg:38.36ms
step:1747/2330 train_time:67014ms step_avg:38.36ms
step:1748/2330 train_time:67055ms step_avg:38.36ms
step:1749/2330 train_time:67090ms step_avg:38.36ms
step:1750/2330 train_time:67131ms step_avg:38.36ms
step:1750/2330 val_loss:5.2105 train_time:67246ms step_avg:38.43ms
step:1751/2330 train_time:67257ms step_avg:38.41ms
step:1752/2330 train_time:67268ms step_avg:38.39ms
step:1753/2330 train_time:67278ms step_avg:38.38ms
step:1754/2330 train_time:67288ms step_avg:38.36ms
step:1755/2330 train_time:67324ms step_avg:38.36ms
step:1756/2330 train_time:67364ms step_avg:38.36ms
step:1757/2330 train_time:67399ms step_avg:38.36ms
step:1758/2330 train_time:67440ms step_avg:38.36ms
step:1759/2330 train_time:67475ms step_avg:38.36ms
step:1760/2330 train_time:67515ms step_avg:38.36ms
step:1761/2330 train_time:67554ms step_avg:38.36ms
step:1762/2330 train_time:67595ms step_avg:38.36ms
step:1763/2330 train_time:67639ms step_avg:38.37ms
step:1764/2330 train_time:67680ms step_avg:38.37ms
step:1765/2330 train_time:67717ms step_avg:38.37ms
step:1766/2330 train_time:67758ms step_avg:38.37ms
step:1767/2330 train_time:67794ms step_avg:38.37ms
step:1768/2330 train_time:67834ms step_avg:38.37ms
step:1769/2330 train_time:67871ms step_avg:38.37ms
step:1770/2330 train_time:67912ms step_avg:38.37ms
step:1771/2330 train_time:67947ms step_avg:38.37ms
step:1772/2330 train_time:67987ms step_avg:38.37ms
step:1773/2330 train_time:68022ms step_avg:38.37ms
step:1774/2330 train_time:68063ms step_avg:38.37ms
step:1775/2330 train_time:68098ms step_avg:38.36ms
step:1776/2330 train_time:68139ms step_avg:38.37ms
step:1777/2330 train_time:68175ms step_avg:38.37ms
step:1778/2330 train_time:68216ms step_avg:38.37ms
step:1779/2330 train_time:68252ms step_avg:38.37ms
step:1780/2330 train_time:68293ms step_avg:38.37ms
step:1781/2330 train_time:68328ms step_avg:38.37ms
step:1782/2330 train_time:68369ms step_avg:38.37ms
step:1783/2330 train_time:68404ms step_avg:38.36ms
step:1784/2330 train_time:68445ms step_avg:38.37ms
step:1785/2330 train_time:68481ms step_avg:38.36ms
step:1786/2330 train_time:68523ms step_avg:38.37ms
step:1787/2330 train_time:68560ms step_avg:38.37ms
step:1788/2330 train_time:68601ms step_avg:38.37ms
step:1789/2330 train_time:68639ms step_avg:38.37ms
step:1790/2330 train_time:68680ms step_avg:38.37ms
step:1791/2330 train_time:68717ms step_avg:38.37ms
step:1792/2330 train_time:68758ms step_avg:38.37ms
step:1793/2330 train_time:68795ms step_avg:38.37ms
step:1794/2330 train_time:68835ms step_avg:38.37ms
step:1795/2330 train_time:68872ms step_avg:38.37ms
step:1796/2330 train_time:68913ms step_avg:38.37ms
step:1797/2330 train_time:68948ms step_avg:38.37ms
step:1798/2330 train_time:68989ms step_avg:38.37ms
step:1799/2330 train_time:69024ms step_avg:38.37ms
step:1800/2330 train_time:69065ms step_avg:38.37ms
step:1801/2330 train_time:69101ms step_avg:38.37ms
step:1802/2330 train_time:69142ms step_avg:38.37ms
step:1803/2330 train_time:69178ms step_avg:38.37ms
step:1804/2330 train_time:69220ms step_avg:38.37ms
step:1805/2330 train_time:69256ms step_avg:38.37ms
step:1806/2330 train_time:69296ms step_avg:38.37ms
step:1807/2330 train_time:69332ms step_avg:38.37ms
step:1808/2330 train_time:69373ms step_avg:38.37ms
step:1809/2330 train_time:69410ms step_avg:38.37ms
step:1810/2330 train_time:69450ms step_avg:38.37ms
step:1811/2330 train_time:69487ms step_avg:38.37ms
step:1812/2330 train_time:69528ms step_avg:38.37ms
step:1813/2330 train_time:69565ms step_avg:38.37ms
step:1814/2330 train_time:69607ms step_avg:38.37ms
step:1815/2330 train_time:69642ms step_avg:38.37ms
step:1816/2330 train_time:69684ms step_avg:38.37ms
step:1817/2330 train_time:69720ms step_avg:38.37ms
step:1818/2330 train_time:69761ms step_avg:38.37ms
step:1819/2330 train_time:69798ms step_avg:38.37ms
step:1820/2330 train_time:69839ms step_avg:38.37ms
step:1821/2330 train_time:69876ms step_avg:38.37ms
step:1822/2330 train_time:69916ms step_avg:38.37ms
step:1823/2330 train_time:69953ms step_avg:38.37ms
step:1824/2330 train_time:69993ms step_avg:38.37ms
step:1825/2330 train_time:70029ms step_avg:38.37ms
step:1826/2330 train_time:70070ms step_avg:38.37ms
step:1827/2330 train_time:70105ms step_avg:38.37ms
step:1828/2330 train_time:70145ms step_avg:38.37ms
step:1829/2330 train_time:70182ms step_avg:38.37ms
step:1830/2330 train_time:70224ms step_avg:38.37ms
step:1831/2330 train_time:70259ms step_avg:38.37ms
step:1832/2330 train_time:70300ms step_avg:38.37ms
step:1833/2330 train_time:70335ms step_avg:38.37ms
step:1834/2330 train_time:70377ms step_avg:38.37ms
step:1835/2330 train_time:70413ms step_avg:38.37ms
step:1836/2330 train_time:70453ms step_avg:38.37ms
step:1837/2330 train_time:70490ms step_avg:38.37ms
step:1838/2330 train_time:70531ms step_avg:38.37ms
step:1839/2330 train_time:70567ms step_avg:38.37ms
step:1840/2330 train_time:70608ms step_avg:38.37ms
step:1841/2330 train_time:70644ms step_avg:38.37ms
step:1842/2330 train_time:70685ms step_avg:38.37ms
step:1843/2330 train_time:70721ms step_avg:38.37ms
step:1844/2330 train_time:70762ms step_avg:38.37ms
step:1845/2330 train_time:70798ms step_avg:38.37ms
step:1846/2330 train_time:70840ms step_avg:38.37ms
step:1847/2330 train_time:70876ms step_avg:38.37ms
step:1848/2330 train_time:70917ms step_avg:38.38ms
step:1849/2330 train_time:70953ms step_avg:38.37ms
step:1850/2330 train_time:70994ms step_avg:38.38ms
step:1851/2330 train_time:71029ms step_avg:38.37ms
step:1852/2330 train_time:71071ms step_avg:38.38ms
step:1853/2330 train_time:71105ms step_avg:38.37ms
step:1854/2330 train_time:71146ms step_avg:38.37ms
step:1855/2330 train_time:71182ms step_avg:38.37ms
step:1856/2330 train_time:71223ms step_avg:38.37ms
step:1857/2330 train_time:71259ms step_avg:38.37ms
step:1858/2330 train_time:71301ms step_avg:38.37ms
step:1859/2330 train_time:71336ms step_avg:38.37ms
step:1860/2330 train_time:71378ms step_avg:38.38ms
step:1861/2330 train_time:71413ms step_avg:38.37ms
step:1862/2330 train_time:71454ms step_avg:38.37ms
step:1863/2330 train_time:71491ms step_avg:38.37ms
step:1864/2330 train_time:71532ms step_avg:38.38ms
step:1865/2330 train_time:71568ms step_avg:38.37ms
step:1866/2330 train_time:71609ms step_avg:38.38ms
step:1867/2330 train_time:71644ms step_avg:38.37ms
step:1868/2330 train_time:71685ms step_avg:38.38ms
step:1869/2330 train_time:71722ms step_avg:38.37ms
step:1870/2330 train_time:71763ms step_avg:38.38ms
step:1871/2330 train_time:71799ms step_avg:38.37ms
step:1872/2330 train_time:71840ms step_avg:38.38ms
step:1873/2330 train_time:71876ms step_avg:38.37ms
step:1874/2330 train_time:71917ms step_avg:38.38ms
step:1875/2330 train_time:71953ms step_avg:38.38ms
step:1876/2330 train_time:71994ms step_avg:38.38ms
step:1877/2330 train_time:72030ms step_avg:38.37ms
step:1878/2330 train_time:72070ms step_avg:38.38ms
step:1879/2330 train_time:72106ms step_avg:38.37ms
step:1880/2330 train_time:72146ms step_avg:38.38ms
step:1881/2330 train_time:72183ms step_avg:38.37ms
step:1882/2330 train_time:72224ms step_avg:38.38ms
step:1883/2330 train_time:72259ms step_avg:38.37ms
step:1884/2330 train_time:72301ms step_avg:38.38ms
step:1885/2330 train_time:72336ms step_avg:38.37ms
step:1886/2330 train_time:72378ms step_avg:38.38ms
step:1887/2330 train_time:72413ms step_avg:38.37ms
step:1888/2330 train_time:72455ms step_avg:38.38ms
step:1889/2330 train_time:72491ms step_avg:38.38ms
step:1890/2330 train_time:72533ms step_avg:38.38ms
step:1891/2330 train_time:72568ms step_avg:38.38ms
step:1892/2330 train_time:72608ms step_avg:38.38ms
step:1893/2330 train_time:72644ms step_avg:38.38ms
step:1894/2330 train_time:72685ms step_avg:38.38ms
step:1895/2330 train_time:72721ms step_avg:38.38ms
step:1896/2330 train_time:72763ms step_avg:38.38ms
step:1897/2330 train_time:72798ms step_avg:38.38ms
step:1898/2330 train_time:72840ms step_avg:38.38ms
step:1899/2330 train_time:72876ms step_avg:38.38ms
step:1900/2330 train_time:72917ms step_avg:38.38ms
step:1901/2330 train_time:72954ms step_avg:38.38ms
step:1902/2330 train_time:72994ms step_avg:38.38ms
step:1903/2330 train_time:73030ms step_avg:38.38ms
step:1904/2330 train_time:73071ms step_avg:38.38ms
step:1905/2330 train_time:73107ms step_avg:38.38ms
step:1906/2330 train_time:73147ms step_avg:38.38ms
step:1907/2330 train_time:73183ms step_avg:38.38ms
step:1908/2330 train_time:73224ms step_avg:38.38ms
step:1909/2330 train_time:73260ms step_avg:38.38ms
step:1910/2330 train_time:73302ms step_avg:38.38ms
step:1911/2330 train_time:73337ms step_avg:38.38ms
step:1912/2330 train_time:73378ms step_avg:38.38ms
step:1913/2330 train_time:73413ms step_avg:38.38ms
step:1914/2330 train_time:73454ms step_avg:38.38ms
step:1915/2330 train_time:73490ms step_avg:38.38ms
step:1916/2330 train_time:73532ms step_avg:38.38ms
step:1917/2330 train_time:73567ms step_avg:38.38ms
step:1918/2330 train_time:73608ms step_avg:38.38ms
step:1919/2330 train_time:73644ms step_avg:38.38ms
step:1920/2330 train_time:73685ms step_avg:38.38ms
step:1921/2330 train_time:73721ms step_avg:38.38ms
step:1922/2330 train_time:73763ms step_avg:38.38ms
step:1923/2330 train_time:73798ms step_avg:38.38ms
step:1924/2330 train_time:73839ms step_avg:38.38ms
step:1925/2330 train_time:73875ms step_avg:38.38ms
step:1926/2330 train_time:73916ms step_avg:38.38ms
step:1927/2330 train_time:73953ms step_avg:38.38ms
step:1928/2330 train_time:73994ms step_avg:38.38ms
step:1929/2330 train_time:74029ms step_avg:38.38ms
step:1930/2330 train_time:74070ms step_avg:38.38ms
step:1931/2330 train_time:74105ms step_avg:38.38ms
step:1932/2330 train_time:74146ms step_avg:38.38ms
step:1933/2330 train_time:74182ms step_avg:38.38ms
step:1934/2330 train_time:74223ms step_avg:38.38ms
step:1935/2330 train_time:74258ms step_avg:38.38ms
step:1936/2330 train_time:74299ms step_avg:38.38ms
step:1937/2330 train_time:74336ms step_avg:38.38ms
step:1938/2330 train_time:74376ms step_avg:38.38ms
step:1939/2330 train_time:74413ms step_avg:38.38ms
step:1940/2330 train_time:74454ms step_avg:38.38ms
step:1941/2330 train_time:74490ms step_avg:38.38ms
step:1942/2330 train_time:74531ms step_avg:38.38ms
step:1943/2330 train_time:74567ms step_avg:38.38ms
step:1944/2330 train_time:74608ms step_avg:38.38ms
step:1945/2330 train_time:74644ms step_avg:38.38ms
step:1946/2330 train_time:74685ms step_avg:38.38ms
step:1947/2330 train_time:74720ms step_avg:38.38ms
step:1948/2330 train_time:74762ms step_avg:38.38ms
step:1949/2330 train_time:74798ms step_avg:38.38ms
step:1950/2330 train_time:74839ms step_avg:38.38ms
step:1951/2330 train_time:74876ms step_avg:38.38ms
step:1952/2330 train_time:74916ms step_avg:38.38ms
step:1953/2330 train_time:74953ms step_avg:38.38ms
step:1954/2330 train_time:74994ms step_avg:38.38ms
step:1955/2330 train_time:75030ms step_avg:38.38ms
step:1956/2330 train_time:75072ms step_avg:38.38ms
step:1957/2330 train_time:75107ms step_avg:38.38ms
step:1958/2330 train_time:75147ms step_avg:38.38ms
step:1959/2330 train_time:75183ms step_avg:38.38ms
step:1960/2330 train_time:75225ms step_avg:38.38ms
step:1961/2330 train_time:75260ms step_avg:38.38ms
step:1962/2330 train_time:75302ms step_avg:38.38ms
step:1963/2330 train_time:75337ms step_avg:38.38ms
step:1964/2330 train_time:75378ms step_avg:38.38ms
step:1965/2330 train_time:75414ms step_avg:38.38ms
step:1966/2330 train_time:75455ms step_avg:38.38ms
step:1967/2330 train_time:75492ms step_avg:38.38ms
step:1968/2330 train_time:75532ms step_avg:38.38ms
step:1969/2330 train_time:75568ms step_avg:38.38ms
step:1970/2330 train_time:75609ms step_avg:38.38ms
step:1971/2330 train_time:75644ms step_avg:38.38ms
step:1972/2330 train_time:75685ms step_avg:38.38ms
step:1973/2330 train_time:75722ms step_avg:38.38ms
step:1974/2330 train_time:75763ms step_avg:38.38ms
step:1975/2330 train_time:75799ms step_avg:38.38ms
step:1976/2330 train_time:75840ms step_avg:38.38ms
step:1977/2330 train_time:75877ms step_avg:38.38ms
step:1978/2330 train_time:75919ms step_avg:38.38ms
step:1979/2330 train_time:75954ms step_avg:38.38ms
step:1980/2330 train_time:75995ms step_avg:38.38ms
step:1981/2330 train_time:76031ms step_avg:38.38ms
step:1982/2330 train_time:76073ms step_avg:38.38ms
step:1983/2330 train_time:76108ms step_avg:38.38ms
step:1984/2330 train_time:76148ms step_avg:38.38ms
step:1985/2330 train_time:76184ms step_avg:38.38ms
step:1986/2330 train_time:76226ms step_avg:38.38ms
step:1987/2330 train_time:76261ms step_avg:38.38ms
step:1988/2330 train_time:76303ms step_avg:38.38ms
step:1989/2330 train_time:76339ms step_avg:38.38ms
step:1990/2330 train_time:76381ms step_avg:38.38ms
step:1991/2330 train_time:76417ms step_avg:38.38ms
step:1992/2330 train_time:76458ms step_avg:38.38ms
step:1993/2330 train_time:76495ms step_avg:38.38ms
step:1994/2330 train_time:76536ms step_avg:38.38ms
step:1995/2330 train_time:76573ms step_avg:38.38ms
step:1996/2330 train_time:76613ms step_avg:38.38ms
step:1997/2330 train_time:76649ms step_avg:38.38ms
step:1998/2330 train_time:76689ms step_avg:38.38ms
step:1999/2330 train_time:76724ms step_avg:38.38ms
step:2000/2330 train_time:76765ms step_avg:38.38ms
step:2000/2330 val_loss:5.1784 train_time:76879ms step_avg:38.44ms
step:2001/2330 train_time:76890ms step_avg:38.43ms
step:2002/2330 train_time:76902ms step_avg:38.41ms
step:2003/2330 train_time:76911ms step_avg:38.40ms
step:2004/2330 train_time:76922ms step_avg:38.38ms
step:2005/2330 train_time:76958ms step_avg:38.38ms
step:2006/2330 train_time:76999ms step_avg:38.38ms
step:2007/2330 train_time:77033ms step_avg:38.38ms
step:2008/2330 train_time:77074ms step_avg:38.38ms
step:2009/2330 train_time:77108ms step_avg:38.38ms
step:2010/2330 train_time:77149ms step_avg:38.38ms
step:2011/2330 train_time:77186ms step_avg:38.38ms
step:2012/2330 train_time:77230ms step_avg:38.38ms
step:2013/2330 train_time:77267ms step_avg:38.38ms
step:2014/2330 train_time:77309ms step_avg:38.39ms
step:2015/2330 train_time:77346ms step_avg:38.39ms
step:2016/2330 train_time:77387ms step_avg:38.39ms
step:2017/2330 train_time:77424ms step_avg:38.39ms
step:2018/2330 train_time:77464ms step_avg:38.39ms
step:2019/2330 train_time:77500ms step_avg:38.39ms
step:2020/2330 train_time:77541ms step_avg:38.39ms
step:2021/2330 train_time:77578ms step_avg:38.39ms
step:2022/2330 train_time:77618ms step_avg:38.39ms
step:2023/2330 train_time:77654ms step_avg:38.39ms
step:2024/2330 train_time:77695ms step_avg:38.39ms
step:2025/2330 train_time:77731ms step_avg:38.39ms
step:2026/2330 train_time:77772ms step_avg:38.39ms
step:2027/2330 train_time:77807ms step_avg:38.39ms
step:2028/2330 train_time:77849ms step_avg:38.39ms
step:2029/2330 train_time:77885ms step_avg:38.39ms
step:2030/2330 train_time:77926ms step_avg:38.39ms
step:2031/2330 train_time:77962ms step_avg:38.39ms
step:2032/2330 train_time:78002ms step_avg:38.39ms
step:2033/2330 train_time:78038ms step_avg:38.39ms
step:2034/2330 train_time:78079ms step_avg:38.39ms
step:2035/2330 train_time:78113ms step_avg:38.38ms
step:2036/2330 train_time:78155ms step_avg:38.39ms
step:2037/2330 train_time:78191ms step_avg:38.39ms
step:2038/2330 train_time:78233ms step_avg:38.39ms
step:2039/2330 train_time:78269ms step_avg:38.39ms
step:2040/2330 train_time:78311ms step_avg:38.39ms
step:2041/2330 train_time:78347ms step_avg:38.39ms
step:2042/2330 train_time:78389ms step_avg:38.39ms
step:2043/2330 train_time:78425ms step_avg:38.39ms
step:2044/2330 train_time:78466ms step_avg:38.39ms
step:2045/2330 train_time:78502ms step_avg:38.39ms
step:2046/2330 train_time:78543ms step_avg:38.39ms
step:2047/2330 train_time:78579ms step_avg:38.39ms
step:2048/2330 train_time:78620ms step_avg:38.39ms
step:2049/2330 train_time:78655ms step_avg:38.39ms
step:2050/2330 train_time:78696ms step_avg:38.39ms
step:2051/2330 train_time:78731ms step_avg:38.39ms
step:2052/2330 train_time:78773ms step_avg:38.39ms
step:2053/2330 train_time:78808ms step_avg:38.39ms
step:2054/2330 train_time:78850ms step_avg:38.39ms
step:2055/2330 train_time:78885ms step_avg:38.39ms
step:2056/2330 train_time:78926ms step_avg:38.39ms
step:2057/2330 train_time:78962ms step_avg:38.39ms
step:2058/2330 train_time:79003ms step_avg:38.39ms
step:2059/2330 train_time:79039ms step_avg:38.39ms
step:2060/2330 train_time:79080ms step_avg:38.39ms
step:2061/2330 train_time:79116ms step_avg:38.39ms
step:2062/2330 train_time:79157ms step_avg:38.39ms
step:2063/2330 train_time:79194ms step_avg:38.39ms
step:2064/2330 train_time:79235ms step_avg:38.39ms
step:2065/2330 train_time:79272ms step_avg:38.39ms
step:2066/2330 train_time:79313ms step_avg:38.39ms
step:2067/2330 train_time:79350ms step_avg:38.39ms
step:2068/2330 train_time:79392ms step_avg:38.39ms
step:2069/2330 train_time:79428ms step_avg:38.39ms
step:2070/2330 train_time:79470ms step_avg:38.39ms
step:2071/2330 train_time:79505ms step_avg:38.39ms
step:2072/2330 train_time:79547ms step_avg:38.39ms
step:2073/2330 train_time:79582ms step_avg:38.39ms
step:2074/2330 train_time:79623ms step_avg:38.39ms
step:2075/2330 train_time:79658ms step_avg:38.39ms
step:2076/2330 train_time:79700ms step_avg:38.39ms
step:2077/2330 train_time:79734ms step_avg:38.39ms
step:2078/2330 train_time:79776ms step_avg:38.39ms
step:2079/2330 train_time:79810ms step_avg:38.39ms
step:2080/2330 train_time:79852ms step_avg:38.39ms
step:2081/2330 train_time:79888ms step_avg:38.39ms
step:2082/2330 train_time:79930ms step_avg:38.39ms
step:2083/2330 train_time:79965ms step_avg:38.39ms
step:2084/2330 train_time:80006ms step_avg:38.39ms
step:2085/2330 train_time:80043ms step_avg:38.39ms
step:2086/2330 train_time:80084ms step_avg:38.39ms
step:2087/2330 train_time:80120ms step_avg:38.39ms
step:2088/2330 train_time:80161ms step_avg:38.39ms
step:2089/2330 train_time:80198ms step_avg:38.39ms
step:2090/2330 train_time:80239ms step_avg:38.39ms
step:2091/2330 train_time:80274ms step_avg:38.39ms
step:2092/2330 train_time:80315ms step_avg:38.39ms
step:2093/2330 train_time:80352ms step_avg:38.39ms
step:2094/2330 train_time:80392ms step_avg:38.39ms
step:2095/2330 train_time:80430ms step_avg:38.39ms
step:2096/2330 train_time:80471ms step_avg:38.39ms
step:2097/2330 train_time:80507ms step_avg:38.39ms
step:2098/2330 train_time:80548ms step_avg:38.39ms
step:2099/2330 train_time:80585ms step_avg:38.39ms
step:2100/2330 train_time:80625ms step_avg:38.39ms
step:2101/2330 train_time:80662ms step_avg:38.39ms
step:2102/2330 train_time:80703ms step_avg:38.39ms
step:2103/2330 train_time:80740ms step_avg:38.39ms
step:2104/2330 train_time:80780ms step_avg:38.39ms
step:2105/2330 train_time:80816ms step_avg:38.39ms
step:2106/2330 train_time:80857ms step_avg:38.39ms
step:2107/2330 train_time:80892ms step_avg:38.39ms
step:2108/2330 train_time:80934ms step_avg:38.39ms
step:2109/2330 train_time:80969ms step_avg:38.39ms
step:2110/2330 train_time:81011ms step_avg:38.39ms
step:2111/2330 train_time:81046ms step_avg:38.39ms
step:2112/2330 train_time:81087ms step_avg:38.39ms
step:2113/2330 train_time:81124ms step_avg:38.39ms
step:2114/2330 train_time:81165ms step_avg:38.39ms
step:2115/2330 train_time:81201ms step_avg:38.39ms
step:2116/2330 train_time:81242ms step_avg:38.39ms
step:2117/2330 train_time:81279ms step_avg:38.39ms
step:2118/2330 train_time:81320ms step_avg:38.39ms
step:2119/2330 train_time:81356ms step_avg:38.39ms
step:2120/2330 train_time:81397ms step_avg:38.39ms
step:2121/2330 train_time:81432ms step_avg:38.39ms
step:2122/2330 train_time:81473ms step_avg:38.39ms
step:2123/2330 train_time:81509ms step_avg:38.39ms
step:2124/2330 train_time:81550ms step_avg:38.39ms
step:2125/2330 train_time:81586ms step_avg:38.39ms
step:2126/2330 train_time:81628ms step_avg:38.40ms
step:2127/2330 train_time:81664ms step_avg:38.39ms
step:2128/2330 train_time:81705ms step_avg:38.40ms
step:2129/2330 train_time:81741ms step_avg:38.39ms
step:2130/2330 train_time:81782ms step_avg:38.40ms
step:2131/2330 train_time:81818ms step_avg:38.39ms
step:2132/2330 train_time:81859ms step_avg:38.40ms
step:2133/2330 train_time:81894ms step_avg:38.39ms
step:2134/2330 train_time:81936ms step_avg:38.40ms
step:2135/2330 train_time:81971ms step_avg:38.39ms
step:2136/2330 train_time:82012ms step_avg:38.40ms
step:2137/2330 train_time:82049ms step_avg:38.39ms
step:2138/2330 train_time:82091ms step_avg:38.40ms
step:2139/2330 train_time:82126ms step_avg:38.39ms
step:2140/2330 train_time:82168ms step_avg:38.40ms
step:2141/2330 train_time:82203ms step_avg:38.39ms
step:2142/2330 train_time:82245ms step_avg:38.40ms
step:2143/2330 train_time:82282ms step_avg:38.40ms
step:2144/2330 train_time:82322ms step_avg:38.40ms
step:2145/2330 train_time:82359ms step_avg:38.40ms
step:2146/2330 train_time:82400ms step_avg:38.40ms
step:2147/2330 train_time:82435ms step_avg:38.40ms
step:2148/2330 train_time:82476ms step_avg:38.40ms
step:2149/2330 train_time:82512ms step_avg:38.40ms
step:2150/2330 train_time:82553ms step_avg:38.40ms
step:2151/2330 train_time:82589ms step_avg:38.40ms
step:2152/2330 train_time:82630ms step_avg:38.40ms
step:2153/2330 train_time:82666ms step_avg:38.40ms
step:2154/2330 train_time:82708ms step_avg:38.40ms
step:2155/2330 train_time:82744ms step_avg:38.40ms
step:2156/2330 train_time:82785ms step_avg:38.40ms
step:2157/2330 train_time:82821ms step_avg:38.40ms
step:2158/2330 train_time:82862ms step_avg:38.40ms
step:2159/2330 train_time:82898ms step_avg:38.40ms
step:2160/2330 train_time:82939ms step_avg:38.40ms
step:2161/2330 train_time:82974ms step_avg:38.40ms
step:2162/2330 train_time:83015ms step_avg:38.40ms
step:2163/2330 train_time:83051ms step_avg:38.40ms
step:2164/2330 train_time:83092ms step_avg:38.40ms
step:2165/2330 train_time:83129ms step_avg:38.40ms
step:2166/2330 train_time:83170ms step_avg:38.40ms
step:2167/2330 train_time:83206ms step_avg:38.40ms
step:2168/2330 train_time:83248ms step_avg:38.40ms
step:2169/2330 train_time:83283ms step_avg:38.40ms
step:2170/2330 train_time:83325ms step_avg:38.40ms
step:2171/2330 train_time:83361ms step_avg:38.40ms
step:2172/2330 train_time:83402ms step_avg:38.40ms
step:2173/2330 train_time:83438ms step_avg:38.40ms
step:2174/2330 train_time:83479ms step_avg:38.40ms
step:2175/2330 train_time:83514ms step_avg:38.40ms
step:2176/2330 train_time:83555ms step_avg:38.40ms
step:2177/2330 train_time:83591ms step_avg:38.40ms
step:2178/2330 train_time:83633ms step_avg:38.40ms
step:2179/2330 train_time:83668ms step_avg:38.40ms
step:2180/2330 train_time:83710ms step_avg:38.40ms
step:2181/2330 train_time:83745ms step_avg:38.40ms
step:2182/2330 train_time:83786ms step_avg:38.40ms
step:2183/2330 train_time:83823ms step_avg:38.40ms
step:2184/2330 train_time:83864ms step_avg:38.40ms
step:2185/2330 train_time:83900ms step_avg:38.40ms
step:2186/2330 train_time:83941ms step_avg:38.40ms
step:2187/2330 train_time:83977ms step_avg:38.40ms
step:2188/2330 train_time:84018ms step_avg:38.40ms
step:2189/2330 train_time:84053ms step_avg:38.40ms
step:2190/2330 train_time:84094ms step_avg:38.40ms
step:2191/2330 train_time:84131ms step_avg:38.40ms
step:2192/2330 train_time:84172ms step_avg:38.40ms
step:2193/2330 train_time:84208ms step_avg:38.40ms
step:2194/2330 train_time:84250ms step_avg:38.40ms
step:2195/2330 train_time:84286ms step_avg:38.40ms
step:2196/2330 train_time:84328ms step_avg:38.40ms
step:2197/2330 train_time:84364ms step_avg:38.40ms
step:2198/2330 train_time:84405ms step_avg:38.40ms
step:2199/2330 train_time:84442ms step_avg:38.40ms
step:2200/2330 train_time:84482ms step_avg:38.40ms
step:2201/2330 train_time:84519ms step_avg:38.40ms
step:2202/2330 train_time:84560ms step_avg:38.40ms
step:2203/2330 train_time:84596ms step_avg:38.40ms
step:2204/2330 train_time:84637ms step_avg:38.40ms
step:2205/2330 train_time:84673ms step_avg:38.40ms
step:2206/2330 train_time:84713ms step_avg:38.40ms
step:2207/2330 train_time:84750ms step_avg:38.40ms
step:2208/2330 train_time:84792ms step_avg:38.40ms
step:2209/2330 train_time:84828ms step_avg:38.40ms
step:2210/2330 train_time:84870ms step_avg:38.40ms
step:2211/2330 train_time:84905ms step_avg:38.40ms
step:2212/2330 train_time:84946ms step_avg:38.40ms
step:2213/2330 train_time:84982ms step_avg:38.40ms
step:2214/2330 train_time:85023ms step_avg:38.40ms
step:2215/2330 train_time:85060ms step_avg:38.40ms
step:2216/2330 train_time:85101ms step_avg:38.40ms
step:2217/2330 train_time:85136ms step_avg:38.40ms
step:2218/2330 train_time:85177ms step_avg:38.40ms
step:2219/2330 train_time:85213ms step_avg:38.40ms
step:2220/2330 train_time:85254ms step_avg:38.40ms
step:2221/2330 train_time:85291ms step_avg:38.40ms
step:2222/2330 train_time:85332ms step_avg:38.40ms
step:2223/2330 train_time:85367ms step_avg:38.40ms
step:2224/2330 train_time:85408ms step_avg:38.40ms
step:2225/2330 train_time:85445ms step_avg:38.40ms
step:2226/2330 train_time:85486ms step_avg:38.40ms
step:2227/2330 train_time:85523ms step_avg:38.40ms
step:2228/2330 train_time:85563ms step_avg:38.40ms
step:2229/2330 train_time:85599ms step_avg:38.40ms
step:2230/2330 train_time:85640ms step_avg:38.40ms
step:2231/2330 train_time:85676ms step_avg:38.40ms
step:2232/2330 train_time:85717ms step_avg:38.40ms
step:2233/2330 train_time:85753ms step_avg:38.40ms
step:2234/2330 train_time:85794ms step_avg:38.40ms
step:2235/2330 train_time:85830ms step_avg:38.40ms
step:2236/2330 train_time:85871ms step_avg:38.40ms
step:2237/2330 train_time:85907ms step_avg:38.40ms
step:2238/2330 train_time:85948ms step_avg:38.40ms
step:2239/2330 train_time:85984ms step_avg:38.40ms
step:2240/2330 train_time:86026ms step_avg:38.40ms
step:2241/2330 train_time:86062ms step_avg:38.40ms
step:2242/2330 train_time:86102ms step_avg:38.40ms
step:2243/2330 train_time:86139ms step_avg:38.40ms
step:2244/2330 train_time:86180ms step_avg:38.40ms
step:2245/2330 train_time:86215ms step_avg:38.40ms
step:2246/2330 train_time:86256ms step_avg:38.40ms
step:2247/2330 train_time:86291ms step_avg:38.40ms
step:2248/2330 train_time:86333ms step_avg:38.40ms
step:2249/2330 train_time:86368ms step_avg:38.40ms
step:2250/2330 train_time:86410ms step_avg:38.40ms
step:2250/2330 val_loss:5.1513 train_time:86525ms step_avg:38.46ms
step:2251/2330 train_time:86536ms step_avg:38.44ms
step:2252/2330 train_time:86547ms step_avg:38.43ms
step:2253/2330 train_time:86557ms step_avg:38.42ms
step:2254/2330 train_time:86568ms step_avg:38.41ms
step:2255/2330 train_time:86602ms step_avg:38.40ms
step:2256/2330 train_time:86643ms step_avg:38.41ms
step:2257/2330 train_time:86678ms step_avg:38.40ms
step:2258/2330 train_time:86718ms step_avg:38.40ms
step:2259/2330 train_time:86753ms step_avg:38.40ms
step:2260/2330 train_time:86793ms step_avg:38.40ms
step:2261/2330 train_time:86832ms step_avg:38.40ms
step:2262/2330 train_time:86874ms step_avg:38.41ms
step:2263/2330 train_time:86915ms step_avg:38.41ms
step:2264/2330 train_time:86956ms step_avg:38.41ms
step:2265/2330 train_time:86994ms step_avg:38.41ms
step:2266/2330 train_time:87034ms step_avg:38.41ms
step:2267/2330 train_time:87070ms step_avg:38.41ms
step:2268/2330 train_time:87112ms step_avg:38.41ms
step:2269/2330 train_time:87146ms step_avg:38.41ms
step:2270/2330 train_time:87187ms step_avg:38.41ms
step:2271/2330 train_time:87222ms step_avg:38.41ms
step:2272/2330 train_time:87263ms step_avg:38.41ms
step:2273/2330 train_time:87298ms step_avg:38.41ms
step:2274/2330 train_time:87339ms step_avg:38.41ms
step:2275/2330 train_time:87375ms step_avg:38.41ms
step:2276/2330 train_time:87415ms step_avg:38.41ms
step:2277/2330 train_time:87451ms step_avg:38.41ms
step:2278/2330 train_time:87492ms step_avg:38.41ms
step:2279/2330 train_time:87527ms step_avg:38.41ms
step:2280/2330 train_time:87568ms step_avg:38.41ms
step:2281/2330 train_time:87603ms step_avg:38.41ms
step:2282/2330 train_time:87644ms step_avg:38.41ms
step:2283/2330 train_time:87679ms step_avg:38.41ms
step:2284/2330 train_time:87720ms step_avg:38.41ms
step:2285/2330 train_time:87756ms step_avg:38.41ms
step:2286/2330 train_time:87797ms step_avg:38.41ms
step:2287/2330 train_time:87835ms step_avg:38.41ms
step:2288/2330 train_time:87876ms step_avg:38.41ms
step:2289/2330 train_time:87916ms step_avg:38.41ms
step:2290/2330 train_time:87957ms step_avg:38.41ms
step:2291/2330 train_time:87996ms step_avg:38.41ms
step:2292/2330 train_time:88037ms step_avg:38.41ms
step:2293/2330 train_time:88074ms step_avg:38.41ms
step:2294/2330 train_time:88115ms step_avg:38.41ms
step:2295/2330 train_time:88151ms step_avg:38.41ms
step:2296/2330 train_time:88192ms step_avg:38.41ms
step:2297/2330 train_time:88227ms step_avg:38.41ms
step:2298/2330 train_time:88267ms step_avg:38.41ms
step:2299/2330 train_time:88303ms step_avg:38.41ms
step:2300/2330 train_time:88344ms step_avg:38.41ms
step:2301/2330 train_time:88378ms step_avg:38.41ms
step:2302/2330 train_time:88420ms step_avg:38.41ms
step:2303/2330 train_time:88454ms step_avg:38.41ms
step:2304/2330 train_time:88495ms step_avg:38.41ms
step:2305/2330 train_time:88530ms step_avg:38.41ms
step:2306/2330 train_time:88571ms step_avg:38.41ms
step:2307/2330 train_time:88607ms step_avg:38.41ms
step:2308/2330 train_time:88648ms step_avg:38.41ms
step:2309/2330 train_time:88684ms step_avg:38.41ms
step:2310/2330 train_time:88725ms step_avg:38.41ms
step:2311/2330 train_time:88762ms step_avg:38.41ms
step:2312/2330 train_time:88803ms step_avg:38.41ms
step:2313/2330 train_time:88840ms step_avg:38.41ms
step:2314/2330 train_time:88882ms step_avg:38.41ms
step:2315/2330 train_time:88918ms step_avg:38.41ms
step:2316/2330 train_time:88960ms step_avg:38.41ms
step:2317/2330 train_time:88996ms step_avg:38.41ms
step:2318/2330 train_time:89038ms step_avg:38.41ms
step:2319/2330 train_time:89073ms step_avg:38.41ms
step:2320/2330 train_time:89114ms step_avg:38.41ms
step:2321/2330 train_time:89150ms step_avg:38.41ms
step:2322/2330 train_time:89191ms step_avg:38.41ms
step:2323/2330 train_time:89226ms step_avg:38.41ms
step:2324/2330 train_time:89267ms step_avg:38.41ms
step:2325/2330 train_time:89302ms step_avg:38.41ms
step:2326/2330 train_time:89344ms step_avg:38.41ms
step:2327/2330 train_time:89379ms step_avg:38.41ms
step:2328/2330 train_time:89420ms step_avg:38.41ms
step:2329/2330 train_time:89456ms step_avg:38.41ms
step:2330/2330 train_time:89497ms step_avg:38.41ms
step:2330/2330 val_loss:5.1445 train_time:89609ms step_avg:38.46ms
peak memory allocated: 27165 MiB reserved: 38888 MiB
