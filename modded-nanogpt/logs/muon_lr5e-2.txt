import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "muon_lr5e-2"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=5e-2, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Oct 28 23:27:01 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   33C    P0             112W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   31C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   28C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   31C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   32C    P0             115W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:93ms step_avg:92.71ms
step:2/2330 train_time:193ms step_avg:96.48ms
step:3/2330 train_time:214ms step_avg:71.45ms
step:4/2330 train_time:250ms step_avg:62.56ms
step:5/2330 train_time:308ms step_avg:61.53ms
step:6/2330 train_time:369ms step_avg:61.44ms
step:7/2330 train_time:428ms step_avg:61.07ms
step:8/2330 train_time:488ms step_avg:61.01ms
step:9/2330 train_time:547ms step_avg:60.76ms
step:10/2330 train_time:608ms step_avg:60.77ms
step:11/2330 train_time:666ms step_avg:60.57ms
step:12/2330 train_time:727ms step_avg:60.58ms
step:13/2330 train_time:786ms step_avg:60.43ms
step:14/2330 train_time:847ms step_avg:60.48ms
step:15/2330 train_time:905ms step_avg:60.36ms
step:16/2330 train_time:966ms step_avg:60.40ms
step:17/2330 train_time:1027ms step_avg:60.39ms
step:18/2330 train_time:1090ms step_avg:60.58ms
step:19/2330 train_time:1154ms step_avg:60.72ms
step:20/2330 train_time:1217ms step_avg:60.83ms
step:21/2330 train_time:1277ms step_avg:60.80ms
step:22/2330 train_time:1338ms step_avg:60.84ms
step:23/2330 train_time:1398ms step_avg:60.79ms
step:24/2330 train_time:1461ms step_avg:60.86ms
step:25/2330 train_time:1520ms step_avg:60.80ms
step:26/2330 train_time:1582ms step_avg:60.85ms
step:27/2330 train_time:1642ms step_avg:60.82ms
step:28/2330 train_time:1704ms step_avg:60.84ms
step:29/2330 train_time:1763ms step_avg:60.79ms
step:30/2330 train_time:1824ms step_avg:60.79ms
step:31/2330 train_time:1883ms step_avg:60.73ms
step:32/2330 train_time:1944ms step_avg:60.74ms
step:33/2330 train_time:2003ms step_avg:60.70ms
step:34/2330 train_time:2066ms step_avg:60.77ms
step:35/2330 train_time:2127ms step_avg:60.77ms
step:36/2330 train_time:2190ms step_avg:60.84ms
step:37/2330 train_time:2251ms step_avg:60.84ms
step:38/2330 train_time:2313ms step_avg:60.86ms
step:39/2330 train_time:2373ms step_avg:60.84ms
step:40/2330 train_time:2434ms step_avg:60.86ms
step:41/2330 train_time:2494ms step_avg:60.82ms
step:42/2330 train_time:2555ms step_avg:60.83ms
step:43/2330 train_time:2614ms step_avg:60.79ms
step:44/2330 train_time:2675ms step_avg:60.80ms
step:45/2330 train_time:2734ms step_avg:60.76ms
step:46/2330 train_time:2795ms step_avg:60.76ms
step:47/2330 train_time:2855ms step_avg:60.74ms
step:48/2330 train_time:2916ms step_avg:60.75ms
step:49/2330 train_time:2976ms step_avg:60.74ms
step:50/2330 train_time:3038ms step_avg:60.76ms
step:51/2330 train_time:3098ms step_avg:60.74ms
step:52/2330 train_time:3161ms step_avg:60.78ms
step:53/2330 train_time:3221ms step_avg:60.76ms
step:54/2330 train_time:3283ms step_avg:60.79ms
step:55/2330 train_time:3343ms step_avg:60.77ms
step:56/2330 train_time:3404ms step_avg:60.79ms
step:57/2330 train_time:3464ms step_avg:60.78ms
step:58/2330 train_time:3527ms step_avg:60.80ms
step:59/2330 train_time:3586ms step_avg:60.78ms
step:60/2330 train_time:3648ms step_avg:60.80ms
step:61/2330 train_time:3707ms step_avg:60.77ms
step:62/2330 train_time:3768ms step_avg:60.78ms
step:63/2330 train_time:3828ms step_avg:60.76ms
step:64/2330 train_time:3889ms step_avg:60.77ms
step:65/2330 train_time:3949ms step_avg:60.75ms
step:66/2330 train_time:4010ms step_avg:60.76ms
step:67/2330 train_time:4070ms step_avg:60.74ms
step:68/2330 train_time:4131ms step_avg:60.75ms
step:69/2330 train_time:4191ms step_avg:60.74ms
step:70/2330 train_time:4253ms step_avg:60.76ms
step:71/2330 train_time:4313ms step_avg:60.74ms
step:72/2330 train_time:4374ms step_avg:60.75ms
step:73/2330 train_time:4433ms step_avg:60.73ms
step:74/2330 train_time:4494ms step_avg:60.73ms
step:75/2330 train_time:4554ms step_avg:60.71ms
step:76/2330 train_time:4615ms step_avg:60.72ms
step:77/2330 train_time:4675ms step_avg:60.71ms
step:78/2330 train_time:4736ms step_avg:60.72ms
step:79/2330 train_time:4795ms step_avg:60.70ms
step:80/2330 train_time:4857ms step_avg:60.71ms
step:81/2330 train_time:4916ms step_avg:60.70ms
step:82/2330 train_time:4979ms step_avg:60.71ms
step:83/2330 train_time:5038ms step_avg:60.70ms
step:84/2330 train_time:5100ms step_avg:60.71ms
step:85/2330 train_time:5160ms step_avg:60.70ms
step:86/2330 train_time:5221ms step_avg:60.71ms
step:87/2330 train_time:5280ms step_avg:60.69ms
step:88/2330 train_time:5342ms step_avg:60.70ms
step:89/2330 train_time:5402ms step_avg:60.69ms
step:90/2330 train_time:5464ms step_avg:60.71ms
step:91/2330 train_time:5524ms step_avg:60.70ms
step:92/2330 train_time:5585ms step_avg:60.71ms
step:93/2330 train_time:5645ms step_avg:60.70ms
step:94/2330 train_time:5707ms step_avg:60.71ms
step:95/2330 train_time:5766ms step_avg:60.69ms
step:96/2330 train_time:5827ms step_avg:60.70ms
step:97/2330 train_time:5887ms step_avg:60.69ms
step:98/2330 train_time:5949ms step_avg:60.70ms
step:99/2330 train_time:6008ms step_avg:60.69ms
step:100/2330 train_time:6070ms step_avg:60.70ms
step:101/2330 train_time:6129ms step_avg:60.68ms
step:102/2330 train_time:6190ms step_avg:60.69ms
step:103/2330 train_time:6249ms step_avg:60.67ms
step:104/2330 train_time:6311ms step_avg:60.68ms
step:105/2330 train_time:6371ms step_avg:60.68ms
step:106/2330 train_time:6433ms step_avg:60.68ms
step:107/2330 train_time:6492ms step_avg:60.67ms
step:108/2330 train_time:6553ms step_avg:60.68ms
step:109/2330 train_time:6612ms step_avg:60.66ms
step:110/2330 train_time:6673ms step_avg:60.66ms
step:111/2330 train_time:6732ms step_avg:60.65ms
step:112/2330 train_time:6793ms step_avg:60.65ms
step:113/2330 train_time:6852ms step_avg:60.63ms
step:114/2330 train_time:6912ms step_avg:60.64ms
step:115/2330 train_time:6972ms step_avg:60.63ms
step:116/2330 train_time:7033ms step_avg:60.63ms
step:117/2330 train_time:7093ms step_avg:60.62ms
step:118/2330 train_time:7154ms step_avg:60.63ms
step:119/2330 train_time:7213ms step_avg:60.61ms
step:120/2330 train_time:7275ms step_avg:60.62ms
step:121/2330 train_time:7334ms step_avg:60.61ms
step:122/2330 train_time:7395ms step_avg:60.62ms
step:123/2330 train_time:7454ms step_avg:60.61ms
step:124/2330 train_time:7515ms step_avg:60.61ms
step:125/2330 train_time:7575ms step_avg:60.60ms
step:126/2330 train_time:7635ms step_avg:60.60ms
step:127/2330 train_time:7694ms step_avg:60.58ms
step:128/2330 train_time:7756ms step_avg:60.59ms
step:129/2330 train_time:7815ms step_avg:60.58ms
step:130/2330 train_time:7876ms step_avg:60.59ms
step:131/2330 train_time:7936ms step_avg:60.58ms
step:132/2330 train_time:7997ms step_avg:60.58ms
step:133/2330 train_time:8056ms step_avg:60.57ms
step:134/2330 train_time:8117ms step_avg:60.58ms
step:135/2330 train_time:8176ms step_avg:60.57ms
step:136/2330 train_time:8238ms step_avg:60.57ms
step:137/2330 train_time:8298ms step_avg:60.57ms
step:138/2330 train_time:8360ms step_avg:60.58ms
step:139/2330 train_time:8419ms step_avg:60.57ms
step:140/2330 train_time:8481ms step_avg:60.58ms
step:141/2330 train_time:8540ms step_avg:60.57ms
step:142/2330 train_time:8601ms step_avg:60.57ms
step:143/2330 train_time:8661ms step_avg:60.56ms
step:144/2330 train_time:8722ms step_avg:60.57ms
step:145/2330 train_time:8781ms step_avg:60.56ms
step:146/2330 train_time:8843ms step_avg:60.57ms
step:147/2330 train_time:8903ms step_avg:60.57ms
step:148/2330 train_time:8965ms step_avg:60.58ms
step:149/2330 train_time:9025ms step_avg:60.57ms
step:150/2330 train_time:9086ms step_avg:60.58ms
step:151/2330 train_time:9146ms step_avg:60.57ms
step:152/2330 train_time:9208ms step_avg:60.58ms
step:153/2330 train_time:9268ms step_avg:60.57ms
step:154/2330 train_time:9329ms step_avg:60.58ms
step:155/2330 train_time:9388ms step_avg:60.57ms
step:156/2330 train_time:9449ms step_avg:60.57ms
step:157/2330 train_time:9509ms step_avg:60.57ms
step:158/2330 train_time:9571ms step_avg:60.58ms
step:159/2330 train_time:9630ms step_avg:60.56ms
step:160/2330 train_time:9691ms step_avg:60.57ms
step:161/2330 train_time:9750ms step_avg:60.56ms
step:162/2330 train_time:9811ms step_avg:60.56ms
step:163/2330 train_time:9871ms step_avg:60.56ms
step:164/2330 train_time:9932ms step_avg:60.56ms
step:165/2330 train_time:9990ms step_avg:60.55ms
step:166/2330 train_time:10051ms step_avg:60.55ms
step:167/2330 train_time:10110ms step_avg:60.54ms
step:168/2330 train_time:10171ms step_avg:60.54ms
step:169/2330 train_time:10230ms step_avg:60.54ms
step:170/2330 train_time:10291ms step_avg:60.54ms
step:171/2330 train_time:10350ms step_avg:60.53ms
step:172/2330 train_time:10411ms step_avg:60.53ms
step:173/2330 train_time:10470ms step_avg:60.52ms
step:174/2330 train_time:10531ms step_avg:60.52ms
step:175/2330 train_time:10590ms step_avg:60.52ms
step:176/2330 train_time:10651ms step_avg:60.52ms
step:177/2330 train_time:10710ms step_avg:60.51ms
step:178/2330 train_time:10772ms step_avg:60.51ms
step:179/2330 train_time:10831ms step_avg:60.51ms
step:180/2330 train_time:10891ms step_avg:60.51ms
step:181/2330 train_time:10950ms step_avg:60.50ms
step:182/2330 train_time:11011ms step_avg:60.50ms
step:183/2330 train_time:11070ms step_avg:60.49ms
step:184/2330 train_time:11131ms step_avg:60.49ms
step:185/2330 train_time:11190ms step_avg:60.49ms
step:186/2330 train_time:11251ms step_avg:60.49ms
step:187/2330 train_time:11310ms step_avg:60.48ms
step:188/2330 train_time:11371ms step_avg:60.49ms
step:189/2330 train_time:11430ms step_avg:60.48ms
step:190/2330 train_time:11491ms step_avg:60.48ms
step:191/2330 train_time:11550ms step_avg:60.47ms
step:192/2330 train_time:11611ms step_avg:60.47ms
step:193/2330 train_time:11671ms step_avg:60.47ms
step:194/2330 train_time:11732ms step_avg:60.47ms
step:195/2330 train_time:11792ms step_avg:60.47ms
step:196/2330 train_time:11853ms step_avg:60.47ms
step:197/2330 train_time:11911ms step_avg:60.46ms
step:198/2330 train_time:11972ms step_avg:60.46ms
step:199/2330 train_time:12031ms step_avg:60.46ms
step:200/2330 train_time:12092ms step_avg:60.46ms
step:201/2330 train_time:12151ms step_avg:60.45ms
step:202/2330 train_time:12212ms step_avg:60.46ms
step:203/2330 train_time:12271ms step_avg:60.45ms
step:204/2330 train_time:12332ms step_avg:60.45ms
step:205/2330 train_time:12391ms step_avg:60.44ms
step:206/2330 train_time:12452ms step_avg:60.45ms
step:207/2330 train_time:12511ms step_avg:60.44ms
step:208/2330 train_time:12573ms step_avg:60.44ms
step:209/2330 train_time:12632ms step_avg:60.44ms
step:210/2330 train_time:12693ms step_avg:60.44ms
step:211/2330 train_time:12752ms step_avg:60.44ms
step:212/2330 train_time:12813ms step_avg:60.44ms
step:213/2330 train_time:12872ms step_avg:60.43ms
step:214/2330 train_time:12933ms step_avg:60.43ms
step:215/2330 train_time:12992ms step_avg:60.43ms
step:216/2330 train_time:13053ms step_avg:60.43ms
step:217/2330 train_time:13112ms step_avg:60.42ms
step:218/2330 train_time:13173ms step_avg:60.43ms
step:219/2330 train_time:13232ms step_avg:60.42ms
step:220/2330 train_time:13293ms step_avg:60.42ms
step:221/2330 train_time:13351ms step_avg:60.41ms
step:222/2330 train_time:13412ms step_avg:60.42ms
step:223/2330 train_time:13471ms step_avg:60.41ms
step:224/2330 train_time:13532ms step_avg:60.41ms
step:225/2330 train_time:13591ms step_avg:60.41ms
step:226/2330 train_time:13652ms step_avg:60.41ms
step:227/2330 train_time:13711ms step_avg:60.40ms
step:228/2330 train_time:13772ms step_avg:60.40ms
step:229/2330 train_time:13831ms step_avg:60.40ms
step:230/2330 train_time:13892ms step_avg:60.40ms
step:231/2330 train_time:13951ms step_avg:60.39ms
step:232/2330 train_time:14012ms step_avg:60.40ms
step:233/2330 train_time:14071ms step_avg:60.39ms
step:234/2330 train_time:14132ms step_avg:60.39ms
step:235/2330 train_time:14192ms step_avg:60.39ms
step:236/2330 train_time:14253ms step_avg:60.39ms
step:237/2330 train_time:14311ms step_avg:60.39ms
step:238/2330 train_time:14372ms step_avg:60.39ms
step:239/2330 train_time:14431ms step_avg:60.38ms
step:240/2330 train_time:14492ms step_avg:60.38ms
step:241/2330 train_time:14551ms step_avg:60.38ms
step:242/2330 train_time:14612ms step_avg:60.38ms
step:243/2330 train_time:14671ms step_avg:60.37ms
step:244/2330 train_time:14732ms step_avg:60.38ms
step:245/2330 train_time:14790ms step_avg:60.37ms
step:246/2330 train_time:14852ms step_avg:60.37ms
step:247/2330 train_time:14910ms step_avg:60.36ms
step:248/2330 train_time:14971ms step_avg:60.37ms
step:249/2330 train_time:15030ms step_avg:60.36ms
step:250/2330 train_time:15091ms step_avg:60.36ms
step:250/2330 val_loss:4.0864 train_time:15154ms step_avg:60.62ms
step:251/2330 train_time:15176ms step_avg:60.46ms
step:252/2330 train_time:15213ms step_avg:60.37ms
step:253/2330 train_time:15276ms step_avg:60.38ms
step:254/2330 train_time:15342ms step_avg:60.40ms
step:255/2330 train_time:15401ms step_avg:60.40ms
step:256/2330 train_time:15462ms step_avg:60.40ms
step:257/2330 train_time:15521ms step_avg:60.39ms
step:258/2330 train_time:15581ms step_avg:60.39ms
step:259/2330 train_time:15640ms step_avg:60.39ms
step:260/2330 train_time:15700ms step_avg:60.39ms
step:261/2330 train_time:15759ms step_avg:60.38ms
step:262/2330 train_time:15819ms step_avg:60.38ms
step:263/2330 train_time:15877ms step_avg:60.37ms
step:264/2330 train_time:15938ms step_avg:60.37ms
step:265/2330 train_time:15997ms step_avg:60.36ms
step:266/2330 train_time:16058ms step_avg:60.37ms
step:267/2330 train_time:16119ms step_avg:60.37ms
step:268/2330 train_time:16182ms step_avg:60.38ms
step:269/2330 train_time:16242ms step_avg:60.38ms
step:270/2330 train_time:16304ms step_avg:60.39ms
step:271/2330 train_time:16363ms step_avg:60.38ms
step:272/2330 train_time:16425ms step_avg:60.38ms
step:273/2330 train_time:16484ms step_avg:60.38ms
step:274/2330 train_time:16545ms step_avg:60.38ms
step:275/2330 train_time:16604ms step_avg:60.38ms
step:276/2330 train_time:16666ms step_avg:60.38ms
step:277/2330 train_time:16725ms step_avg:60.38ms
step:278/2330 train_time:16786ms step_avg:60.38ms
step:279/2330 train_time:16844ms step_avg:60.37ms
step:280/2330 train_time:16905ms step_avg:60.38ms
step:281/2330 train_time:16964ms step_avg:60.37ms
step:282/2330 train_time:17025ms step_avg:60.37ms
step:283/2330 train_time:17084ms step_avg:60.37ms
step:284/2330 train_time:17146ms step_avg:60.37ms
step:285/2330 train_time:17206ms step_avg:60.37ms
step:286/2330 train_time:17268ms step_avg:60.38ms
step:287/2330 train_time:17328ms step_avg:60.38ms
step:288/2330 train_time:17389ms step_avg:60.38ms
step:289/2330 train_time:17449ms step_avg:60.38ms
step:290/2330 train_time:17510ms step_avg:60.38ms
step:291/2330 train_time:17569ms step_avg:60.37ms
step:292/2330 train_time:17631ms step_avg:60.38ms
step:293/2330 train_time:17690ms step_avg:60.38ms
step:294/2330 train_time:17751ms step_avg:60.38ms
step:295/2330 train_time:17810ms step_avg:60.37ms
step:296/2330 train_time:17871ms step_avg:60.38ms
step:297/2330 train_time:17931ms step_avg:60.37ms
step:298/2330 train_time:17992ms step_avg:60.38ms
step:299/2330 train_time:18051ms step_avg:60.37ms
step:300/2330 train_time:18113ms step_avg:60.38ms
step:301/2330 train_time:18173ms step_avg:60.38ms
step:302/2330 train_time:18235ms step_avg:60.38ms
step:303/2330 train_time:18294ms step_avg:60.38ms
step:304/2330 train_time:18355ms step_avg:60.38ms
step:305/2330 train_time:18415ms step_avg:60.38ms
step:306/2330 train_time:18477ms step_avg:60.38ms
step:307/2330 train_time:18537ms step_avg:60.38ms
step:308/2330 train_time:18598ms step_avg:60.38ms
step:309/2330 train_time:18657ms step_avg:60.38ms
step:310/2330 train_time:18718ms step_avg:60.38ms
step:311/2330 train_time:18777ms step_avg:60.38ms
step:312/2330 train_time:18839ms step_avg:60.38ms
step:313/2330 train_time:18898ms step_avg:60.38ms
step:314/2330 train_time:18959ms step_avg:60.38ms
step:315/2330 train_time:19019ms step_avg:60.38ms
step:316/2330 train_time:19080ms step_avg:60.38ms
step:317/2330 train_time:19139ms step_avg:60.37ms
step:318/2330 train_time:19199ms step_avg:60.38ms
step:319/2330 train_time:19258ms step_avg:60.37ms
step:320/2330 train_time:19319ms step_avg:60.37ms
step:321/2330 train_time:19378ms step_avg:60.37ms
step:322/2330 train_time:19440ms step_avg:60.37ms
step:323/2330 train_time:19499ms step_avg:60.37ms
step:324/2330 train_time:19560ms step_avg:60.37ms
step:325/2330 train_time:19618ms step_avg:60.36ms
step:326/2330 train_time:19680ms step_avg:60.37ms
step:327/2330 train_time:19739ms step_avg:60.37ms
step:328/2330 train_time:19801ms step_avg:60.37ms
step:329/2330 train_time:19860ms step_avg:60.36ms
step:330/2330 train_time:19921ms step_avg:60.37ms
step:331/2330 train_time:19980ms step_avg:60.36ms
step:332/2330 train_time:20041ms step_avg:60.37ms
step:333/2330 train_time:20100ms step_avg:60.36ms
step:334/2330 train_time:20161ms step_avg:60.36ms
step:335/2330 train_time:20220ms step_avg:60.36ms
step:336/2330 train_time:20281ms step_avg:60.36ms
step:337/2330 train_time:20340ms step_avg:60.36ms
step:338/2330 train_time:20401ms step_avg:60.36ms
step:339/2330 train_time:20460ms step_avg:60.35ms
step:340/2330 train_time:20521ms step_avg:60.36ms
step:341/2330 train_time:20580ms step_avg:60.35ms
step:342/2330 train_time:20642ms step_avg:60.36ms
step:343/2330 train_time:20700ms step_avg:60.35ms
step:344/2330 train_time:20761ms step_avg:60.35ms
step:345/2330 train_time:20820ms step_avg:60.35ms
step:346/2330 train_time:20881ms step_avg:60.35ms
step:347/2330 train_time:20941ms step_avg:60.35ms
step:348/2330 train_time:21001ms step_avg:60.35ms
step:349/2330 train_time:21060ms step_avg:60.34ms
step:350/2330 train_time:21121ms step_avg:60.35ms
step:351/2330 train_time:21180ms step_avg:60.34ms
step:352/2330 train_time:21241ms step_avg:60.34ms
step:353/2330 train_time:21300ms step_avg:60.34ms
step:354/2330 train_time:21361ms step_avg:60.34ms
step:355/2330 train_time:21420ms step_avg:60.34ms
step:356/2330 train_time:21481ms step_avg:60.34ms
step:357/2330 train_time:21540ms step_avg:60.34ms
step:358/2330 train_time:21601ms step_avg:60.34ms
step:359/2330 train_time:21660ms step_avg:60.33ms
step:360/2330 train_time:21721ms step_avg:60.34ms
step:361/2330 train_time:21780ms step_avg:60.33ms
step:362/2330 train_time:21841ms step_avg:60.33ms
step:363/2330 train_time:21899ms step_avg:60.33ms
step:364/2330 train_time:21960ms step_avg:60.33ms
step:365/2330 train_time:22020ms step_avg:60.33ms
step:366/2330 train_time:22080ms step_avg:60.33ms
step:367/2330 train_time:22140ms step_avg:60.33ms
step:368/2330 train_time:22201ms step_avg:60.33ms
step:369/2330 train_time:22259ms step_avg:60.32ms
step:370/2330 train_time:22320ms step_avg:60.32ms
step:371/2330 train_time:22378ms step_avg:60.32ms
step:372/2330 train_time:22440ms step_avg:60.32ms
step:373/2330 train_time:22498ms step_avg:60.32ms
step:374/2330 train_time:22559ms step_avg:60.32ms
step:375/2330 train_time:22618ms step_avg:60.32ms
step:376/2330 train_time:22679ms step_avg:60.32ms
step:377/2330 train_time:22739ms step_avg:60.32ms
step:378/2330 train_time:22800ms step_avg:60.32ms
step:379/2330 train_time:22859ms step_avg:60.31ms
step:380/2330 train_time:22920ms step_avg:60.32ms
step:381/2330 train_time:22980ms step_avg:60.31ms
step:382/2330 train_time:23041ms step_avg:60.32ms
step:383/2330 train_time:23100ms step_avg:60.31ms
step:384/2330 train_time:23160ms step_avg:60.31ms
step:385/2330 train_time:23220ms step_avg:60.31ms
step:386/2330 train_time:23281ms step_avg:60.31ms
step:387/2330 train_time:23340ms step_avg:60.31ms
step:388/2330 train_time:23401ms step_avg:60.31ms
step:389/2330 train_time:23460ms step_avg:60.31ms
step:390/2330 train_time:23521ms step_avg:60.31ms
step:391/2330 train_time:23580ms step_avg:60.31ms
step:392/2330 train_time:23641ms step_avg:60.31ms
step:393/2330 train_time:23700ms step_avg:60.31ms
step:394/2330 train_time:23761ms step_avg:60.31ms
step:395/2330 train_time:23820ms step_avg:60.30ms
step:396/2330 train_time:23881ms step_avg:60.31ms
step:397/2330 train_time:23940ms step_avg:60.30ms
step:398/2330 train_time:24001ms step_avg:60.30ms
step:399/2330 train_time:24059ms step_avg:60.30ms
step:400/2330 train_time:24121ms step_avg:60.30ms
step:401/2330 train_time:24179ms step_avg:60.30ms
step:402/2330 train_time:24241ms step_avg:60.30ms
step:403/2330 train_time:24300ms step_avg:60.30ms
step:404/2330 train_time:24361ms step_avg:60.30ms
step:405/2330 train_time:24420ms step_avg:60.30ms
step:406/2330 train_time:24481ms step_avg:60.30ms
step:407/2330 train_time:24540ms step_avg:60.29ms
step:408/2330 train_time:24601ms step_avg:60.30ms
step:409/2330 train_time:24660ms step_avg:60.29ms
step:410/2330 train_time:24721ms step_avg:60.29ms
step:411/2330 train_time:24780ms step_avg:60.29ms
step:412/2330 train_time:24842ms step_avg:60.30ms
step:413/2330 train_time:24901ms step_avg:60.29ms
step:414/2330 train_time:24962ms step_avg:60.29ms
step:415/2330 train_time:25020ms step_avg:60.29ms
step:416/2330 train_time:25081ms step_avg:60.29ms
step:417/2330 train_time:25141ms step_avg:60.29ms
step:418/2330 train_time:25202ms step_avg:60.29ms
step:419/2330 train_time:25261ms step_avg:60.29ms
step:420/2330 train_time:25322ms step_avg:60.29ms
step:421/2330 train_time:25381ms step_avg:60.29ms
step:422/2330 train_time:25442ms step_avg:60.29ms
step:423/2330 train_time:25500ms step_avg:60.28ms
step:424/2330 train_time:25561ms step_avg:60.29ms
step:425/2330 train_time:25619ms step_avg:60.28ms
step:426/2330 train_time:25680ms step_avg:60.28ms
step:427/2330 train_time:25740ms step_avg:60.28ms
step:428/2330 train_time:25801ms step_avg:60.28ms
step:429/2330 train_time:25860ms step_avg:60.28ms
step:430/2330 train_time:25921ms step_avg:60.28ms
step:431/2330 train_time:25980ms step_avg:60.28ms
step:432/2330 train_time:26041ms step_avg:60.28ms
step:433/2330 train_time:26099ms step_avg:60.28ms
step:434/2330 train_time:26160ms step_avg:60.28ms
step:435/2330 train_time:26219ms step_avg:60.27ms
step:436/2330 train_time:26280ms step_avg:60.28ms
step:437/2330 train_time:26340ms step_avg:60.27ms
step:438/2330 train_time:26401ms step_avg:60.28ms
step:439/2330 train_time:26460ms step_avg:60.27ms
step:440/2330 train_time:26521ms step_avg:60.28ms
step:441/2330 train_time:26580ms step_avg:60.27ms
step:442/2330 train_time:26641ms step_avg:60.27ms
step:443/2330 train_time:26700ms step_avg:60.27ms
step:444/2330 train_time:26761ms step_avg:60.27ms
step:445/2330 train_time:26819ms step_avg:60.27ms
step:446/2330 train_time:26880ms step_avg:60.27ms
step:447/2330 train_time:26939ms step_avg:60.27ms
step:448/2330 train_time:27000ms step_avg:60.27ms
step:449/2330 train_time:27060ms step_avg:60.27ms
step:450/2330 train_time:27120ms step_avg:60.27ms
step:451/2330 train_time:27179ms step_avg:60.26ms
step:452/2330 train_time:27241ms step_avg:60.27ms
step:453/2330 train_time:27300ms step_avg:60.26ms
step:454/2330 train_time:27361ms step_avg:60.27ms
step:455/2330 train_time:27419ms step_avg:60.26ms
step:456/2330 train_time:27481ms step_avg:60.26ms
step:457/2330 train_time:27540ms step_avg:60.26ms
step:458/2330 train_time:27601ms step_avg:60.26ms
step:459/2330 train_time:27660ms step_avg:60.26ms
step:460/2330 train_time:27721ms step_avg:60.26ms
step:461/2330 train_time:27780ms step_avg:60.26ms
step:462/2330 train_time:27841ms step_avg:60.26ms
step:463/2330 train_time:27900ms step_avg:60.26ms
step:464/2330 train_time:27961ms step_avg:60.26ms
step:465/2330 train_time:28020ms step_avg:60.26ms
step:466/2330 train_time:28080ms step_avg:60.26ms
step:467/2330 train_time:28139ms step_avg:60.26ms
step:468/2330 train_time:28200ms step_avg:60.26ms
step:469/2330 train_time:28259ms step_avg:60.25ms
step:470/2330 train_time:28320ms step_avg:60.26ms
step:471/2330 train_time:28379ms step_avg:60.25ms
step:472/2330 train_time:28441ms step_avg:60.26ms
step:473/2330 train_time:28500ms step_avg:60.25ms
step:474/2330 train_time:28561ms step_avg:60.25ms
step:475/2330 train_time:28619ms step_avg:60.25ms
step:476/2330 train_time:28680ms step_avg:60.25ms
step:477/2330 train_time:28740ms step_avg:60.25ms
step:478/2330 train_time:28801ms step_avg:60.25ms
step:479/2330 train_time:28859ms step_avg:60.25ms
step:480/2330 train_time:28920ms step_avg:60.25ms
step:481/2330 train_time:28979ms step_avg:60.25ms
step:482/2330 train_time:29040ms step_avg:60.25ms
step:483/2330 train_time:29100ms step_avg:60.25ms
step:484/2330 train_time:29160ms step_avg:60.25ms
step:485/2330 train_time:29219ms step_avg:60.25ms
step:486/2330 train_time:29280ms step_avg:60.25ms
step:487/2330 train_time:29339ms step_avg:60.24ms
step:488/2330 train_time:29400ms step_avg:60.25ms
step:489/2330 train_time:29459ms step_avg:60.24ms
step:490/2330 train_time:29520ms step_avg:60.24ms
step:491/2330 train_time:29579ms step_avg:60.24ms
step:492/2330 train_time:29640ms step_avg:60.24ms
step:493/2330 train_time:29700ms step_avg:60.24ms
step:494/2330 train_time:29761ms step_avg:60.24ms
step:495/2330 train_time:29820ms step_avg:60.24ms
step:496/2330 train_time:29881ms step_avg:60.24ms
step:497/2330 train_time:29940ms step_avg:60.24ms
step:498/2330 train_time:30001ms step_avg:60.24ms
step:499/2330 train_time:30059ms step_avg:60.24ms
step:500/2330 train_time:30120ms step_avg:60.24ms
step:500/2330 val_loss:3.8188 train_time:30184ms step_avg:60.37ms
step:501/2330 train_time:30207ms step_avg:60.29ms
step:502/2330 train_time:30243ms step_avg:60.24ms
step:503/2330 train_time:30307ms step_avg:60.25ms
step:504/2330 train_time:30374ms step_avg:60.27ms
step:505/2330 train_time:30434ms step_avg:60.27ms
step:506/2330 train_time:30495ms step_avg:60.27ms
step:507/2330 train_time:30554ms step_avg:60.27ms
step:508/2330 train_time:30615ms step_avg:60.27ms
step:509/2330 train_time:30674ms step_avg:60.26ms
step:510/2330 train_time:30734ms step_avg:60.26ms
step:511/2330 train_time:30792ms step_avg:60.26ms
step:512/2330 train_time:30852ms step_avg:60.26ms
step:513/2330 train_time:30911ms step_avg:60.25ms
step:514/2330 train_time:30971ms step_avg:60.26ms
step:515/2330 train_time:31029ms step_avg:60.25ms
step:516/2330 train_time:31090ms step_avg:60.25ms
step:517/2330 train_time:31150ms step_avg:60.25ms
step:518/2330 train_time:31212ms step_avg:60.26ms
step:519/2330 train_time:31274ms step_avg:60.26ms
step:520/2330 train_time:31336ms step_avg:60.26ms
step:521/2330 train_time:31396ms step_avg:60.26ms
step:522/2330 train_time:31457ms step_avg:60.26ms
step:523/2330 train_time:31517ms step_avg:60.26ms
step:524/2330 train_time:31579ms step_avg:60.26ms
step:525/2330 train_time:31638ms step_avg:60.26ms
step:526/2330 train_time:31700ms step_avg:60.27ms
step:527/2330 train_time:31759ms step_avg:60.26ms
step:528/2330 train_time:31820ms step_avg:60.27ms
step:529/2330 train_time:31879ms step_avg:60.26ms
step:530/2330 train_time:31940ms step_avg:60.26ms
step:531/2330 train_time:31999ms step_avg:60.26ms
step:532/2330 train_time:32060ms step_avg:60.26ms
step:533/2330 train_time:32119ms step_avg:60.26ms
step:534/2330 train_time:32181ms step_avg:60.26ms
step:535/2330 train_time:32241ms step_avg:60.26ms
step:536/2330 train_time:32303ms step_avg:60.27ms
step:537/2330 train_time:32363ms step_avg:60.27ms
step:538/2330 train_time:32426ms step_avg:60.27ms
step:539/2330 train_time:32486ms step_avg:60.27ms
step:540/2330 train_time:32547ms step_avg:60.27ms
step:541/2330 train_time:32606ms step_avg:60.27ms
step:542/2330 train_time:32668ms step_avg:60.27ms
step:543/2330 train_time:32727ms step_avg:60.27ms
step:544/2330 train_time:32789ms step_avg:60.27ms
step:545/2330 train_time:32849ms step_avg:60.27ms
step:546/2330 train_time:32911ms step_avg:60.28ms
step:547/2330 train_time:32970ms step_avg:60.27ms
step:548/2330 train_time:33031ms step_avg:60.28ms
step:549/2330 train_time:33090ms step_avg:60.27ms
step:550/2330 train_time:33151ms step_avg:60.27ms
step:551/2330 train_time:33211ms step_avg:60.27ms
step:552/2330 train_time:33272ms step_avg:60.28ms
step:553/2330 train_time:33332ms step_avg:60.27ms
step:554/2330 train_time:33393ms step_avg:60.28ms
step:555/2330 train_time:33452ms step_avg:60.27ms
step:556/2330 train_time:33514ms step_avg:60.28ms
step:557/2330 train_time:33573ms step_avg:60.27ms
step:558/2330 train_time:33634ms step_avg:60.28ms
step:559/2330 train_time:33693ms step_avg:60.27ms
step:560/2330 train_time:33755ms step_avg:60.28ms
step:561/2330 train_time:33813ms step_avg:60.27ms
step:562/2330 train_time:33875ms step_avg:60.28ms
step:563/2330 train_time:33934ms step_avg:60.27ms
step:564/2330 train_time:33995ms step_avg:60.27ms
step:565/2330 train_time:34054ms step_avg:60.27ms
step:566/2330 train_time:34115ms step_avg:60.27ms
step:567/2330 train_time:34174ms step_avg:60.27ms
step:568/2330 train_time:34235ms step_avg:60.27ms
step:569/2330 train_time:34295ms step_avg:60.27ms
step:570/2330 train_time:34356ms step_avg:60.27ms
step:571/2330 train_time:34416ms step_avg:60.27ms
step:572/2330 train_time:34477ms step_avg:60.27ms
step:573/2330 train_time:34536ms step_avg:60.27ms
step:574/2330 train_time:34597ms step_avg:60.27ms
step:575/2330 train_time:34656ms step_avg:60.27ms
step:576/2330 train_time:34717ms step_avg:60.27ms
step:577/2330 train_time:34777ms step_avg:60.27ms
step:578/2330 train_time:34838ms step_avg:60.27ms
step:579/2330 train_time:34897ms step_avg:60.27ms
step:580/2330 train_time:34959ms step_avg:60.27ms
step:581/2330 train_time:35018ms step_avg:60.27ms
step:582/2330 train_time:35081ms step_avg:60.28ms
step:583/2330 train_time:35140ms step_avg:60.27ms
step:584/2330 train_time:35201ms step_avg:60.28ms
step:585/2330 train_time:35260ms step_avg:60.27ms
step:586/2330 train_time:35322ms step_avg:60.28ms
step:587/2330 train_time:35382ms step_avg:60.28ms
step:588/2330 train_time:35443ms step_avg:60.28ms
step:589/2330 train_time:35504ms step_avg:60.28ms
step:590/2330 train_time:35565ms step_avg:60.28ms
step:591/2330 train_time:35625ms step_avg:60.28ms
step:592/2330 train_time:35687ms step_avg:60.28ms
step:593/2330 train_time:35746ms step_avg:60.28ms
step:594/2330 train_time:35807ms step_avg:60.28ms
step:595/2330 train_time:35867ms step_avg:60.28ms
step:596/2330 train_time:35929ms step_avg:60.28ms
step:597/2330 train_time:35988ms step_avg:60.28ms
step:598/2330 train_time:36050ms step_avg:60.28ms
step:599/2330 train_time:36110ms step_avg:60.28ms
step:600/2330 train_time:36172ms step_avg:60.29ms
step:601/2330 train_time:36230ms step_avg:60.28ms
step:602/2330 train_time:36292ms step_avg:60.29ms
step:603/2330 train_time:36352ms step_avg:60.28ms
step:604/2330 train_time:36413ms step_avg:60.29ms
step:605/2330 train_time:36473ms step_avg:60.29ms
step:606/2330 train_time:36534ms step_avg:60.29ms
step:607/2330 train_time:36593ms step_avg:60.29ms
step:608/2330 train_time:36654ms step_avg:60.29ms
step:609/2330 train_time:36714ms step_avg:60.29ms
step:610/2330 train_time:36775ms step_avg:60.29ms
step:611/2330 train_time:36834ms step_avg:60.28ms
step:612/2330 train_time:36894ms step_avg:60.28ms
step:613/2330 train_time:36953ms step_avg:60.28ms
step:614/2330 train_time:37014ms step_avg:60.28ms
step:615/2330 train_time:37073ms step_avg:60.28ms
step:616/2330 train_time:37134ms step_avg:60.28ms
step:617/2330 train_time:37193ms step_avg:60.28ms
step:618/2330 train_time:37255ms step_avg:60.28ms
step:619/2330 train_time:37314ms step_avg:60.28ms
step:620/2330 train_time:37375ms step_avg:60.28ms
step:621/2330 train_time:37434ms step_avg:60.28ms
step:622/2330 train_time:37495ms step_avg:60.28ms
step:623/2330 train_time:37555ms step_avg:60.28ms
step:624/2330 train_time:37616ms step_avg:60.28ms
step:625/2330 train_time:37675ms step_avg:60.28ms
step:626/2330 train_time:37736ms step_avg:60.28ms
step:627/2330 train_time:37795ms step_avg:60.28ms
step:628/2330 train_time:37856ms step_avg:60.28ms
step:629/2330 train_time:37915ms step_avg:60.28ms
step:630/2330 train_time:37976ms step_avg:60.28ms
step:631/2330 train_time:38035ms step_avg:60.28ms
step:632/2330 train_time:38096ms step_avg:60.28ms
step:633/2330 train_time:38155ms step_avg:60.28ms
step:634/2330 train_time:38217ms step_avg:60.28ms
step:635/2330 train_time:38275ms step_avg:60.28ms
step:636/2330 train_time:38336ms step_avg:60.28ms
step:637/2330 train_time:38395ms step_avg:60.28ms
step:638/2330 train_time:38457ms step_avg:60.28ms
step:639/2330 train_time:38516ms step_avg:60.28ms
step:640/2330 train_time:38578ms step_avg:60.28ms
step:641/2330 train_time:38637ms step_avg:60.28ms
step:642/2330 train_time:38698ms step_avg:60.28ms
step:643/2330 train_time:38758ms step_avg:60.28ms
step:644/2330 train_time:38820ms step_avg:60.28ms
step:645/2330 train_time:38880ms step_avg:60.28ms
step:646/2330 train_time:38942ms step_avg:60.28ms
step:647/2330 train_time:39001ms step_avg:60.28ms
step:648/2330 train_time:39062ms step_avg:60.28ms
step:649/2330 train_time:39122ms step_avg:60.28ms
step:650/2330 train_time:39183ms step_avg:60.28ms
step:651/2330 train_time:39243ms step_avg:60.28ms
step:652/2330 train_time:39305ms step_avg:60.28ms
step:653/2330 train_time:39365ms step_avg:60.28ms
step:654/2330 train_time:39427ms step_avg:60.29ms
step:655/2330 train_time:39487ms step_avg:60.28ms
step:656/2330 train_time:39548ms step_avg:60.29ms
step:657/2330 train_time:39608ms step_avg:60.29ms
step:658/2330 train_time:39670ms step_avg:60.29ms
step:659/2330 train_time:39730ms step_avg:60.29ms
step:660/2330 train_time:39791ms step_avg:60.29ms
step:661/2330 train_time:39851ms step_avg:60.29ms
step:662/2330 train_time:39912ms step_avg:60.29ms
step:663/2330 train_time:39972ms step_avg:60.29ms
step:664/2330 train_time:40033ms step_avg:60.29ms
step:665/2330 train_time:40093ms step_avg:60.29ms
step:666/2330 train_time:40154ms step_avg:60.29ms
step:667/2330 train_time:40213ms step_avg:60.29ms
step:668/2330 train_time:40274ms step_avg:60.29ms
step:669/2330 train_time:40334ms step_avg:60.29ms
step:670/2330 train_time:40396ms step_avg:60.29ms
step:671/2330 train_time:40455ms step_avg:60.29ms
step:672/2330 train_time:40516ms step_avg:60.29ms
step:673/2330 train_time:40574ms step_avg:60.29ms
step:674/2330 train_time:40635ms step_avg:60.29ms
step:675/2330 train_time:40694ms step_avg:60.29ms
step:676/2330 train_time:40756ms step_avg:60.29ms
step:677/2330 train_time:40815ms step_avg:60.29ms
step:678/2330 train_time:40877ms step_avg:60.29ms
step:679/2330 train_time:40936ms step_avg:60.29ms
step:680/2330 train_time:40997ms step_avg:60.29ms
step:681/2330 train_time:41057ms step_avg:60.29ms
step:682/2330 train_time:41118ms step_avg:60.29ms
step:683/2330 train_time:41178ms step_avg:60.29ms
step:684/2330 train_time:41239ms step_avg:60.29ms
step:685/2330 train_time:41298ms step_avg:60.29ms
step:686/2330 train_time:41360ms step_avg:60.29ms
step:687/2330 train_time:41420ms step_avg:60.29ms
step:688/2330 train_time:41482ms step_avg:60.29ms
step:689/2330 train_time:41541ms step_avg:60.29ms
step:690/2330 train_time:41602ms step_avg:60.29ms
step:691/2330 train_time:41661ms step_avg:60.29ms
step:692/2330 train_time:41722ms step_avg:60.29ms
step:693/2330 train_time:41781ms step_avg:60.29ms
step:694/2330 train_time:41843ms step_avg:60.29ms
step:695/2330 train_time:41903ms step_avg:60.29ms
step:696/2330 train_time:41965ms step_avg:60.29ms
step:697/2330 train_time:42025ms step_avg:60.29ms
step:698/2330 train_time:42086ms step_avg:60.30ms
step:699/2330 train_time:42146ms step_avg:60.29ms
step:700/2330 train_time:42207ms step_avg:60.30ms
step:701/2330 train_time:42266ms step_avg:60.29ms
step:702/2330 train_time:42328ms step_avg:60.30ms
step:703/2330 train_time:42387ms step_avg:60.29ms
step:704/2330 train_time:42448ms step_avg:60.30ms
step:705/2330 train_time:42508ms step_avg:60.29ms
step:706/2330 train_time:42569ms step_avg:60.30ms
step:707/2330 train_time:42628ms step_avg:60.29ms
step:708/2330 train_time:42690ms step_avg:60.30ms
step:709/2330 train_time:42749ms step_avg:60.29ms
step:710/2330 train_time:42810ms step_avg:60.30ms
step:711/2330 train_time:42870ms step_avg:60.29ms
step:712/2330 train_time:42931ms step_avg:60.30ms
step:713/2330 train_time:42990ms step_avg:60.30ms
step:714/2330 train_time:43052ms step_avg:60.30ms
step:715/2330 train_time:43111ms step_avg:60.29ms
step:716/2330 train_time:43172ms step_avg:60.30ms
step:717/2330 train_time:43231ms step_avg:60.29ms
step:718/2330 train_time:43292ms step_avg:60.29ms
step:719/2330 train_time:43351ms step_avg:60.29ms
step:720/2330 train_time:43412ms step_avg:60.29ms
step:721/2330 train_time:43472ms step_avg:60.29ms
step:722/2330 train_time:43533ms step_avg:60.29ms
step:723/2330 train_time:43591ms step_avg:60.29ms
step:724/2330 train_time:43652ms step_avg:60.29ms
step:725/2330 train_time:43712ms step_avg:60.29ms
step:726/2330 train_time:43773ms step_avg:60.29ms
step:727/2330 train_time:43832ms step_avg:60.29ms
step:728/2330 train_time:43893ms step_avg:60.29ms
step:729/2330 train_time:43953ms step_avg:60.29ms
step:730/2330 train_time:44014ms step_avg:60.29ms
step:731/2330 train_time:44073ms step_avg:60.29ms
step:732/2330 train_time:44134ms step_avg:60.29ms
step:733/2330 train_time:44193ms step_avg:60.29ms
step:734/2330 train_time:44255ms step_avg:60.29ms
step:735/2330 train_time:44314ms step_avg:60.29ms
step:736/2330 train_time:44376ms step_avg:60.29ms
step:737/2330 train_time:44434ms step_avg:60.29ms
step:738/2330 train_time:44495ms step_avg:60.29ms
step:739/2330 train_time:44554ms step_avg:60.29ms
step:740/2330 train_time:44615ms step_avg:60.29ms
step:741/2330 train_time:44675ms step_avg:60.29ms
step:742/2330 train_time:44735ms step_avg:60.29ms
step:743/2330 train_time:44795ms step_avg:60.29ms
step:744/2330 train_time:44856ms step_avg:60.29ms
step:745/2330 train_time:44915ms step_avg:60.29ms
step:746/2330 train_time:44976ms step_avg:60.29ms
step:747/2330 train_time:45036ms step_avg:60.29ms
step:748/2330 train_time:45097ms step_avg:60.29ms
step:749/2330 train_time:45156ms step_avg:60.29ms
step:750/2330 train_time:45217ms step_avg:60.29ms
step:750/2330 val_loss:3.6872 train_time:45281ms step_avg:60.37ms
step:751/2330 train_time:45305ms step_avg:60.33ms
step:752/2330 train_time:45341ms step_avg:60.29ms
step:753/2330 train_time:45407ms step_avg:60.30ms
step:754/2330 train_time:45470ms step_avg:60.31ms
step:755/2330 train_time:45530ms step_avg:60.30ms
step:756/2330 train_time:45592ms step_avg:60.31ms
step:757/2330 train_time:45651ms step_avg:60.31ms
step:758/2330 train_time:45712ms step_avg:60.31ms
step:759/2330 train_time:45770ms step_avg:60.30ms
step:760/2330 train_time:45830ms step_avg:60.30ms
step:761/2330 train_time:45889ms step_avg:60.30ms
step:762/2330 train_time:45949ms step_avg:60.30ms
step:763/2330 train_time:46007ms step_avg:60.30ms
step:764/2330 train_time:46067ms step_avg:60.30ms
step:765/2330 train_time:46126ms step_avg:60.29ms
step:766/2330 train_time:46187ms step_avg:60.30ms
step:767/2330 train_time:46247ms step_avg:60.30ms
step:768/2330 train_time:46310ms step_avg:60.30ms
step:769/2330 train_time:46371ms step_avg:60.30ms
step:770/2330 train_time:46434ms step_avg:60.30ms
step:771/2330 train_time:46495ms step_avg:60.30ms
step:772/2330 train_time:46557ms step_avg:60.31ms
step:773/2330 train_time:46617ms step_avg:60.31ms
step:774/2330 train_time:46679ms step_avg:60.31ms
step:775/2330 train_time:46740ms step_avg:60.31ms
step:776/2330 train_time:46802ms step_avg:60.31ms
step:777/2330 train_time:46861ms step_avg:60.31ms
step:778/2330 train_time:46923ms step_avg:60.31ms
step:779/2330 train_time:46983ms step_avg:60.31ms
step:780/2330 train_time:47045ms step_avg:60.31ms
step:781/2330 train_time:47104ms step_avg:60.31ms
step:782/2330 train_time:47165ms step_avg:60.31ms
step:783/2330 train_time:47225ms step_avg:60.31ms
step:784/2330 train_time:47287ms step_avg:60.32ms
step:785/2330 train_time:47348ms step_avg:60.32ms
step:786/2330 train_time:47410ms step_avg:60.32ms
step:787/2330 train_time:47470ms step_avg:60.32ms
step:788/2330 train_time:47532ms step_avg:60.32ms
step:789/2330 train_time:47592ms step_avg:60.32ms
step:790/2330 train_time:47655ms step_avg:60.32ms
step:791/2330 train_time:47715ms step_avg:60.32ms
step:792/2330 train_time:47777ms step_avg:60.32ms
step:793/2330 train_time:47837ms step_avg:60.32ms
step:794/2330 train_time:47899ms step_avg:60.33ms
step:795/2330 train_time:47959ms step_avg:60.33ms
step:796/2330 train_time:48021ms step_avg:60.33ms
step:797/2330 train_time:48081ms step_avg:60.33ms
step:798/2330 train_time:48143ms step_avg:60.33ms
step:799/2330 train_time:48203ms step_avg:60.33ms
step:800/2330 train_time:48265ms step_avg:60.33ms
step:801/2330 train_time:48326ms step_avg:60.33ms
step:802/2330 train_time:48388ms step_avg:60.33ms
step:803/2330 train_time:48448ms step_avg:60.33ms
step:804/2330 train_time:48510ms step_avg:60.34ms
step:805/2330 train_time:48570ms step_avg:60.34ms
step:806/2330 train_time:48632ms step_avg:60.34ms
step:807/2330 train_time:48693ms step_avg:60.34ms
step:808/2330 train_time:48755ms step_avg:60.34ms
step:809/2330 train_time:48814ms step_avg:60.34ms
step:810/2330 train_time:48877ms step_avg:60.34ms
step:811/2330 train_time:48936ms step_avg:60.34ms
step:812/2330 train_time:48998ms step_avg:60.34ms
step:813/2330 train_time:49058ms step_avg:60.34ms
step:814/2330 train_time:49121ms step_avg:60.34ms
step:815/2330 train_time:49182ms step_avg:60.35ms
step:816/2330 train_time:49245ms step_avg:60.35ms
step:817/2330 train_time:49305ms step_avg:60.35ms
step:818/2330 train_time:49367ms step_avg:60.35ms
step:819/2330 train_time:49428ms step_avg:60.35ms
step:820/2330 train_time:49489ms step_avg:60.35ms
step:821/2330 train_time:49549ms step_avg:60.35ms
step:822/2330 train_time:49611ms step_avg:60.35ms
step:823/2330 train_time:49671ms step_avg:60.35ms
step:824/2330 train_time:49733ms step_avg:60.36ms
step:825/2330 train_time:49793ms step_avg:60.35ms
step:826/2330 train_time:49855ms step_avg:60.36ms
step:827/2330 train_time:49915ms step_avg:60.36ms
step:828/2330 train_time:49976ms step_avg:60.36ms
step:829/2330 train_time:50037ms step_avg:60.36ms
step:830/2330 train_time:50099ms step_avg:60.36ms
step:831/2330 train_time:50159ms step_avg:60.36ms
step:832/2330 train_time:50221ms step_avg:60.36ms
step:833/2330 train_time:50282ms step_avg:60.36ms
step:834/2330 train_time:50344ms step_avg:60.37ms
step:835/2330 train_time:50405ms step_avg:60.37ms
step:836/2330 train_time:50467ms step_avg:60.37ms
step:837/2330 train_time:50527ms step_avg:60.37ms
step:838/2330 train_time:50589ms step_avg:60.37ms
step:839/2330 train_time:50650ms step_avg:60.37ms
step:840/2330 train_time:50712ms step_avg:60.37ms
step:841/2330 train_time:50771ms step_avg:60.37ms
step:842/2330 train_time:50832ms step_avg:60.37ms
step:843/2330 train_time:50892ms step_avg:60.37ms
step:844/2330 train_time:50954ms step_avg:60.37ms
step:845/2330 train_time:51014ms step_avg:60.37ms
step:846/2330 train_time:51077ms step_avg:60.37ms
step:847/2330 train_time:51138ms step_avg:60.38ms
step:848/2330 train_time:51200ms step_avg:60.38ms
step:849/2330 train_time:51261ms step_avg:60.38ms
step:850/2330 train_time:51323ms step_avg:60.38ms
step:851/2330 train_time:51384ms step_avg:60.38ms
step:852/2330 train_time:51446ms step_avg:60.38ms
step:853/2330 train_time:51506ms step_avg:60.38ms
step:854/2330 train_time:51568ms step_avg:60.38ms
step:855/2330 train_time:51628ms step_avg:60.38ms
step:856/2330 train_time:51690ms step_avg:60.39ms
step:857/2330 train_time:51750ms step_avg:60.38ms
step:858/2330 train_time:51812ms step_avg:60.39ms
step:859/2330 train_time:51871ms step_avg:60.39ms
step:860/2330 train_time:51933ms step_avg:60.39ms
step:861/2330 train_time:51993ms step_avg:60.39ms
step:862/2330 train_time:52055ms step_avg:60.39ms
step:863/2330 train_time:52116ms step_avg:60.39ms
step:864/2330 train_time:52178ms step_avg:60.39ms
step:865/2330 train_time:52238ms step_avg:60.39ms
step:866/2330 train_time:52301ms step_avg:60.39ms
step:867/2330 train_time:52361ms step_avg:60.39ms
step:868/2330 train_time:52423ms step_avg:60.40ms
step:869/2330 train_time:52484ms step_avg:60.40ms
step:870/2330 train_time:52546ms step_avg:60.40ms
step:871/2330 train_time:52606ms step_avg:60.40ms
step:872/2330 train_time:52668ms step_avg:60.40ms
step:873/2330 train_time:52729ms step_avg:60.40ms
step:874/2330 train_time:52791ms step_avg:60.40ms
step:875/2330 train_time:52851ms step_avg:60.40ms
step:876/2330 train_time:52913ms step_avg:60.40ms
step:877/2330 train_time:52972ms step_avg:60.40ms
step:878/2330 train_time:53034ms step_avg:60.40ms
step:879/2330 train_time:53094ms step_avg:60.40ms
step:880/2330 train_time:53156ms step_avg:60.40ms
step:881/2330 train_time:53216ms step_avg:60.40ms
step:882/2330 train_time:53279ms step_avg:60.41ms
step:883/2330 train_time:53339ms step_avg:60.41ms
step:884/2330 train_time:53402ms step_avg:60.41ms
step:885/2330 train_time:53462ms step_avg:60.41ms
step:886/2330 train_time:53524ms step_avg:60.41ms
step:887/2330 train_time:53585ms step_avg:60.41ms
step:888/2330 train_time:53647ms step_avg:60.41ms
step:889/2330 train_time:53707ms step_avg:60.41ms
step:890/2330 train_time:53769ms step_avg:60.41ms
step:891/2330 train_time:53829ms step_avg:60.41ms
step:892/2330 train_time:53892ms step_avg:60.42ms
step:893/2330 train_time:53951ms step_avg:60.42ms
step:894/2330 train_time:54013ms step_avg:60.42ms
step:895/2330 train_time:54072ms step_avg:60.42ms
step:896/2330 train_time:54134ms step_avg:60.42ms
step:897/2330 train_time:54194ms step_avg:60.42ms
step:898/2330 train_time:54257ms step_avg:60.42ms
step:899/2330 train_time:54318ms step_avg:60.42ms
step:900/2330 train_time:54380ms step_avg:60.42ms
step:901/2330 train_time:54440ms step_avg:60.42ms
step:902/2330 train_time:54503ms step_avg:60.42ms
step:903/2330 train_time:54563ms step_avg:60.42ms
step:904/2330 train_time:54626ms step_avg:60.43ms
step:905/2330 train_time:54686ms step_avg:60.43ms
step:906/2330 train_time:54747ms step_avg:60.43ms
step:907/2330 train_time:54808ms step_avg:60.43ms
step:908/2330 train_time:54870ms step_avg:60.43ms
step:909/2330 train_time:54930ms step_avg:60.43ms
step:910/2330 train_time:54991ms step_avg:60.43ms
step:911/2330 train_time:55051ms step_avg:60.43ms
step:912/2330 train_time:55113ms step_avg:60.43ms
step:913/2330 train_time:55173ms step_avg:60.43ms
step:914/2330 train_time:55235ms step_avg:60.43ms
step:915/2330 train_time:55295ms step_avg:60.43ms
step:916/2330 train_time:55358ms step_avg:60.43ms
step:917/2330 train_time:55419ms step_avg:60.44ms
step:918/2330 train_time:55481ms step_avg:60.44ms
step:919/2330 train_time:55542ms step_avg:60.44ms
step:920/2330 train_time:55604ms step_avg:60.44ms
step:921/2330 train_time:55665ms step_avg:60.44ms
step:922/2330 train_time:55727ms step_avg:60.44ms
step:923/2330 train_time:55786ms step_avg:60.44ms
step:924/2330 train_time:55848ms step_avg:60.44ms
step:925/2330 train_time:55908ms step_avg:60.44ms
step:926/2330 train_time:55969ms step_avg:60.44ms
step:927/2330 train_time:56029ms step_avg:60.44ms
step:928/2330 train_time:56091ms step_avg:60.44ms
step:929/2330 train_time:56151ms step_avg:60.44ms
step:930/2330 train_time:56212ms step_avg:60.44ms
step:931/2330 train_time:56272ms step_avg:60.44ms
step:932/2330 train_time:56334ms step_avg:60.44ms
step:933/2330 train_time:56394ms step_avg:60.44ms
step:934/2330 train_time:56457ms step_avg:60.45ms
step:935/2330 train_time:56518ms step_avg:60.45ms
step:936/2330 train_time:56580ms step_avg:60.45ms
step:937/2330 train_time:56640ms step_avg:60.45ms
step:938/2330 train_time:56702ms step_avg:60.45ms
step:939/2330 train_time:56762ms step_avg:60.45ms
step:940/2330 train_time:56824ms step_avg:60.45ms
step:941/2330 train_time:56884ms step_avg:60.45ms
step:942/2330 train_time:56946ms step_avg:60.45ms
step:943/2330 train_time:57006ms step_avg:60.45ms
step:944/2330 train_time:57068ms step_avg:60.45ms
step:945/2330 train_time:57128ms step_avg:60.45ms
step:946/2330 train_time:57189ms step_avg:60.45ms
step:947/2330 train_time:57249ms step_avg:60.45ms
step:948/2330 train_time:57311ms step_avg:60.45ms
step:949/2330 train_time:57371ms step_avg:60.45ms
step:950/2330 train_time:57433ms step_avg:60.46ms
step:951/2330 train_time:57493ms step_avg:60.46ms
step:952/2330 train_time:57556ms step_avg:60.46ms
step:953/2330 train_time:57616ms step_avg:60.46ms
step:954/2330 train_time:57678ms step_avg:60.46ms
step:955/2330 train_time:57738ms step_avg:60.46ms
step:956/2330 train_time:57801ms step_avg:60.46ms
step:957/2330 train_time:57862ms step_avg:60.46ms
step:958/2330 train_time:57925ms step_avg:60.46ms
step:959/2330 train_time:57985ms step_avg:60.46ms
step:960/2330 train_time:58047ms step_avg:60.47ms
step:961/2330 train_time:58106ms step_avg:60.46ms
step:962/2330 train_time:58168ms step_avg:60.47ms
step:963/2330 train_time:58227ms step_avg:60.46ms
step:964/2330 train_time:58290ms step_avg:60.47ms
step:965/2330 train_time:58349ms step_avg:60.47ms
step:966/2330 train_time:58411ms step_avg:60.47ms
step:967/2330 train_time:58471ms step_avg:60.47ms
step:968/2330 train_time:58533ms step_avg:60.47ms
step:969/2330 train_time:58593ms step_avg:60.47ms
step:970/2330 train_time:58655ms step_avg:60.47ms
step:971/2330 train_time:58716ms step_avg:60.47ms
step:972/2330 train_time:58779ms step_avg:60.47ms
step:973/2330 train_time:58839ms step_avg:60.47ms
step:974/2330 train_time:58902ms step_avg:60.47ms
step:975/2330 train_time:58963ms step_avg:60.47ms
step:976/2330 train_time:59025ms step_avg:60.48ms
step:977/2330 train_time:59084ms step_avg:60.47ms
step:978/2330 train_time:59146ms step_avg:60.48ms
step:979/2330 train_time:59206ms step_avg:60.48ms
step:980/2330 train_time:59267ms step_avg:60.48ms
step:981/2330 train_time:59327ms step_avg:60.48ms
step:982/2330 train_time:59389ms step_avg:60.48ms
step:983/2330 train_time:59448ms step_avg:60.48ms
step:984/2330 train_time:59510ms step_avg:60.48ms
step:985/2330 train_time:59570ms step_avg:60.48ms
step:986/2330 train_time:59632ms step_avg:60.48ms
step:987/2330 train_time:59692ms step_avg:60.48ms
step:988/2330 train_time:59755ms step_avg:60.48ms
step:989/2330 train_time:59815ms step_avg:60.48ms
step:990/2330 train_time:59878ms step_avg:60.48ms
step:991/2330 train_time:59938ms step_avg:60.48ms
step:992/2330 train_time:60001ms step_avg:60.48ms
step:993/2330 train_time:60061ms step_avg:60.48ms
step:994/2330 train_time:60124ms step_avg:60.49ms
step:995/2330 train_time:60184ms step_avg:60.49ms
step:996/2330 train_time:60246ms step_avg:60.49ms
step:997/2330 train_time:60306ms step_avg:60.49ms
step:998/2330 train_time:60368ms step_avg:60.49ms
step:999/2330 train_time:60428ms step_avg:60.49ms
step:1000/2330 train_time:60490ms step_avg:60.49ms
step:1000/2330 val_loss:3.5739 train_time:60553ms step_avg:60.55ms
step:1001/2330 train_time:60577ms step_avg:60.52ms
step:1002/2330 train_time:60613ms step_avg:60.49ms
step:1003/2330 train_time:60676ms step_avg:60.49ms
step:1004/2330 train_time:60740ms step_avg:60.50ms
step:1005/2330 train_time:60800ms step_avg:60.50ms
step:1006/2330 train_time:60862ms step_avg:60.50ms
step:1007/2330 train_time:60921ms step_avg:60.50ms
step:1008/2330 train_time:60983ms step_avg:60.50ms
step:1009/2330 train_time:61043ms step_avg:60.50ms
step:1010/2330 train_time:61104ms step_avg:60.50ms
step:1011/2330 train_time:61163ms step_avg:60.50ms
step:1012/2330 train_time:61224ms step_avg:60.50ms
step:1013/2330 train_time:61283ms step_avg:60.50ms
step:1014/2330 train_time:61344ms step_avg:60.50ms
step:1015/2330 train_time:61403ms step_avg:60.50ms
step:1016/2330 train_time:61468ms step_avg:60.50ms
step:1017/2330 train_time:61532ms step_avg:60.50ms
step:1018/2330 train_time:61594ms step_avg:60.50ms
step:1019/2330 train_time:61655ms step_avg:60.51ms
step:1020/2330 train_time:61717ms step_avg:60.51ms
step:1021/2330 train_time:61778ms step_avg:60.51ms
step:1022/2330 train_time:61840ms step_avg:60.51ms
step:1023/2330 train_time:61899ms step_avg:60.51ms
step:1024/2330 train_time:61962ms step_avg:60.51ms
step:1025/2330 train_time:62021ms step_avg:60.51ms
step:1026/2330 train_time:62083ms step_avg:60.51ms
step:1027/2330 train_time:62142ms step_avg:60.51ms
step:1028/2330 train_time:62204ms step_avg:60.51ms
step:1029/2330 train_time:62263ms step_avg:60.51ms
step:1030/2330 train_time:62324ms step_avg:60.51ms
step:1031/2330 train_time:62385ms step_avg:60.51ms
step:1032/2330 train_time:62448ms step_avg:60.51ms
step:1033/2330 train_time:62508ms step_avg:60.51ms
step:1034/2330 train_time:62570ms step_avg:60.51ms
step:1035/2330 train_time:62632ms step_avg:60.51ms
step:1036/2330 train_time:62694ms step_avg:60.52ms
step:1037/2330 train_time:62754ms step_avg:60.52ms
step:1038/2330 train_time:62817ms step_avg:60.52ms
step:1039/2330 train_time:62877ms step_avg:60.52ms
step:1040/2330 train_time:62940ms step_avg:60.52ms
step:1041/2330 train_time:63000ms step_avg:60.52ms
step:1042/2330 train_time:63061ms step_avg:60.52ms
step:1043/2330 train_time:63121ms step_avg:60.52ms
step:1044/2330 train_time:63183ms step_avg:60.52ms
step:1045/2330 train_time:63242ms step_avg:60.52ms
step:1046/2330 train_time:63304ms step_avg:60.52ms
step:1047/2330 train_time:63365ms step_avg:60.52ms
step:1048/2330 train_time:63427ms step_avg:60.52ms
step:1049/2330 train_time:63488ms step_avg:60.52ms
step:1050/2330 train_time:63550ms step_avg:60.52ms
step:1051/2330 train_time:63611ms step_avg:60.52ms
step:1052/2330 train_time:63673ms step_avg:60.53ms
step:1053/2330 train_time:63733ms step_avg:60.52ms
step:1054/2330 train_time:63795ms step_avg:60.53ms
step:1055/2330 train_time:63856ms step_avg:60.53ms
step:1056/2330 train_time:63918ms step_avg:60.53ms
step:1057/2330 train_time:63978ms step_avg:60.53ms
step:1058/2330 train_time:64039ms step_avg:60.53ms
step:1059/2330 train_time:64099ms step_avg:60.53ms
step:1060/2330 train_time:64162ms step_avg:60.53ms
step:1061/2330 train_time:64222ms step_avg:60.53ms
step:1062/2330 train_time:64283ms step_avg:60.53ms
step:1063/2330 train_time:64343ms step_avg:60.53ms
step:1064/2330 train_time:64405ms step_avg:60.53ms
step:1065/2330 train_time:64466ms step_avg:60.53ms
step:1066/2330 train_time:64529ms step_avg:60.53ms
step:1067/2330 train_time:64589ms step_avg:60.53ms
step:1068/2330 train_time:64651ms step_avg:60.53ms
step:1069/2330 train_time:64711ms step_avg:60.53ms
step:1070/2330 train_time:64774ms step_avg:60.54ms
step:1071/2330 train_time:64834ms step_avg:60.54ms
step:1072/2330 train_time:64896ms step_avg:60.54ms
step:1073/2330 train_time:64957ms step_avg:60.54ms
step:1074/2330 train_time:65020ms step_avg:60.54ms
step:1075/2330 train_time:65079ms step_avg:60.54ms
step:1076/2330 train_time:65141ms step_avg:60.54ms
step:1077/2330 train_time:65201ms step_avg:60.54ms
step:1078/2330 train_time:65263ms step_avg:60.54ms
step:1079/2330 train_time:65323ms step_avg:60.54ms
step:1080/2330 train_time:65385ms step_avg:60.54ms
step:1081/2330 train_time:65445ms step_avg:60.54ms
step:1082/2330 train_time:65507ms step_avg:60.54ms
step:1083/2330 train_time:65567ms step_avg:60.54ms
step:1084/2330 train_time:65629ms step_avg:60.54ms
step:1085/2330 train_time:65689ms step_avg:60.54ms
step:1086/2330 train_time:65751ms step_avg:60.54ms
step:1087/2330 train_time:65811ms step_avg:60.54ms
step:1088/2330 train_time:65873ms step_avg:60.55ms
step:1089/2330 train_time:65933ms step_avg:60.54ms
step:1090/2330 train_time:65996ms step_avg:60.55ms
step:1091/2330 train_time:66057ms step_avg:60.55ms
step:1092/2330 train_time:66120ms step_avg:60.55ms
step:1093/2330 train_time:66180ms step_avg:60.55ms
step:1094/2330 train_time:66242ms step_avg:60.55ms
step:1095/2330 train_time:66302ms step_avg:60.55ms
step:1096/2330 train_time:66364ms step_avg:60.55ms
step:1097/2330 train_time:66424ms step_avg:60.55ms
step:1098/2330 train_time:66486ms step_avg:60.55ms
step:1099/2330 train_time:66546ms step_avg:60.55ms
step:1100/2330 train_time:66609ms step_avg:60.55ms
step:1101/2330 train_time:66669ms step_avg:60.55ms
step:1102/2330 train_time:66731ms step_avg:60.55ms
step:1103/2330 train_time:66791ms step_avg:60.55ms
step:1104/2330 train_time:66853ms step_avg:60.56ms
step:1105/2330 train_time:66913ms step_avg:60.56ms
step:1106/2330 train_time:66976ms step_avg:60.56ms
step:1107/2330 train_time:67037ms step_avg:60.56ms
step:1108/2330 train_time:67100ms step_avg:60.56ms
step:1109/2330 train_time:67160ms step_avg:60.56ms
step:1110/2330 train_time:67223ms step_avg:60.56ms
step:1111/2330 train_time:67283ms step_avg:60.56ms
step:1112/2330 train_time:67345ms step_avg:60.56ms
step:1113/2330 train_time:67405ms step_avg:60.56ms
step:1114/2330 train_time:67467ms step_avg:60.56ms
step:1115/2330 train_time:67527ms step_avg:60.56ms
step:1116/2330 train_time:67589ms step_avg:60.56ms
step:1117/2330 train_time:67649ms step_avg:60.56ms
step:1118/2330 train_time:67711ms step_avg:60.56ms
step:1119/2330 train_time:67771ms step_avg:60.56ms
step:1120/2330 train_time:67834ms step_avg:60.57ms
step:1121/2330 train_time:67893ms step_avg:60.56ms
step:1122/2330 train_time:67955ms step_avg:60.57ms
step:1123/2330 train_time:68016ms step_avg:60.57ms
step:1124/2330 train_time:68079ms step_avg:60.57ms
step:1125/2330 train_time:68139ms step_avg:60.57ms
step:1126/2330 train_time:68202ms step_avg:60.57ms
step:1127/2330 train_time:68263ms step_avg:60.57ms
step:1128/2330 train_time:68325ms step_avg:60.57ms
step:1129/2330 train_time:68385ms step_avg:60.57ms
step:1130/2330 train_time:68447ms step_avg:60.57ms
step:1131/2330 train_time:68508ms step_avg:60.57ms
step:1132/2330 train_time:68569ms step_avg:60.57ms
step:1133/2330 train_time:68629ms step_avg:60.57ms
step:1134/2330 train_time:68691ms step_avg:60.57ms
step:1135/2330 train_time:68751ms step_avg:60.57ms
step:1136/2330 train_time:68813ms step_avg:60.58ms
step:1137/2330 train_time:68874ms step_avg:60.57ms
step:1138/2330 train_time:68936ms step_avg:60.58ms
step:1139/2330 train_time:68995ms step_avg:60.58ms
step:1140/2330 train_time:69058ms step_avg:60.58ms
step:1141/2330 train_time:69119ms step_avg:60.58ms
step:1142/2330 train_time:69181ms step_avg:60.58ms
step:1143/2330 train_time:69241ms step_avg:60.58ms
step:1144/2330 train_time:69303ms step_avg:60.58ms
step:1145/2330 train_time:69363ms step_avg:60.58ms
step:1146/2330 train_time:69425ms step_avg:60.58ms
step:1147/2330 train_time:69485ms step_avg:60.58ms
step:1148/2330 train_time:69547ms step_avg:60.58ms
step:1149/2330 train_time:69607ms step_avg:60.58ms
step:1150/2330 train_time:69669ms step_avg:60.58ms
step:1151/2330 train_time:69729ms step_avg:60.58ms
step:1152/2330 train_time:69791ms step_avg:60.58ms
step:1153/2330 train_time:69851ms step_avg:60.58ms
step:1154/2330 train_time:69913ms step_avg:60.58ms
step:1155/2330 train_time:69973ms step_avg:60.58ms
step:1156/2330 train_time:70035ms step_avg:60.58ms
step:1157/2330 train_time:70096ms step_avg:60.58ms
step:1158/2330 train_time:70159ms step_avg:60.59ms
step:1159/2330 train_time:70220ms step_avg:60.59ms
step:1160/2330 train_time:70282ms step_avg:60.59ms
step:1161/2330 train_time:70342ms step_avg:60.59ms
step:1162/2330 train_time:70403ms step_avg:60.59ms
step:1163/2330 train_time:70463ms step_avg:60.59ms
step:1164/2330 train_time:70525ms step_avg:60.59ms
step:1165/2330 train_time:70585ms step_avg:60.59ms
step:1166/2330 train_time:70647ms step_avg:60.59ms
step:1167/2330 train_time:70707ms step_avg:60.59ms
step:1168/2330 train_time:70769ms step_avg:60.59ms
step:1169/2330 train_time:70829ms step_avg:60.59ms
step:1170/2330 train_time:70891ms step_avg:60.59ms
step:1171/2330 train_time:70951ms step_avg:60.59ms
step:1172/2330 train_time:71013ms step_avg:60.59ms
step:1173/2330 train_time:71073ms step_avg:60.59ms
step:1174/2330 train_time:71136ms step_avg:60.59ms
step:1175/2330 train_time:71197ms step_avg:60.59ms
step:1176/2330 train_time:71259ms step_avg:60.59ms
step:1177/2330 train_time:71319ms step_avg:60.59ms
step:1178/2330 train_time:71380ms step_avg:60.59ms
step:1179/2330 train_time:71440ms step_avg:60.59ms
step:1180/2330 train_time:71503ms step_avg:60.60ms
step:1181/2330 train_time:71563ms step_avg:60.60ms
step:1182/2330 train_time:71626ms step_avg:60.60ms
step:1183/2330 train_time:71686ms step_avg:60.60ms
step:1184/2330 train_time:71748ms step_avg:60.60ms
step:1185/2330 train_time:71808ms step_avg:60.60ms
step:1186/2330 train_time:71870ms step_avg:60.60ms
step:1187/2330 train_time:71930ms step_avg:60.60ms
step:1188/2330 train_time:71991ms step_avg:60.60ms
step:1189/2330 train_time:72052ms step_avg:60.60ms
step:1190/2330 train_time:72115ms step_avg:60.60ms
step:1191/2330 train_time:72176ms step_avg:60.60ms
step:1192/2330 train_time:72238ms step_avg:60.60ms
step:1193/2330 train_time:72298ms step_avg:60.60ms
step:1194/2330 train_time:72360ms step_avg:60.60ms
step:1195/2330 train_time:72421ms step_avg:60.60ms
step:1196/2330 train_time:72483ms step_avg:60.60ms
step:1197/2330 train_time:72543ms step_avg:60.60ms
step:1198/2330 train_time:72605ms step_avg:60.61ms
step:1199/2330 train_time:72665ms step_avg:60.60ms
step:1200/2330 train_time:72727ms step_avg:60.61ms
step:1201/2330 train_time:72788ms step_avg:60.61ms
step:1202/2330 train_time:72850ms step_avg:60.61ms
step:1203/2330 train_time:72910ms step_avg:60.61ms
step:1204/2330 train_time:72971ms step_avg:60.61ms
step:1205/2330 train_time:73031ms step_avg:60.61ms
step:1206/2330 train_time:73094ms step_avg:60.61ms
step:1207/2330 train_time:73155ms step_avg:60.61ms
step:1208/2330 train_time:73218ms step_avg:60.61ms
step:1209/2330 train_time:73279ms step_avg:60.61ms
step:1210/2330 train_time:73341ms step_avg:60.61ms
step:1211/2330 train_time:73401ms step_avg:60.61ms
step:1212/2330 train_time:73463ms step_avg:60.61ms
step:1213/2330 train_time:73523ms step_avg:60.61ms
step:1214/2330 train_time:73585ms step_avg:60.61ms
step:1215/2330 train_time:73645ms step_avg:60.61ms
step:1216/2330 train_time:73707ms step_avg:60.61ms
step:1217/2330 train_time:73767ms step_avg:60.61ms
step:1218/2330 train_time:73829ms step_avg:60.61ms
step:1219/2330 train_time:73889ms step_avg:60.61ms
step:1220/2330 train_time:73950ms step_avg:60.61ms
step:1221/2330 train_time:74010ms step_avg:60.61ms
step:1222/2330 train_time:74072ms step_avg:60.62ms
step:1223/2330 train_time:74132ms step_avg:60.62ms
step:1224/2330 train_time:74195ms step_avg:60.62ms
step:1225/2330 train_time:74256ms step_avg:60.62ms
step:1226/2330 train_time:74319ms step_avg:60.62ms
step:1227/2330 train_time:74379ms step_avg:60.62ms
step:1228/2330 train_time:74442ms step_avg:60.62ms
step:1229/2330 train_time:74503ms step_avg:60.62ms
step:1230/2330 train_time:74565ms step_avg:60.62ms
step:1231/2330 train_time:74625ms step_avg:60.62ms
step:1232/2330 train_time:74686ms step_avg:60.62ms
step:1233/2330 train_time:74747ms step_avg:60.62ms
step:1234/2330 train_time:74809ms step_avg:60.62ms
step:1235/2330 train_time:74869ms step_avg:60.62ms
step:1236/2330 train_time:74931ms step_avg:60.62ms
step:1237/2330 train_time:74991ms step_avg:60.62ms
step:1238/2330 train_time:75052ms step_avg:60.62ms
step:1239/2330 train_time:75112ms step_avg:60.62ms
step:1240/2330 train_time:75175ms step_avg:60.63ms
step:1241/2330 train_time:75236ms step_avg:60.63ms
step:1242/2330 train_time:75298ms step_avg:60.63ms
step:1243/2330 train_time:75359ms step_avg:60.63ms
step:1244/2330 train_time:75421ms step_avg:60.63ms
step:1245/2330 train_time:75481ms step_avg:60.63ms
step:1246/2330 train_time:75544ms step_avg:60.63ms
step:1247/2330 train_time:75604ms step_avg:60.63ms
step:1248/2330 train_time:75666ms step_avg:60.63ms
step:1249/2330 train_time:75726ms step_avg:60.63ms
step:1250/2330 train_time:75788ms step_avg:60.63ms
step:1250/2330 val_loss:3.5136 train_time:75852ms step_avg:60.68ms
step:1251/2330 train_time:75876ms step_avg:60.65ms
step:1252/2330 train_time:75913ms step_avg:60.63ms
step:1253/2330 train_time:75978ms step_avg:60.64ms
step:1254/2330 train_time:76044ms step_avg:60.64ms
step:1255/2330 train_time:76105ms step_avg:60.64ms
step:1256/2330 train_time:76167ms step_avg:60.64ms
step:1257/2330 train_time:76226ms step_avg:60.64ms
step:1258/2330 train_time:76287ms step_avg:60.64ms
step:1259/2330 train_time:76347ms step_avg:60.64ms
step:1260/2330 train_time:76407ms step_avg:60.64ms
step:1261/2330 train_time:76467ms step_avg:60.64ms
step:1262/2330 train_time:76528ms step_avg:60.64ms
step:1263/2330 train_time:76587ms step_avg:60.64ms
step:1264/2330 train_time:76648ms step_avg:60.64ms
step:1265/2330 train_time:76707ms step_avg:60.64ms
step:1266/2330 train_time:76769ms step_avg:60.64ms
step:1267/2330 train_time:76830ms step_avg:60.64ms
step:1268/2330 train_time:76893ms step_avg:60.64ms
step:1269/2330 train_time:76956ms step_avg:60.64ms
step:1270/2330 train_time:77019ms step_avg:60.65ms
step:1271/2330 train_time:77080ms step_avg:60.64ms
step:1272/2330 train_time:77142ms step_avg:60.65ms
step:1273/2330 train_time:77202ms step_avg:60.65ms
step:1274/2330 train_time:77265ms step_avg:60.65ms
step:1275/2330 train_time:77325ms step_avg:60.65ms
step:1276/2330 train_time:77386ms step_avg:60.65ms
step:1277/2330 train_time:77445ms step_avg:60.65ms
step:1278/2330 train_time:77507ms step_avg:60.65ms
step:1279/2330 train_time:77566ms step_avg:60.65ms
step:1280/2330 train_time:77628ms step_avg:60.65ms
step:1281/2330 train_time:77687ms step_avg:60.65ms
step:1282/2330 train_time:77749ms step_avg:60.65ms
step:1283/2330 train_time:77809ms step_avg:60.65ms
step:1284/2330 train_time:77871ms step_avg:60.65ms
step:1285/2330 train_time:77931ms step_avg:60.65ms
step:1286/2330 train_time:77994ms step_avg:60.65ms
step:1287/2330 train_time:78055ms step_avg:60.65ms
step:1288/2330 train_time:78118ms step_avg:60.65ms
step:1289/2330 train_time:78178ms step_avg:60.65ms
step:1290/2330 train_time:78241ms step_avg:60.65ms
step:1291/2330 train_time:78301ms step_avg:60.65ms
step:1292/2330 train_time:78363ms step_avg:60.65ms
step:1293/2330 train_time:78423ms step_avg:60.65ms
step:1294/2330 train_time:78485ms step_avg:60.65ms
step:1295/2330 train_time:78544ms step_avg:60.65ms
step:1296/2330 train_time:78606ms step_avg:60.65ms
step:1297/2330 train_time:78666ms step_avg:60.65ms
step:1298/2330 train_time:78727ms step_avg:60.65ms
step:1299/2330 train_time:78787ms step_avg:60.65ms
step:1300/2330 train_time:78849ms step_avg:60.65ms
step:1301/2330 train_time:78908ms step_avg:60.65ms
step:1302/2330 train_time:78971ms step_avg:60.65ms
step:1303/2330 train_time:79032ms step_avg:60.65ms
step:1304/2330 train_time:79095ms step_avg:60.66ms
step:1305/2330 train_time:79155ms step_avg:60.66ms
step:1306/2330 train_time:79218ms step_avg:60.66ms
step:1307/2330 train_time:79278ms step_avg:60.66ms
step:1308/2330 train_time:79340ms step_avg:60.66ms
step:1309/2330 train_time:79400ms step_avg:60.66ms
step:1310/2330 train_time:79462ms step_avg:60.66ms
step:1311/2330 train_time:79522ms step_avg:60.66ms
step:1312/2330 train_time:79584ms step_avg:60.66ms
step:1313/2330 train_time:79644ms step_avg:60.66ms
step:1314/2330 train_time:79706ms step_avg:60.66ms
step:1315/2330 train_time:79765ms step_avg:60.66ms
step:1316/2330 train_time:79827ms step_avg:60.66ms
step:1317/2330 train_time:79887ms step_avg:60.66ms
step:1318/2330 train_time:79950ms step_avg:60.66ms
step:1319/2330 train_time:80009ms step_avg:60.66ms
step:1320/2330 train_time:80071ms step_avg:60.66ms
step:1321/2330 train_time:80132ms step_avg:60.66ms
step:1322/2330 train_time:80194ms step_avg:60.66ms
step:1323/2330 train_time:80255ms step_avg:60.66ms
step:1324/2330 train_time:80317ms step_avg:60.66ms
step:1325/2330 train_time:80378ms step_avg:60.66ms
step:1326/2330 train_time:80441ms step_avg:60.66ms
step:1327/2330 train_time:80500ms step_avg:60.66ms
step:1328/2330 train_time:80562ms step_avg:60.66ms
step:1329/2330 train_time:80622ms step_avg:60.66ms
step:1330/2330 train_time:80684ms step_avg:60.66ms
step:1331/2330 train_time:80744ms step_avg:60.66ms
step:1332/2330 train_time:80806ms step_avg:60.67ms
step:1333/2330 train_time:80866ms step_avg:60.66ms
step:1334/2330 train_time:80928ms step_avg:60.67ms
step:1335/2330 train_time:80988ms step_avg:60.66ms
step:1336/2330 train_time:81050ms step_avg:60.67ms
step:1337/2330 train_time:81109ms step_avg:60.67ms
step:1338/2330 train_time:81171ms step_avg:60.67ms
step:1339/2330 train_time:81231ms step_avg:60.67ms
step:1340/2330 train_time:81293ms step_avg:60.67ms
step:1341/2330 train_time:81353ms step_avg:60.67ms
step:1342/2330 train_time:81416ms step_avg:60.67ms
step:1343/2330 train_time:81475ms step_avg:60.67ms
step:1344/2330 train_time:81538ms step_avg:60.67ms
step:1345/2330 train_time:81598ms step_avg:60.67ms
step:1346/2330 train_time:81661ms step_avg:60.67ms
step:1347/2330 train_time:81721ms step_avg:60.67ms
step:1348/2330 train_time:81783ms step_avg:60.67ms
step:1349/2330 train_time:81843ms step_avg:60.67ms
step:1350/2330 train_time:81905ms step_avg:60.67ms
step:1351/2330 train_time:81966ms step_avg:60.67ms
step:1352/2330 train_time:82028ms step_avg:60.67ms
step:1353/2330 train_time:82088ms step_avg:60.67ms
step:1354/2330 train_time:82150ms step_avg:60.67ms
step:1355/2330 train_time:82209ms step_avg:60.67ms
step:1356/2330 train_time:82271ms step_avg:60.67ms
step:1357/2330 train_time:82331ms step_avg:60.67ms
step:1358/2330 train_time:82393ms step_avg:60.67ms
step:1359/2330 train_time:82453ms step_avg:60.67ms
step:1360/2330 train_time:82515ms step_avg:60.67ms
step:1361/2330 train_time:82575ms step_avg:60.67ms
step:1362/2330 train_time:82639ms step_avg:60.67ms
step:1363/2330 train_time:82699ms step_avg:60.67ms
step:1364/2330 train_time:82761ms step_avg:60.68ms
step:1365/2330 train_time:82821ms step_avg:60.67ms
step:1366/2330 train_time:82884ms step_avg:60.68ms
step:1367/2330 train_time:82944ms step_avg:60.68ms
step:1368/2330 train_time:83007ms step_avg:60.68ms
step:1369/2330 train_time:83066ms step_avg:60.68ms
step:1370/2330 train_time:83128ms step_avg:60.68ms
step:1371/2330 train_time:83189ms step_avg:60.68ms
step:1372/2330 train_time:83251ms step_avg:60.68ms
step:1373/2330 train_time:83310ms step_avg:60.68ms
step:1374/2330 train_time:83372ms step_avg:60.68ms
step:1375/2330 train_time:83431ms step_avg:60.68ms
step:1376/2330 train_time:83494ms step_avg:60.68ms
step:1377/2330 train_time:83555ms step_avg:60.68ms
step:1378/2330 train_time:83617ms step_avg:60.68ms
step:1379/2330 train_time:83677ms step_avg:60.68ms
step:1380/2330 train_time:83739ms step_avg:60.68ms
step:1381/2330 train_time:83799ms step_avg:60.68ms
step:1382/2330 train_time:83862ms step_avg:60.68ms
step:1383/2330 train_time:83923ms step_avg:60.68ms
step:1384/2330 train_time:83985ms step_avg:60.68ms
step:1385/2330 train_time:84045ms step_avg:60.68ms
step:1386/2330 train_time:84107ms step_avg:60.68ms
step:1387/2330 train_time:84167ms step_avg:60.68ms
step:1388/2330 train_time:84229ms step_avg:60.68ms
step:1389/2330 train_time:84289ms step_avg:60.68ms
step:1390/2330 train_time:84351ms step_avg:60.68ms
step:1391/2330 train_time:84411ms step_avg:60.68ms
step:1392/2330 train_time:84472ms step_avg:60.68ms
step:1393/2330 train_time:84533ms step_avg:60.68ms
step:1394/2330 train_time:84595ms step_avg:60.69ms
step:1395/2330 train_time:84656ms step_avg:60.69ms
step:1396/2330 train_time:84718ms step_avg:60.69ms
step:1397/2330 train_time:84778ms step_avg:60.69ms
step:1398/2330 train_time:84841ms step_avg:60.69ms
step:1399/2330 train_time:84901ms step_avg:60.69ms
step:1400/2330 train_time:84964ms step_avg:60.69ms
step:1401/2330 train_time:85024ms step_avg:60.69ms
step:1402/2330 train_time:85086ms step_avg:60.69ms
step:1403/2330 train_time:85146ms step_avg:60.69ms
step:1404/2330 train_time:85208ms step_avg:60.69ms
step:1405/2330 train_time:85267ms step_avg:60.69ms
step:1406/2330 train_time:85329ms step_avg:60.69ms
step:1407/2330 train_time:85390ms step_avg:60.69ms
step:1408/2330 train_time:85451ms step_avg:60.69ms
step:1409/2330 train_time:85511ms step_avg:60.69ms
step:1410/2330 train_time:85573ms step_avg:60.69ms
step:1411/2330 train_time:85634ms step_avg:60.69ms
step:1412/2330 train_time:85696ms step_avg:60.69ms
step:1413/2330 train_time:85756ms step_avg:60.69ms
step:1414/2330 train_time:85818ms step_avg:60.69ms
step:1415/2330 train_time:85879ms step_avg:60.69ms
step:1416/2330 train_time:85941ms step_avg:60.69ms
step:1417/2330 train_time:86002ms step_avg:60.69ms
step:1418/2330 train_time:86065ms step_avg:60.69ms
step:1419/2330 train_time:86125ms step_avg:60.69ms
step:1420/2330 train_time:86186ms step_avg:60.69ms
step:1421/2330 train_time:86247ms step_avg:60.69ms
step:1422/2330 train_time:86309ms step_avg:60.70ms
step:1423/2330 train_time:86368ms step_avg:60.69ms
step:1424/2330 train_time:86430ms step_avg:60.70ms
step:1425/2330 train_time:86490ms step_avg:60.69ms
step:1426/2330 train_time:86552ms step_avg:60.70ms
step:1427/2330 train_time:86612ms step_avg:60.70ms
step:1428/2330 train_time:86674ms step_avg:60.70ms
step:1429/2330 train_time:86735ms step_avg:60.70ms
step:1430/2330 train_time:86797ms step_avg:60.70ms
step:1431/2330 train_time:86857ms step_avg:60.70ms
step:1432/2330 train_time:86920ms step_avg:60.70ms
step:1433/2330 train_time:86980ms step_avg:60.70ms
step:1434/2330 train_time:87042ms step_avg:60.70ms
step:1435/2330 train_time:87103ms step_avg:60.70ms
step:1436/2330 train_time:87165ms step_avg:60.70ms
step:1437/2330 train_time:87225ms step_avg:60.70ms
step:1438/2330 train_time:87287ms step_avg:60.70ms
step:1439/2330 train_time:87347ms step_avg:60.70ms
step:1440/2330 train_time:87409ms step_avg:60.70ms
step:1441/2330 train_time:87469ms step_avg:60.70ms
step:1442/2330 train_time:87532ms step_avg:60.70ms
step:1443/2330 train_time:87591ms step_avg:60.70ms
step:1444/2330 train_time:87653ms step_avg:60.70ms
step:1445/2330 train_time:87713ms step_avg:60.70ms
step:1446/2330 train_time:87776ms step_avg:60.70ms
step:1447/2330 train_time:87836ms step_avg:60.70ms
step:1448/2330 train_time:87898ms step_avg:60.70ms
step:1449/2330 train_time:87958ms step_avg:60.70ms
step:1450/2330 train_time:88020ms step_avg:60.70ms
step:1451/2330 train_time:88082ms step_avg:60.70ms
step:1452/2330 train_time:88144ms step_avg:60.71ms
step:1453/2330 train_time:88204ms step_avg:60.70ms
step:1454/2330 train_time:88266ms step_avg:60.71ms
step:1455/2330 train_time:88326ms step_avg:60.70ms
step:1456/2330 train_time:88388ms step_avg:60.71ms
step:1457/2330 train_time:88447ms step_avg:60.71ms
step:1458/2330 train_time:88509ms step_avg:60.71ms
step:1459/2330 train_time:88569ms step_avg:60.70ms
step:1460/2330 train_time:88630ms step_avg:60.71ms
step:1461/2330 train_time:88690ms step_avg:60.71ms
step:1462/2330 train_time:88752ms step_avg:60.71ms
step:1463/2330 train_time:88812ms step_avg:60.71ms
step:1464/2330 train_time:88875ms step_avg:60.71ms
step:1465/2330 train_time:88936ms step_avg:60.71ms
step:1466/2330 train_time:88998ms step_avg:60.71ms
step:1467/2330 train_time:89059ms step_avg:60.71ms
step:1468/2330 train_time:89122ms step_avg:60.71ms
step:1469/2330 train_time:89182ms step_avg:60.71ms
step:1470/2330 train_time:89244ms step_avg:60.71ms
step:1471/2330 train_time:89304ms step_avg:60.71ms
step:1472/2330 train_time:89366ms step_avg:60.71ms
step:1473/2330 train_time:89426ms step_avg:60.71ms
step:1474/2330 train_time:89488ms step_avg:60.71ms
step:1475/2330 train_time:89548ms step_avg:60.71ms
step:1476/2330 train_time:89609ms step_avg:60.71ms
step:1477/2330 train_time:89669ms step_avg:60.71ms
step:1478/2330 train_time:89731ms step_avg:60.71ms
step:1479/2330 train_time:89791ms step_avg:60.71ms
step:1480/2330 train_time:89853ms step_avg:60.71ms
step:1481/2330 train_time:89914ms step_avg:60.71ms
step:1482/2330 train_time:89976ms step_avg:60.71ms
step:1483/2330 train_time:90037ms step_avg:60.71ms
step:1484/2330 train_time:90099ms step_avg:60.71ms
step:1485/2330 train_time:90159ms step_avg:60.71ms
step:1486/2330 train_time:90222ms step_avg:60.71ms
step:1487/2330 train_time:90283ms step_avg:60.71ms
step:1488/2330 train_time:90345ms step_avg:60.72ms
step:1489/2330 train_time:90405ms step_avg:60.72ms
step:1490/2330 train_time:90467ms step_avg:60.72ms
step:1491/2330 train_time:90527ms step_avg:60.72ms
step:1492/2330 train_time:90589ms step_avg:60.72ms
step:1493/2330 train_time:90649ms step_avg:60.72ms
step:1494/2330 train_time:90710ms step_avg:60.72ms
step:1495/2330 train_time:90770ms step_avg:60.72ms
step:1496/2330 train_time:90832ms step_avg:60.72ms
step:1497/2330 train_time:90892ms step_avg:60.72ms
step:1498/2330 train_time:90954ms step_avg:60.72ms
step:1499/2330 train_time:91014ms step_avg:60.72ms
step:1500/2330 train_time:91077ms step_avg:60.72ms
step:1500/2330 val_loss:3.4515 train_time:91142ms step_avg:60.76ms
step:1501/2330 train_time:91167ms step_avg:60.74ms
step:1502/2330 train_time:91203ms step_avg:60.72ms
step:1503/2330 train_time:91269ms step_avg:60.72ms
step:1504/2330 train_time:91335ms step_avg:60.73ms
step:1505/2330 train_time:91395ms step_avg:60.73ms
step:1506/2330 train_time:91457ms step_avg:60.73ms
step:1507/2330 train_time:91517ms step_avg:60.73ms
step:1508/2330 train_time:91578ms step_avg:60.73ms
step:1509/2330 train_time:91637ms step_avg:60.73ms
step:1510/2330 train_time:91698ms step_avg:60.73ms
step:1511/2330 train_time:91757ms step_avg:60.73ms
step:1512/2330 train_time:91819ms step_avg:60.73ms
step:1513/2330 train_time:91878ms step_avg:60.73ms
step:1514/2330 train_time:91940ms step_avg:60.73ms
step:1515/2330 train_time:91999ms step_avg:60.73ms
step:1516/2330 train_time:92061ms step_avg:60.73ms
step:1517/2330 train_time:92122ms step_avg:60.73ms
step:1518/2330 train_time:92185ms step_avg:60.73ms
step:1519/2330 train_time:92247ms step_avg:60.73ms
step:1520/2330 train_time:92310ms step_avg:60.73ms
step:1521/2330 train_time:92370ms step_avg:60.73ms
step:1522/2330 train_time:92432ms step_avg:60.73ms
step:1523/2330 train_time:92492ms step_avg:60.73ms
step:1524/2330 train_time:92554ms step_avg:60.73ms
step:1525/2330 train_time:92613ms step_avg:60.73ms
step:1526/2330 train_time:92674ms step_avg:60.73ms
step:1527/2330 train_time:92734ms step_avg:60.73ms
step:1528/2330 train_time:92795ms step_avg:60.73ms
step:1529/2330 train_time:92855ms step_avg:60.73ms
step:1530/2330 train_time:92917ms step_avg:60.73ms
step:1531/2330 train_time:92977ms step_avg:60.73ms
step:1532/2330 train_time:93040ms step_avg:60.73ms
step:1533/2330 train_time:93101ms step_avg:60.73ms
step:1534/2330 train_time:93164ms step_avg:60.73ms
step:1535/2330 train_time:93225ms step_avg:60.73ms
step:1536/2330 train_time:93288ms step_avg:60.73ms
step:1537/2330 train_time:93349ms step_avg:60.73ms
step:1538/2330 train_time:93412ms step_avg:60.74ms
step:1539/2330 train_time:93472ms step_avg:60.74ms
step:1540/2330 train_time:93535ms step_avg:60.74ms
step:1541/2330 train_time:93595ms step_avg:60.74ms
step:1542/2330 train_time:93657ms step_avg:60.74ms
step:1543/2330 train_time:93717ms step_avg:60.74ms
step:1544/2330 train_time:93779ms step_avg:60.74ms
step:1545/2330 train_time:93839ms step_avg:60.74ms
step:1546/2330 train_time:93901ms step_avg:60.74ms
step:1547/2330 train_time:93961ms step_avg:60.74ms
step:1548/2330 train_time:94023ms step_avg:60.74ms
step:1549/2330 train_time:94084ms step_avg:60.74ms
step:1550/2330 train_time:94147ms step_avg:60.74ms
step:1551/2330 train_time:94208ms step_avg:60.74ms
step:1552/2330 train_time:94271ms step_avg:60.74ms
step:1553/2330 train_time:94331ms step_avg:60.74ms
step:1554/2330 train_time:94394ms step_avg:60.74ms
step:1555/2330 train_time:94455ms step_avg:60.74ms
step:1556/2330 train_time:94518ms step_avg:60.74ms
step:1557/2330 train_time:94578ms step_avg:60.74ms
step:1558/2330 train_time:94641ms step_avg:60.74ms
step:1559/2330 train_time:94701ms step_avg:60.74ms
step:1560/2330 train_time:94764ms step_avg:60.75ms
step:1561/2330 train_time:94824ms step_avg:60.75ms
step:1562/2330 train_time:94886ms step_avg:60.75ms
step:1563/2330 train_time:94947ms step_avg:60.75ms
step:1564/2330 train_time:95009ms step_avg:60.75ms
step:1565/2330 train_time:95069ms step_avg:60.75ms
step:1566/2330 train_time:95132ms step_avg:60.75ms
step:1567/2330 train_time:95193ms step_avg:60.75ms
step:1568/2330 train_time:95255ms step_avg:60.75ms
step:1569/2330 train_time:95316ms step_avg:60.75ms
step:1570/2330 train_time:95378ms step_avg:60.75ms
step:1571/2330 train_time:95440ms step_avg:60.75ms
step:1572/2330 train_time:95502ms step_avg:60.75ms
step:1573/2330 train_time:95563ms step_avg:60.75ms
step:1574/2330 train_time:95625ms step_avg:60.75ms
step:1575/2330 train_time:95685ms step_avg:60.75ms
step:1576/2330 train_time:95747ms step_avg:60.75ms
step:1577/2330 train_time:95807ms step_avg:60.75ms
step:1578/2330 train_time:95871ms step_avg:60.75ms
step:1579/2330 train_time:95930ms step_avg:60.75ms
step:1580/2330 train_time:95992ms step_avg:60.75ms
step:1581/2330 train_time:96052ms step_avg:60.75ms
step:1582/2330 train_time:96115ms step_avg:60.76ms
step:1583/2330 train_time:96176ms step_avg:60.76ms
step:1584/2330 train_time:96238ms step_avg:60.76ms
step:1585/2330 train_time:96299ms step_avg:60.76ms
step:1586/2330 train_time:96363ms step_avg:60.76ms
step:1587/2330 train_time:96423ms step_avg:60.76ms
step:1588/2330 train_time:96485ms step_avg:60.76ms
step:1589/2330 train_time:96546ms step_avg:60.76ms
step:1590/2330 train_time:96608ms step_avg:60.76ms
step:1591/2330 train_time:96669ms step_avg:60.76ms
step:1592/2330 train_time:96731ms step_avg:60.76ms
step:1593/2330 train_time:96791ms step_avg:60.76ms
step:1594/2330 train_time:96853ms step_avg:60.76ms
step:1595/2330 train_time:96913ms step_avg:60.76ms
step:1596/2330 train_time:96975ms step_avg:60.76ms
step:1597/2330 train_time:97036ms step_avg:60.76ms
step:1598/2330 train_time:97098ms step_avg:60.76ms
step:1599/2330 train_time:97159ms step_avg:60.76ms
step:1600/2330 train_time:97221ms step_avg:60.76ms
step:1601/2330 train_time:97282ms step_avg:60.76ms
step:1602/2330 train_time:97345ms step_avg:60.76ms
step:1603/2330 train_time:97405ms step_avg:60.76ms
step:1604/2330 train_time:97468ms step_avg:60.77ms
step:1605/2330 train_time:97529ms step_avg:60.77ms
step:1606/2330 train_time:97591ms step_avg:60.77ms
step:1607/2330 train_time:97651ms step_avg:60.77ms
step:1608/2330 train_time:97714ms step_avg:60.77ms
step:1609/2330 train_time:97774ms step_avg:60.77ms
step:1610/2330 train_time:97836ms step_avg:60.77ms
step:1611/2330 train_time:97897ms step_avg:60.77ms
step:1612/2330 train_time:97959ms step_avg:60.77ms
step:1613/2330 train_time:98019ms step_avg:60.77ms
step:1614/2330 train_time:98082ms step_avg:60.77ms
step:1615/2330 train_time:98142ms step_avg:60.77ms
step:1616/2330 train_time:98205ms step_avg:60.77ms
step:1617/2330 train_time:98265ms step_avg:60.77ms
step:1618/2330 train_time:98327ms step_avg:60.77ms
step:1619/2330 train_time:98387ms step_avg:60.77ms
step:1620/2330 train_time:98450ms step_avg:60.77ms
step:1621/2330 train_time:98510ms step_avg:60.77ms
step:1622/2330 train_time:98572ms step_avg:60.77ms
step:1623/2330 train_time:98632ms step_avg:60.77ms
step:1624/2330 train_time:98694ms step_avg:60.77ms
step:1625/2330 train_time:98755ms step_avg:60.77ms
step:1626/2330 train_time:98817ms step_avg:60.77ms
step:1627/2330 train_time:98878ms step_avg:60.77ms
step:1628/2330 train_time:98940ms step_avg:60.77ms
step:1629/2330 train_time:99001ms step_avg:60.77ms
step:1630/2330 train_time:99063ms step_avg:60.77ms
step:1631/2330 train_time:99123ms step_avg:60.77ms
step:1632/2330 train_time:99186ms step_avg:60.78ms
step:1633/2330 train_time:99247ms step_avg:60.78ms
step:1634/2330 train_time:99309ms step_avg:60.78ms
step:1635/2330 train_time:99369ms step_avg:60.78ms
step:1636/2330 train_time:99431ms step_avg:60.78ms
step:1637/2330 train_time:99491ms step_avg:60.78ms
step:1638/2330 train_time:99554ms step_avg:60.78ms
step:1639/2330 train_time:99614ms step_avg:60.78ms
step:1640/2330 train_time:99677ms step_avg:60.78ms
step:1641/2330 train_time:99737ms step_avg:60.78ms
step:1642/2330 train_time:99799ms step_avg:60.78ms
step:1643/2330 train_time:99860ms step_avg:60.78ms
step:1644/2330 train_time:99923ms step_avg:60.78ms
step:1645/2330 train_time:99983ms step_avg:60.78ms
step:1646/2330 train_time:100046ms step_avg:60.78ms
step:1647/2330 train_time:100106ms step_avg:60.78ms
step:1648/2330 train_time:100169ms step_avg:60.78ms
step:1649/2330 train_time:100228ms step_avg:60.78ms
step:1650/2330 train_time:100290ms step_avg:60.78ms
step:1651/2330 train_time:100351ms step_avg:60.78ms
step:1652/2330 train_time:100413ms step_avg:60.78ms
step:1653/2330 train_time:100473ms step_avg:60.78ms
step:1654/2330 train_time:100535ms step_avg:60.78ms
step:1655/2330 train_time:100595ms step_avg:60.78ms
step:1656/2330 train_time:100658ms step_avg:60.78ms
step:1657/2330 train_time:100718ms step_avg:60.78ms
step:1658/2330 train_time:100780ms step_avg:60.78ms
step:1659/2330 train_time:100840ms step_avg:60.78ms
step:1660/2330 train_time:100903ms step_avg:60.78ms
step:1661/2330 train_time:100964ms step_avg:60.79ms
step:1662/2330 train_time:101026ms step_avg:60.79ms
step:1663/2330 train_time:101087ms step_avg:60.79ms
step:1664/2330 train_time:101149ms step_avg:60.79ms
step:1665/2330 train_time:101210ms step_avg:60.79ms
step:1666/2330 train_time:101272ms step_avg:60.79ms
step:1667/2330 train_time:101332ms step_avg:60.79ms
step:1668/2330 train_time:101394ms step_avg:60.79ms
step:1669/2330 train_time:101455ms step_avg:60.79ms
step:1670/2330 train_time:101518ms step_avg:60.79ms
step:1671/2330 train_time:101578ms step_avg:60.79ms
step:1672/2330 train_time:101641ms step_avg:60.79ms
step:1673/2330 train_time:101701ms step_avg:60.79ms
step:1674/2330 train_time:101764ms step_avg:60.79ms
step:1675/2330 train_time:101824ms step_avg:60.79ms
step:1676/2330 train_time:101887ms step_avg:60.79ms
step:1677/2330 train_time:101947ms step_avg:60.79ms
step:1678/2330 train_time:102009ms step_avg:60.79ms
step:1679/2330 train_time:102070ms step_avg:60.79ms
step:1680/2330 train_time:102133ms step_avg:60.79ms
step:1681/2330 train_time:102193ms step_avg:60.79ms
step:1682/2330 train_time:102255ms step_avg:60.79ms
step:1683/2330 train_time:102316ms step_avg:60.79ms
step:1684/2330 train_time:102379ms step_avg:60.80ms
step:1685/2330 train_time:102439ms step_avg:60.79ms
step:1686/2330 train_time:102502ms step_avg:60.80ms
step:1687/2330 train_time:102563ms step_avg:60.80ms
step:1688/2330 train_time:102625ms step_avg:60.80ms
step:1689/2330 train_time:102685ms step_avg:60.80ms
step:1690/2330 train_time:102747ms step_avg:60.80ms
step:1691/2330 train_time:102807ms step_avg:60.80ms
step:1692/2330 train_time:102870ms step_avg:60.80ms
step:1693/2330 train_time:102930ms step_avg:60.80ms
step:1694/2330 train_time:102992ms step_avg:60.80ms
step:1695/2330 train_time:103052ms step_avg:60.80ms
step:1696/2330 train_time:103114ms step_avg:60.80ms
step:1697/2330 train_time:103174ms step_avg:60.80ms
step:1698/2330 train_time:103237ms step_avg:60.80ms
step:1699/2330 train_time:103297ms step_avg:60.80ms
step:1700/2330 train_time:103360ms step_avg:60.80ms
step:1701/2330 train_time:103420ms step_avg:60.80ms
step:1702/2330 train_time:103483ms step_avg:60.80ms
step:1703/2330 train_time:103543ms step_avg:60.80ms
step:1704/2330 train_time:103606ms step_avg:60.80ms
step:1705/2330 train_time:103666ms step_avg:60.80ms
step:1706/2330 train_time:103729ms step_avg:60.80ms
step:1707/2330 train_time:103789ms step_avg:60.80ms
step:1708/2330 train_time:103851ms step_avg:60.80ms
step:1709/2330 train_time:103911ms step_avg:60.80ms
step:1710/2330 train_time:103974ms step_avg:60.80ms
step:1711/2330 train_time:104034ms step_avg:60.80ms
step:1712/2330 train_time:104097ms step_avg:60.80ms
step:1713/2330 train_time:104158ms step_avg:60.80ms
step:1714/2330 train_time:104220ms step_avg:60.81ms
step:1715/2330 train_time:104281ms step_avg:60.81ms
step:1716/2330 train_time:104344ms step_avg:60.81ms
step:1717/2330 train_time:104404ms step_avg:60.81ms
step:1718/2330 train_time:104466ms step_avg:60.81ms
step:1719/2330 train_time:104526ms step_avg:60.81ms
step:1720/2330 train_time:104588ms step_avg:60.81ms
step:1721/2330 train_time:104649ms step_avg:60.81ms
step:1722/2330 train_time:104711ms step_avg:60.81ms
step:1723/2330 train_time:104771ms step_avg:60.81ms
step:1724/2330 train_time:104834ms step_avg:60.81ms
step:1725/2330 train_time:104894ms step_avg:60.81ms
step:1726/2330 train_time:104957ms step_avg:60.81ms
step:1727/2330 train_time:105017ms step_avg:60.81ms
step:1728/2330 train_time:105080ms step_avg:60.81ms
step:1729/2330 train_time:105140ms step_avg:60.81ms
step:1730/2330 train_time:105203ms step_avg:60.81ms
step:1731/2330 train_time:105263ms step_avg:60.81ms
step:1732/2330 train_time:105326ms step_avg:60.81ms
step:1733/2330 train_time:105386ms step_avg:60.81ms
step:1734/2330 train_time:105449ms step_avg:60.81ms
step:1735/2330 train_time:105509ms step_avg:60.81ms
step:1736/2330 train_time:105571ms step_avg:60.81ms
step:1737/2330 train_time:105632ms step_avg:60.81ms
step:1738/2330 train_time:105693ms step_avg:60.81ms
step:1739/2330 train_time:105753ms step_avg:60.81ms
step:1740/2330 train_time:105816ms step_avg:60.81ms
step:1741/2330 train_time:105876ms step_avg:60.81ms
step:1742/2330 train_time:105938ms step_avg:60.81ms
step:1743/2330 train_time:105999ms step_avg:60.81ms
step:1744/2330 train_time:106061ms step_avg:60.81ms
step:1745/2330 train_time:106122ms step_avg:60.81ms
step:1746/2330 train_time:106185ms step_avg:60.82ms
step:1747/2330 train_time:106245ms step_avg:60.82ms
step:1748/2330 train_time:106307ms step_avg:60.82ms
step:1749/2330 train_time:106368ms step_avg:60.82ms
step:1750/2330 train_time:106430ms step_avg:60.82ms
step:1750/2330 val_loss:3.3820 train_time:106495ms step_avg:60.85ms
step:1751/2330 train_time:106519ms step_avg:60.83ms
step:1752/2330 train_time:106558ms step_avg:60.82ms
step:1753/2330 train_time:106625ms step_avg:60.82ms
step:1754/2330 train_time:106689ms step_avg:60.83ms
step:1755/2330 train_time:106749ms step_avg:60.83ms
step:1756/2330 train_time:106812ms step_avg:60.83ms
step:1757/2330 train_time:106872ms step_avg:60.83ms
step:1758/2330 train_time:106934ms step_avg:60.83ms
step:1759/2330 train_time:106994ms step_avg:60.83ms
step:1760/2330 train_time:107057ms step_avg:60.83ms
step:1761/2330 train_time:107116ms step_avg:60.83ms
step:1762/2330 train_time:107178ms step_avg:60.83ms
step:1763/2330 train_time:107238ms step_avg:60.83ms
step:1764/2330 train_time:107299ms step_avg:60.83ms
step:1765/2330 train_time:107359ms step_avg:60.83ms
step:1766/2330 train_time:107422ms step_avg:60.83ms
step:1767/2330 train_time:107483ms step_avg:60.83ms
step:1768/2330 train_time:107549ms step_avg:60.83ms
step:1769/2330 train_time:107610ms step_avg:60.83ms
step:1770/2330 train_time:107674ms step_avg:60.83ms
step:1771/2330 train_time:107735ms step_avg:60.83ms
step:1772/2330 train_time:107798ms step_avg:60.83ms
step:1773/2330 train_time:107859ms step_avg:60.83ms
step:1774/2330 train_time:107922ms step_avg:60.84ms
step:1775/2330 train_time:107983ms step_avg:60.84ms
step:1776/2330 train_time:108045ms step_avg:60.84ms
step:1777/2330 train_time:108106ms step_avg:60.84ms
step:1778/2330 train_time:108168ms step_avg:60.84ms
step:1779/2330 train_time:108228ms step_avg:60.84ms
step:1780/2330 train_time:108290ms step_avg:60.84ms
step:1781/2330 train_time:108349ms step_avg:60.84ms
step:1782/2330 train_time:108411ms step_avg:60.84ms
step:1783/2330 train_time:108472ms step_avg:60.84ms
step:1784/2330 train_time:108535ms step_avg:60.84ms
step:1785/2330 train_time:108596ms step_avg:60.84ms
step:1786/2330 train_time:108659ms step_avg:60.84ms
step:1787/2330 train_time:108720ms step_avg:60.84ms
step:1788/2330 train_time:108783ms step_avg:60.84ms
step:1789/2330 train_time:108843ms step_avg:60.84ms
step:1790/2330 train_time:108906ms step_avg:60.84ms
step:1791/2330 train_time:108967ms step_avg:60.84ms
step:1792/2330 train_time:109029ms step_avg:60.84ms
step:1793/2330 train_time:109089ms step_avg:60.84ms
step:1794/2330 train_time:109151ms step_avg:60.84ms
step:1795/2330 train_time:109211ms step_avg:60.84ms
step:1796/2330 train_time:109273ms step_avg:60.84ms
step:1797/2330 train_time:109333ms step_avg:60.84ms
step:1798/2330 train_time:109394ms step_avg:60.84ms
step:1799/2330 train_time:109455ms step_avg:60.84ms
step:1800/2330 train_time:109518ms step_avg:60.84ms
step:1801/2330 train_time:109580ms step_avg:60.84ms
step:1802/2330 train_time:109643ms step_avg:60.85ms
step:1803/2330 train_time:109704ms step_avg:60.85ms
step:1804/2330 train_time:109767ms step_avg:60.85ms
step:1805/2330 train_time:109827ms step_avg:60.85ms
step:1806/2330 train_time:109889ms step_avg:60.85ms
step:1807/2330 train_time:109949ms step_avg:60.85ms
step:1808/2330 train_time:110011ms step_avg:60.85ms
step:1809/2330 train_time:110072ms step_avg:60.85ms
step:1810/2330 train_time:110134ms step_avg:60.85ms
step:1811/2330 train_time:110196ms step_avg:60.85ms
step:1812/2330 train_time:110258ms step_avg:60.85ms
step:1813/2330 train_time:110318ms step_avg:60.85ms
step:1814/2330 train_time:110380ms step_avg:60.85ms
step:1815/2330 train_time:110441ms step_avg:60.85ms
step:1816/2330 train_time:110505ms step_avg:60.85ms
step:1817/2330 train_time:110565ms step_avg:60.85ms
step:1818/2330 train_time:110627ms step_avg:60.85ms
step:1819/2330 train_time:110687ms step_avg:60.85ms
step:1820/2330 train_time:110750ms step_avg:60.85ms
step:1821/2330 train_time:110810ms step_avg:60.85ms
step:1822/2330 train_time:110873ms step_avg:60.85ms
step:1823/2330 train_time:110934ms step_avg:60.85ms
step:1824/2330 train_time:110996ms step_avg:60.85ms
step:1825/2330 train_time:111057ms step_avg:60.85ms
step:1826/2330 train_time:111120ms step_avg:60.85ms
step:1827/2330 train_time:111180ms step_avg:60.85ms
step:1828/2330 train_time:111243ms step_avg:60.86ms
step:1829/2330 train_time:111303ms step_avg:60.85ms
step:1830/2330 train_time:111366ms step_avg:60.86ms
step:1831/2330 train_time:111426ms step_avg:60.86ms
step:1832/2330 train_time:111488ms step_avg:60.86ms
step:1833/2330 train_time:111549ms step_avg:60.86ms
step:1834/2330 train_time:111611ms step_avg:60.86ms
step:1835/2330 train_time:111671ms step_avg:60.86ms
step:1836/2330 train_time:111734ms step_avg:60.86ms
step:1837/2330 train_time:111795ms step_avg:60.86ms
step:1838/2330 train_time:111857ms step_avg:60.86ms
step:1839/2330 train_time:111918ms step_avg:60.86ms
step:1840/2330 train_time:111981ms step_avg:60.86ms
step:1841/2330 train_time:112042ms step_avg:60.86ms
step:1842/2330 train_time:112104ms step_avg:60.86ms
step:1843/2330 train_time:112165ms step_avg:60.86ms
step:1844/2330 train_time:112227ms step_avg:60.86ms
step:1845/2330 train_time:112287ms step_avg:60.86ms
step:1846/2330 train_time:112349ms step_avg:60.86ms
step:1847/2330 train_time:112410ms step_avg:60.86ms
step:1848/2330 train_time:112472ms step_avg:60.86ms
step:1849/2330 train_time:112532ms step_avg:60.86ms
step:1850/2330 train_time:112595ms step_avg:60.86ms
step:1851/2330 train_time:112656ms step_avg:60.86ms
step:1852/2330 train_time:112718ms step_avg:60.86ms
step:1853/2330 train_time:112779ms step_avg:60.86ms
step:1854/2330 train_time:112843ms step_avg:60.86ms
step:1855/2330 train_time:112903ms step_avg:60.86ms
step:1856/2330 train_time:112966ms step_avg:60.87ms
step:1857/2330 train_time:113027ms step_avg:60.87ms
step:1858/2330 train_time:113089ms step_avg:60.87ms
step:1859/2330 train_time:113149ms step_avg:60.87ms
step:1860/2330 train_time:113211ms step_avg:60.87ms
step:1861/2330 train_time:113272ms step_avg:60.87ms
step:1862/2330 train_time:113334ms step_avg:60.87ms
step:1863/2330 train_time:113396ms step_avg:60.87ms
step:1864/2330 train_time:113459ms step_avg:60.87ms
step:1865/2330 train_time:113520ms step_avg:60.87ms
step:1866/2330 train_time:113583ms step_avg:60.87ms
step:1867/2330 train_time:113643ms step_avg:60.87ms
step:1868/2330 train_time:113706ms step_avg:60.87ms
step:1869/2330 train_time:113767ms step_avg:60.87ms
step:1870/2330 train_time:113829ms step_avg:60.87ms
step:1871/2330 train_time:113889ms step_avg:60.87ms
step:1872/2330 train_time:113951ms step_avg:60.87ms
step:1873/2330 train_time:114011ms step_avg:60.87ms
step:1874/2330 train_time:114074ms step_avg:60.87ms
step:1875/2330 train_time:114134ms step_avg:60.87ms
step:1876/2330 train_time:114196ms step_avg:60.87ms
step:1877/2330 train_time:114257ms step_avg:60.87ms
step:1878/2330 train_time:114319ms step_avg:60.87ms
step:1879/2330 train_time:114380ms step_avg:60.87ms
step:1880/2330 train_time:114443ms step_avg:60.87ms
step:1881/2330 train_time:114503ms step_avg:60.87ms
step:1882/2330 train_time:114566ms step_avg:60.87ms
step:1883/2330 train_time:114627ms step_avg:60.87ms
step:1884/2330 train_time:114689ms step_avg:60.88ms
step:1885/2330 train_time:114750ms step_avg:60.88ms
step:1886/2330 train_time:114812ms step_avg:60.88ms
step:1887/2330 train_time:114872ms step_avg:60.88ms
step:1888/2330 train_time:114934ms step_avg:60.88ms
step:1889/2330 train_time:114994ms step_avg:60.88ms
step:1890/2330 train_time:115057ms step_avg:60.88ms
step:1891/2330 train_time:115118ms step_avg:60.88ms
step:1892/2330 train_time:115180ms step_avg:60.88ms
step:1893/2330 train_time:115240ms step_avg:60.88ms
step:1894/2330 train_time:115303ms step_avg:60.88ms
step:1895/2330 train_time:115364ms step_avg:60.88ms
step:1896/2330 train_time:115427ms step_avg:60.88ms
step:1897/2330 train_time:115488ms step_avg:60.88ms
step:1898/2330 train_time:115550ms step_avg:60.88ms
step:1899/2330 train_time:115611ms step_avg:60.88ms
step:1900/2330 train_time:115673ms step_avg:60.88ms
step:1901/2330 train_time:115733ms step_avg:60.88ms
step:1902/2330 train_time:115795ms step_avg:60.88ms
step:1903/2330 train_time:115855ms step_avg:60.88ms
step:1904/2330 train_time:115917ms step_avg:60.88ms
step:1905/2330 train_time:115978ms step_avg:60.88ms
step:1906/2330 train_time:116040ms step_avg:60.88ms
step:1907/2330 train_time:116101ms step_avg:60.88ms
step:1908/2330 train_time:116163ms step_avg:60.88ms
step:1909/2330 train_time:116223ms step_avg:60.88ms
step:1910/2330 train_time:116285ms step_avg:60.88ms
step:1911/2330 train_time:116346ms step_avg:60.88ms
step:1912/2330 train_time:116409ms step_avg:60.88ms
step:1913/2330 train_time:116470ms step_avg:60.88ms
step:1914/2330 train_time:116532ms step_avg:60.88ms
step:1915/2330 train_time:116592ms step_avg:60.88ms
step:1916/2330 train_time:116655ms step_avg:60.88ms
step:1917/2330 train_time:116714ms step_avg:60.88ms
step:1918/2330 train_time:116777ms step_avg:60.88ms
step:1919/2330 train_time:116837ms step_avg:60.88ms
step:1920/2330 train_time:116899ms step_avg:60.88ms
step:1921/2330 train_time:116960ms step_avg:60.88ms
step:1922/2330 train_time:117023ms step_avg:60.89ms
step:1923/2330 train_time:117084ms step_avg:60.89ms
step:1924/2330 train_time:117147ms step_avg:60.89ms
step:1925/2330 train_time:117207ms step_avg:60.89ms
step:1926/2330 train_time:117270ms step_avg:60.89ms
step:1927/2330 train_time:117330ms step_avg:60.89ms
step:1928/2330 train_time:117392ms step_avg:60.89ms
step:1929/2330 train_time:117453ms step_avg:60.89ms
step:1930/2330 train_time:117515ms step_avg:60.89ms
step:1931/2330 train_time:117575ms step_avg:60.89ms
step:1932/2330 train_time:117637ms step_avg:60.89ms
step:1933/2330 train_time:117697ms step_avg:60.89ms
step:1934/2330 train_time:117760ms step_avg:60.89ms
step:1935/2330 train_time:117820ms step_avg:60.89ms
step:1936/2330 train_time:117883ms step_avg:60.89ms
step:1937/2330 train_time:117943ms step_avg:60.89ms
step:1938/2330 train_time:118006ms step_avg:60.89ms
step:1939/2330 train_time:118067ms step_avg:60.89ms
step:1940/2330 train_time:118129ms step_avg:60.89ms
step:1941/2330 train_time:118189ms step_avg:60.89ms
step:1942/2330 train_time:118252ms step_avg:60.89ms
step:1943/2330 train_time:118312ms step_avg:60.89ms
step:1944/2330 train_time:118375ms step_avg:60.89ms
step:1945/2330 train_time:118435ms step_avg:60.89ms
step:1946/2330 train_time:118498ms step_avg:60.89ms
step:1947/2330 train_time:118558ms step_avg:60.89ms
step:1948/2330 train_time:118620ms step_avg:60.89ms
step:1949/2330 train_time:118681ms step_avg:60.89ms
step:1950/2330 train_time:118744ms step_avg:60.89ms
step:1951/2330 train_time:118804ms step_avg:60.89ms
step:1952/2330 train_time:118867ms step_avg:60.90ms
step:1953/2330 train_time:118927ms step_avg:60.89ms
step:1954/2330 train_time:118989ms step_avg:60.90ms
step:1955/2330 train_time:119049ms step_avg:60.89ms
step:1956/2330 train_time:119112ms step_avg:60.90ms
step:1957/2330 train_time:119173ms step_avg:60.90ms
step:1958/2330 train_time:119235ms step_avg:60.90ms
step:1959/2330 train_time:119295ms step_avg:60.90ms
step:1960/2330 train_time:119357ms step_avg:60.90ms
step:1961/2330 train_time:119419ms step_avg:60.90ms
step:1962/2330 train_time:119481ms step_avg:60.90ms
step:1963/2330 train_time:119542ms step_avg:60.90ms
step:1964/2330 train_time:119605ms step_avg:60.90ms
step:1965/2330 train_time:119665ms step_avg:60.90ms
step:1966/2330 train_time:119727ms step_avg:60.90ms
step:1967/2330 train_time:119787ms step_avg:60.90ms
step:1968/2330 train_time:119850ms step_avg:60.90ms
step:1969/2330 train_time:119910ms step_avg:60.90ms
step:1970/2330 train_time:119972ms step_avg:60.90ms
step:1971/2330 train_time:120032ms step_avg:60.90ms
step:1972/2330 train_time:120094ms step_avg:60.90ms
step:1973/2330 train_time:120154ms step_avg:60.90ms
step:1974/2330 train_time:120217ms step_avg:60.90ms
step:1975/2330 train_time:120276ms step_avg:60.90ms
step:1976/2330 train_time:120338ms step_avg:60.90ms
step:1977/2330 train_time:120399ms step_avg:60.90ms
step:1978/2330 train_time:120461ms step_avg:60.90ms
step:1979/2330 train_time:120522ms step_avg:60.90ms
step:1980/2330 train_time:120584ms step_avg:60.90ms
step:1981/2330 train_time:120645ms step_avg:60.90ms
step:1982/2330 train_time:120708ms step_avg:60.90ms
step:1983/2330 train_time:120768ms step_avg:60.90ms
step:1984/2330 train_time:120831ms step_avg:60.90ms
step:1985/2330 train_time:120891ms step_avg:60.90ms
step:1986/2330 train_time:120954ms step_avg:60.90ms
step:1987/2330 train_time:121014ms step_avg:60.90ms
step:1988/2330 train_time:121077ms step_avg:60.90ms
step:1989/2330 train_time:121138ms step_avg:60.90ms
step:1990/2330 train_time:121200ms step_avg:60.90ms
step:1991/2330 train_time:121260ms step_avg:60.90ms
step:1992/2330 train_time:121323ms step_avg:60.90ms
step:1993/2330 train_time:121383ms step_avg:60.90ms
step:1994/2330 train_time:121446ms step_avg:60.91ms
step:1995/2330 train_time:121507ms step_avg:60.91ms
step:1996/2330 train_time:121570ms step_avg:60.91ms
step:1997/2330 train_time:121630ms step_avg:60.91ms
step:1998/2330 train_time:121692ms step_avg:60.91ms
step:1999/2330 train_time:121752ms step_avg:60.91ms
step:2000/2330 train_time:121814ms step_avg:60.91ms
step:2000/2330 val_loss:3.3325 train_time:121879ms step_avg:60.94ms
step:2001/2330 train_time:121903ms step_avg:60.92ms
step:2002/2330 train_time:121941ms step_avg:60.91ms
step:2003/2330 train_time:122007ms step_avg:60.91ms
step:2004/2330 train_time:122071ms step_avg:60.91ms
step:2005/2330 train_time:122131ms step_avg:60.91ms
step:2006/2330 train_time:122194ms step_avg:60.91ms
step:2007/2330 train_time:122253ms step_avg:60.91ms
step:2008/2330 train_time:122315ms step_avg:60.91ms
step:2009/2330 train_time:122375ms step_avg:60.91ms
step:2010/2330 train_time:122437ms step_avg:60.91ms
step:2011/2330 train_time:122497ms step_avg:60.91ms
step:2012/2330 train_time:122559ms step_avg:60.91ms
step:2013/2330 train_time:122619ms step_avg:60.91ms
step:2014/2330 train_time:122681ms step_avg:60.91ms
step:2015/2330 train_time:122741ms step_avg:60.91ms
step:2016/2330 train_time:122803ms step_avg:60.91ms
step:2017/2330 train_time:122865ms step_avg:60.91ms
step:2018/2330 train_time:122930ms step_avg:60.92ms
step:2019/2330 train_time:122991ms step_avg:60.92ms
step:2020/2330 train_time:123055ms step_avg:60.92ms
step:2021/2330 train_time:123116ms step_avg:60.92ms
step:2022/2330 train_time:123180ms step_avg:60.92ms
step:2023/2330 train_time:123241ms step_avg:60.92ms
step:2024/2330 train_time:123304ms step_avg:60.92ms
step:2025/2330 train_time:123365ms step_avg:60.92ms
step:2026/2330 train_time:123427ms step_avg:60.92ms
step:2027/2330 train_time:123487ms step_avg:60.92ms
step:2028/2330 train_time:123549ms step_avg:60.92ms
step:2029/2330 train_time:123609ms step_avg:60.92ms
step:2030/2330 train_time:123671ms step_avg:60.92ms
step:2031/2330 train_time:123731ms step_avg:60.92ms
step:2032/2330 train_time:123793ms step_avg:60.92ms
step:2033/2330 train_time:123853ms step_avg:60.92ms
step:2034/2330 train_time:123917ms step_avg:60.92ms
step:2035/2330 train_time:123978ms step_avg:60.92ms
step:2036/2330 train_time:124042ms step_avg:60.92ms
step:2037/2330 train_time:124103ms step_avg:60.92ms
step:2038/2330 train_time:124166ms step_avg:60.93ms
step:2039/2330 train_time:124227ms step_avg:60.93ms
step:2040/2330 train_time:124290ms step_avg:60.93ms
step:2041/2330 train_time:124350ms step_avg:60.93ms
step:2042/2330 train_time:124412ms step_avg:60.93ms
step:2043/2330 train_time:124471ms step_avg:60.93ms
step:2044/2330 train_time:124533ms step_avg:60.93ms
step:2045/2330 train_time:124593ms step_avg:60.93ms
step:2046/2330 train_time:124655ms step_avg:60.93ms
step:2047/2330 train_time:124715ms step_avg:60.93ms
step:2048/2330 train_time:124778ms step_avg:60.93ms
step:2049/2330 train_time:124838ms step_avg:60.93ms
step:2050/2330 train_time:124901ms step_avg:60.93ms
step:2051/2330 train_time:124962ms step_avg:60.93ms
step:2052/2330 train_time:125025ms step_avg:60.93ms
step:2053/2330 train_time:125086ms step_avg:60.93ms
step:2054/2330 train_time:125149ms step_avg:60.93ms
step:2055/2330 train_time:125209ms step_avg:60.93ms
step:2056/2330 train_time:125272ms step_avg:60.93ms
step:2057/2330 train_time:125332ms step_avg:60.93ms
step:2058/2330 train_time:125394ms step_avg:60.93ms
step:2059/2330 train_time:125455ms step_avg:60.93ms
step:2060/2330 train_time:125517ms step_avg:60.93ms
step:2061/2330 train_time:125577ms step_avg:60.93ms
step:2062/2330 train_time:125639ms step_avg:60.93ms
step:2063/2330 train_time:125699ms step_avg:60.93ms
step:2064/2330 train_time:125762ms step_avg:60.93ms
step:2065/2330 train_time:125822ms step_avg:60.93ms
step:2066/2330 train_time:125885ms step_avg:60.93ms
step:2067/2330 train_time:125946ms step_avg:60.93ms
step:2068/2330 train_time:126008ms step_avg:60.93ms
step:2069/2330 train_time:126069ms step_avg:60.93ms
step:2070/2330 train_time:126132ms step_avg:60.93ms
step:2071/2330 train_time:126192ms step_avg:60.93ms
step:2072/2330 train_time:126255ms step_avg:60.93ms
step:2073/2330 train_time:126316ms step_avg:60.93ms
step:2074/2330 train_time:126379ms step_avg:60.93ms
step:2075/2330 train_time:126440ms step_avg:60.93ms
step:2076/2330 train_time:126502ms step_avg:60.94ms
step:2077/2330 train_time:126563ms step_avg:60.94ms
step:2078/2330 train_time:126625ms step_avg:60.94ms
step:2079/2330 train_time:126686ms step_avg:60.94ms
step:2080/2330 train_time:126748ms step_avg:60.94ms
step:2081/2330 train_time:126808ms step_avg:60.94ms
step:2082/2330 train_time:126871ms step_avg:60.94ms
step:2083/2330 train_time:126931ms step_avg:60.94ms
step:2084/2330 train_time:126994ms step_avg:60.94ms
step:2085/2330 train_time:127054ms step_avg:60.94ms
step:2086/2330 train_time:127116ms step_avg:60.94ms
step:2087/2330 train_time:127177ms step_avg:60.94ms
step:2088/2330 train_time:127240ms step_avg:60.94ms
step:2089/2330 train_time:127301ms step_avg:60.94ms
step:2090/2330 train_time:127365ms step_avg:60.94ms
step:2091/2330 train_time:127426ms step_avg:60.94ms
step:2092/2330 train_time:127488ms step_avg:60.94ms
step:2093/2330 train_time:127548ms step_avg:60.94ms
step:2094/2330 train_time:127610ms step_avg:60.94ms
step:2095/2330 train_time:127671ms step_avg:60.94ms
step:2096/2330 train_time:127733ms step_avg:60.94ms
step:2097/2330 train_time:127793ms step_avg:60.94ms
step:2098/2330 train_time:127856ms step_avg:60.94ms
step:2099/2330 train_time:127917ms step_avg:60.94ms
step:2100/2330 train_time:127980ms step_avg:60.94ms
step:2101/2330 train_time:128040ms step_avg:60.94ms
step:2102/2330 train_time:128103ms step_avg:60.94ms
step:2103/2330 train_time:128165ms step_avg:60.94ms
step:2104/2330 train_time:128227ms step_avg:60.94ms
step:2105/2330 train_time:128288ms step_avg:60.94ms
step:2106/2330 train_time:128350ms step_avg:60.94ms
step:2107/2330 train_time:128411ms step_avg:60.94ms
step:2108/2330 train_time:128473ms step_avg:60.95ms
step:2109/2330 train_time:128533ms step_avg:60.94ms
step:2110/2330 train_time:128596ms step_avg:60.95ms
step:2111/2330 train_time:128656ms step_avg:60.95ms
step:2112/2330 train_time:128718ms step_avg:60.95ms
step:2113/2330 train_time:128778ms step_avg:60.95ms
step:2114/2330 train_time:128841ms step_avg:60.95ms
step:2115/2330 train_time:128901ms step_avg:60.95ms
step:2116/2330 train_time:128964ms step_avg:60.95ms
step:2117/2330 train_time:129025ms step_avg:60.95ms
step:2118/2330 train_time:129088ms step_avg:60.95ms
step:2119/2330 train_time:129148ms step_avg:60.95ms
step:2120/2330 train_time:129211ms step_avg:60.95ms
step:2121/2330 train_time:129271ms step_avg:60.95ms
step:2122/2330 train_time:129333ms step_avg:60.95ms
step:2123/2330 train_time:129394ms step_avg:60.95ms
step:2124/2330 train_time:129456ms step_avg:60.95ms
step:2125/2330 train_time:129517ms step_avg:60.95ms
step:2126/2330 train_time:129579ms step_avg:60.95ms
step:2127/2330 train_time:129640ms step_avg:60.95ms
step:2128/2330 train_time:129703ms step_avg:60.95ms
step:2129/2330 train_time:129764ms step_avg:60.95ms
step:2130/2330 train_time:129827ms step_avg:60.95ms
step:2131/2330 train_time:129888ms step_avg:60.95ms
step:2132/2330 train_time:129950ms step_avg:60.95ms
step:2133/2330 train_time:130010ms step_avg:60.95ms
step:2134/2330 train_time:130073ms step_avg:60.95ms
step:2135/2330 train_time:130133ms step_avg:60.95ms
step:2136/2330 train_time:130195ms step_avg:60.95ms
step:2137/2330 train_time:130255ms step_avg:60.95ms
step:2138/2330 train_time:130318ms step_avg:60.95ms
step:2139/2330 train_time:130378ms step_avg:60.95ms
step:2140/2330 train_time:130441ms step_avg:60.95ms
step:2141/2330 train_time:130502ms step_avg:60.95ms
step:2142/2330 train_time:130565ms step_avg:60.95ms
step:2143/2330 train_time:130625ms step_avg:60.95ms
step:2144/2330 train_time:130688ms step_avg:60.96ms
step:2145/2330 train_time:130748ms step_avg:60.95ms
step:2146/2330 train_time:130810ms step_avg:60.96ms
step:2147/2330 train_time:130870ms step_avg:60.95ms
step:2148/2330 train_time:130932ms step_avg:60.96ms
step:2149/2330 train_time:130993ms step_avg:60.96ms
step:2150/2330 train_time:131055ms step_avg:60.96ms
step:2151/2330 train_time:131116ms step_avg:60.96ms
step:2152/2330 train_time:131179ms step_avg:60.96ms
step:2153/2330 train_time:131239ms step_avg:60.96ms
step:2154/2330 train_time:131301ms step_avg:60.96ms
step:2155/2330 train_time:131362ms step_avg:60.96ms
step:2156/2330 train_time:131424ms step_avg:60.96ms
step:2157/2330 train_time:131485ms step_avg:60.96ms
step:2158/2330 train_time:131548ms step_avg:60.96ms
step:2159/2330 train_time:131609ms step_avg:60.96ms
step:2160/2330 train_time:131671ms step_avg:60.96ms
step:2161/2330 train_time:131731ms step_avg:60.96ms
step:2162/2330 train_time:131794ms step_avg:60.96ms
step:2163/2330 train_time:131854ms step_avg:60.96ms
step:2164/2330 train_time:131917ms step_avg:60.96ms
step:2165/2330 train_time:131977ms step_avg:60.96ms
step:2166/2330 train_time:132040ms step_avg:60.96ms
step:2167/2330 train_time:132101ms step_avg:60.96ms
step:2168/2330 train_time:132163ms step_avg:60.96ms
step:2169/2330 train_time:132224ms step_avg:60.96ms
step:2170/2330 train_time:132286ms step_avg:60.96ms
step:2171/2330 train_time:132347ms step_avg:60.96ms
step:2172/2330 train_time:132410ms step_avg:60.96ms
step:2173/2330 train_time:132470ms step_avg:60.96ms
step:2174/2330 train_time:132532ms step_avg:60.96ms
step:2175/2330 train_time:132593ms step_avg:60.96ms
step:2176/2330 train_time:132655ms step_avg:60.96ms
step:2177/2330 train_time:132716ms step_avg:60.96ms
step:2178/2330 train_time:132779ms step_avg:60.96ms
step:2179/2330 train_time:132839ms step_avg:60.96ms
step:2180/2330 train_time:132902ms step_avg:60.96ms
step:2181/2330 train_time:132963ms step_avg:60.96ms
step:2182/2330 train_time:133026ms step_avg:60.97ms
step:2183/2330 train_time:133086ms step_avg:60.96ms
step:2184/2330 train_time:133149ms step_avg:60.97ms
step:2185/2330 train_time:133209ms step_avg:60.97ms
step:2186/2330 train_time:133272ms step_avg:60.97ms
step:2187/2330 train_time:133332ms step_avg:60.97ms
step:2188/2330 train_time:133395ms step_avg:60.97ms
step:2189/2330 train_time:133456ms step_avg:60.97ms
step:2190/2330 train_time:133518ms step_avg:60.97ms
step:2191/2330 train_time:133580ms step_avg:60.97ms
step:2192/2330 train_time:133643ms step_avg:60.97ms
step:2193/2330 train_time:133703ms step_avg:60.97ms
step:2194/2330 train_time:133766ms step_avg:60.97ms
step:2195/2330 train_time:133827ms step_avg:60.97ms
step:2196/2330 train_time:133890ms step_avg:60.97ms
step:2197/2330 train_time:133950ms step_avg:60.97ms
step:2198/2330 train_time:134012ms step_avg:60.97ms
step:2199/2330 train_time:134073ms step_avg:60.97ms
step:2200/2330 train_time:134135ms step_avg:60.97ms
step:2201/2330 train_time:134195ms step_avg:60.97ms
step:2202/2330 train_time:134258ms step_avg:60.97ms
step:2203/2330 train_time:134319ms step_avg:60.97ms
step:2204/2330 train_time:134382ms step_avg:60.97ms
step:2205/2330 train_time:134442ms step_avg:60.97ms
step:2206/2330 train_time:134505ms step_avg:60.97ms
step:2207/2330 train_time:134565ms step_avg:60.97ms
step:2208/2330 train_time:134627ms step_avg:60.97ms
step:2209/2330 train_time:134688ms step_avg:60.97ms
step:2210/2330 train_time:134751ms step_avg:60.97ms
step:2211/2330 train_time:134811ms step_avg:60.97ms
step:2212/2330 train_time:134873ms step_avg:60.97ms
step:2213/2330 train_time:134933ms step_avg:60.97ms
step:2214/2330 train_time:134996ms step_avg:60.97ms
step:2215/2330 train_time:135056ms step_avg:60.97ms
step:2216/2330 train_time:135119ms step_avg:60.97ms
step:2217/2330 train_time:135180ms step_avg:60.97ms
step:2218/2330 train_time:135243ms step_avg:60.98ms
step:2219/2330 train_time:135303ms step_avg:60.97ms
step:2220/2330 train_time:135366ms step_avg:60.98ms
step:2221/2330 train_time:135427ms step_avg:60.98ms
step:2222/2330 train_time:135490ms step_avg:60.98ms
step:2223/2330 train_time:135550ms step_avg:60.98ms
step:2224/2330 train_time:135612ms step_avg:60.98ms
step:2225/2330 train_time:135672ms step_avg:60.98ms
step:2226/2330 train_time:135734ms step_avg:60.98ms
step:2227/2330 train_time:135795ms step_avg:60.98ms
step:2228/2330 train_time:135858ms step_avg:60.98ms
step:2229/2330 train_time:135918ms step_avg:60.98ms
step:2230/2330 train_time:135981ms step_avg:60.98ms
step:2231/2330 train_time:136041ms step_avg:60.98ms
step:2232/2330 train_time:136104ms step_avg:60.98ms
step:2233/2330 train_time:136166ms step_avg:60.98ms
step:2234/2330 train_time:136228ms step_avg:60.98ms
step:2235/2330 train_time:136288ms step_avg:60.98ms
step:2236/2330 train_time:136351ms step_avg:60.98ms
step:2237/2330 train_time:136411ms step_avg:60.98ms
step:2238/2330 train_time:136474ms step_avg:60.98ms
step:2239/2330 train_time:136534ms step_avg:60.98ms
step:2240/2330 train_time:136597ms step_avg:60.98ms
step:2241/2330 train_time:136657ms step_avg:60.98ms
step:2242/2330 train_time:136720ms step_avg:60.98ms
step:2243/2330 train_time:136781ms step_avg:60.98ms
step:2244/2330 train_time:136844ms step_avg:60.98ms
step:2245/2330 train_time:136904ms step_avg:60.98ms
step:2246/2330 train_time:136967ms step_avg:60.98ms
step:2247/2330 train_time:137028ms step_avg:60.98ms
step:2248/2330 train_time:137090ms step_avg:60.98ms
step:2249/2330 train_time:137151ms step_avg:60.98ms
step:2250/2330 train_time:137213ms step_avg:60.98ms
step:2250/2330 val_loss:3.2921 train_time:137277ms step_avg:61.01ms
step:2251/2330 train_time:137300ms step_avg:61.00ms
step:2252/2330 train_time:137341ms step_avg:60.99ms
step:2253/2330 train_time:137406ms step_avg:60.99ms
step:2254/2330 train_time:137470ms step_avg:60.99ms
step:2255/2330 train_time:137530ms step_avg:60.99ms
step:2256/2330 train_time:137592ms step_avg:60.99ms
step:2257/2330 train_time:137652ms step_avg:60.99ms
step:2258/2330 train_time:137714ms step_avg:60.99ms
step:2259/2330 train_time:137774ms step_avg:60.99ms
step:2260/2330 train_time:137836ms step_avg:60.99ms
step:2261/2330 train_time:137896ms step_avg:60.99ms
step:2262/2330 train_time:137958ms step_avg:60.99ms
step:2263/2330 train_time:138019ms step_avg:60.99ms
step:2264/2330 train_time:138082ms step_avg:60.99ms
step:2265/2330 train_time:138142ms step_avg:60.99ms
step:2266/2330 train_time:138205ms step_avg:60.99ms
step:2267/2330 train_time:138267ms step_avg:60.99ms
step:2268/2330 train_time:138331ms step_avg:60.99ms
step:2269/2330 train_time:138392ms step_avg:60.99ms
step:2270/2330 train_time:138455ms step_avg:60.99ms
step:2271/2330 train_time:138516ms step_avg:60.99ms
step:2272/2330 train_time:138579ms step_avg:60.99ms
step:2273/2330 train_time:138639ms step_avg:60.99ms
step:2274/2330 train_time:138701ms step_avg:60.99ms
step:2275/2330 train_time:138762ms step_avg:60.99ms
step:2276/2330 train_time:138824ms step_avg:60.99ms
step:2277/2330 train_time:138884ms step_avg:60.99ms
step:2278/2330 train_time:138946ms step_avg:60.99ms
step:2279/2330 train_time:139006ms step_avg:60.99ms
step:2280/2330 train_time:139069ms step_avg:61.00ms
step:2281/2330 train_time:139129ms step_avg:60.99ms
step:2282/2330 train_time:139190ms step_avg:60.99ms
step:2283/2330 train_time:139251ms step_avg:60.99ms
step:2284/2330 train_time:139314ms step_avg:61.00ms
step:2285/2330 train_time:139374ms step_avg:61.00ms
step:2286/2330 train_time:139438ms step_avg:61.00ms
step:2287/2330 train_time:139499ms step_avg:61.00ms
step:2288/2330 train_time:139562ms step_avg:61.00ms
step:2289/2330 train_time:139622ms step_avg:61.00ms
step:2290/2330 train_time:139685ms step_avg:61.00ms
step:2291/2330 train_time:139745ms step_avg:61.00ms
step:2292/2330 train_time:139807ms step_avg:61.00ms
step:2293/2330 train_time:139867ms step_avg:61.00ms
step:2294/2330 train_time:139929ms step_avg:61.00ms
step:2295/2330 train_time:139990ms step_avg:61.00ms
step:2296/2330 train_time:140052ms step_avg:61.00ms
step:2297/2330 train_time:140112ms step_avg:61.00ms
step:2298/2330 train_time:140174ms step_avg:61.00ms
step:2299/2330 train_time:140234ms step_avg:61.00ms
step:2300/2330 train_time:140296ms step_avg:61.00ms
step:2301/2330 train_time:140357ms step_avg:61.00ms
step:2302/2330 train_time:140420ms step_avg:61.00ms
step:2303/2330 train_time:140481ms step_avg:61.00ms
step:2304/2330 train_time:140544ms step_avg:61.00ms
step:2305/2330 train_time:140604ms step_avg:61.00ms
step:2306/2330 train_time:140667ms step_avg:61.00ms
step:2307/2330 train_time:140727ms step_avg:61.00ms
step:2308/2330 train_time:140789ms step_avg:61.00ms
step:2309/2330 train_time:140849ms step_avg:61.00ms
step:2310/2330 train_time:140912ms step_avg:61.00ms
step:2311/2330 train_time:140971ms step_avg:61.00ms
step:2312/2330 train_time:141034ms step_avg:61.00ms
step:2313/2330 train_time:141094ms step_avg:61.00ms
step:2314/2330 train_time:141156ms step_avg:61.00ms
step:2315/2330 train_time:141217ms step_avg:61.00ms
step:2316/2330 train_time:141280ms step_avg:61.00ms
step:2317/2330 train_time:141340ms step_avg:61.00ms
step:2318/2330 train_time:141403ms step_avg:61.00ms
step:2319/2330 train_time:141463ms step_avg:61.00ms
step:2320/2330 train_time:141527ms step_avg:61.00ms
step:2321/2330 train_time:141588ms step_avg:61.00ms
step:2322/2330 train_time:141650ms step_avg:61.00ms
step:2323/2330 train_time:141710ms step_avg:61.00ms
step:2324/2330 train_time:141772ms step_avg:61.00ms
step:2325/2330 train_time:141833ms step_avg:61.00ms
step:2326/2330 train_time:141896ms step_avg:61.00ms
step:2327/2330 train_time:141956ms step_avg:61.00ms
step:2328/2330 train_time:142018ms step_avg:61.00ms
step:2329/2330 train_time:142078ms step_avg:61.00ms
step:2330/2330 train_time:142141ms step_avg:61.00ms
step:2330/2330 val_loss:3.2776 train_time:142206ms step_avg:61.03ms
peak memory allocated: 29512 MiB reserved: 44076 MiB
