import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        # self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "mlp_adam_lr7e-2"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
# embed_params = [p for n, p in model.named_parameters() if "embed" in n]
embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=1e-1, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizer2 = torch.optim.Adam(
    hidden_matrix_params + gate_params,
    lr=7e-2,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 04:54:34 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   32C    P0             111W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   31C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   28C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   31C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   32C    P0             115W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.05ms
step:1/2330 train_time:80ms step_avg:80.50ms
step:2/2330 train_time:178ms step_avg:89.07ms
step:3/2330 train_time:190ms step_avg:63.30ms
step:4/2330 train_time:201ms step_avg:50.33ms
step:5/2330 train_time:211ms step_avg:42.22ms
step:6/2330 train_time:221ms step_avg:36.88ms
step:7/2330 train_time:339ms step_avg:48.43ms
step:8/2330 train_time:379ms step_avg:47.35ms
step:9/2330 train_time:412ms step_avg:45.79ms
step:10/2330 train_time:452ms step_avg:45.20ms
step:11/2330 train_time:485ms step_avg:44.10ms
step:12/2330 train_time:525ms step_avg:43.75ms
step:13/2330 train_time:558ms step_avg:42.95ms
step:14/2330 train_time:598ms step_avg:42.73ms
step:15/2330 train_time:632ms step_avg:42.11ms
step:16/2330 train_time:671ms step_avg:41.97ms
step:17/2330 train_time:705ms step_avg:41.47ms
step:18/2330 train_time:745ms step_avg:41.40ms
step:19/2330 train_time:778ms step_avg:40.97ms
step:20/2330 train_time:819ms step_avg:40.93ms
step:21/2330 train_time:852ms step_avg:40.58ms
step:22/2330 train_time:892ms step_avg:40.55ms
step:23/2330 train_time:926ms step_avg:40.25ms
step:24/2330 train_time:966ms step_avg:40.24ms
step:25/2330 train_time:999ms step_avg:39.97ms
step:26/2330 train_time:1039ms step_avg:39.97ms
step:27/2330 train_time:1073ms step_avg:39.74ms
step:28/2330 train_time:1113ms step_avg:39.75ms
step:29/2330 train_time:1147ms step_avg:39.54ms
step:30/2330 train_time:1187ms step_avg:39.56ms
step:31/2330 train_time:1222ms step_avg:39.43ms
step:32/2330 train_time:1264ms step_avg:39.49ms
step:33/2330 train_time:1300ms step_avg:39.41ms
step:34/2330 train_time:1341ms step_avg:39.43ms
step:35/2330 train_time:1377ms step_avg:39.34ms
step:36/2330 train_time:1417ms step_avg:39.36ms
step:37/2330 train_time:1452ms step_avg:39.25ms
step:38/2330 train_time:1492ms step_avg:39.27ms
step:39/2330 train_time:1527ms step_avg:39.14ms
step:40/2330 train_time:1567ms step_avg:39.16ms
step:41/2330 train_time:1600ms step_avg:39.03ms
step:42/2330 train_time:1640ms step_avg:39.05ms
step:43/2330 train_time:1674ms step_avg:38.94ms
step:44/2330 train_time:1715ms step_avg:38.97ms
step:45/2330 train_time:1749ms step_avg:38.86ms
step:46/2330 train_time:1788ms step_avg:38.88ms
step:47/2330 train_time:1822ms step_avg:38.77ms
step:48/2330 train_time:1862ms step_avg:38.79ms
step:49/2330 train_time:1896ms step_avg:38.69ms
step:50/2330 train_time:1936ms step_avg:38.72ms
step:51/2330 train_time:1970ms step_avg:38.63ms
step:52/2330 train_time:2010ms step_avg:38.65ms
step:53/2330 train_time:2043ms step_avg:38.56ms
step:54/2330 train_time:2083ms step_avg:38.58ms
step:55/2330 train_time:2117ms step_avg:38.49ms
step:56/2330 train_time:2157ms step_avg:38.53ms
step:57/2330 train_time:2192ms step_avg:38.46ms
step:58/2330 train_time:2233ms step_avg:38.50ms
step:59/2330 train_time:2267ms step_avg:38.43ms
step:60/2330 train_time:2308ms step_avg:38.47ms
step:61/2330 train_time:2343ms step_avg:38.41ms
step:62/2330 train_time:2384ms step_avg:38.45ms
step:63/2330 train_time:2419ms step_avg:38.40ms
step:64/2330 train_time:2459ms step_avg:38.42ms
step:65/2330 train_time:2495ms step_avg:38.38ms
step:66/2330 train_time:2535ms step_avg:38.41ms
step:67/2330 train_time:2569ms step_avg:38.35ms
step:68/2330 train_time:2610ms step_avg:38.38ms
step:69/2330 train_time:2644ms step_avg:38.32ms
step:70/2330 train_time:2684ms step_avg:38.35ms
step:71/2330 train_time:2718ms step_avg:38.28ms
step:72/2330 train_time:2758ms step_avg:38.31ms
step:73/2330 train_time:2793ms step_avg:38.26ms
step:74/2330 train_time:2833ms step_avg:38.29ms
step:75/2330 train_time:2867ms step_avg:38.23ms
step:76/2330 train_time:2907ms step_avg:38.26ms
step:77/2330 train_time:2942ms step_avg:38.20ms
step:78/2330 train_time:2982ms step_avg:38.23ms
step:79/2330 train_time:3016ms step_avg:38.17ms
step:80/2330 train_time:3056ms step_avg:38.20ms
step:81/2330 train_time:3091ms step_avg:38.16ms
step:82/2330 train_time:3131ms step_avg:38.19ms
step:83/2330 train_time:3165ms step_avg:38.14ms
step:84/2330 train_time:3206ms step_avg:38.17ms
step:85/2330 train_time:3240ms step_avg:38.12ms
step:86/2330 train_time:3280ms step_avg:38.14ms
step:87/2330 train_time:3316ms step_avg:38.12ms
step:88/2330 train_time:3357ms step_avg:38.15ms
step:89/2330 train_time:3392ms step_avg:38.11ms
step:90/2330 train_time:3433ms step_avg:38.14ms
step:91/2330 train_time:3467ms step_avg:38.10ms
step:92/2330 train_time:3508ms step_avg:38.13ms
step:93/2330 train_time:3543ms step_avg:38.10ms
step:94/2330 train_time:3583ms step_avg:38.12ms
step:95/2330 train_time:3618ms step_avg:38.08ms
step:96/2330 train_time:3658ms step_avg:38.10ms
step:97/2330 train_time:3692ms step_avg:38.07ms
step:98/2330 train_time:3733ms step_avg:38.09ms
step:99/2330 train_time:3767ms step_avg:38.05ms
step:100/2330 train_time:3808ms step_avg:38.08ms
step:101/2330 train_time:3842ms step_avg:38.04ms
step:102/2330 train_time:3882ms step_avg:38.06ms
step:103/2330 train_time:3917ms step_avg:38.03ms
step:104/2330 train_time:3957ms step_avg:38.05ms
step:105/2330 train_time:3992ms step_avg:38.02ms
step:106/2330 train_time:4032ms step_avg:38.04ms
step:107/2330 train_time:4067ms step_avg:38.01ms
step:108/2330 train_time:4107ms step_avg:38.03ms
step:109/2330 train_time:4142ms step_avg:38.00ms
step:110/2330 train_time:4183ms step_avg:38.03ms
step:111/2330 train_time:4217ms step_avg:37.99ms
step:112/2330 train_time:4258ms step_avg:38.02ms
step:113/2330 train_time:4292ms step_avg:37.98ms
step:114/2330 train_time:4333ms step_avg:38.01ms
step:115/2330 train_time:4368ms step_avg:37.98ms
step:116/2330 train_time:4409ms step_avg:38.01ms
step:117/2330 train_time:4444ms step_avg:37.98ms
step:118/2330 train_time:4485ms step_avg:38.00ms
step:119/2330 train_time:4519ms step_avg:37.98ms
step:120/2330 train_time:4559ms step_avg:37.99ms
step:121/2330 train_time:4594ms step_avg:37.97ms
step:122/2330 train_time:4635ms step_avg:37.99ms
step:123/2330 train_time:4670ms step_avg:37.97ms
step:124/2330 train_time:4710ms step_avg:37.99ms
step:125/2330 train_time:4746ms step_avg:37.96ms
step:126/2330 train_time:4786ms step_avg:37.98ms
step:127/2330 train_time:4821ms step_avg:37.96ms
step:128/2330 train_time:4861ms step_avg:37.97ms
step:129/2330 train_time:4895ms step_avg:37.95ms
step:130/2330 train_time:4936ms step_avg:37.97ms
step:131/2330 train_time:4971ms step_avg:37.94ms
step:132/2330 train_time:5011ms step_avg:37.96ms
step:133/2330 train_time:5045ms step_avg:37.94ms
step:134/2330 train_time:5086ms step_avg:37.95ms
step:135/2330 train_time:5120ms step_avg:37.93ms
step:136/2330 train_time:5161ms step_avg:37.95ms
step:137/2330 train_time:5195ms step_avg:37.92ms
step:138/2330 train_time:5236ms step_avg:37.94ms
step:139/2330 train_time:5271ms step_avg:37.92ms
step:140/2330 train_time:5311ms step_avg:37.94ms
step:141/2330 train_time:5347ms step_avg:37.92ms
step:142/2330 train_time:5387ms step_avg:37.94ms
step:143/2330 train_time:5422ms step_avg:37.92ms
step:144/2330 train_time:5462ms step_avg:37.93ms
step:145/2330 train_time:5497ms step_avg:37.91ms
step:146/2330 train_time:5537ms step_avg:37.93ms
step:147/2330 train_time:5573ms step_avg:37.91ms
step:148/2330 train_time:5614ms step_avg:37.93ms
step:149/2330 train_time:5648ms step_avg:37.91ms
step:150/2330 train_time:5688ms step_avg:37.92ms
step:151/2330 train_time:5724ms step_avg:37.90ms
step:152/2330 train_time:5764ms step_avg:37.92ms
step:153/2330 train_time:5799ms step_avg:37.90ms
step:154/2330 train_time:5839ms step_avg:37.92ms
step:155/2330 train_time:5874ms step_avg:37.90ms
step:156/2330 train_time:5915ms step_avg:37.91ms
step:157/2330 train_time:5950ms step_avg:37.90ms
step:158/2330 train_time:5990ms step_avg:37.91ms
step:159/2330 train_time:6025ms step_avg:37.89ms
step:160/2330 train_time:6065ms step_avg:37.91ms
step:161/2330 train_time:6100ms step_avg:37.89ms
step:162/2330 train_time:6140ms step_avg:37.90ms
step:163/2330 train_time:6175ms step_avg:37.88ms
step:164/2330 train_time:6216ms step_avg:37.90ms
step:165/2330 train_time:6251ms step_avg:37.89ms
step:166/2330 train_time:6292ms step_avg:37.90ms
step:167/2330 train_time:6327ms step_avg:37.89ms
step:168/2330 train_time:6367ms step_avg:37.90ms
step:169/2330 train_time:6403ms step_avg:37.89ms
step:170/2330 train_time:6444ms step_avg:37.91ms
step:171/2330 train_time:6479ms step_avg:37.89ms
step:172/2330 train_time:6519ms step_avg:37.90ms
step:173/2330 train_time:6555ms step_avg:37.89ms
step:174/2330 train_time:6595ms step_avg:37.90ms
step:175/2330 train_time:6630ms step_avg:37.89ms
step:176/2330 train_time:6671ms step_avg:37.90ms
step:177/2330 train_time:6707ms step_avg:37.89ms
step:178/2330 train_time:6748ms step_avg:37.91ms
step:179/2330 train_time:6783ms step_avg:37.89ms
step:180/2330 train_time:6823ms step_avg:37.91ms
step:181/2330 train_time:6858ms step_avg:37.89ms
step:182/2330 train_time:6898ms step_avg:37.90ms
step:183/2330 train_time:6933ms step_avg:37.88ms
step:184/2330 train_time:6973ms step_avg:37.90ms
step:185/2330 train_time:7008ms step_avg:37.88ms
step:186/2330 train_time:7049ms step_avg:37.90ms
step:187/2330 train_time:7083ms step_avg:37.88ms
step:188/2330 train_time:7124ms step_avg:37.89ms
step:189/2330 train_time:7158ms step_avg:37.87ms
step:190/2330 train_time:7198ms step_avg:37.89ms
step:191/2330 train_time:7233ms step_avg:37.87ms
step:192/2330 train_time:7274ms step_avg:37.88ms
step:193/2330 train_time:7308ms step_avg:37.87ms
step:194/2330 train_time:7349ms step_avg:37.88ms
step:195/2330 train_time:7384ms step_avg:37.87ms
step:196/2330 train_time:7425ms step_avg:37.88ms
step:197/2330 train_time:7460ms step_avg:37.87ms
step:198/2330 train_time:7500ms step_avg:37.88ms
step:199/2330 train_time:7535ms step_avg:37.87ms
step:200/2330 train_time:7576ms step_avg:37.88ms
step:201/2330 train_time:7611ms step_avg:37.87ms
step:202/2330 train_time:7652ms step_avg:37.88ms
step:203/2330 train_time:7686ms step_avg:37.86ms
step:204/2330 train_time:7727ms step_avg:37.88ms
step:205/2330 train_time:7762ms step_avg:37.86ms
step:206/2330 train_time:7802ms step_avg:37.88ms
step:207/2330 train_time:7837ms step_avg:37.86ms
step:208/2330 train_time:7877ms step_avg:37.87ms
step:209/2330 train_time:7912ms step_avg:37.86ms
step:210/2330 train_time:7953ms step_avg:37.87ms
step:211/2330 train_time:7987ms step_avg:37.85ms
step:212/2330 train_time:8028ms step_avg:37.87ms
step:213/2330 train_time:8063ms step_avg:37.85ms
step:214/2330 train_time:8104ms step_avg:37.87ms
step:215/2330 train_time:8138ms step_avg:37.85ms
step:216/2330 train_time:8178ms step_avg:37.86ms
step:217/2330 train_time:8213ms step_avg:37.85ms
step:218/2330 train_time:8254ms step_avg:37.86ms
step:219/2330 train_time:8288ms step_avg:37.85ms
step:220/2330 train_time:8329ms step_avg:37.86ms
step:221/2330 train_time:8364ms step_avg:37.85ms
step:222/2330 train_time:8405ms step_avg:37.86ms
step:223/2330 train_time:8440ms step_avg:37.85ms
step:224/2330 train_time:8480ms step_avg:37.86ms
step:225/2330 train_time:8515ms step_avg:37.85ms
step:226/2330 train_time:8556ms step_avg:37.86ms
step:227/2330 train_time:8591ms step_avg:37.85ms
step:228/2330 train_time:8632ms step_avg:37.86ms
step:229/2330 train_time:8667ms step_avg:37.85ms
step:230/2330 train_time:8708ms step_avg:37.86ms
step:231/2330 train_time:8743ms step_avg:37.85ms
step:232/2330 train_time:8783ms step_avg:37.86ms
step:233/2330 train_time:8818ms step_avg:37.84ms
step:234/2330 train_time:8858ms step_avg:37.86ms
step:235/2330 train_time:8893ms step_avg:37.84ms
step:236/2330 train_time:8933ms step_avg:37.85ms
step:237/2330 train_time:8968ms step_avg:37.84ms
step:238/2330 train_time:9009ms step_avg:37.85ms
step:239/2330 train_time:9044ms step_avg:37.84ms
step:240/2330 train_time:9085ms step_avg:37.86ms
step:241/2330 train_time:9120ms step_avg:37.84ms
step:242/2330 train_time:9160ms step_avg:37.85ms
step:243/2330 train_time:9195ms step_avg:37.84ms
step:244/2330 train_time:9235ms step_avg:37.85ms
step:245/2330 train_time:9270ms step_avg:37.84ms
step:246/2330 train_time:9311ms step_avg:37.85ms
step:247/2330 train_time:9346ms step_avg:37.84ms
step:248/2330 train_time:9387ms step_avg:37.85ms
step:249/2330 train_time:9423ms step_avg:37.84ms
step:250/2330 train_time:9463ms step_avg:37.85ms
step:250/2330 val_loss:5.9931 train_time:9574ms step_avg:38.29ms
step:251/2330 train_time:9585ms step_avg:38.19ms
step:252/2330 train_time:9596ms step_avg:38.08ms
step:253/2330 train_time:9606ms step_avg:37.97ms
step:254/2330 train_time:9616ms step_avg:37.86ms
step:255/2330 train_time:9651ms step_avg:37.85ms
step:256/2330 train_time:9691ms step_avg:37.86ms
step:257/2330 train_time:9725ms step_avg:37.84ms
step:258/2330 train_time:9766ms step_avg:37.85ms
step:259/2330 train_time:9800ms step_avg:37.84ms
step:260/2330 train_time:9840ms step_avg:37.85ms
step:261/2330 train_time:9875ms step_avg:37.83ms
step:262/2330 train_time:9916ms step_avg:37.85ms
step:263/2330 train_time:9955ms step_avg:37.85ms
step:264/2330 train_time:9996ms step_avg:37.86ms
step:265/2330 train_time:10032ms step_avg:37.86ms
step:266/2330 train_time:10072ms step_avg:37.87ms
step:267/2330 train_time:10107ms step_avg:37.85ms
step:268/2330 train_time:10148ms step_avg:37.87ms
step:269/2330 train_time:10182ms step_avg:37.85ms
step:270/2330 train_time:10223ms step_avg:37.86ms
step:271/2330 train_time:10257ms step_avg:37.85ms
step:272/2330 train_time:10298ms step_avg:37.86ms
step:273/2330 train_time:10332ms step_avg:37.85ms
step:274/2330 train_time:10373ms step_avg:37.86ms
step:275/2330 train_time:10408ms step_avg:37.85ms
step:276/2330 train_time:10448ms step_avg:37.86ms
step:277/2330 train_time:10483ms step_avg:37.84ms
step:278/2330 train_time:10524ms step_avg:37.86ms
step:279/2330 train_time:10559ms step_avg:37.85ms
step:280/2330 train_time:10600ms step_avg:37.86ms
step:281/2330 train_time:10635ms step_avg:37.85ms
step:282/2330 train_time:10676ms step_avg:37.86ms
step:283/2330 train_time:10710ms step_avg:37.85ms
step:284/2330 train_time:10751ms step_avg:37.85ms
step:285/2330 train_time:10786ms step_avg:37.84ms
step:286/2330 train_time:10826ms step_avg:37.85ms
step:287/2330 train_time:10862ms step_avg:37.85ms
step:288/2330 train_time:10903ms step_avg:37.86ms
step:289/2330 train_time:10939ms step_avg:37.85ms
step:290/2330 train_time:10980ms step_avg:37.86ms
step:291/2330 train_time:11015ms step_avg:37.85ms
step:292/2330 train_time:11056ms step_avg:37.86ms
step:293/2330 train_time:11091ms step_avg:37.85ms
step:294/2330 train_time:11131ms step_avg:37.86ms
step:295/2330 train_time:11166ms step_avg:37.85ms
step:296/2330 train_time:11207ms step_avg:37.86ms
step:297/2330 train_time:11242ms step_avg:37.85ms
step:298/2330 train_time:11282ms step_avg:37.86ms
step:299/2330 train_time:11316ms step_avg:37.85ms
step:300/2330 train_time:11357ms step_avg:37.86ms
step:301/2330 train_time:11392ms step_avg:37.85ms
step:302/2330 train_time:11432ms step_avg:37.85ms
step:303/2330 train_time:11467ms step_avg:37.85ms
step:304/2330 train_time:11508ms step_avg:37.86ms
step:305/2330 train_time:11543ms step_avg:37.85ms
step:306/2330 train_time:11584ms step_avg:37.86ms
step:307/2330 train_time:11618ms step_avg:37.84ms
step:308/2330 train_time:11659ms step_avg:37.85ms
step:309/2330 train_time:11693ms step_avg:37.84ms
step:310/2330 train_time:11734ms step_avg:37.85ms
step:311/2330 train_time:11769ms step_avg:37.84ms
step:312/2330 train_time:11810ms step_avg:37.85ms
step:313/2330 train_time:11845ms step_avg:37.84ms
step:314/2330 train_time:11886ms step_avg:37.85ms
step:315/2330 train_time:11921ms step_avg:37.84ms
step:316/2330 train_time:11962ms step_avg:37.85ms
step:317/2330 train_time:11997ms step_avg:37.85ms
step:318/2330 train_time:12038ms step_avg:37.85ms
step:319/2330 train_time:12072ms step_avg:37.84ms
step:320/2330 train_time:12114ms step_avg:37.85ms
step:321/2330 train_time:12149ms step_avg:37.85ms
step:322/2330 train_time:12189ms step_avg:37.85ms
step:323/2330 train_time:12224ms step_avg:37.85ms
step:324/2330 train_time:12265ms step_avg:37.85ms
step:325/2330 train_time:12300ms step_avg:37.85ms
step:326/2330 train_time:12341ms step_avg:37.85ms
step:327/2330 train_time:12375ms step_avg:37.84ms
step:328/2330 train_time:12416ms step_avg:37.85ms
step:329/2330 train_time:12450ms step_avg:37.84ms
step:330/2330 train_time:12491ms step_avg:37.85ms
step:331/2330 train_time:12526ms step_avg:37.84ms
step:332/2330 train_time:12567ms step_avg:37.85ms
step:333/2330 train_time:12603ms step_avg:37.85ms
step:334/2330 train_time:12643ms step_avg:37.85ms
step:335/2330 train_time:12678ms step_avg:37.84ms
step:336/2330 train_time:12718ms step_avg:37.85ms
step:337/2330 train_time:12753ms step_avg:37.84ms
step:338/2330 train_time:12794ms step_avg:37.85ms
step:339/2330 train_time:12829ms step_avg:37.84ms
step:340/2330 train_time:12870ms step_avg:37.85ms
step:341/2330 train_time:12905ms step_avg:37.84ms
step:342/2330 train_time:12946ms step_avg:37.85ms
step:343/2330 train_time:12982ms step_avg:37.85ms
step:344/2330 train_time:13023ms step_avg:37.86ms
step:345/2330 train_time:13057ms step_avg:37.85ms
step:346/2330 train_time:13098ms step_avg:37.86ms
step:347/2330 train_time:13132ms step_avg:37.85ms
step:348/2330 train_time:13173ms step_avg:37.85ms
step:349/2330 train_time:13208ms step_avg:37.84ms
step:350/2330 train_time:13249ms step_avg:37.85ms
step:351/2330 train_time:13284ms step_avg:37.84ms
step:352/2330 train_time:13324ms step_avg:37.85ms
step:353/2330 train_time:13359ms step_avg:37.84ms
step:354/2330 train_time:13399ms step_avg:37.85ms
step:355/2330 train_time:13434ms step_avg:37.84ms
step:356/2330 train_time:13475ms step_avg:37.85ms
step:357/2330 train_time:13510ms step_avg:37.84ms
step:358/2330 train_time:13551ms step_avg:37.85ms
step:359/2330 train_time:13586ms step_avg:37.84ms
step:360/2330 train_time:13627ms step_avg:37.85ms
step:361/2330 train_time:13662ms step_avg:37.84ms
step:362/2330 train_time:13703ms step_avg:37.85ms
step:363/2330 train_time:13737ms step_avg:37.84ms
step:364/2330 train_time:13778ms step_avg:37.85ms
step:365/2330 train_time:13813ms step_avg:37.84ms
step:366/2330 train_time:13854ms step_avg:37.85ms
step:367/2330 train_time:13890ms step_avg:37.85ms
step:368/2330 train_time:13930ms step_avg:37.85ms
step:369/2330 train_time:13966ms step_avg:37.85ms
step:370/2330 train_time:14007ms step_avg:37.86ms
step:371/2330 train_time:14043ms step_avg:37.85ms
step:372/2330 train_time:14084ms step_avg:37.86ms
step:373/2330 train_time:14119ms step_avg:37.85ms
step:374/2330 train_time:14160ms step_avg:37.86ms
step:375/2330 train_time:14194ms step_avg:37.85ms
step:376/2330 train_time:14235ms step_avg:37.86ms
step:377/2330 train_time:14271ms step_avg:37.85ms
step:378/2330 train_time:14312ms step_avg:37.86ms
step:379/2330 train_time:14348ms step_avg:37.86ms
step:380/2330 train_time:14388ms step_avg:37.86ms
step:381/2330 train_time:14424ms step_avg:37.86ms
step:382/2330 train_time:14465ms step_avg:37.87ms
step:383/2330 train_time:14499ms step_avg:37.86ms
step:384/2330 train_time:14540ms step_avg:37.86ms
step:385/2330 train_time:14575ms step_avg:37.86ms
step:386/2330 train_time:14616ms step_avg:37.87ms
step:387/2330 train_time:14652ms step_avg:37.86ms
step:388/2330 train_time:14693ms step_avg:37.87ms
step:389/2330 train_time:14728ms step_avg:37.86ms
step:390/2330 train_time:14769ms step_avg:37.87ms
step:391/2330 train_time:14804ms step_avg:37.86ms
step:392/2330 train_time:14845ms step_avg:37.87ms
step:393/2330 train_time:14879ms step_avg:37.86ms
step:394/2330 train_time:14920ms step_avg:37.87ms
step:395/2330 train_time:14955ms step_avg:37.86ms
step:396/2330 train_time:14996ms step_avg:37.87ms
step:397/2330 train_time:15031ms step_avg:37.86ms
step:398/2330 train_time:15072ms step_avg:37.87ms
step:399/2330 train_time:15107ms step_avg:37.86ms
step:400/2330 train_time:15148ms step_avg:37.87ms
step:401/2330 train_time:15183ms step_avg:37.86ms
step:402/2330 train_time:15224ms step_avg:37.87ms
step:403/2330 train_time:15259ms step_avg:37.86ms
step:404/2330 train_time:15299ms step_avg:37.87ms
step:405/2330 train_time:15334ms step_avg:37.86ms
step:406/2330 train_time:15375ms step_avg:37.87ms
step:407/2330 train_time:15411ms step_avg:37.86ms
step:408/2330 train_time:15451ms step_avg:37.87ms
step:409/2330 train_time:15487ms step_avg:37.87ms
step:410/2330 train_time:15528ms step_avg:37.87ms
step:411/2330 train_time:15564ms step_avg:37.87ms
step:412/2330 train_time:15605ms step_avg:37.88ms
step:413/2330 train_time:15641ms step_avg:37.87ms
step:414/2330 train_time:15682ms step_avg:37.88ms
step:415/2330 train_time:15717ms step_avg:37.87ms
step:416/2330 train_time:15758ms step_avg:37.88ms
step:417/2330 train_time:15792ms step_avg:37.87ms
step:418/2330 train_time:15833ms step_avg:37.88ms
step:419/2330 train_time:15868ms step_avg:37.87ms
step:420/2330 train_time:15910ms step_avg:37.88ms
step:421/2330 train_time:15946ms step_avg:37.88ms
step:422/2330 train_time:15987ms step_avg:37.88ms
step:423/2330 train_time:16023ms step_avg:37.88ms
step:424/2330 train_time:16064ms step_avg:37.89ms
step:425/2330 train_time:16099ms step_avg:37.88ms
step:426/2330 train_time:16140ms step_avg:37.89ms
step:427/2330 train_time:16174ms step_avg:37.88ms
step:428/2330 train_time:16215ms step_avg:37.89ms
step:429/2330 train_time:16251ms step_avg:37.88ms
step:430/2330 train_time:16292ms step_avg:37.89ms
step:431/2330 train_time:16327ms step_avg:37.88ms
step:432/2330 train_time:16367ms step_avg:37.89ms
step:433/2330 train_time:16402ms step_avg:37.88ms
step:434/2330 train_time:16443ms step_avg:37.89ms
step:435/2330 train_time:16478ms step_avg:37.88ms
step:436/2330 train_time:16519ms step_avg:37.89ms
step:437/2330 train_time:16555ms step_avg:37.88ms
step:438/2330 train_time:16596ms step_avg:37.89ms
step:439/2330 train_time:16631ms step_avg:37.88ms
step:440/2330 train_time:16672ms step_avg:37.89ms
step:441/2330 train_time:16708ms step_avg:37.89ms
step:442/2330 train_time:16748ms step_avg:37.89ms
step:443/2330 train_time:16784ms step_avg:37.89ms
step:444/2330 train_time:16825ms step_avg:37.89ms
step:445/2330 train_time:16859ms step_avg:37.89ms
step:446/2330 train_time:16900ms step_avg:37.89ms
step:447/2330 train_time:16936ms step_avg:37.89ms
step:448/2330 train_time:16977ms step_avg:37.89ms
step:449/2330 train_time:17012ms step_avg:37.89ms
step:450/2330 train_time:17054ms step_avg:37.90ms
step:451/2330 train_time:17088ms step_avg:37.89ms
step:452/2330 train_time:17130ms step_avg:37.90ms
step:453/2330 train_time:17166ms step_avg:37.89ms
step:454/2330 train_time:17207ms step_avg:37.90ms
step:455/2330 train_time:17243ms step_avg:37.90ms
step:456/2330 train_time:17284ms step_avg:37.90ms
step:457/2330 train_time:17318ms step_avg:37.90ms
step:458/2330 train_time:17359ms step_avg:37.90ms
step:459/2330 train_time:17394ms step_avg:37.89ms
step:460/2330 train_time:17434ms step_avg:37.90ms
step:461/2330 train_time:17470ms step_avg:37.90ms
step:462/2330 train_time:17511ms step_avg:37.90ms
step:463/2330 train_time:17546ms step_avg:37.90ms
step:464/2330 train_time:17588ms step_avg:37.90ms
step:465/2330 train_time:17622ms step_avg:37.90ms
step:466/2330 train_time:17663ms step_avg:37.90ms
step:467/2330 train_time:17697ms step_avg:37.90ms
step:468/2330 train_time:17738ms step_avg:37.90ms
step:469/2330 train_time:17773ms step_avg:37.90ms
step:470/2330 train_time:17815ms step_avg:37.90ms
step:471/2330 train_time:17850ms step_avg:37.90ms
step:472/2330 train_time:17891ms step_avg:37.90ms
step:473/2330 train_time:17927ms step_avg:37.90ms
step:474/2330 train_time:17969ms step_avg:37.91ms
step:475/2330 train_time:18003ms step_avg:37.90ms
step:476/2330 train_time:18045ms step_avg:37.91ms
step:477/2330 train_time:18079ms step_avg:37.90ms
step:478/2330 train_time:18120ms step_avg:37.91ms
step:479/2330 train_time:18155ms step_avg:37.90ms
step:480/2330 train_time:18197ms step_avg:37.91ms
step:481/2330 train_time:18232ms step_avg:37.90ms
step:482/2330 train_time:18273ms step_avg:37.91ms
step:483/2330 train_time:18308ms step_avg:37.91ms
step:484/2330 train_time:18349ms step_avg:37.91ms
step:485/2330 train_time:18384ms step_avg:37.90ms
step:486/2330 train_time:18425ms step_avg:37.91ms
step:487/2330 train_time:18460ms step_avg:37.91ms
step:488/2330 train_time:18500ms step_avg:37.91ms
step:489/2330 train_time:18535ms step_avg:37.90ms
step:490/2330 train_time:18577ms step_avg:37.91ms
step:491/2330 train_time:18612ms step_avg:37.91ms
step:492/2330 train_time:18654ms step_avg:37.91ms
step:493/2330 train_time:18689ms step_avg:37.91ms
step:494/2330 train_time:18729ms step_avg:37.91ms
step:495/2330 train_time:18765ms step_avg:37.91ms
step:496/2330 train_time:18806ms step_avg:37.91ms
step:497/2330 train_time:18841ms step_avg:37.91ms
step:498/2330 train_time:18881ms step_avg:37.91ms
step:499/2330 train_time:18916ms step_avg:37.91ms
step:500/2330 train_time:18957ms step_avg:37.91ms
step:500/2330 val_loss:5.6898 train_time:19070ms step_avg:38.14ms
step:501/2330 train_time:19082ms step_avg:38.09ms
step:502/2330 train_time:19093ms step_avg:38.03ms
step:503/2330 train_time:19102ms step_avg:37.98ms
step:504/2330 train_time:19113ms step_avg:37.92ms
step:505/2330 train_time:19148ms step_avg:37.92ms
step:506/2330 train_time:19188ms step_avg:37.92ms
step:507/2330 train_time:19223ms step_avg:37.91ms
step:508/2330 train_time:19264ms step_avg:37.92ms
step:509/2330 train_time:19298ms step_avg:37.91ms
step:510/2330 train_time:19338ms step_avg:37.92ms
step:511/2330 train_time:19375ms step_avg:37.92ms
step:512/2330 train_time:19415ms step_avg:37.92ms
step:513/2330 train_time:19454ms step_avg:37.92ms
step:514/2330 train_time:19496ms step_avg:37.93ms
step:515/2330 train_time:19530ms step_avg:37.92ms
step:516/2330 train_time:19571ms step_avg:37.93ms
step:517/2330 train_time:19607ms step_avg:37.92ms
step:518/2330 train_time:19648ms step_avg:37.93ms
step:519/2330 train_time:19684ms step_avg:37.93ms
step:520/2330 train_time:19725ms step_avg:37.93ms
step:521/2330 train_time:19760ms step_avg:37.93ms
step:522/2330 train_time:19801ms step_avg:37.93ms
step:523/2330 train_time:19836ms step_avg:37.93ms
step:524/2330 train_time:19876ms step_avg:37.93ms
step:525/2330 train_time:19910ms step_avg:37.92ms
step:526/2330 train_time:19952ms step_avg:37.93ms
step:527/2330 train_time:19987ms step_avg:37.93ms
step:528/2330 train_time:20029ms step_avg:37.93ms
step:529/2330 train_time:20064ms step_avg:37.93ms
step:530/2330 train_time:20106ms step_avg:37.93ms
step:531/2330 train_time:20141ms step_avg:37.93ms
step:532/2330 train_time:20182ms step_avg:37.94ms
step:533/2330 train_time:20216ms step_avg:37.93ms
step:534/2330 train_time:20257ms step_avg:37.93ms
step:535/2330 train_time:20292ms step_avg:37.93ms
step:536/2330 train_time:20333ms step_avg:37.94ms
step:537/2330 train_time:20368ms step_avg:37.93ms
step:538/2330 train_time:20410ms step_avg:37.94ms
step:539/2330 train_time:20446ms step_avg:37.93ms
step:540/2330 train_time:20488ms step_avg:37.94ms
step:541/2330 train_time:20524ms step_avg:37.94ms
step:542/2330 train_time:20566ms step_avg:37.94ms
step:543/2330 train_time:20601ms step_avg:37.94ms
step:544/2330 train_time:20643ms step_avg:37.95ms
step:545/2330 train_time:20677ms step_avg:37.94ms
step:546/2330 train_time:20718ms step_avg:37.94ms
step:547/2330 train_time:20752ms step_avg:37.94ms
step:548/2330 train_time:20793ms step_avg:37.94ms
step:549/2330 train_time:20828ms step_avg:37.94ms
step:550/2330 train_time:20869ms step_avg:37.94ms
step:551/2330 train_time:20904ms step_avg:37.94ms
step:552/2330 train_time:20945ms step_avg:37.94ms
step:553/2330 train_time:20980ms step_avg:37.94ms
step:554/2330 train_time:21021ms step_avg:37.94ms
step:555/2330 train_time:21056ms step_avg:37.94ms
step:556/2330 train_time:21097ms step_avg:37.94ms
step:557/2330 train_time:21132ms step_avg:37.94ms
step:558/2330 train_time:21173ms step_avg:37.94ms
step:559/2330 train_time:21209ms step_avg:37.94ms
step:560/2330 train_time:21250ms step_avg:37.95ms
step:561/2330 train_time:21285ms step_avg:37.94ms
step:562/2330 train_time:21326ms step_avg:37.95ms
step:563/2330 train_time:21361ms step_avg:37.94ms
step:564/2330 train_time:21402ms step_avg:37.95ms
step:565/2330 train_time:21438ms step_avg:37.94ms
step:566/2330 train_time:21480ms step_avg:37.95ms
step:567/2330 train_time:21514ms step_avg:37.94ms
step:568/2330 train_time:21555ms step_avg:37.95ms
step:569/2330 train_time:21591ms step_avg:37.95ms
step:570/2330 train_time:21632ms step_avg:37.95ms
step:571/2330 train_time:21667ms step_avg:37.95ms
step:572/2330 train_time:21708ms step_avg:37.95ms
step:573/2330 train_time:21744ms step_avg:37.95ms
step:574/2330 train_time:21785ms step_avg:37.95ms
step:575/2330 train_time:21820ms step_avg:37.95ms
step:576/2330 train_time:21861ms step_avg:37.95ms
step:577/2330 train_time:21897ms step_avg:37.95ms
step:578/2330 train_time:21938ms step_avg:37.95ms
step:579/2330 train_time:21972ms step_avg:37.95ms
step:580/2330 train_time:22013ms step_avg:37.95ms
step:581/2330 train_time:22049ms step_avg:37.95ms
step:582/2330 train_time:22090ms step_avg:37.96ms
step:583/2330 train_time:22126ms step_avg:37.95ms
step:584/2330 train_time:22167ms step_avg:37.96ms
step:585/2330 train_time:22202ms step_avg:37.95ms
step:586/2330 train_time:22243ms step_avg:37.96ms
step:587/2330 train_time:22278ms step_avg:37.95ms
step:588/2330 train_time:22319ms step_avg:37.96ms
step:589/2330 train_time:22354ms step_avg:37.95ms
step:590/2330 train_time:22395ms step_avg:37.96ms
step:591/2330 train_time:22430ms step_avg:37.95ms
step:592/2330 train_time:22472ms step_avg:37.96ms
step:593/2330 train_time:22506ms step_avg:37.95ms
step:594/2330 train_time:22547ms step_avg:37.96ms
step:595/2330 train_time:22583ms step_avg:37.95ms
step:596/2330 train_time:22624ms step_avg:37.96ms
step:597/2330 train_time:22660ms step_avg:37.96ms
step:598/2330 train_time:22702ms step_avg:37.96ms
step:599/2330 train_time:22737ms step_avg:37.96ms
step:600/2330 train_time:22779ms step_avg:37.96ms
step:601/2330 train_time:22813ms step_avg:37.96ms
step:602/2330 train_time:22855ms step_avg:37.96ms
step:603/2330 train_time:22890ms step_avg:37.96ms
step:604/2330 train_time:22931ms step_avg:37.97ms
step:605/2330 train_time:22966ms step_avg:37.96ms
step:606/2330 train_time:23007ms step_avg:37.97ms
step:607/2330 train_time:23044ms step_avg:37.96ms
step:608/2330 train_time:23085ms step_avg:37.97ms
step:609/2330 train_time:23120ms step_avg:37.96ms
step:610/2330 train_time:23161ms step_avg:37.97ms
step:611/2330 train_time:23196ms step_avg:37.96ms
step:612/2330 train_time:23237ms step_avg:37.97ms
step:613/2330 train_time:23272ms step_avg:37.96ms
step:614/2330 train_time:23313ms step_avg:37.97ms
step:615/2330 train_time:23349ms step_avg:37.97ms
step:616/2330 train_time:23390ms step_avg:37.97ms
step:617/2330 train_time:23425ms step_avg:37.97ms
step:618/2330 train_time:23467ms step_avg:37.97ms
step:619/2330 train_time:23502ms step_avg:37.97ms
step:620/2330 train_time:23544ms step_avg:37.97ms
step:621/2330 train_time:23579ms step_avg:37.97ms
step:622/2330 train_time:23620ms step_avg:37.97ms
step:623/2330 train_time:23655ms step_avg:37.97ms
step:624/2330 train_time:23696ms step_avg:37.97ms
step:625/2330 train_time:23730ms step_avg:37.97ms
step:626/2330 train_time:23771ms step_avg:37.97ms
step:627/2330 train_time:23807ms step_avg:37.97ms
step:628/2330 train_time:23848ms step_avg:37.97ms
step:629/2330 train_time:23883ms step_avg:37.97ms
step:630/2330 train_time:23925ms step_avg:37.98ms
step:631/2330 train_time:23960ms step_avg:37.97ms
step:632/2330 train_time:24002ms step_avg:37.98ms
step:633/2330 train_time:24036ms step_avg:37.97ms
step:634/2330 train_time:24077ms step_avg:37.98ms
step:635/2330 train_time:24114ms step_avg:37.97ms
step:636/2330 train_time:24152ms step_avg:37.98ms
step:637/2330 train_time:24187ms step_avg:37.97ms
step:638/2330 train_time:24229ms step_avg:37.98ms
step:639/2330 train_time:24264ms step_avg:37.97ms
step:640/2330 train_time:24305ms step_avg:37.98ms
step:641/2330 train_time:24341ms step_avg:37.97ms
step:642/2330 train_time:24383ms step_avg:37.98ms
step:643/2330 train_time:24418ms step_avg:37.97ms
step:644/2330 train_time:24459ms step_avg:37.98ms
step:645/2330 train_time:24494ms step_avg:37.98ms
step:646/2330 train_time:24535ms step_avg:37.98ms
step:647/2330 train_time:24570ms step_avg:37.98ms
step:648/2330 train_time:24611ms step_avg:37.98ms
step:649/2330 train_time:24647ms step_avg:37.98ms
step:650/2330 train_time:24688ms step_avg:37.98ms
step:651/2330 train_time:24724ms step_avg:37.98ms
step:652/2330 train_time:24765ms step_avg:37.98ms
step:653/2330 train_time:24800ms step_avg:37.98ms
step:654/2330 train_time:24842ms step_avg:37.98ms
step:655/2330 train_time:24876ms step_avg:37.98ms
step:656/2330 train_time:24917ms step_avg:37.98ms
step:657/2330 train_time:24952ms step_avg:37.98ms
step:658/2330 train_time:24993ms step_avg:37.98ms
step:659/2330 train_time:25028ms step_avg:37.98ms
step:660/2330 train_time:25069ms step_avg:37.98ms
step:661/2330 train_time:25105ms step_avg:37.98ms
step:662/2330 train_time:25146ms step_avg:37.98ms
step:663/2330 train_time:25182ms step_avg:37.98ms
step:664/2330 train_time:25223ms step_avg:37.99ms
step:665/2330 train_time:25258ms step_avg:37.98ms
step:666/2330 train_time:25299ms step_avg:37.99ms
step:667/2330 train_time:25334ms step_avg:37.98ms
step:668/2330 train_time:25374ms step_avg:37.99ms
step:669/2330 train_time:25410ms step_avg:37.98ms
step:670/2330 train_time:25452ms step_avg:37.99ms
step:671/2330 train_time:25486ms step_avg:37.98ms
step:672/2330 train_time:25528ms step_avg:37.99ms
step:673/2330 train_time:25562ms step_avg:37.98ms
step:674/2330 train_time:25603ms step_avg:37.99ms
step:675/2330 train_time:25638ms step_avg:37.98ms
step:676/2330 train_time:25680ms step_avg:37.99ms
step:677/2330 train_time:25714ms step_avg:37.98ms
step:678/2330 train_time:25755ms step_avg:37.99ms
step:679/2330 train_time:25790ms step_avg:37.98ms
step:680/2330 train_time:25832ms step_avg:37.99ms
step:681/2330 train_time:25867ms step_avg:37.98ms
step:682/2330 train_time:25908ms step_avg:37.99ms
step:683/2330 train_time:25942ms step_avg:37.98ms
step:684/2330 train_time:25983ms step_avg:37.99ms
step:685/2330 train_time:26019ms step_avg:37.98ms
step:686/2330 train_time:26060ms step_avg:37.99ms
step:687/2330 train_time:26094ms step_avg:37.98ms
step:688/2330 train_time:26135ms step_avg:37.99ms
step:689/2330 train_time:26170ms step_avg:37.98ms
step:690/2330 train_time:26210ms step_avg:37.99ms
step:691/2330 train_time:26246ms step_avg:37.98ms
step:692/2330 train_time:26287ms step_avg:37.99ms
step:693/2330 train_time:26323ms step_avg:37.98ms
step:694/2330 train_time:26364ms step_avg:37.99ms
step:695/2330 train_time:26399ms step_avg:37.98ms
step:696/2330 train_time:26441ms step_avg:37.99ms
step:697/2330 train_time:26476ms step_avg:37.99ms
step:698/2330 train_time:26517ms step_avg:37.99ms
step:699/2330 train_time:26552ms step_avg:37.99ms
step:700/2330 train_time:26593ms step_avg:37.99ms
step:701/2330 train_time:26628ms step_avg:37.99ms
step:702/2330 train_time:26669ms step_avg:37.99ms
step:703/2330 train_time:26704ms step_avg:37.99ms
step:704/2330 train_time:26745ms step_avg:37.99ms
step:705/2330 train_time:26780ms step_avg:37.99ms
step:706/2330 train_time:26821ms step_avg:37.99ms
step:707/2330 train_time:26856ms step_avg:37.99ms
step:708/2330 train_time:26897ms step_avg:37.99ms
step:709/2330 train_time:26931ms step_avg:37.99ms
step:710/2330 train_time:26972ms step_avg:37.99ms
step:711/2330 train_time:27008ms step_avg:37.99ms
step:712/2330 train_time:27049ms step_avg:37.99ms
step:713/2330 train_time:27085ms step_avg:37.99ms
step:714/2330 train_time:27126ms step_avg:37.99ms
step:715/2330 train_time:27161ms step_avg:37.99ms
step:716/2330 train_time:27203ms step_avg:37.99ms
step:717/2330 train_time:27237ms step_avg:37.99ms
step:718/2330 train_time:27279ms step_avg:37.99ms
step:719/2330 train_time:27313ms step_avg:37.99ms
step:720/2330 train_time:27354ms step_avg:37.99ms
step:721/2330 train_time:27389ms step_avg:37.99ms
step:722/2330 train_time:27431ms step_avg:37.99ms
step:723/2330 train_time:27466ms step_avg:37.99ms
step:724/2330 train_time:27507ms step_avg:37.99ms
step:725/2330 train_time:27543ms step_avg:37.99ms
step:726/2330 train_time:27584ms step_avg:37.99ms
step:727/2330 train_time:27618ms step_avg:37.99ms
step:728/2330 train_time:27660ms step_avg:37.99ms
step:729/2330 train_time:27695ms step_avg:37.99ms
step:730/2330 train_time:27736ms step_avg:37.99ms
step:731/2330 train_time:27770ms step_avg:37.99ms
step:732/2330 train_time:27811ms step_avg:37.99ms
step:733/2330 train_time:27846ms step_avg:37.99ms
step:734/2330 train_time:27887ms step_avg:37.99ms
step:735/2330 train_time:27923ms step_avg:37.99ms
step:736/2330 train_time:27964ms step_avg:37.99ms
step:737/2330 train_time:27999ms step_avg:37.99ms
step:738/2330 train_time:28040ms step_avg:38.00ms
step:739/2330 train_time:28075ms step_avg:37.99ms
step:740/2330 train_time:28116ms step_avg:38.00ms
step:741/2330 train_time:28151ms step_avg:37.99ms
step:742/2330 train_time:28192ms step_avg:37.99ms
step:743/2330 train_time:28227ms step_avg:37.99ms
step:744/2330 train_time:28269ms step_avg:38.00ms
step:745/2330 train_time:28304ms step_avg:37.99ms
step:746/2330 train_time:28345ms step_avg:38.00ms
step:747/2330 train_time:28381ms step_avg:37.99ms
step:748/2330 train_time:28423ms step_avg:38.00ms
step:749/2330 train_time:28458ms step_avg:37.99ms
step:750/2330 train_time:28500ms step_avg:38.00ms
step:750/2330 val_loss:5.5645 train_time:28611ms step_avg:38.15ms
step:751/2330 train_time:28622ms step_avg:38.11ms
step:752/2330 train_time:28632ms step_avg:38.07ms
step:753/2330 train_time:28642ms step_avg:38.04ms
step:754/2330 train_time:28653ms step_avg:38.00ms
step:755/2330 train_time:28687ms step_avg:38.00ms
step:756/2330 train_time:28728ms step_avg:38.00ms
step:757/2330 train_time:28762ms step_avg:38.00ms
step:758/2330 train_time:28803ms step_avg:38.00ms
step:759/2330 train_time:28837ms step_avg:37.99ms
step:760/2330 train_time:28878ms step_avg:38.00ms
step:761/2330 train_time:28913ms step_avg:37.99ms
step:762/2330 train_time:28956ms step_avg:38.00ms
step:763/2330 train_time:28993ms step_avg:38.00ms
step:764/2330 train_time:29035ms step_avg:38.00ms
step:765/2330 train_time:29070ms step_avg:38.00ms
step:766/2330 train_time:29112ms step_avg:38.01ms
step:767/2330 train_time:29146ms step_avg:38.00ms
step:768/2330 train_time:29187ms step_avg:38.00ms
step:769/2330 train_time:29222ms step_avg:38.00ms
step:770/2330 train_time:29263ms step_avg:38.00ms
step:771/2330 train_time:29298ms step_avg:38.00ms
step:772/2330 train_time:29339ms step_avg:38.00ms
step:773/2330 train_time:29373ms step_avg:38.00ms
step:774/2330 train_time:29414ms step_avg:38.00ms
step:775/2330 train_time:29449ms step_avg:38.00ms
step:776/2330 train_time:29489ms step_avg:38.00ms
step:777/2330 train_time:29525ms step_avg:38.00ms
step:778/2330 train_time:29566ms step_avg:38.00ms
step:779/2330 train_time:29602ms step_avg:38.00ms
step:780/2330 train_time:29643ms step_avg:38.00ms
step:781/2330 train_time:29679ms step_avg:38.00ms
step:782/2330 train_time:29720ms step_avg:38.00ms
step:783/2330 train_time:29755ms step_avg:38.00ms
step:784/2330 train_time:29796ms step_avg:38.01ms
step:785/2330 train_time:29831ms step_avg:38.00ms
step:786/2330 train_time:29872ms step_avg:38.00ms
step:787/2330 train_time:29907ms step_avg:38.00ms
step:788/2330 train_time:29948ms step_avg:38.00ms
step:789/2330 train_time:29985ms step_avg:38.00ms
step:790/2330 train_time:30026ms step_avg:38.01ms
step:791/2330 train_time:30062ms step_avg:38.00ms
step:792/2330 train_time:30104ms step_avg:38.01ms
step:793/2330 train_time:30138ms step_avg:38.01ms
step:794/2330 train_time:30179ms step_avg:38.01ms
step:795/2330 train_time:30214ms step_avg:38.00ms
step:796/2330 train_time:30255ms step_avg:38.01ms
step:797/2330 train_time:30289ms step_avg:38.00ms
step:798/2330 train_time:30330ms step_avg:38.01ms
step:799/2330 train_time:30365ms step_avg:38.00ms
step:800/2330 train_time:30406ms step_avg:38.01ms
step:801/2330 train_time:30441ms step_avg:38.00ms
step:802/2330 train_time:30481ms step_avg:38.01ms
step:803/2330 train_time:30518ms step_avg:38.00ms
step:804/2330 train_time:30559ms step_avg:38.01ms
step:805/2330 train_time:30595ms step_avg:38.01ms
step:806/2330 train_time:30636ms step_avg:38.01ms
step:807/2330 train_time:30671ms step_avg:38.01ms
step:808/2330 train_time:30712ms step_avg:38.01ms
step:809/2330 train_time:30747ms step_avg:38.01ms
step:810/2330 train_time:30787ms step_avg:38.01ms
step:811/2330 train_time:30823ms step_avg:38.01ms
step:812/2330 train_time:30865ms step_avg:38.01ms
step:813/2330 train_time:30899ms step_avg:38.01ms
step:814/2330 train_time:30940ms step_avg:38.01ms
step:815/2330 train_time:30976ms step_avg:38.01ms
step:816/2330 train_time:31017ms step_avg:38.01ms
step:817/2330 train_time:31054ms step_avg:38.01ms
step:818/2330 train_time:31095ms step_avg:38.01ms
step:819/2330 train_time:31130ms step_avg:38.01ms
step:820/2330 train_time:31171ms step_avg:38.01ms
step:821/2330 train_time:31206ms step_avg:38.01ms
step:822/2330 train_time:31247ms step_avg:38.01ms
step:823/2330 train_time:31281ms step_avg:38.01ms
step:824/2330 train_time:31322ms step_avg:38.01ms
step:825/2330 train_time:31357ms step_avg:38.01ms
step:826/2330 train_time:31398ms step_avg:38.01ms
step:827/2330 train_time:31433ms step_avg:38.01ms
step:828/2330 train_time:31475ms step_avg:38.01ms
step:829/2330 train_time:31510ms step_avg:38.01ms
step:830/2330 train_time:31551ms step_avg:38.01ms
step:831/2330 train_time:31586ms step_avg:38.01ms
step:832/2330 train_time:31627ms step_avg:38.01ms
step:833/2330 train_time:31664ms step_avg:38.01ms
step:834/2330 train_time:31705ms step_avg:38.02ms
step:835/2330 train_time:31741ms step_avg:38.01ms
step:836/2330 train_time:31782ms step_avg:38.02ms
step:837/2330 train_time:31819ms step_avg:38.02ms
step:838/2330 train_time:31860ms step_avg:38.02ms
step:839/2330 train_time:31897ms step_avg:38.02ms
step:840/2330 train_time:31938ms step_avg:38.02ms
step:841/2330 train_time:31974ms step_avg:38.02ms
step:842/2330 train_time:32015ms step_avg:38.02ms
step:843/2330 train_time:32051ms step_avg:38.02ms
step:844/2330 train_time:32092ms step_avg:38.02ms
step:845/2330 train_time:32128ms step_avg:38.02ms
step:846/2330 train_time:32169ms step_avg:38.02ms
step:847/2330 train_time:32205ms step_avg:38.02ms
step:848/2330 train_time:32246ms step_avg:38.03ms
step:849/2330 train_time:32282ms step_avg:38.02ms
step:850/2330 train_time:32323ms step_avg:38.03ms
step:851/2330 train_time:32359ms step_avg:38.02ms
step:852/2330 train_time:32400ms step_avg:38.03ms
step:853/2330 train_time:32436ms step_avg:38.03ms
step:854/2330 train_time:32477ms step_avg:38.03ms
step:855/2330 train_time:32513ms step_avg:38.03ms
step:856/2330 train_time:32554ms step_avg:38.03ms
step:857/2330 train_time:32590ms step_avg:38.03ms
step:858/2330 train_time:32631ms step_avg:38.03ms
step:859/2330 train_time:32666ms step_avg:38.03ms
step:860/2330 train_time:32707ms step_avg:38.03ms
step:861/2330 train_time:32743ms step_avg:38.03ms
step:862/2330 train_time:32784ms step_avg:38.03ms
step:863/2330 train_time:32819ms step_avg:38.03ms
step:864/2330 train_time:32860ms step_avg:38.03ms
step:865/2330 train_time:32896ms step_avg:38.03ms
step:866/2330 train_time:32938ms step_avg:38.03ms
step:867/2330 train_time:32973ms step_avg:38.03ms
step:868/2330 train_time:33015ms step_avg:38.04ms
step:869/2330 train_time:33050ms step_avg:38.03ms
step:870/2330 train_time:33091ms step_avg:38.04ms
step:871/2330 train_time:33126ms step_avg:38.03ms
step:872/2330 train_time:33167ms step_avg:38.04ms
step:873/2330 train_time:33203ms step_avg:38.03ms
step:874/2330 train_time:33243ms step_avg:38.04ms
step:875/2330 train_time:33280ms step_avg:38.03ms
step:876/2330 train_time:33320ms step_avg:38.04ms
step:877/2330 train_time:33356ms step_avg:38.03ms
step:878/2330 train_time:33397ms step_avg:38.04ms
step:879/2330 train_time:33432ms step_avg:38.03ms
step:880/2330 train_time:33473ms step_avg:38.04ms
step:881/2330 train_time:33509ms step_avg:38.04ms
step:882/2330 train_time:33550ms step_avg:38.04ms
step:883/2330 train_time:33585ms step_avg:38.03ms
step:884/2330 train_time:33626ms step_avg:38.04ms
step:885/2330 train_time:33662ms step_avg:38.04ms
step:886/2330 train_time:33704ms step_avg:38.04ms
step:887/2330 train_time:33739ms step_avg:38.04ms
step:888/2330 train_time:33780ms step_avg:38.04ms
step:889/2330 train_time:33816ms step_avg:38.04ms
step:890/2330 train_time:33857ms step_avg:38.04ms
step:891/2330 train_time:33893ms step_avg:38.04ms
step:892/2330 train_time:33934ms step_avg:38.04ms
step:893/2330 train_time:33969ms step_avg:38.04ms
step:894/2330 train_time:34011ms step_avg:38.04ms
step:895/2330 train_time:34046ms step_avg:38.04ms
step:896/2330 train_time:34087ms step_avg:38.04ms
step:897/2330 train_time:34123ms step_avg:38.04ms
step:898/2330 train_time:34164ms step_avg:38.04ms
step:899/2330 train_time:34201ms step_avg:38.04ms
step:900/2330 train_time:34242ms step_avg:38.05ms
step:901/2330 train_time:34278ms step_avg:38.04ms
step:902/2330 train_time:34318ms step_avg:38.05ms
step:903/2330 train_time:34355ms step_avg:38.04ms
step:904/2330 train_time:34395ms step_avg:38.05ms
step:905/2330 train_time:34432ms step_avg:38.05ms
step:906/2330 train_time:34472ms step_avg:38.05ms
step:907/2330 train_time:34508ms step_avg:38.05ms
step:908/2330 train_time:34549ms step_avg:38.05ms
step:909/2330 train_time:34585ms step_avg:38.05ms
step:910/2330 train_time:34626ms step_avg:38.05ms
step:911/2330 train_time:34662ms step_avg:38.05ms
step:912/2330 train_time:34703ms step_avg:38.05ms
step:913/2330 train_time:34738ms step_avg:38.05ms
step:914/2330 train_time:34779ms step_avg:38.05ms
step:915/2330 train_time:34814ms step_avg:38.05ms
step:916/2330 train_time:34856ms step_avg:38.05ms
step:917/2330 train_time:34891ms step_avg:38.05ms
step:918/2330 train_time:34932ms step_avg:38.05ms
step:919/2330 train_time:34968ms step_avg:38.05ms
step:920/2330 train_time:35009ms step_avg:38.05ms
step:921/2330 train_time:35044ms step_avg:38.05ms
step:922/2330 train_time:35085ms step_avg:38.05ms
step:923/2330 train_time:35121ms step_avg:38.05ms
step:924/2330 train_time:35162ms step_avg:38.05ms
step:925/2330 train_time:35197ms step_avg:38.05ms
step:926/2330 train_time:35238ms step_avg:38.05ms
step:927/2330 train_time:35274ms step_avg:38.05ms
step:928/2330 train_time:35315ms step_avg:38.05ms
step:929/2330 train_time:35351ms step_avg:38.05ms
step:930/2330 train_time:35391ms step_avg:38.06ms
step:931/2330 train_time:35428ms step_avg:38.05ms
step:932/2330 train_time:35468ms step_avg:38.06ms
step:933/2330 train_time:35504ms step_avg:38.05ms
step:934/2330 train_time:35545ms step_avg:38.06ms
step:935/2330 train_time:35581ms step_avg:38.05ms
step:936/2330 train_time:35623ms step_avg:38.06ms
step:937/2330 train_time:35658ms step_avg:38.06ms
step:938/2330 train_time:35700ms step_avg:38.06ms
step:939/2330 train_time:35735ms step_avg:38.06ms
step:940/2330 train_time:35776ms step_avg:38.06ms
step:941/2330 train_time:35812ms step_avg:38.06ms
step:942/2330 train_time:35853ms step_avg:38.06ms
step:943/2330 train_time:35888ms step_avg:38.06ms
step:944/2330 train_time:35929ms step_avg:38.06ms
step:945/2330 train_time:35965ms step_avg:38.06ms
step:946/2330 train_time:36007ms step_avg:38.06ms
step:947/2330 train_time:36042ms step_avg:38.06ms
step:948/2330 train_time:36083ms step_avg:38.06ms
step:949/2330 train_time:36118ms step_avg:38.06ms
step:950/2330 train_time:36159ms step_avg:38.06ms
step:951/2330 train_time:36194ms step_avg:38.06ms
step:952/2330 train_time:36235ms step_avg:38.06ms
step:953/2330 train_time:36272ms step_avg:38.06ms
step:954/2330 train_time:36313ms step_avg:38.06ms
step:955/2330 train_time:36349ms step_avg:38.06ms
step:956/2330 train_time:36389ms step_avg:38.06ms
step:957/2330 train_time:36425ms step_avg:38.06ms
step:958/2330 train_time:36467ms step_avg:38.07ms
step:959/2330 train_time:36502ms step_avg:38.06ms
step:960/2330 train_time:36544ms step_avg:38.07ms
step:961/2330 train_time:36579ms step_avg:38.06ms
step:962/2330 train_time:36620ms step_avg:38.07ms
step:963/2330 train_time:36656ms step_avg:38.06ms
step:964/2330 train_time:36697ms step_avg:38.07ms
step:965/2330 train_time:36732ms step_avg:38.06ms
step:966/2330 train_time:36773ms step_avg:38.07ms
step:967/2330 train_time:36810ms step_avg:38.07ms
step:968/2330 train_time:36851ms step_avg:38.07ms
step:969/2330 train_time:36886ms step_avg:38.07ms
step:970/2330 train_time:36927ms step_avg:38.07ms
step:971/2330 train_time:36963ms step_avg:38.07ms
step:972/2330 train_time:37004ms step_avg:38.07ms
step:973/2330 train_time:37040ms step_avg:38.07ms
step:974/2330 train_time:37081ms step_avg:38.07ms
step:975/2330 train_time:37116ms step_avg:38.07ms
step:976/2330 train_time:37157ms step_avg:38.07ms
step:977/2330 train_time:37192ms step_avg:38.07ms
step:978/2330 train_time:37234ms step_avg:38.07ms
step:979/2330 train_time:37269ms step_avg:38.07ms
step:980/2330 train_time:37309ms step_avg:38.07ms
step:981/2330 train_time:37345ms step_avg:38.07ms
step:982/2330 train_time:37386ms step_avg:38.07ms
step:983/2330 train_time:37422ms step_avg:38.07ms
step:984/2330 train_time:37464ms step_avg:38.07ms
step:985/2330 train_time:37499ms step_avg:38.07ms
step:986/2330 train_time:37540ms step_avg:38.07ms
step:987/2330 train_time:37576ms step_avg:38.07ms
step:988/2330 train_time:37617ms step_avg:38.07ms
step:989/2330 train_time:37653ms step_avg:38.07ms
step:990/2330 train_time:37694ms step_avg:38.07ms
step:991/2330 train_time:37730ms step_avg:38.07ms
step:992/2330 train_time:37770ms step_avg:38.07ms
step:993/2330 train_time:37806ms step_avg:38.07ms
step:994/2330 train_time:37847ms step_avg:38.08ms
step:995/2330 train_time:37884ms step_avg:38.07ms
step:996/2330 train_time:37925ms step_avg:38.08ms
step:997/2330 train_time:37960ms step_avg:38.07ms
step:998/2330 train_time:38001ms step_avg:38.08ms
step:999/2330 train_time:38036ms step_avg:38.07ms
step:1000/2330 train_time:38077ms step_avg:38.08ms
step:1000/2330 val_loss:5.5031 train_time:38190ms step_avg:38.19ms
step:1001/2330 train_time:38202ms step_avg:38.16ms
step:1002/2330 train_time:38212ms step_avg:38.14ms
step:1003/2330 train_time:38221ms step_avg:38.11ms
step:1004/2330 train_time:38233ms step_avg:38.08ms
step:1005/2330 train_time:38269ms step_avg:38.08ms
step:1006/2330 train_time:38309ms step_avg:38.08ms
step:1007/2330 train_time:38344ms step_avg:38.08ms
step:1008/2330 train_time:38384ms step_avg:38.08ms
step:1009/2330 train_time:38419ms step_avg:38.08ms
step:1010/2330 train_time:38460ms step_avg:38.08ms
step:1011/2330 train_time:38497ms step_avg:38.08ms
step:1012/2330 train_time:38540ms step_avg:38.08ms
step:1013/2330 train_time:38577ms step_avg:38.08ms
step:1014/2330 train_time:38618ms step_avg:38.09ms
step:1015/2330 train_time:38656ms step_avg:38.08ms
step:1016/2330 train_time:38697ms step_avg:38.09ms
step:1017/2330 train_time:38733ms step_avg:38.09ms
step:1018/2330 train_time:38775ms step_avg:38.09ms
step:1019/2330 train_time:38811ms step_avg:38.09ms
step:1020/2330 train_time:38852ms step_avg:38.09ms
step:1021/2330 train_time:38887ms step_avg:38.09ms
step:1022/2330 train_time:38928ms step_avg:38.09ms
step:1023/2330 train_time:38962ms step_avg:38.09ms
step:1024/2330 train_time:39004ms step_avg:38.09ms
step:1025/2330 train_time:39038ms step_avg:38.09ms
step:1026/2330 train_time:39079ms step_avg:38.09ms
step:1027/2330 train_time:39115ms step_avg:38.09ms
step:1028/2330 train_time:39157ms step_avg:38.09ms
step:1029/2330 train_time:39192ms step_avg:38.09ms
step:1030/2330 train_time:39234ms step_avg:38.09ms
step:1031/2330 train_time:39269ms step_avg:38.09ms
step:1032/2330 train_time:39310ms step_avg:38.09ms
step:1033/2330 train_time:39344ms step_avg:38.09ms
step:1034/2330 train_time:39385ms step_avg:38.09ms
step:1035/2330 train_time:39420ms step_avg:38.09ms
step:1036/2330 train_time:39463ms step_avg:38.09ms
step:1037/2330 train_time:39497ms step_avg:38.09ms
step:1038/2330 train_time:39539ms step_avg:38.09ms
step:1039/2330 train_time:39575ms step_avg:38.09ms
step:1040/2330 train_time:39616ms step_avg:38.09ms
step:1041/2330 train_time:39653ms step_avg:38.09ms
step:1042/2330 train_time:39694ms step_avg:38.09ms
step:1043/2330 train_time:39730ms step_avg:38.09ms
step:1044/2330 train_time:39771ms step_avg:38.10ms
step:1045/2330 train_time:39807ms step_avg:38.09ms
step:1046/2330 train_time:39848ms step_avg:38.10ms
step:1047/2330 train_time:39883ms step_avg:38.09ms
step:1048/2330 train_time:39924ms step_avg:38.10ms
step:1049/2330 train_time:39960ms step_avg:38.09ms
step:1050/2330 train_time:40001ms step_avg:38.10ms
step:1051/2330 train_time:40036ms step_avg:38.09ms
step:1052/2330 train_time:40077ms step_avg:38.10ms
step:1053/2330 train_time:40113ms step_avg:38.09ms
step:1054/2330 train_time:40155ms step_avg:38.10ms
step:1055/2330 train_time:40190ms step_avg:38.10ms
step:1056/2330 train_time:40232ms step_avg:38.10ms
step:1057/2330 train_time:40267ms step_avg:38.10ms
step:1058/2330 train_time:40309ms step_avg:38.10ms
step:1059/2330 train_time:40344ms step_avg:38.10ms
step:1060/2330 train_time:40385ms step_avg:38.10ms
step:1061/2330 train_time:40420ms step_avg:38.10ms
step:1062/2330 train_time:40462ms step_avg:38.10ms
step:1063/2330 train_time:40497ms step_avg:38.10ms
step:1064/2330 train_time:40538ms step_avg:38.10ms
step:1065/2330 train_time:40574ms step_avg:38.10ms
step:1066/2330 train_time:40616ms step_avg:38.10ms
step:1067/2330 train_time:40652ms step_avg:38.10ms
step:1068/2330 train_time:40693ms step_avg:38.10ms
step:1069/2330 train_time:40729ms step_avg:38.10ms
step:1070/2330 train_time:40770ms step_avg:38.10ms
step:1071/2330 train_time:40806ms step_avg:38.10ms
step:1072/2330 train_time:40847ms step_avg:38.10ms
step:1073/2330 train_time:40882ms step_avg:38.10ms
step:1074/2330 train_time:40924ms step_avg:38.10ms
step:1075/2330 train_time:40959ms step_avg:38.10ms
step:1076/2330 train_time:41001ms step_avg:38.10ms
step:1077/2330 train_time:41036ms step_avg:38.10ms
step:1078/2330 train_time:41077ms step_avg:38.10ms
step:1079/2330 train_time:41113ms step_avg:38.10ms
step:1080/2330 train_time:41155ms step_avg:38.11ms
step:1081/2330 train_time:41190ms step_avg:38.10ms
step:1082/2330 train_time:41232ms step_avg:38.11ms
step:1083/2330 train_time:41267ms step_avg:38.10ms
step:1084/2330 train_time:41309ms step_avg:38.11ms
step:1085/2330 train_time:41344ms step_avg:38.10ms
step:1086/2330 train_time:41385ms step_avg:38.11ms
step:1087/2330 train_time:41421ms step_avg:38.11ms
step:1088/2330 train_time:41462ms step_avg:38.11ms
step:1089/2330 train_time:41499ms step_avg:38.11ms
step:1090/2330 train_time:41540ms step_avg:38.11ms
step:1091/2330 train_time:41577ms step_avg:38.11ms
step:1092/2330 train_time:41618ms step_avg:38.11ms
step:1093/2330 train_time:41655ms step_avg:38.11ms
step:1094/2330 train_time:41696ms step_avg:38.11ms
step:1095/2330 train_time:41732ms step_avg:38.11ms
step:1096/2330 train_time:41774ms step_avg:38.11ms
step:1097/2330 train_time:41810ms step_avg:38.11ms
step:1098/2330 train_time:41851ms step_avg:38.12ms
step:1099/2330 train_time:41886ms step_avg:38.11ms
step:1100/2330 train_time:41927ms step_avg:38.12ms
step:1101/2330 train_time:41963ms step_avg:38.11ms
step:1102/2330 train_time:42004ms step_avg:38.12ms
step:1103/2330 train_time:42039ms step_avg:38.11ms
step:1104/2330 train_time:42080ms step_avg:38.12ms
step:1105/2330 train_time:42116ms step_avg:38.11ms
step:1106/2330 train_time:42157ms step_avg:38.12ms
step:1107/2330 train_time:42193ms step_avg:38.11ms
step:1108/2330 train_time:42234ms step_avg:38.12ms
step:1109/2330 train_time:42270ms step_avg:38.12ms
step:1110/2330 train_time:42311ms step_avg:38.12ms
step:1111/2330 train_time:42346ms step_avg:38.12ms
step:1112/2330 train_time:42387ms step_avg:38.12ms
step:1113/2330 train_time:42422ms step_avg:38.12ms
step:1114/2330 train_time:42464ms step_avg:38.12ms
step:1115/2330 train_time:42499ms step_avg:38.12ms
step:1116/2330 train_time:42540ms step_avg:38.12ms
step:1117/2330 train_time:42576ms step_avg:38.12ms
step:1118/2330 train_time:42618ms step_avg:38.12ms
step:1119/2330 train_time:42654ms step_avg:38.12ms
step:1120/2330 train_time:42696ms step_avg:38.12ms
step:1121/2330 train_time:42732ms step_avg:38.12ms
step:1122/2330 train_time:42773ms step_avg:38.12ms
step:1123/2330 train_time:42808ms step_avg:38.12ms
step:1124/2330 train_time:42850ms step_avg:38.12ms
step:1125/2330 train_time:42885ms step_avg:38.12ms
step:1126/2330 train_time:42925ms step_avg:38.12ms
step:1127/2330 train_time:42961ms step_avg:38.12ms
step:1128/2330 train_time:43002ms step_avg:38.12ms
step:1129/2330 train_time:43037ms step_avg:38.12ms
step:1130/2330 train_time:43078ms step_avg:38.12ms
step:1131/2330 train_time:43114ms step_avg:38.12ms
step:1132/2330 train_time:43155ms step_avg:38.12ms
step:1133/2330 train_time:43191ms step_avg:38.12ms
step:1134/2330 train_time:43232ms step_avg:38.12ms
step:1135/2330 train_time:43268ms step_avg:38.12ms
step:1136/2330 train_time:43308ms step_avg:38.12ms
step:1137/2330 train_time:43344ms step_avg:38.12ms
step:1138/2330 train_time:43385ms step_avg:38.12ms
step:1139/2330 train_time:43421ms step_avg:38.12ms
step:1140/2330 train_time:43463ms step_avg:38.13ms
step:1141/2330 train_time:43498ms step_avg:38.12ms
step:1142/2330 train_time:43539ms step_avg:38.13ms
step:1143/2330 train_time:43576ms step_avg:38.12ms
step:1144/2330 train_time:43617ms step_avg:38.13ms
step:1145/2330 train_time:43653ms step_avg:38.12ms
step:1146/2330 train_time:43694ms step_avg:38.13ms
step:1147/2330 train_time:43731ms step_avg:38.13ms
step:1148/2330 train_time:43772ms step_avg:38.13ms
step:1149/2330 train_time:43808ms step_avg:38.13ms
step:1150/2330 train_time:43849ms step_avg:38.13ms
step:1151/2330 train_time:43885ms step_avg:38.13ms
step:1152/2330 train_time:43926ms step_avg:38.13ms
step:1153/2330 train_time:43961ms step_avg:38.13ms
step:1154/2330 train_time:44002ms step_avg:38.13ms
step:1155/2330 train_time:44037ms step_avg:38.13ms
step:1156/2330 train_time:44078ms step_avg:38.13ms
step:1157/2330 train_time:44113ms step_avg:38.13ms
step:1158/2330 train_time:44155ms step_avg:38.13ms
step:1159/2330 train_time:44190ms step_avg:38.13ms
step:1160/2330 train_time:44232ms step_avg:38.13ms
step:1161/2330 train_time:44266ms step_avg:38.13ms
step:1162/2330 train_time:44307ms step_avg:38.13ms
step:1163/2330 train_time:44343ms step_avg:38.13ms
step:1164/2330 train_time:44384ms step_avg:38.13ms
step:1165/2330 train_time:44420ms step_avg:38.13ms
step:1166/2330 train_time:44462ms step_avg:38.13ms
step:1167/2330 train_time:44498ms step_avg:38.13ms
step:1168/2330 train_time:44538ms step_avg:38.13ms
step:1169/2330 train_time:44574ms step_avg:38.13ms
step:1170/2330 train_time:44616ms step_avg:38.13ms
step:1171/2330 train_time:44651ms step_avg:38.13ms
step:1172/2330 train_time:44692ms step_avg:38.13ms
step:1173/2330 train_time:44728ms step_avg:38.13ms
step:1174/2330 train_time:44769ms step_avg:38.13ms
step:1175/2330 train_time:44804ms step_avg:38.13ms
step:1176/2330 train_time:44845ms step_avg:38.13ms
step:1177/2330 train_time:44880ms step_avg:38.13ms
step:1178/2330 train_time:44922ms step_avg:38.13ms
step:1179/2330 train_time:44957ms step_avg:38.13ms
step:1180/2330 train_time:44998ms step_avg:38.13ms
step:1181/2330 train_time:45034ms step_avg:38.13ms
step:1182/2330 train_time:45075ms step_avg:38.13ms
step:1183/2330 train_time:45111ms step_avg:38.13ms
step:1184/2330 train_time:45153ms step_avg:38.14ms
step:1185/2330 train_time:45188ms step_avg:38.13ms
step:1186/2330 train_time:45229ms step_avg:38.14ms
step:1187/2330 train_time:45264ms step_avg:38.13ms
step:1188/2330 train_time:45305ms step_avg:38.14ms
step:1189/2330 train_time:45341ms step_avg:38.13ms
step:1190/2330 train_time:45383ms step_avg:38.14ms
step:1191/2330 train_time:45419ms step_avg:38.14ms
step:1192/2330 train_time:45460ms step_avg:38.14ms
step:1193/2330 train_time:45498ms step_avg:38.14ms
step:1194/2330 train_time:45538ms step_avg:38.14ms
step:1195/2330 train_time:45574ms step_avg:38.14ms
step:1196/2330 train_time:45615ms step_avg:38.14ms
step:1197/2330 train_time:45651ms step_avg:38.14ms
step:1198/2330 train_time:45692ms step_avg:38.14ms
step:1199/2330 train_time:45728ms step_avg:38.14ms
step:1200/2330 train_time:45769ms step_avg:38.14ms
step:1201/2330 train_time:45806ms step_avg:38.14ms
step:1202/2330 train_time:45847ms step_avg:38.14ms
step:1203/2330 train_time:45882ms step_avg:38.14ms
step:1204/2330 train_time:45924ms step_avg:38.14ms
step:1205/2330 train_time:45959ms step_avg:38.14ms
step:1206/2330 train_time:46001ms step_avg:38.14ms
step:1207/2330 train_time:46036ms step_avg:38.14ms
step:1208/2330 train_time:46077ms step_avg:38.14ms
step:1209/2330 train_time:46113ms step_avg:38.14ms
step:1210/2330 train_time:46154ms step_avg:38.14ms
step:1211/2330 train_time:46190ms step_avg:38.14ms
step:1212/2330 train_time:46231ms step_avg:38.14ms
step:1213/2330 train_time:46265ms step_avg:38.14ms
step:1214/2330 train_time:46306ms step_avg:38.14ms
step:1215/2330 train_time:46342ms step_avg:38.14ms
step:1216/2330 train_time:46383ms step_avg:38.14ms
step:1217/2330 train_time:46419ms step_avg:38.14ms
step:1218/2330 train_time:46460ms step_avg:38.14ms
step:1219/2330 train_time:46496ms step_avg:38.14ms
step:1220/2330 train_time:46537ms step_avg:38.15ms
step:1221/2330 train_time:46573ms step_avg:38.14ms
step:1222/2330 train_time:46615ms step_avg:38.15ms
step:1223/2330 train_time:46650ms step_avg:38.14ms
step:1224/2330 train_time:46692ms step_avg:38.15ms
step:1225/2330 train_time:46727ms step_avg:38.14ms
step:1226/2330 train_time:46769ms step_avg:38.15ms
step:1227/2330 train_time:46806ms step_avg:38.15ms
step:1228/2330 train_time:46847ms step_avg:38.15ms
step:1229/2330 train_time:46882ms step_avg:38.15ms
step:1230/2330 train_time:46923ms step_avg:38.15ms
step:1231/2330 train_time:46960ms step_avg:38.15ms
step:1232/2330 train_time:47001ms step_avg:38.15ms
step:1233/2330 train_time:47035ms step_avg:38.15ms
step:1234/2330 train_time:47076ms step_avg:38.15ms
step:1235/2330 train_time:47112ms step_avg:38.15ms
step:1236/2330 train_time:47153ms step_avg:38.15ms
step:1237/2330 train_time:47188ms step_avg:38.15ms
step:1238/2330 train_time:47229ms step_avg:38.15ms
step:1239/2330 train_time:47265ms step_avg:38.15ms
step:1240/2330 train_time:47306ms step_avg:38.15ms
step:1241/2330 train_time:47341ms step_avg:38.15ms
step:1242/2330 train_time:47382ms step_avg:38.15ms
step:1243/2330 train_time:47418ms step_avg:38.15ms
step:1244/2330 train_time:47460ms step_avg:38.15ms
step:1245/2330 train_time:47495ms step_avg:38.15ms
step:1246/2330 train_time:47536ms step_avg:38.15ms
step:1247/2330 train_time:47571ms step_avg:38.15ms
step:1248/2330 train_time:47613ms step_avg:38.15ms
step:1249/2330 train_time:47648ms step_avg:38.15ms
step:1250/2330 train_time:47689ms step_avg:38.15ms
step:1250/2330 val_loss:5.5302 train_time:47801ms step_avg:38.24ms
step:1251/2330 train_time:47813ms step_avg:38.22ms
step:1252/2330 train_time:47824ms step_avg:38.20ms
step:1253/2330 train_time:47834ms step_avg:38.18ms
step:1254/2330 train_time:47844ms step_avg:38.15ms
step:1255/2330 train_time:47878ms step_avg:38.15ms
step:1256/2330 train_time:47918ms step_avg:38.15ms
step:1257/2330 train_time:47953ms step_avg:38.15ms
step:1258/2330 train_time:47993ms step_avg:38.15ms
step:1259/2330 train_time:48028ms step_avg:38.15ms
step:1260/2330 train_time:48069ms step_avg:38.15ms
step:1261/2330 train_time:48106ms step_avg:38.15ms
step:1262/2330 train_time:48148ms step_avg:38.15ms
step:1263/2330 train_time:48191ms step_avg:38.16ms
step:1264/2330 train_time:48232ms step_avg:38.16ms
step:1265/2330 train_time:48268ms step_avg:38.16ms
step:1266/2330 train_time:48309ms step_avg:38.16ms
step:1267/2330 train_time:48344ms step_avg:38.16ms
step:1268/2330 train_time:48386ms step_avg:38.16ms
step:1269/2330 train_time:48421ms step_avg:38.16ms
step:1270/2330 train_time:48461ms step_avg:38.16ms
step:1271/2330 train_time:48497ms step_avg:38.16ms
step:1272/2330 train_time:48537ms step_avg:38.16ms
step:1273/2330 train_time:48573ms step_avg:38.16ms
step:1274/2330 train_time:48613ms step_avg:38.16ms
step:1275/2330 train_time:48648ms step_avg:38.16ms
step:1276/2330 train_time:48690ms step_avg:38.16ms
step:1277/2330 train_time:48725ms step_avg:38.16ms
step:1278/2330 train_time:48767ms step_avg:38.16ms
step:1279/2330 train_time:48802ms step_avg:38.16ms
step:1280/2330 train_time:48844ms step_avg:38.16ms
step:1281/2330 train_time:48879ms step_avg:38.16ms
step:1282/2330 train_time:48920ms step_avg:38.16ms
step:1283/2330 train_time:48955ms step_avg:38.16ms
step:1284/2330 train_time:48995ms step_avg:38.16ms
step:1285/2330 train_time:49031ms step_avg:38.16ms
step:1286/2330 train_time:49073ms step_avg:38.16ms
step:1287/2330 train_time:49110ms step_avg:38.16ms
step:1288/2330 train_time:49152ms step_avg:38.16ms
step:1289/2330 train_time:49187ms step_avg:38.16ms
step:1290/2330 train_time:49230ms step_avg:38.16ms
step:1291/2330 train_time:49266ms step_avg:38.16ms
step:1292/2330 train_time:49307ms step_avg:38.16ms
step:1293/2330 train_time:49344ms step_avg:38.16ms
step:1294/2330 train_time:49385ms step_avg:38.16ms
step:1295/2330 train_time:49420ms step_avg:38.16ms
step:1296/2330 train_time:49461ms step_avg:38.16ms
step:1297/2330 train_time:49497ms step_avg:38.16ms
step:1298/2330 train_time:49538ms step_avg:38.16ms
step:1299/2330 train_time:49573ms step_avg:38.16ms
step:1300/2330 train_time:49613ms step_avg:38.16ms
step:1301/2330 train_time:49648ms step_avg:38.16ms
step:1302/2330 train_time:49689ms step_avg:38.16ms
step:1303/2330 train_time:49725ms step_avg:38.16ms
step:1304/2330 train_time:49766ms step_avg:38.16ms
step:1305/2330 train_time:49801ms step_avg:38.16ms
step:1306/2330 train_time:49842ms step_avg:38.16ms
step:1307/2330 train_time:49878ms step_avg:38.16ms
step:1308/2330 train_time:49919ms step_avg:38.16ms
step:1309/2330 train_time:49954ms step_avg:38.16ms
step:1310/2330 train_time:49995ms step_avg:38.16ms
step:1311/2330 train_time:50031ms step_avg:38.16ms
step:1312/2330 train_time:50073ms step_avg:38.17ms
step:1313/2330 train_time:50108ms step_avg:38.16ms
step:1314/2330 train_time:50150ms step_avg:38.17ms
step:1315/2330 train_time:50186ms step_avg:38.16ms
step:1316/2330 train_time:50228ms step_avg:38.17ms
step:1317/2330 train_time:50265ms step_avg:38.17ms
step:1318/2330 train_time:50306ms step_avg:38.17ms
step:1319/2330 train_time:50341ms step_avg:38.17ms
step:1320/2330 train_time:50382ms step_avg:38.17ms
step:1321/2330 train_time:50419ms step_avg:38.17ms
step:1322/2330 train_time:50460ms step_avg:38.17ms
step:1323/2330 train_time:50495ms step_avg:38.17ms
step:1324/2330 train_time:50535ms step_avg:38.17ms
step:1325/2330 train_time:50571ms step_avg:38.17ms
step:1326/2330 train_time:50612ms step_avg:38.17ms
step:1327/2330 train_time:50647ms step_avg:38.17ms
step:1328/2330 train_time:50689ms step_avg:38.17ms
step:1329/2330 train_time:50724ms step_avg:38.17ms
step:1330/2330 train_time:50765ms step_avg:38.17ms
step:1331/2330 train_time:50801ms step_avg:38.17ms
step:1332/2330 train_time:50842ms step_avg:38.17ms
step:1333/2330 train_time:50878ms step_avg:38.17ms
step:1334/2330 train_time:50919ms step_avg:38.17ms
step:1335/2330 train_time:50954ms step_avg:38.17ms
step:1336/2330 train_time:50995ms step_avg:38.17ms
step:1337/2330 train_time:51031ms step_avg:38.17ms
step:1338/2330 train_time:51072ms step_avg:38.17ms
step:1339/2330 train_time:51107ms step_avg:38.17ms
step:1340/2330 train_time:51149ms step_avg:38.17ms
step:1341/2330 train_time:51184ms step_avg:38.17ms
step:1342/2330 train_time:51226ms step_avg:38.17ms
step:1343/2330 train_time:51262ms step_avg:38.17ms
step:1344/2330 train_time:51303ms step_avg:38.17ms
step:1345/2330 train_time:51339ms step_avg:38.17ms
step:1346/2330 train_time:51381ms step_avg:38.17ms
step:1347/2330 train_time:51416ms step_avg:38.17ms
step:1348/2330 train_time:51457ms step_avg:38.17ms
step:1349/2330 train_time:51493ms step_avg:38.17ms
step:1350/2330 train_time:51534ms step_avg:38.17ms
step:1351/2330 train_time:51569ms step_avg:38.17ms
step:1352/2330 train_time:51611ms step_avg:38.17ms
step:1353/2330 train_time:51645ms step_avg:38.17ms
step:1354/2330 train_time:51687ms step_avg:38.17ms
step:1355/2330 train_time:51723ms step_avg:38.17ms
step:1356/2330 train_time:51764ms step_avg:38.17ms
step:1357/2330 train_time:51799ms step_avg:38.17ms
step:1358/2330 train_time:51840ms step_avg:38.17ms
step:1359/2330 train_time:51876ms step_avg:38.17ms
step:1360/2330 train_time:51917ms step_avg:38.17ms
step:1361/2330 train_time:51952ms step_avg:38.17ms
step:1362/2330 train_time:51993ms step_avg:38.17ms
step:1363/2330 train_time:52029ms step_avg:38.17ms
step:1364/2330 train_time:52071ms step_avg:38.18ms
step:1365/2330 train_time:52106ms step_avg:38.17ms
step:1366/2330 train_time:52147ms step_avg:38.18ms
step:1367/2330 train_time:52183ms step_avg:38.17ms
step:1368/2330 train_time:52225ms step_avg:38.18ms
step:1369/2330 train_time:52261ms step_avg:38.17ms
step:1370/2330 train_time:52303ms step_avg:38.18ms
step:1371/2330 train_time:52338ms step_avg:38.18ms
step:1372/2330 train_time:52379ms step_avg:38.18ms
step:1373/2330 train_time:52415ms step_avg:38.18ms
step:1374/2330 train_time:52456ms step_avg:38.18ms
step:1375/2330 train_time:52491ms step_avg:38.18ms
step:1376/2330 train_time:52532ms step_avg:38.18ms
step:1377/2330 train_time:52567ms step_avg:38.18ms
step:1378/2330 train_time:52609ms step_avg:38.18ms
step:1379/2330 train_time:52644ms step_avg:38.18ms
step:1380/2330 train_time:52685ms step_avg:38.18ms
step:1381/2330 train_time:52721ms step_avg:38.18ms
step:1382/2330 train_time:52762ms step_avg:38.18ms
step:1383/2330 train_time:52798ms step_avg:38.18ms
step:1384/2330 train_time:52838ms step_avg:38.18ms
step:1385/2330 train_time:52874ms step_avg:38.18ms
step:1386/2330 train_time:52914ms step_avg:38.18ms
step:1387/2330 train_time:52950ms step_avg:38.18ms
step:1388/2330 train_time:52991ms step_avg:38.18ms
step:1389/2330 train_time:53027ms step_avg:38.18ms
step:1390/2330 train_time:53068ms step_avg:38.18ms
step:1391/2330 train_time:53104ms step_avg:38.18ms
step:1392/2330 train_time:53145ms step_avg:38.18ms
step:1393/2330 train_time:53181ms step_avg:38.18ms
step:1394/2330 train_time:53223ms step_avg:38.18ms
step:1395/2330 train_time:53258ms step_avg:38.18ms
step:1396/2330 train_time:53299ms step_avg:38.18ms
step:1397/2330 train_time:53334ms step_avg:38.18ms
step:1398/2330 train_time:53376ms step_avg:38.18ms
step:1399/2330 train_time:53411ms step_avg:38.18ms
step:1400/2330 train_time:53452ms step_avg:38.18ms
step:1401/2330 train_time:53488ms step_avg:38.18ms
step:1402/2330 train_time:53530ms step_avg:38.18ms
step:1403/2330 train_time:53566ms step_avg:38.18ms
step:1404/2330 train_time:53607ms step_avg:38.18ms
step:1405/2330 train_time:53642ms step_avg:38.18ms
step:1406/2330 train_time:53683ms step_avg:38.18ms
step:1407/2330 train_time:53718ms step_avg:38.18ms
step:1408/2330 train_time:53759ms step_avg:38.18ms
step:1409/2330 train_time:53795ms step_avg:38.18ms
step:1410/2330 train_time:53836ms step_avg:38.18ms
step:1411/2330 train_time:53871ms step_avg:38.18ms
step:1412/2330 train_time:53913ms step_avg:38.18ms
step:1413/2330 train_time:53947ms step_avg:38.18ms
step:1414/2330 train_time:53988ms step_avg:38.18ms
step:1415/2330 train_time:54024ms step_avg:38.18ms
step:1416/2330 train_time:54066ms step_avg:38.18ms
step:1417/2330 train_time:54101ms step_avg:38.18ms
step:1418/2330 train_time:54143ms step_avg:38.18ms
step:1419/2330 train_time:54179ms step_avg:38.18ms
step:1420/2330 train_time:54220ms step_avg:38.18ms
step:1421/2330 train_time:54255ms step_avg:38.18ms
step:1422/2330 train_time:54296ms step_avg:38.18ms
step:1423/2330 train_time:54332ms step_avg:38.18ms
step:1424/2330 train_time:54373ms step_avg:38.18ms
step:1425/2330 train_time:54409ms step_avg:38.18ms
step:1426/2330 train_time:54450ms step_avg:38.18ms
step:1427/2330 train_time:54485ms step_avg:38.18ms
step:1428/2330 train_time:54527ms step_avg:38.18ms
step:1429/2330 train_time:54562ms step_avg:38.18ms
step:1430/2330 train_time:54604ms step_avg:38.18ms
step:1431/2330 train_time:54640ms step_avg:38.18ms
step:1432/2330 train_time:54681ms step_avg:38.18ms
step:1433/2330 train_time:54717ms step_avg:38.18ms
step:1434/2330 train_time:54758ms step_avg:38.19ms
step:1435/2330 train_time:54793ms step_avg:38.18ms
step:1436/2330 train_time:54834ms step_avg:38.19ms
step:1437/2330 train_time:54868ms step_avg:38.18ms
step:1438/2330 train_time:54909ms step_avg:38.18ms
step:1439/2330 train_time:54946ms step_avg:38.18ms
step:1440/2330 train_time:54987ms step_avg:38.19ms
step:1441/2330 train_time:55023ms step_avg:38.18ms
step:1442/2330 train_time:55064ms step_avg:38.19ms
step:1443/2330 train_time:55100ms step_avg:38.18ms
step:1444/2330 train_time:55141ms step_avg:38.19ms
step:1445/2330 train_time:55177ms step_avg:38.18ms
step:1446/2330 train_time:55218ms step_avg:38.19ms
step:1447/2330 train_time:55253ms step_avg:38.18ms
step:1448/2330 train_time:55294ms step_avg:38.19ms
step:1449/2330 train_time:55329ms step_avg:38.18ms
step:1450/2330 train_time:55371ms step_avg:38.19ms
step:1451/2330 train_time:55406ms step_avg:38.18ms
step:1452/2330 train_time:55447ms step_avg:38.19ms
step:1453/2330 train_time:55483ms step_avg:38.19ms
step:1454/2330 train_time:55524ms step_avg:38.19ms
step:1455/2330 train_time:55561ms step_avg:38.19ms
step:1456/2330 train_time:55602ms step_avg:38.19ms
step:1457/2330 train_time:55638ms step_avg:38.19ms
step:1458/2330 train_time:55679ms step_avg:38.19ms
step:1459/2330 train_time:55715ms step_avg:38.19ms
step:1460/2330 train_time:55756ms step_avg:38.19ms
step:1461/2330 train_time:55791ms step_avg:38.19ms
step:1462/2330 train_time:55832ms step_avg:38.19ms
step:1463/2330 train_time:55867ms step_avg:38.19ms
step:1464/2330 train_time:55909ms step_avg:38.19ms
step:1465/2330 train_time:55944ms step_avg:38.19ms
step:1466/2330 train_time:55985ms step_avg:38.19ms
step:1467/2330 train_time:56021ms step_avg:38.19ms
step:1468/2330 train_time:56063ms step_avg:38.19ms
step:1469/2330 train_time:56099ms step_avg:38.19ms
step:1470/2330 train_time:56140ms step_avg:38.19ms
step:1471/2330 train_time:56175ms step_avg:38.19ms
step:1472/2330 train_time:56216ms step_avg:38.19ms
step:1473/2330 train_time:56251ms step_avg:38.19ms
step:1474/2330 train_time:56293ms step_avg:38.19ms
step:1475/2330 train_time:56328ms step_avg:38.19ms
step:1476/2330 train_time:56370ms step_avg:38.19ms
step:1477/2330 train_time:56405ms step_avg:38.19ms
step:1478/2330 train_time:56446ms step_avg:38.19ms
step:1479/2330 train_time:56482ms step_avg:38.19ms
step:1480/2330 train_time:56523ms step_avg:38.19ms
step:1481/2330 train_time:56559ms step_avg:38.19ms
step:1482/2330 train_time:56601ms step_avg:38.19ms
step:1483/2330 train_time:56637ms step_avg:38.19ms
step:1484/2330 train_time:56677ms step_avg:38.19ms
step:1485/2330 train_time:56713ms step_avg:38.19ms
step:1486/2330 train_time:56754ms step_avg:38.19ms
step:1487/2330 train_time:56790ms step_avg:38.19ms
step:1488/2330 train_time:56831ms step_avg:38.19ms
step:1489/2330 train_time:56866ms step_avg:38.19ms
step:1490/2330 train_time:56908ms step_avg:38.19ms
step:1491/2330 train_time:56943ms step_avg:38.19ms
step:1492/2330 train_time:56985ms step_avg:38.19ms
step:1493/2330 train_time:57021ms step_avg:38.19ms
step:1494/2330 train_time:57063ms step_avg:38.19ms
step:1495/2330 train_time:57098ms step_avg:38.19ms
step:1496/2330 train_time:57139ms step_avg:38.19ms
step:1497/2330 train_time:57175ms step_avg:38.19ms
step:1498/2330 train_time:57216ms step_avg:38.19ms
step:1499/2330 train_time:57251ms step_avg:38.19ms
step:1500/2330 train_time:57293ms step_avg:38.20ms
step:1500/2330 val_loss:5.4495 train_time:57405ms step_avg:38.27ms
step:1501/2330 train_time:57417ms step_avg:38.25ms
step:1502/2330 train_time:57428ms step_avg:38.23ms
step:1503/2330 train_time:57437ms step_avg:38.21ms
step:1504/2330 train_time:57448ms step_avg:38.20ms
step:1505/2330 train_time:57483ms step_avg:38.19ms
step:1506/2330 train_time:57523ms step_avg:38.20ms
step:1507/2330 train_time:57558ms step_avg:38.19ms
step:1508/2330 train_time:57599ms step_avg:38.20ms
step:1509/2330 train_time:57634ms step_avg:38.19ms
step:1510/2330 train_time:57674ms step_avg:38.19ms
step:1511/2330 train_time:57710ms step_avg:38.19ms
step:1512/2330 train_time:57754ms step_avg:38.20ms
step:1513/2330 train_time:57791ms step_avg:38.20ms
step:1514/2330 train_time:57832ms step_avg:38.20ms
step:1515/2330 train_time:57869ms step_avg:38.20ms
step:1516/2330 train_time:57910ms step_avg:38.20ms
step:1517/2330 train_time:57945ms step_avg:38.20ms
step:1518/2330 train_time:57986ms step_avg:38.20ms
step:1519/2330 train_time:58022ms step_avg:38.20ms
step:1520/2330 train_time:58063ms step_avg:38.20ms
step:1521/2330 train_time:58098ms step_avg:38.20ms
step:1522/2330 train_time:58139ms step_avg:38.20ms
step:1523/2330 train_time:58174ms step_avg:38.20ms
step:1524/2330 train_time:58214ms step_avg:38.20ms
step:1525/2330 train_time:58249ms step_avg:38.20ms
step:1526/2330 train_time:58290ms step_avg:38.20ms
step:1527/2330 train_time:58325ms step_avg:38.20ms
step:1528/2330 train_time:58367ms step_avg:38.20ms
step:1529/2330 train_time:58402ms step_avg:38.20ms
step:1530/2330 train_time:58445ms step_avg:38.20ms
step:1531/2330 train_time:58479ms step_avg:38.20ms
step:1532/2330 train_time:58521ms step_avg:38.20ms
step:1533/2330 train_time:58556ms step_avg:38.20ms
step:1534/2330 train_time:58596ms step_avg:38.20ms
step:1535/2330 train_time:58632ms step_avg:38.20ms
step:1536/2330 train_time:58673ms step_avg:38.20ms
step:1537/2330 train_time:58709ms step_avg:38.20ms
step:1538/2330 train_time:58751ms step_avg:38.20ms
step:1539/2330 train_time:58787ms step_avg:38.20ms
step:1540/2330 train_time:58829ms step_avg:38.20ms
step:1541/2330 train_time:58865ms step_avg:38.20ms
step:1542/2330 train_time:58906ms step_avg:38.20ms
step:1543/2330 train_time:58942ms step_avg:38.20ms
step:1544/2330 train_time:58983ms step_avg:38.20ms
step:1545/2330 train_time:59019ms step_avg:38.20ms
step:1546/2330 train_time:59060ms step_avg:38.20ms
step:1547/2330 train_time:59095ms step_avg:38.20ms
step:1548/2330 train_time:59136ms step_avg:38.20ms
step:1549/2330 train_time:59171ms step_avg:38.20ms
step:1550/2330 train_time:59211ms step_avg:38.20ms
step:1551/2330 train_time:59246ms step_avg:38.20ms
step:1552/2330 train_time:59288ms step_avg:38.20ms
step:1553/2330 train_time:59323ms step_avg:38.20ms
step:1554/2330 train_time:59364ms step_avg:38.20ms
step:1555/2330 train_time:59399ms step_avg:38.20ms
step:1556/2330 train_time:59440ms step_avg:38.20ms
step:1557/2330 train_time:59475ms step_avg:38.20ms
step:1558/2330 train_time:59516ms step_avg:38.20ms
step:1559/2330 train_time:59552ms step_avg:38.20ms
step:1560/2330 train_time:59592ms step_avg:38.20ms
step:1561/2330 train_time:59628ms step_avg:38.20ms
step:1562/2330 train_time:59669ms step_avg:38.20ms
step:1563/2330 train_time:59705ms step_avg:38.20ms
step:1564/2330 train_time:59747ms step_avg:38.20ms
step:1565/2330 train_time:59782ms step_avg:38.20ms
step:1566/2330 train_time:59824ms step_avg:38.20ms
step:1567/2330 train_time:59861ms step_avg:38.20ms
step:1568/2330 train_time:59902ms step_avg:38.20ms
step:1569/2330 train_time:59938ms step_avg:38.20ms
step:1570/2330 train_time:59979ms step_avg:38.20ms
step:1571/2330 train_time:60014ms step_avg:38.20ms
step:1572/2330 train_time:60055ms step_avg:38.20ms
step:1573/2330 train_time:60090ms step_avg:38.20ms
step:1574/2330 train_time:60131ms step_avg:38.20ms
step:1575/2330 train_time:60166ms step_avg:38.20ms
step:1576/2330 train_time:60208ms step_avg:38.20ms
step:1577/2330 train_time:60243ms step_avg:38.20ms
step:1578/2330 train_time:60284ms step_avg:38.20ms
step:1579/2330 train_time:60320ms step_avg:38.20ms
step:1580/2330 train_time:60362ms step_avg:38.20ms
step:1581/2330 train_time:60396ms step_avg:38.20ms
step:1582/2330 train_time:60438ms step_avg:38.20ms
step:1583/2330 train_time:60472ms step_avg:38.20ms
step:1584/2330 train_time:60513ms step_avg:38.20ms
step:1585/2330 train_time:60548ms step_avg:38.20ms
step:1586/2330 train_time:60590ms step_avg:38.20ms
step:1587/2330 train_time:60625ms step_avg:38.20ms
step:1588/2330 train_time:60666ms step_avg:38.20ms
step:1589/2330 train_time:60701ms step_avg:38.20ms
step:1590/2330 train_time:60743ms step_avg:38.20ms
step:1591/2330 train_time:60779ms step_avg:38.20ms
step:1592/2330 train_time:60821ms step_avg:38.20ms
step:1593/2330 train_time:60856ms step_avg:38.20ms
step:1594/2330 train_time:60897ms step_avg:38.20ms
step:1595/2330 train_time:60932ms step_avg:38.20ms
step:1596/2330 train_time:60973ms step_avg:38.20ms
step:1597/2330 train_time:61009ms step_avg:38.20ms
step:1598/2330 train_time:61050ms step_avg:38.20ms
step:1599/2330 train_time:61085ms step_avg:38.20ms
step:1600/2330 train_time:61127ms step_avg:38.20ms
step:1601/2330 train_time:61162ms step_avg:38.20ms
step:1602/2330 train_time:61203ms step_avg:38.20ms
step:1603/2330 train_time:61238ms step_avg:38.20ms
step:1604/2330 train_time:61280ms step_avg:38.20ms
step:1605/2330 train_time:61315ms step_avg:38.20ms
step:1606/2330 train_time:61356ms step_avg:38.20ms
step:1607/2330 train_time:61391ms step_avg:38.20ms
step:1608/2330 train_time:61432ms step_avg:38.20ms
step:1609/2330 train_time:61468ms step_avg:38.20ms
step:1610/2330 train_time:61509ms step_avg:38.20ms
step:1611/2330 train_time:61544ms step_avg:38.20ms
step:1612/2330 train_time:61585ms step_avg:38.20ms
step:1613/2330 train_time:61621ms step_avg:38.20ms
step:1614/2330 train_time:61662ms step_avg:38.20ms
step:1615/2330 train_time:61697ms step_avg:38.20ms
step:1616/2330 train_time:61739ms step_avg:38.20ms
step:1617/2330 train_time:61773ms step_avg:38.20ms
step:1618/2330 train_time:61815ms step_avg:38.20ms
step:1619/2330 train_time:61850ms step_avg:38.20ms
step:1620/2330 train_time:61891ms step_avg:38.20ms
step:1621/2330 train_time:61927ms step_avg:38.20ms
step:1622/2330 train_time:61968ms step_avg:38.20ms
step:1623/2330 train_time:62004ms step_avg:38.20ms
step:1624/2330 train_time:62045ms step_avg:38.20ms
step:1625/2330 train_time:62081ms step_avg:38.20ms
step:1626/2330 train_time:62122ms step_avg:38.21ms
step:1627/2330 train_time:62157ms step_avg:38.20ms
step:1628/2330 train_time:62199ms step_avg:38.21ms
step:1629/2330 train_time:62234ms step_avg:38.20ms
step:1630/2330 train_time:62275ms step_avg:38.21ms
step:1631/2330 train_time:62310ms step_avg:38.20ms
step:1632/2330 train_time:62351ms step_avg:38.21ms
step:1633/2330 train_time:62386ms step_avg:38.20ms
step:1634/2330 train_time:62428ms step_avg:38.21ms
step:1635/2330 train_time:62463ms step_avg:38.20ms
step:1636/2330 train_time:62505ms step_avg:38.21ms
step:1637/2330 train_time:62541ms step_avg:38.20ms
step:1638/2330 train_time:62582ms step_avg:38.21ms
step:1639/2330 train_time:62617ms step_avg:38.20ms
step:1640/2330 train_time:62659ms step_avg:38.21ms
step:1641/2330 train_time:62693ms step_avg:38.20ms
step:1642/2330 train_time:62734ms step_avg:38.21ms
step:1643/2330 train_time:62769ms step_avg:38.20ms
step:1644/2330 train_time:62811ms step_avg:38.21ms
step:1645/2330 train_time:62846ms step_avg:38.20ms
step:1646/2330 train_time:62888ms step_avg:38.21ms
step:1647/2330 train_time:62923ms step_avg:38.20ms
step:1648/2330 train_time:62965ms step_avg:38.21ms
step:1649/2330 train_time:63000ms step_avg:38.20ms
step:1650/2330 train_time:63041ms step_avg:38.21ms
step:1651/2330 train_time:63077ms step_avg:38.21ms
step:1652/2330 train_time:63118ms step_avg:38.21ms
step:1653/2330 train_time:63153ms step_avg:38.20ms
step:1654/2330 train_time:63193ms step_avg:38.21ms
step:1655/2330 train_time:63229ms step_avg:38.20ms
step:1656/2330 train_time:63270ms step_avg:38.21ms
step:1657/2330 train_time:63306ms step_avg:38.21ms
step:1658/2330 train_time:63347ms step_avg:38.21ms
step:1659/2330 train_time:63382ms step_avg:38.21ms
step:1660/2330 train_time:63424ms step_avg:38.21ms
step:1661/2330 train_time:63459ms step_avg:38.21ms
step:1662/2330 train_time:63500ms step_avg:38.21ms
step:1663/2330 train_time:63535ms step_avg:38.21ms
step:1664/2330 train_time:63576ms step_avg:38.21ms
step:1665/2330 train_time:63612ms step_avg:38.21ms
step:1666/2330 train_time:63653ms step_avg:38.21ms
step:1667/2330 train_time:63689ms step_avg:38.21ms
step:1668/2330 train_time:63730ms step_avg:38.21ms
step:1669/2330 train_time:63765ms step_avg:38.21ms
step:1670/2330 train_time:63807ms step_avg:38.21ms
step:1671/2330 train_time:63842ms step_avg:38.21ms
step:1672/2330 train_time:63883ms step_avg:38.21ms
step:1673/2330 train_time:63918ms step_avg:38.21ms
step:1674/2330 train_time:63959ms step_avg:38.21ms
step:1675/2330 train_time:63995ms step_avg:38.21ms
step:1676/2330 train_time:64036ms step_avg:38.21ms
step:1677/2330 train_time:64071ms step_avg:38.21ms
step:1678/2330 train_time:64112ms step_avg:38.21ms
step:1679/2330 train_time:64147ms step_avg:38.21ms
step:1680/2330 train_time:64189ms step_avg:38.21ms
step:1681/2330 train_time:64224ms step_avg:38.21ms
step:1682/2330 train_time:64265ms step_avg:38.21ms
step:1683/2330 train_time:64301ms step_avg:38.21ms
step:1684/2330 train_time:64343ms step_avg:38.21ms
step:1685/2330 train_time:64377ms step_avg:38.21ms
step:1686/2330 train_time:64419ms step_avg:38.21ms
step:1687/2330 train_time:64453ms step_avg:38.21ms
step:1688/2330 train_time:64494ms step_avg:38.21ms
step:1689/2330 train_time:64530ms step_avg:38.21ms
step:1690/2330 train_time:64571ms step_avg:38.21ms
step:1691/2330 train_time:64607ms step_avg:38.21ms
step:1692/2330 train_time:64648ms step_avg:38.21ms
step:1693/2330 train_time:64683ms step_avg:38.21ms
step:1694/2330 train_time:64725ms step_avg:38.21ms
step:1695/2330 train_time:64760ms step_avg:38.21ms
step:1696/2330 train_time:64802ms step_avg:38.21ms
step:1697/2330 train_time:64837ms step_avg:38.21ms
step:1698/2330 train_time:64878ms step_avg:38.21ms
step:1699/2330 train_time:64913ms step_avg:38.21ms
step:1700/2330 train_time:64954ms step_avg:38.21ms
step:1701/2330 train_time:64990ms step_avg:38.21ms
step:1702/2330 train_time:65031ms step_avg:38.21ms
step:1703/2330 train_time:65067ms step_avg:38.21ms
step:1704/2330 train_time:65108ms step_avg:38.21ms
step:1705/2330 train_time:65143ms step_avg:38.21ms
step:1706/2330 train_time:65184ms step_avg:38.21ms
step:1707/2330 train_time:65221ms step_avg:38.21ms
step:1708/2330 train_time:65262ms step_avg:38.21ms
step:1709/2330 train_time:65298ms step_avg:38.21ms
step:1710/2330 train_time:65339ms step_avg:38.21ms
step:1711/2330 train_time:65375ms step_avg:38.21ms
step:1712/2330 train_time:65415ms step_avg:38.21ms
step:1713/2330 train_time:65451ms step_avg:38.21ms
step:1714/2330 train_time:65492ms step_avg:38.21ms
step:1715/2330 train_time:65527ms step_avg:38.21ms
step:1716/2330 train_time:65569ms step_avg:38.21ms
step:1717/2330 train_time:65604ms step_avg:38.21ms
step:1718/2330 train_time:65645ms step_avg:38.21ms
step:1719/2330 train_time:65680ms step_avg:38.21ms
step:1720/2330 train_time:65722ms step_avg:38.21ms
step:1721/2330 train_time:65756ms step_avg:38.21ms
step:1722/2330 train_time:65797ms step_avg:38.21ms
step:1723/2330 train_time:65832ms step_avg:38.21ms
step:1724/2330 train_time:65874ms step_avg:38.21ms
step:1725/2330 train_time:65909ms step_avg:38.21ms
step:1726/2330 train_time:65950ms step_avg:38.21ms
step:1727/2330 train_time:65986ms step_avg:38.21ms
step:1728/2330 train_time:66028ms step_avg:38.21ms
step:1729/2330 train_time:66063ms step_avg:38.21ms
step:1730/2330 train_time:66104ms step_avg:38.21ms
step:1731/2330 train_time:66139ms step_avg:38.21ms
step:1732/2330 train_time:66181ms step_avg:38.21ms
step:1733/2330 train_time:66216ms step_avg:38.21ms
step:1734/2330 train_time:66257ms step_avg:38.21ms
step:1735/2330 train_time:66292ms step_avg:38.21ms
step:1736/2330 train_time:66333ms step_avg:38.21ms
step:1737/2330 train_time:66369ms step_avg:38.21ms
step:1738/2330 train_time:66411ms step_avg:38.21ms
step:1739/2330 train_time:66446ms step_avg:38.21ms
step:1740/2330 train_time:66487ms step_avg:38.21ms
step:1741/2330 train_time:66523ms step_avg:38.21ms
step:1742/2330 train_time:66564ms step_avg:38.21ms
step:1743/2330 train_time:66600ms step_avg:38.21ms
step:1744/2330 train_time:66641ms step_avg:38.21ms
step:1745/2330 train_time:66676ms step_avg:38.21ms
step:1746/2330 train_time:66717ms step_avg:38.21ms
step:1747/2330 train_time:66753ms step_avg:38.21ms
step:1748/2330 train_time:66794ms step_avg:38.21ms
step:1749/2330 train_time:66829ms step_avg:38.21ms
step:1750/2330 train_time:66871ms step_avg:38.21ms
step:1750/2330 val_loss:5.3946 train_time:66983ms step_avg:38.28ms
step:1751/2330 train_time:66995ms step_avg:38.26ms
step:1752/2330 train_time:67006ms step_avg:38.25ms
step:1753/2330 train_time:67015ms step_avg:38.23ms
step:1754/2330 train_time:67026ms step_avg:38.21ms
step:1755/2330 train_time:67060ms step_avg:38.21ms
step:1756/2330 train_time:67100ms step_avg:38.21ms
step:1757/2330 train_time:67135ms step_avg:38.21ms
step:1758/2330 train_time:67175ms step_avg:38.21ms
step:1759/2330 train_time:67210ms step_avg:38.21ms
step:1760/2330 train_time:67251ms step_avg:38.21ms
step:1761/2330 train_time:67286ms step_avg:38.21ms
step:1762/2330 train_time:67327ms step_avg:38.21ms
step:1763/2330 train_time:67370ms step_avg:38.21ms
step:1764/2330 train_time:67410ms step_avg:38.21ms
step:1765/2330 train_time:67448ms step_avg:38.21ms
step:1766/2330 train_time:67489ms step_avg:38.22ms
step:1767/2330 train_time:67526ms step_avg:38.21ms
step:1768/2330 train_time:67566ms step_avg:38.22ms
step:1769/2330 train_time:67602ms step_avg:38.21ms
step:1770/2330 train_time:67642ms step_avg:38.22ms
step:1771/2330 train_time:67677ms step_avg:38.21ms
step:1772/2330 train_time:67718ms step_avg:38.22ms
step:1773/2330 train_time:67754ms step_avg:38.21ms
step:1774/2330 train_time:67795ms step_avg:38.22ms
step:1775/2330 train_time:67830ms step_avg:38.21ms
step:1776/2330 train_time:67871ms step_avg:38.22ms
step:1777/2330 train_time:67907ms step_avg:38.21ms
step:1778/2330 train_time:67951ms step_avg:38.22ms
step:1779/2330 train_time:67986ms step_avg:38.22ms
step:1780/2330 train_time:68029ms step_avg:38.22ms
step:1781/2330 train_time:68064ms step_avg:38.22ms
step:1782/2330 train_time:68105ms step_avg:38.22ms
step:1783/2330 train_time:68140ms step_avg:38.22ms
step:1784/2330 train_time:68181ms step_avg:38.22ms
step:1785/2330 train_time:68215ms step_avg:38.22ms
step:1786/2330 train_time:68257ms step_avg:38.22ms
step:1787/2330 train_time:68293ms step_avg:38.22ms
step:1788/2330 train_time:68335ms step_avg:38.22ms
step:1789/2330 train_time:68370ms step_avg:38.22ms
step:1790/2330 train_time:68411ms step_avg:38.22ms
step:1791/2330 train_time:68447ms step_avg:38.22ms
step:1792/2330 train_time:68488ms step_avg:38.22ms
step:1793/2330 train_time:68524ms step_avg:38.22ms
step:1794/2330 train_time:68565ms step_avg:38.22ms
step:1795/2330 train_time:68601ms step_avg:38.22ms
step:1796/2330 train_time:68641ms step_avg:38.22ms
step:1797/2330 train_time:68676ms step_avg:38.22ms
step:1798/2330 train_time:68717ms step_avg:38.22ms
step:1799/2330 train_time:68753ms step_avg:38.22ms
step:1800/2330 train_time:68794ms step_avg:38.22ms
step:1801/2330 train_time:68829ms step_avg:38.22ms
step:1802/2330 train_time:68870ms step_avg:38.22ms
step:1803/2330 train_time:68907ms step_avg:38.22ms
step:1804/2330 train_time:68948ms step_avg:38.22ms
step:1805/2330 train_time:68983ms step_avg:38.22ms
step:1806/2330 train_time:69025ms step_avg:38.22ms
step:1807/2330 train_time:69059ms step_avg:38.22ms
step:1808/2330 train_time:69101ms step_avg:38.22ms
step:1809/2330 train_time:69135ms step_avg:38.22ms
step:1810/2330 train_time:69177ms step_avg:38.22ms
step:1811/2330 train_time:69212ms step_avg:38.22ms
step:1812/2330 train_time:69253ms step_avg:38.22ms
step:1813/2330 train_time:69290ms step_avg:38.22ms
step:1814/2330 train_time:69331ms step_avg:38.22ms
step:1815/2330 train_time:69367ms step_avg:38.22ms
step:1816/2330 train_time:69408ms step_avg:38.22ms
step:1817/2330 train_time:69443ms step_avg:38.22ms
step:1818/2330 train_time:69485ms step_avg:38.22ms
step:1819/2330 train_time:69520ms step_avg:38.22ms
step:1820/2330 train_time:69561ms step_avg:38.22ms
step:1821/2330 train_time:69597ms step_avg:38.22ms
step:1822/2330 train_time:69639ms step_avg:38.22ms
step:1823/2330 train_time:69673ms step_avg:38.22ms
step:1824/2330 train_time:69714ms step_avg:38.22ms
step:1825/2330 train_time:69750ms step_avg:38.22ms
step:1826/2330 train_time:69791ms step_avg:38.22ms
step:1827/2330 train_time:69827ms step_avg:38.22ms
step:1828/2330 train_time:69868ms step_avg:38.22ms
step:1829/2330 train_time:69904ms step_avg:38.22ms
step:1830/2330 train_time:69945ms step_avg:38.22ms
step:1831/2330 train_time:69980ms step_avg:38.22ms
step:1832/2330 train_time:70021ms step_avg:38.22ms
step:1833/2330 train_time:70056ms step_avg:38.22ms
step:1834/2330 train_time:70098ms step_avg:38.22ms
step:1835/2330 train_time:70133ms step_avg:38.22ms
step:1836/2330 train_time:70174ms step_avg:38.22ms
step:1837/2330 train_time:70209ms step_avg:38.22ms
step:1838/2330 train_time:70251ms step_avg:38.22ms
step:1839/2330 train_time:70286ms step_avg:38.22ms
step:1840/2330 train_time:70328ms step_avg:38.22ms
step:1841/2330 train_time:70363ms step_avg:38.22ms
step:1842/2330 train_time:70404ms step_avg:38.22ms
step:1843/2330 train_time:70439ms step_avg:38.22ms
step:1844/2330 train_time:70480ms step_avg:38.22ms
step:1845/2330 train_time:70515ms step_avg:38.22ms
step:1846/2330 train_time:70557ms step_avg:38.22ms
step:1847/2330 train_time:70592ms step_avg:38.22ms
step:1848/2330 train_time:70634ms step_avg:38.22ms
step:1849/2330 train_time:70670ms step_avg:38.22ms
step:1850/2330 train_time:70711ms step_avg:38.22ms
step:1851/2330 train_time:70747ms step_avg:38.22ms
step:1852/2330 train_time:70788ms step_avg:38.22ms
step:1853/2330 train_time:70824ms step_avg:38.22ms
step:1854/2330 train_time:70865ms step_avg:38.22ms
step:1855/2330 train_time:70900ms step_avg:38.22ms
step:1856/2330 train_time:70941ms step_avg:38.22ms
step:1857/2330 train_time:70976ms step_avg:38.22ms
step:1858/2330 train_time:71018ms step_avg:38.22ms
step:1859/2330 train_time:71053ms step_avg:38.22ms
step:1860/2330 train_time:71094ms step_avg:38.22ms
step:1861/2330 train_time:71130ms step_avg:38.22ms
step:1862/2330 train_time:71171ms step_avg:38.22ms
step:1863/2330 train_time:71207ms step_avg:38.22ms
step:1864/2330 train_time:71248ms step_avg:38.22ms
step:1865/2330 train_time:71284ms step_avg:38.22ms
step:1866/2330 train_time:71325ms step_avg:38.22ms
step:1867/2330 train_time:71360ms step_avg:38.22ms
step:1868/2330 train_time:71401ms step_avg:38.22ms
step:1869/2330 train_time:71436ms step_avg:38.22ms
step:1870/2330 train_time:71477ms step_avg:38.22ms
step:1871/2330 train_time:71513ms step_avg:38.22ms
step:1872/2330 train_time:71554ms step_avg:38.22ms
step:1873/2330 train_time:71589ms step_avg:38.22ms
step:1874/2330 train_time:71631ms step_avg:38.22ms
step:1875/2330 train_time:71667ms step_avg:38.22ms
step:1876/2330 train_time:71708ms step_avg:38.22ms
step:1877/2330 train_time:71743ms step_avg:38.22ms
step:1878/2330 train_time:71785ms step_avg:38.22ms
step:1879/2330 train_time:71820ms step_avg:38.22ms
step:1880/2330 train_time:71861ms step_avg:38.22ms
step:1881/2330 train_time:71896ms step_avg:38.22ms
step:1882/2330 train_time:71938ms step_avg:38.22ms
step:1883/2330 train_time:71973ms step_avg:38.22ms
step:1884/2330 train_time:72015ms step_avg:38.22ms
step:1885/2330 train_time:72050ms step_avg:38.22ms
step:1886/2330 train_time:72091ms step_avg:38.22ms
step:1887/2330 train_time:72127ms step_avg:38.22ms
step:1888/2330 train_time:72168ms step_avg:38.22ms
step:1889/2330 train_time:72203ms step_avg:38.22ms
step:1890/2330 train_time:72245ms step_avg:38.22ms
step:1891/2330 train_time:72279ms step_avg:38.22ms
step:1892/2330 train_time:72320ms step_avg:38.22ms
step:1893/2330 train_time:72356ms step_avg:38.22ms
step:1894/2330 train_time:72397ms step_avg:38.22ms
step:1895/2330 train_time:72433ms step_avg:38.22ms
step:1896/2330 train_time:72474ms step_avg:38.22ms
step:1897/2330 train_time:72510ms step_avg:38.22ms
step:1898/2330 train_time:72551ms step_avg:38.22ms
step:1899/2330 train_time:72587ms step_avg:38.22ms
step:1900/2330 train_time:72628ms step_avg:38.23ms
step:1901/2330 train_time:72664ms step_avg:38.22ms
step:1902/2330 train_time:72705ms step_avg:38.23ms
step:1903/2330 train_time:72740ms step_avg:38.22ms
step:1904/2330 train_time:72781ms step_avg:38.23ms
step:1905/2330 train_time:72817ms step_avg:38.22ms
step:1906/2330 train_time:72858ms step_avg:38.23ms
step:1907/2330 train_time:72893ms step_avg:38.22ms
step:1908/2330 train_time:72935ms step_avg:38.23ms
step:1909/2330 train_time:72970ms step_avg:38.22ms
step:1910/2330 train_time:73010ms step_avg:38.23ms
step:1911/2330 train_time:73047ms step_avg:38.22ms
step:1912/2330 train_time:73088ms step_avg:38.23ms
step:1913/2330 train_time:73123ms step_avg:38.22ms
step:1914/2330 train_time:73165ms step_avg:38.23ms
step:1915/2330 train_time:73199ms step_avg:38.22ms
step:1916/2330 train_time:73241ms step_avg:38.23ms
step:1917/2330 train_time:73275ms step_avg:38.22ms
step:1918/2330 train_time:73317ms step_avg:38.23ms
step:1919/2330 train_time:73352ms step_avg:38.22ms
step:1920/2330 train_time:73393ms step_avg:38.23ms
step:1921/2330 train_time:73428ms step_avg:38.22ms
step:1922/2330 train_time:73470ms step_avg:38.23ms
step:1923/2330 train_time:73505ms step_avg:38.22ms
step:1924/2330 train_time:73546ms step_avg:38.23ms
step:1925/2330 train_time:73582ms step_avg:38.22ms
step:1926/2330 train_time:73623ms step_avg:38.23ms
step:1927/2330 train_time:73658ms step_avg:38.22ms
step:1928/2330 train_time:73699ms step_avg:38.23ms
step:1929/2330 train_time:73735ms step_avg:38.22ms
step:1930/2330 train_time:73776ms step_avg:38.23ms
step:1931/2330 train_time:73811ms step_avg:38.22ms
step:1932/2330 train_time:73853ms step_avg:38.23ms
step:1933/2330 train_time:73888ms step_avg:38.22ms
step:1934/2330 train_time:73929ms step_avg:38.23ms
step:1935/2330 train_time:73965ms step_avg:38.22ms
step:1936/2330 train_time:74006ms step_avg:38.23ms
step:1937/2330 train_time:74041ms step_avg:38.22ms
step:1938/2330 train_time:74082ms step_avg:38.23ms
step:1939/2330 train_time:74117ms step_avg:38.22ms
step:1940/2330 train_time:74158ms step_avg:38.23ms
step:1941/2330 train_time:74194ms step_avg:38.22ms
step:1942/2330 train_time:74235ms step_avg:38.23ms
step:1943/2330 train_time:74271ms step_avg:38.22ms
step:1944/2330 train_time:74312ms step_avg:38.23ms
step:1945/2330 train_time:74348ms step_avg:38.22ms
step:1946/2330 train_time:74389ms step_avg:38.23ms
step:1947/2330 train_time:74425ms step_avg:38.23ms
step:1948/2330 train_time:74466ms step_avg:38.23ms
step:1949/2330 train_time:74501ms step_avg:38.23ms
step:1950/2330 train_time:74542ms step_avg:38.23ms
step:1951/2330 train_time:74576ms step_avg:38.22ms
step:1952/2330 train_time:74617ms step_avg:38.23ms
step:1953/2330 train_time:74655ms step_avg:38.23ms
step:1954/2330 train_time:74696ms step_avg:38.23ms
step:1955/2330 train_time:74732ms step_avg:38.23ms
step:1956/2330 train_time:74773ms step_avg:38.23ms
step:1957/2330 train_time:74809ms step_avg:38.23ms
step:1958/2330 train_time:74850ms step_avg:38.23ms
step:1959/2330 train_time:74886ms step_avg:38.23ms
step:1960/2330 train_time:74927ms step_avg:38.23ms
step:1961/2330 train_time:74963ms step_avg:38.23ms
step:1962/2330 train_time:75004ms step_avg:38.23ms
step:1963/2330 train_time:75039ms step_avg:38.23ms
step:1964/2330 train_time:75079ms step_avg:38.23ms
step:1965/2330 train_time:75115ms step_avg:38.23ms
step:1966/2330 train_time:75156ms step_avg:38.23ms
step:1967/2330 train_time:75192ms step_avg:38.23ms
step:1968/2330 train_time:75233ms step_avg:38.23ms
step:1969/2330 train_time:75269ms step_avg:38.23ms
step:1970/2330 train_time:75311ms step_avg:38.23ms
step:1971/2330 train_time:75346ms step_avg:38.23ms
step:1972/2330 train_time:75387ms step_avg:38.23ms
step:1973/2330 train_time:75423ms step_avg:38.23ms
step:1974/2330 train_time:75464ms step_avg:38.23ms
step:1975/2330 train_time:75499ms step_avg:38.23ms
step:1976/2330 train_time:75540ms step_avg:38.23ms
step:1977/2330 train_time:75575ms step_avg:38.23ms
step:1978/2330 train_time:75617ms step_avg:38.23ms
step:1979/2330 train_time:75652ms step_avg:38.23ms
step:1980/2330 train_time:75694ms step_avg:38.23ms
step:1981/2330 train_time:75729ms step_avg:38.23ms
step:1982/2330 train_time:75770ms step_avg:38.23ms
step:1983/2330 train_time:75805ms step_avg:38.23ms
step:1984/2330 train_time:75847ms step_avg:38.23ms
step:1985/2330 train_time:75882ms step_avg:38.23ms
step:1986/2330 train_time:75923ms step_avg:38.23ms
step:1987/2330 train_time:75959ms step_avg:38.23ms
step:1988/2330 train_time:76000ms step_avg:38.23ms
step:1989/2330 train_time:76035ms step_avg:38.23ms
step:1990/2330 train_time:76076ms step_avg:38.23ms
step:1991/2330 train_time:76112ms step_avg:38.23ms
step:1992/2330 train_time:76154ms step_avg:38.23ms
step:1993/2330 train_time:76189ms step_avg:38.23ms
step:1994/2330 train_time:76231ms step_avg:38.23ms
step:1995/2330 train_time:76266ms step_avg:38.23ms
step:1996/2330 train_time:76308ms step_avg:38.23ms
step:1997/2330 train_time:76343ms step_avg:38.23ms
step:1998/2330 train_time:76385ms step_avg:38.23ms
step:1999/2330 train_time:76420ms step_avg:38.23ms
step:2000/2330 train_time:76461ms step_avg:38.23ms
step:2000/2330 val_loss:5.3490 train_time:76572ms step_avg:38.29ms
step:2001/2330 train_time:76584ms step_avg:38.27ms
step:2002/2330 train_time:76595ms step_avg:38.26ms
step:2003/2330 train_time:76604ms step_avg:38.24ms
step:2004/2330 train_time:76616ms step_avg:38.23ms
step:2005/2330 train_time:76649ms step_avg:38.23ms
step:2006/2330 train_time:76690ms step_avg:38.23ms
step:2007/2330 train_time:76724ms step_avg:38.23ms
step:2008/2330 train_time:76765ms step_avg:38.23ms
step:2009/2330 train_time:76800ms step_avg:38.23ms
step:2010/2330 train_time:76841ms step_avg:38.23ms
step:2011/2330 train_time:76877ms step_avg:38.23ms
step:2012/2330 train_time:76918ms step_avg:38.23ms
step:2013/2330 train_time:76959ms step_avg:38.23ms
step:2014/2330 train_time:77000ms step_avg:38.23ms
step:2015/2330 train_time:77037ms step_avg:38.23ms
step:2016/2330 train_time:77078ms step_avg:38.23ms
step:2017/2330 train_time:77115ms step_avg:38.23ms
step:2018/2330 train_time:77156ms step_avg:38.23ms
step:2019/2330 train_time:77191ms step_avg:38.23ms
step:2020/2330 train_time:77231ms step_avg:38.23ms
step:2021/2330 train_time:77266ms step_avg:38.23ms
step:2022/2330 train_time:77307ms step_avg:38.23ms
step:2023/2330 train_time:77342ms step_avg:38.23ms
step:2024/2330 train_time:77382ms step_avg:38.23ms
step:2025/2330 train_time:77417ms step_avg:38.23ms
step:2026/2330 train_time:77458ms step_avg:38.23ms
step:2027/2330 train_time:77493ms step_avg:38.23ms
step:2028/2330 train_time:77535ms step_avg:38.23ms
step:2029/2330 train_time:77570ms step_avg:38.23ms
step:2030/2330 train_time:77611ms step_avg:38.23ms
step:2031/2330 train_time:77647ms step_avg:38.23ms
step:2032/2330 train_time:77689ms step_avg:38.23ms
step:2033/2330 train_time:77723ms step_avg:38.23ms
step:2034/2330 train_time:77764ms step_avg:38.23ms
step:2035/2330 train_time:77799ms step_avg:38.23ms
step:2036/2330 train_time:77840ms step_avg:38.23ms
step:2037/2330 train_time:77876ms step_avg:38.23ms
step:2038/2330 train_time:77918ms step_avg:38.23ms
step:2039/2330 train_time:77953ms step_avg:38.23ms
step:2040/2330 train_time:77994ms step_avg:38.23ms
step:2041/2330 train_time:78030ms step_avg:38.23ms
step:2042/2330 train_time:78071ms step_avg:38.23ms
step:2043/2330 train_time:78107ms step_avg:38.23ms
step:2044/2330 train_time:78148ms step_avg:38.23ms
step:2045/2330 train_time:78185ms step_avg:38.23ms
step:2046/2330 train_time:78225ms step_avg:38.23ms
step:2047/2330 train_time:78260ms step_avg:38.23ms
step:2048/2330 train_time:78301ms step_avg:38.23ms
step:2049/2330 train_time:78336ms step_avg:38.23ms
step:2050/2330 train_time:78378ms step_avg:38.23ms
step:2051/2330 train_time:78412ms step_avg:38.23ms
step:2052/2330 train_time:78454ms step_avg:38.23ms
step:2053/2330 train_time:78488ms step_avg:38.23ms
step:2054/2330 train_time:78530ms step_avg:38.23ms
step:2055/2330 train_time:78564ms step_avg:38.23ms
step:2056/2330 train_time:78607ms step_avg:38.23ms
step:2057/2330 train_time:78642ms step_avg:38.23ms
step:2058/2330 train_time:78683ms step_avg:38.23ms
step:2059/2330 train_time:78719ms step_avg:38.23ms
step:2060/2330 train_time:78760ms step_avg:38.23ms
step:2061/2330 train_time:78795ms step_avg:38.23ms
step:2062/2330 train_time:78836ms step_avg:38.23ms
step:2063/2330 train_time:78872ms step_avg:38.23ms
step:2064/2330 train_time:78912ms step_avg:38.23ms
step:2065/2330 train_time:78949ms step_avg:38.23ms
step:2066/2330 train_time:78990ms step_avg:38.23ms
step:2067/2330 train_time:79026ms step_avg:38.23ms
step:2068/2330 train_time:79067ms step_avg:38.23ms
step:2069/2330 train_time:79104ms step_avg:38.23ms
step:2070/2330 train_time:79144ms step_avg:38.23ms
step:2071/2330 train_time:79181ms step_avg:38.23ms
step:2072/2330 train_time:79222ms step_avg:38.23ms
step:2073/2330 train_time:79257ms step_avg:38.23ms
step:2074/2330 train_time:79298ms step_avg:38.23ms
step:2075/2330 train_time:79333ms step_avg:38.23ms
step:2076/2330 train_time:79374ms step_avg:38.23ms
step:2077/2330 train_time:79409ms step_avg:38.23ms
step:2078/2330 train_time:79450ms step_avg:38.23ms
step:2079/2330 train_time:79485ms step_avg:38.23ms
step:2080/2330 train_time:79526ms step_avg:38.23ms
step:2081/2330 train_time:79561ms step_avg:38.23ms
step:2082/2330 train_time:79603ms step_avg:38.23ms
step:2083/2330 train_time:79637ms step_avg:38.23ms
step:2084/2330 train_time:79679ms step_avg:38.23ms
step:2085/2330 train_time:79714ms step_avg:38.23ms
step:2086/2330 train_time:79756ms step_avg:38.23ms
step:2087/2330 train_time:79790ms step_avg:38.23ms
step:2088/2330 train_time:79831ms step_avg:38.23ms
step:2089/2330 train_time:79867ms step_avg:38.23ms
step:2090/2330 train_time:79908ms step_avg:38.23ms
step:2091/2330 train_time:79944ms step_avg:38.23ms
step:2092/2330 train_time:79986ms step_avg:38.23ms
step:2093/2330 train_time:80020ms step_avg:38.23ms
step:2094/2330 train_time:80062ms step_avg:38.23ms
step:2095/2330 train_time:80098ms step_avg:38.23ms
step:2096/2330 train_time:80140ms step_avg:38.23ms
step:2097/2330 train_time:80175ms step_avg:38.23ms
step:2098/2330 train_time:80216ms step_avg:38.23ms
step:2099/2330 train_time:80251ms step_avg:38.23ms
step:2100/2330 train_time:80292ms step_avg:38.23ms
step:2101/2330 train_time:80328ms step_avg:38.23ms
step:2102/2330 train_time:80369ms step_avg:38.23ms
step:2103/2330 train_time:80404ms step_avg:38.23ms
step:2104/2330 train_time:80446ms step_avg:38.23ms
step:2105/2330 train_time:80481ms step_avg:38.23ms
step:2106/2330 train_time:80522ms step_avg:38.23ms
step:2107/2330 train_time:80557ms step_avg:38.23ms
step:2108/2330 train_time:80599ms step_avg:38.23ms
step:2109/2330 train_time:80634ms step_avg:38.23ms
step:2110/2330 train_time:80675ms step_avg:38.23ms
step:2111/2330 train_time:80710ms step_avg:38.23ms
step:2112/2330 train_time:80751ms step_avg:38.23ms
step:2113/2330 train_time:80787ms step_avg:38.23ms
step:2114/2330 train_time:80829ms step_avg:38.23ms
step:2115/2330 train_time:80864ms step_avg:38.23ms
step:2116/2330 train_time:80906ms step_avg:38.24ms
step:2117/2330 train_time:80941ms step_avg:38.23ms
step:2118/2330 train_time:80982ms step_avg:38.24ms
step:2119/2330 train_time:81017ms step_avg:38.23ms
step:2120/2330 train_time:81058ms step_avg:38.24ms
step:2121/2330 train_time:81093ms step_avg:38.23ms
step:2122/2330 train_time:81135ms step_avg:38.23ms
step:2123/2330 train_time:81169ms step_avg:38.23ms
step:2124/2330 train_time:81210ms step_avg:38.23ms
step:2125/2330 train_time:81246ms step_avg:38.23ms
step:2126/2330 train_time:81287ms step_avg:38.23ms
step:2127/2330 train_time:81323ms step_avg:38.23ms
step:2128/2330 train_time:81364ms step_avg:38.24ms
step:2129/2330 train_time:81399ms step_avg:38.23ms
step:2130/2330 train_time:81441ms step_avg:38.24ms
step:2131/2330 train_time:81476ms step_avg:38.23ms
step:2132/2330 train_time:81517ms step_avg:38.23ms
step:2133/2330 train_time:81552ms step_avg:38.23ms
step:2134/2330 train_time:81593ms step_avg:38.23ms
step:2135/2330 train_time:81628ms step_avg:38.23ms
step:2136/2330 train_time:81669ms step_avg:38.23ms
step:2137/2330 train_time:81704ms step_avg:38.23ms
step:2138/2330 train_time:81745ms step_avg:38.23ms
step:2139/2330 train_time:81781ms step_avg:38.23ms
step:2140/2330 train_time:81823ms step_avg:38.24ms
step:2141/2330 train_time:81859ms step_avg:38.23ms
step:2142/2330 train_time:81900ms step_avg:38.24ms
step:2143/2330 train_time:81936ms step_avg:38.23ms
step:2144/2330 train_time:81978ms step_avg:38.24ms
step:2145/2330 train_time:82012ms step_avg:38.23ms
step:2146/2330 train_time:82053ms step_avg:38.24ms
step:2147/2330 train_time:82088ms step_avg:38.23ms
step:2148/2330 train_time:82130ms step_avg:38.24ms
step:2149/2330 train_time:82165ms step_avg:38.23ms
step:2150/2330 train_time:82206ms step_avg:38.24ms
step:2151/2330 train_time:82242ms step_avg:38.23ms
step:2152/2330 train_time:82284ms step_avg:38.24ms
step:2153/2330 train_time:82318ms step_avg:38.23ms
step:2154/2330 train_time:82360ms step_avg:38.24ms
step:2155/2330 train_time:82395ms step_avg:38.23ms
step:2156/2330 train_time:82436ms step_avg:38.24ms
step:2157/2330 train_time:82471ms step_avg:38.23ms
step:2158/2330 train_time:82512ms step_avg:38.24ms
step:2159/2330 train_time:82547ms step_avg:38.23ms
step:2160/2330 train_time:82588ms step_avg:38.24ms
step:2161/2330 train_time:82623ms step_avg:38.23ms
step:2162/2330 train_time:82664ms step_avg:38.24ms
step:2163/2330 train_time:82699ms step_avg:38.23ms
step:2164/2330 train_time:82741ms step_avg:38.24ms
step:2165/2330 train_time:82776ms step_avg:38.23ms
step:2166/2330 train_time:82818ms step_avg:38.24ms
step:2167/2330 train_time:82853ms step_avg:38.23ms
step:2168/2330 train_time:82894ms step_avg:38.24ms
step:2169/2330 train_time:82930ms step_avg:38.23ms
step:2170/2330 train_time:82971ms step_avg:38.24ms
step:2171/2330 train_time:83006ms step_avg:38.23ms
step:2172/2330 train_time:83047ms step_avg:38.24ms
step:2173/2330 train_time:83083ms step_avg:38.23ms
step:2174/2330 train_time:83124ms step_avg:38.24ms
step:2175/2330 train_time:83160ms step_avg:38.23ms
step:2176/2330 train_time:83201ms step_avg:38.24ms
step:2177/2330 train_time:83237ms step_avg:38.23ms
step:2178/2330 train_time:83278ms step_avg:38.24ms
step:2179/2330 train_time:83313ms step_avg:38.23ms
step:2180/2330 train_time:83354ms step_avg:38.24ms
step:2181/2330 train_time:83389ms step_avg:38.23ms
step:2182/2330 train_time:83430ms step_avg:38.24ms
step:2183/2330 train_time:83465ms step_avg:38.23ms
step:2184/2330 train_time:83507ms step_avg:38.24ms
step:2185/2330 train_time:83542ms step_avg:38.23ms
step:2186/2330 train_time:83583ms step_avg:38.24ms
step:2187/2330 train_time:83619ms step_avg:38.23ms
step:2188/2330 train_time:83660ms step_avg:38.24ms
step:2189/2330 train_time:83696ms step_avg:38.23ms
step:2190/2330 train_time:83736ms step_avg:38.24ms
step:2191/2330 train_time:83772ms step_avg:38.23ms
step:2192/2330 train_time:83813ms step_avg:38.24ms
step:2193/2330 train_time:83849ms step_avg:38.23ms
step:2194/2330 train_time:83890ms step_avg:38.24ms
step:2195/2330 train_time:83925ms step_avg:38.23ms
step:2196/2330 train_time:83966ms step_avg:38.24ms
step:2197/2330 train_time:84002ms step_avg:38.24ms
step:2198/2330 train_time:84043ms step_avg:38.24ms
step:2199/2330 train_time:84079ms step_avg:38.24ms
step:2200/2330 train_time:84121ms step_avg:38.24ms
step:2201/2330 train_time:84156ms step_avg:38.24ms
step:2202/2330 train_time:84198ms step_avg:38.24ms
step:2203/2330 train_time:84233ms step_avg:38.24ms
step:2204/2330 train_time:84274ms step_avg:38.24ms
step:2205/2330 train_time:84310ms step_avg:38.24ms
step:2206/2330 train_time:84350ms step_avg:38.24ms
step:2207/2330 train_time:84386ms step_avg:38.24ms
step:2208/2330 train_time:84428ms step_avg:38.24ms
step:2209/2330 train_time:84464ms step_avg:38.24ms
step:2210/2330 train_time:84505ms step_avg:38.24ms
step:2211/2330 train_time:84540ms step_avg:38.24ms
step:2212/2330 train_time:84582ms step_avg:38.24ms
step:2213/2330 train_time:84617ms step_avg:38.24ms
step:2214/2330 train_time:84658ms step_avg:38.24ms
step:2215/2330 train_time:84694ms step_avg:38.24ms
step:2216/2330 train_time:84735ms step_avg:38.24ms
step:2217/2330 train_time:84771ms step_avg:38.24ms
step:2218/2330 train_time:84811ms step_avg:38.24ms
step:2219/2330 train_time:84848ms step_avg:38.24ms
step:2220/2330 train_time:84889ms step_avg:38.24ms
step:2221/2330 train_time:84923ms step_avg:38.24ms
step:2222/2330 train_time:84964ms step_avg:38.24ms
step:2223/2330 train_time:85000ms step_avg:38.24ms
step:2224/2330 train_time:85041ms step_avg:38.24ms
step:2225/2330 train_time:85077ms step_avg:38.24ms
step:2226/2330 train_time:85118ms step_avg:38.24ms
step:2227/2330 train_time:85153ms step_avg:38.24ms
step:2228/2330 train_time:85195ms step_avg:38.24ms
step:2229/2330 train_time:85230ms step_avg:38.24ms
step:2230/2330 train_time:85271ms step_avg:38.24ms
step:2231/2330 train_time:85305ms step_avg:38.24ms
step:2232/2330 train_time:85347ms step_avg:38.24ms
step:2233/2330 train_time:85383ms step_avg:38.24ms
step:2234/2330 train_time:85424ms step_avg:38.24ms
step:2235/2330 train_time:85460ms step_avg:38.24ms
step:2236/2330 train_time:85501ms step_avg:38.24ms
step:2237/2330 train_time:85537ms step_avg:38.24ms
step:2238/2330 train_time:85578ms step_avg:38.24ms
step:2239/2330 train_time:85613ms step_avg:38.24ms
step:2240/2330 train_time:85655ms step_avg:38.24ms
step:2241/2330 train_time:85690ms step_avg:38.24ms
step:2242/2330 train_time:85731ms step_avg:38.24ms
step:2243/2330 train_time:85767ms step_avg:38.24ms
step:2244/2330 train_time:85808ms step_avg:38.24ms
step:2245/2330 train_time:85843ms step_avg:38.24ms
step:2246/2330 train_time:85885ms step_avg:38.24ms
step:2247/2330 train_time:85920ms step_avg:38.24ms
step:2248/2330 train_time:85960ms step_avg:38.24ms
step:2249/2330 train_time:85997ms step_avg:38.24ms
step:2250/2330 train_time:86038ms step_avg:38.24ms
step:2250/2330 val_loss:5.3134 train_time:86150ms step_avg:38.29ms
step:2251/2330 train_time:86161ms step_avg:38.28ms
step:2252/2330 train_time:86172ms step_avg:38.26ms
step:2253/2330 train_time:86181ms step_avg:38.25ms
step:2254/2330 train_time:86192ms step_avg:38.24ms
step:2255/2330 train_time:86229ms step_avg:38.24ms
step:2256/2330 train_time:86269ms step_avg:38.24ms
step:2257/2330 train_time:86304ms step_avg:38.24ms
step:2258/2330 train_time:86345ms step_avg:38.24ms
step:2259/2330 train_time:86379ms step_avg:38.24ms
step:2260/2330 train_time:86420ms step_avg:38.24ms
step:2261/2330 train_time:86458ms step_avg:38.24ms
step:2262/2330 train_time:86499ms step_avg:38.24ms
step:2263/2330 train_time:86541ms step_avg:38.24ms
step:2264/2330 train_time:86582ms step_avg:38.24ms
step:2265/2330 train_time:86621ms step_avg:38.24ms
step:2266/2330 train_time:86661ms step_avg:38.24ms
step:2267/2330 train_time:86697ms step_avg:38.24ms
step:2268/2330 train_time:86738ms step_avg:38.24ms
step:2269/2330 train_time:86773ms step_avg:38.24ms
step:2270/2330 train_time:86814ms step_avg:38.24ms
step:2271/2330 train_time:86849ms step_avg:38.24ms
step:2272/2330 train_time:86890ms step_avg:38.24ms
step:2273/2330 train_time:86925ms step_avg:38.24ms
step:2274/2330 train_time:86966ms step_avg:38.24ms
step:2275/2330 train_time:87002ms step_avg:38.24ms
step:2276/2330 train_time:87043ms step_avg:38.24ms
step:2277/2330 train_time:87077ms step_avg:38.24ms
step:2278/2330 train_time:87119ms step_avg:38.24ms
step:2279/2330 train_time:87154ms step_avg:38.24ms
step:2280/2330 train_time:87195ms step_avg:38.24ms
step:2281/2330 train_time:87230ms step_avg:38.24ms
step:2282/2330 train_time:87271ms step_avg:38.24ms
step:2283/2330 train_time:87306ms step_avg:38.24ms
step:2284/2330 train_time:87347ms step_avg:38.24ms
step:2285/2330 train_time:87382ms step_avg:38.24ms
step:2286/2330 train_time:87424ms step_avg:38.24ms
step:2287/2330 train_time:87460ms step_avg:38.24ms
step:2288/2330 train_time:87502ms step_avg:38.24ms
step:2289/2330 train_time:87538ms step_avg:38.24ms
step:2290/2330 train_time:87579ms step_avg:38.24ms
step:2291/2330 train_time:87616ms step_avg:38.24ms
step:2292/2330 train_time:87657ms step_avg:38.24ms
step:2293/2330 train_time:87693ms step_avg:38.24ms
step:2294/2330 train_time:87735ms step_avg:38.25ms
step:2295/2330 train_time:87770ms step_avg:38.24ms
step:2296/2330 train_time:87811ms step_avg:38.25ms
step:2297/2330 train_time:87846ms step_avg:38.24ms
step:2298/2330 train_time:87887ms step_avg:38.24ms
step:2299/2330 train_time:87922ms step_avg:38.24ms
step:2300/2330 train_time:87963ms step_avg:38.24ms
step:2301/2330 train_time:87998ms step_avg:38.24ms
step:2302/2330 train_time:88038ms step_avg:38.24ms
step:2303/2330 train_time:88074ms step_avg:38.24ms
step:2304/2330 train_time:88115ms step_avg:38.24ms
step:2305/2330 train_time:88150ms step_avg:38.24ms
step:2306/2330 train_time:88191ms step_avg:38.24ms
step:2307/2330 train_time:88227ms step_avg:38.24ms
step:2308/2330 train_time:88268ms step_avg:38.24ms
step:2309/2330 train_time:88303ms step_avg:38.24ms
step:2310/2330 train_time:88345ms step_avg:38.24ms
step:2311/2330 train_time:88380ms step_avg:38.24ms
step:2312/2330 train_time:88421ms step_avg:38.24ms
step:2313/2330 train_time:88457ms step_avg:38.24ms
step:2314/2330 train_time:88497ms step_avg:38.24ms
step:2315/2330 train_time:88535ms step_avg:38.24ms
step:2316/2330 train_time:88576ms step_avg:38.25ms
step:2317/2330 train_time:88612ms step_avg:38.24ms
step:2318/2330 train_time:88654ms step_avg:38.25ms
step:2319/2330 train_time:88690ms step_avg:38.25ms
step:2320/2330 train_time:88732ms step_avg:38.25ms
step:2321/2330 train_time:88767ms step_avg:38.24ms
step:2322/2330 train_time:88808ms step_avg:38.25ms
step:2323/2330 train_time:88844ms step_avg:38.25ms
step:2324/2330 train_time:88886ms step_avg:38.25ms
step:2325/2330 train_time:88922ms step_avg:38.25ms
step:2326/2330 train_time:88963ms step_avg:38.25ms
step:2327/2330 train_time:88998ms step_avg:38.25ms
step:2328/2330 train_time:89039ms step_avg:38.25ms
step:2329/2330 train_time:89074ms step_avg:38.25ms
step:2330/2330 train_time:89115ms step_avg:38.25ms
step:2330/2330 val_loss:5.3046 train_time:89226ms step_avg:38.29ms
peak memory allocated: 27165 MiB reserved: 38888 MiB
