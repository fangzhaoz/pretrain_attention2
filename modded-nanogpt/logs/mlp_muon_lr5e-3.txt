import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        # self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "mlp_muon_lr5e-3"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
# embed_params = [p for n, p in model.named_parameters() if "embed" in n]
embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=5e-3, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 04:05:44 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   31C    P0             111W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   30C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   28C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   28C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   30C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   28C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   31C    P0             114W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:74ms step_avg:74.15ms
step:2/2330 train_time:149ms step_avg:74.64ms
step:3/2330 train_time:164ms step_avg:54.66ms
step:4/2330 train_time:177ms step_avg:44.25ms
step:5/2330 train_time:189ms step_avg:37.70ms
step:6/2330 train_time:220ms step_avg:36.66ms
step:7/2330 train_time:253ms step_avg:36.20ms
step:8/2330 train_time:297ms step_avg:37.14ms
step:9/2330 train_time:332ms step_avg:36.91ms
step:10/2330 train_time:376ms step_avg:37.64ms
step:11/2330 train_time:411ms step_avg:37.32ms
step:12/2330 train_time:454ms step_avg:37.85ms
step:13/2330 train_time:489ms step_avg:37.60ms
step:14/2330 train_time:533ms step_avg:38.10ms
step:15/2330 train_time:568ms step_avg:37.90ms
step:16/2330 train_time:613ms step_avg:38.30ms
step:17/2330 train_time:648ms step_avg:38.09ms
step:18/2330 train_time:691ms step_avg:38.40ms
step:19/2330 train_time:726ms step_avg:38.22ms
step:20/2330 train_time:770ms step_avg:38.50ms
step:21/2330 train_time:805ms step_avg:38.32ms
step:22/2330 train_time:849ms step_avg:38.59ms
step:23/2330 train_time:884ms step_avg:38.42ms
step:24/2330 train_time:927ms step_avg:38.64ms
step:25/2330 train_time:963ms step_avg:38.50ms
step:26/2330 train_time:1010ms step_avg:38.86ms
step:27/2330 train_time:1049ms step_avg:38.86ms
step:28/2330 train_time:1098ms step_avg:39.22ms
step:29/2330 train_time:1135ms step_avg:39.14ms
step:30/2330 train_time:1180ms step_avg:39.34ms
step:31/2330 train_time:1215ms step_avg:39.20ms
step:32/2330 train_time:1260ms step_avg:39.37ms
step:33/2330 train_time:1296ms step_avg:39.26ms
step:34/2330 train_time:1340ms step_avg:39.40ms
step:35/2330 train_time:1375ms step_avg:39.29ms
step:36/2330 train_time:1419ms step_avg:39.43ms
step:37/2330 train_time:1454ms step_avg:39.30ms
step:38/2330 train_time:1499ms step_avg:39.44ms
step:39/2330 train_time:1534ms step_avg:39.33ms
step:40/2330 train_time:1578ms step_avg:39.45ms
step:41/2330 train_time:1614ms step_avg:39.36ms
step:42/2330 train_time:1659ms step_avg:39.50ms
step:43/2330 train_time:1694ms step_avg:39.39ms
step:44/2330 train_time:1739ms step_avg:39.53ms
step:45/2330 train_time:1775ms step_avg:39.45ms
step:46/2330 train_time:1820ms step_avg:39.56ms
step:47/2330 train_time:1855ms step_avg:39.47ms
step:48/2330 train_time:1900ms step_avg:39.59ms
step:49/2330 train_time:1936ms step_avg:39.51ms
step:50/2330 train_time:1982ms step_avg:39.64ms
step:51/2330 train_time:2019ms step_avg:39.59ms
step:52/2330 train_time:2066ms step_avg:39.73ms
step:53/2330 train_time:2105ms step_avg:39.71ms
step:54/2330 train_time:2152ms step_avg:39.85ms
step:55/2330 train_time:2187ms step_avg:39.77ms
step:56/2330 train_time:2232ms step_avg:39.86ms
step:57/2330 train_time:2268ms step_avg:39.80ms
step:58/2330 train_time:2314ms step_avg:39.89ms
step:59/2330 train_time:2349ms step_avg:39.82ms
step:60/2330 train_time:2394ms step_avg:39.90ms
step:61/2330 train_time:2429ms step_avg:39.82ms
step:62/2330 train_time:2473ms step_avg:39.88ms
step:63/2330 train_time:2508ms step_avg:39.82ms
step:64/2330 train_time:2553ms step_avg:39.89ms
step:65/2330 train_time:2588ms step_avg:39.82ms
step:66/2330 train_time:2633ms step_avg:39.89ms
step:67/2330 train_time:2668ms step_avg:39.82ms
step:68/2330 train_time:2713ms step_avg:39.90ms
step:69/2330 train_time:2748ms step_avg:39.83ms
step:70/2330 train_time:2792ms step_avg:39.89ms
step:71/2330 train_time:2827ms step_avg:39.82ms
step:72/2330 train_time:2873ms step_avg:39.91ms
step:73/2330 train_time:2908ms step_avg:39.84ms
step:74/2330 train_time:2953ms step_avg:39.91ms
step:75/2330 train_time:2988ms step_avg:39.84ms
step:76/2330 train_time:3034ms step_avg:39.93ms
step:77/2330 train_time:3070ms step_avg:39.87ms
step:78/2330 train_time:3116ms step_avg:39.95ms
step:79/2330 train_time:3151ms step_avg:39.89ms
step:80/2330 train_time:3198ms step_avg:39.97ms
step:81/2330 train_time:3234ms step_avg:39.93ms
step:82/2330 train_time:3279ms step_avg:39.99ms
step:83/2330 train_time:3315ms step_avg:39.94ms
step:84/2330 train_time:3360ms step_avg:40.00ms
step:85/2330 train_time:3395ms step_avg:39.94ms
step:86/2330 train_time:3439ms step_avg:39.99ms
step:87/2330 train_time:3475ms step_avg:39.94ms
step:88/2330 train_time:3519ms step_avg:39.99ms
step:89/2330 train_time:3555ms step_avg:39.94ms
step:90/2330 train_time:3599ms step_avg:39.99ms
step:91/2330 train_time:3634ms step_avg:39.93ms
step:92/2330 train_time:3678ms step_avg:39.98ms
step:93/2330 train_time:3714ms step_avg:39.93ms
step:94/2330 train_time:3758ms step_avg:39.98ms
step:95/2330 train_time:3794ms step_avg:39.94ms
step:96/2330 train_time:3839ms step_avg:39.99ms
step:97/2330 train_time:3874ms step_avg:39.94ms
step:98/2330 train_time:3919ms step_avg:39.99ms
step:99/2330 train_time:3955ms step_avg:39.95ms
step:100/2330 train_time:4000ms step_avg:40.00ms
step:101/2330 train_time:4036ms step_avg:39.96ms
step:102/2330 train_time:4081ms step_avg:40.01ms
step:103/2330 train_time:4117ms step_avg:39.97ms
step:104/2330 train_time:4163ms step_avg:40.03ms
step:105/2330 train_time:4200ms step_avg:40.00ms
step:106/2330 train_time:4248ms step_avg:40.07ms
step:107/2330 train_time:4284ms step_avg:40.04ms
step:108/2330 train_time:4329ms step_avg:40.08ms
step:109/2330 train_time:4365ms step_avg:40.04ms
step:110/2330 train_time:4410ms step_avg:40.09ms
step:111/2330 train_time:4446ms step_avg:40.05ms
step:112/2330 train_time:4491ms step_avg:40.10ms
step:113/2330 train_time:4526ms step_avg:40.05ms
step:114/2330 train_time:4571ms step_avg:40.10ms
step:115/2330 train_time:4606ms step_avg:40.06ms
step:116/2330 train_time:4652ms step_avg:40.10ms
step:117/2330 train_time:4688ms step_avg:40.07ms
step:118/2330 train_time:4733ms step_avg:40.11ms
step:119/2330 train_time:4768ms step_avg:40.07ms
step:120/2330 train_time:4813ms step_avg:40.11ms
step:121/2330 train_time:4848ms step_avg:40.07ms
step:122/2330 train_time:4894ms step_avg:40.11ms
step:123/2330 train_time:4929ms step_avg:40.07ms
step:124/2330 train_time:4974ms step_avg:40.11ms
step:125/2330 train_time:5009ms step_avg:40.07ms
step:126/2330 train_time:5053ms step_avg:40.11ms
step:127/2330 train_time:5090ms step_avg:40.08ms
step:128/2330 train_time:5135ms step_avg:40.12ms
step:129/2330 train_time:5171ms step_avg:40.08ms
step:130/2330 train_time:5216ms step_avg:40.12ms
step:131/2330 train_time:5251ms step_avg:40.09ms
step:132/2330 train_time:5297ms step_avg:40.13ms
step:133/2330 train_time:5332ms step_avg:40.09ms
step:134/2330 train_time:5377ms step_avg:40.13ms
step:135/2330 train_time:5413ms step_avg:40.10ms
step:136/2330 train_time:5457ms step_avg:40.13ms
step:137/2330 train_time:5493ms step_avg:40.09ms
step:138/2330 train_time:5540ms step_avg:40.14ms
step:139/2330 train_time:5576ms step_avg:40.11ms
step:140/2330 train_time:5621ms step_avg:40.15ms
step:141/2330 train_time:5656ms step_avg:40.12ms
step:142/2330 train_time:5701ms step_avg:40.15ms
step:143/2330 train_time:5737ms step_avg:40.12ms
step:144/2330 train_time:5782ms step_avg:40.15ms
step:145/2330 train_time:5817ms step_avg:40.12ms
step:146/2330 train_time:5862ms step_avg:40.15ms
step:147/2330 train_time:5897ms step_avg:40.12ms
step:148/2330 train_time:5943ms step_avg:40.15ms
step:149/2330 train_time:5978ms step_avg:40.12ms
step:150/2330 train_time:6023ms step_avg:40.16ms
step:151/2330 train_time:6059ms step_avg:40.13ms
step:152/2330 train_time:6105ms step_avg:40.17ms
step:153/2330 train_time:6141ms step_avg:40.14ms
step:154/2330 train_time:6186ms step_avg:40.17ms
step:155/2330 train_time:6223ms step_avg:40.15ms
step:156/2330 train_time:6268ms step_avg:40.18ms
step:157/2330 train_time:6304ms step_avg:40.15ms
step:158/2330 train_time:6350ms step_avg:40.19ms
step:159/2330 train_time:6384ms step_avg:40.15ms
step:160/2330 train_time:6430ms step_avg:40.18ms
step:161/2330 train_time:6465ms step_avg:40.16ms
step:162/2330 train_time:6512ms step_avg:40.19ms
step:163/2330 train_time:6547ms step_avg:40.17ms
step:164/2330 train_time:6592ms step_avg:40.20ms
step:165/2330 train_time:6627ms step_avg:40.17ms
step:166/2330 train_time:6672ms step_avg:40.19ms
step:167/2330 train_time:6707ms step_avg:40.16ms
step:168/2330 train_time:6752ms step_avg:40.19ms
step:169/2330 train_time:6788ms step_avg:40.16ms
step:170/2330 train_time:6833ms step_avg:40.20ms
step:171/2330 train_time:6868ms step_avg:40.16ms
step:172/2330 train_time:6913ms step_avg:40.19ms
step:173/2330 train_time:6948ms step_avg:40.16ms
step:174/2330 train_time:6993ms step_avg:40.19ms
step:175/2330 train_time:7029ms step_avg:40.17ms
step:176/2330 train_time:7074ms step_avg:40.20ms
step:177/2330 train_time:7110ms step_avg:40.17ms
step:178/2330 train_time:7156ms step_avg:40.20ms
step:179/2330 train_time:7192ms step_avg:40.18ms
step:180/2330 train_time:7236ms step_avg:40.20ms
step:181/2330 train_time:7271ms step_avg:40.17ms
step:182/2330 train_time:7316ms step_avg:40.20ms
step:183/2330 train_time:7352ms step_avg:40.18ms
step:184/2330 train_time:7398ms step_avg:40.21ms
step:185/2330 train_time:7434ms step_avg:40.18ms
step:186/2330 train_time:7480ms step_avg:40.21ms
step:187/2330 train_time:7516ms step_avg:40.19ms
step:188/2330 train_time:7562ms step_avg:40.22ms
step:189/2330 train_time:7598ms step_avg:40.20ms
step:190/2330 train_time:7643ms step_avg:40.23ms
step:191/2330 train_time:7678ms step_avg:40.20ms
step:192/2330 train_time:7723ms step_avg:40.22ms
step:193/2330 train_time:7759ms step_avg:40.20ms
step:194/2330 train_time:7805ms step_avg:40.23ms
step:195/2330 train_time:7842ms step_avg:40.21ms
step:196/2330 train_time:7887ms step_avg:40.24ms
step:197/2330 train_time:7922ms step_avg:40.21ms
step:198/2330 train_time:7967ms step_avg:40.24ms
step:199/2330 train_time:8003ms step_avg:40.22ms
step:200/2330 train_time:8048ms step_avg:40.24ms
step:201/2330 train_time:8084ms step_avg:40.22ms
step:202/2330 train_time:8130ms step_avg:40.25ms
step:203/2330 train_time:8165ms step_avg:40.22ms
step:204/2330 train_time:8210ms step_avg:40.24ms
step:205/2330 train_time:8246ms step_avg:40.23ms
step:206/2330 train_time:8292ms step_avg:40.25ms
step:207/2330 train_time:8327ms step_avg:40.23ms
step:208/2330 train_time:8372ms step_avg:40.25ms
step:209/2330 train_time:8408ms step_avg:40.23ms
step:210/2330 train_time:8453ms step_avg:40.25ms
step:211/2330 train_time:8489ms step_avg:40.23ms
step:212/2330 train_time:8535ms step_avg:40.26ms
step:213/2330 train_time:8571ms step_avg:40.24ms
step:214/2330 train_time:8615ms step_avg:40.26ms
step:215/2330 train_time:8650ms step_avg:40.23ms
step:216/2330 train_time:8696ms step_avg:40.26ms
step:217/2330 train_time:8732ms step_avg:40.24ms
step:218/2330 train_time:8777ms step_avg:40.26ms
step:219/2330 train_time:8812ms step_avg:40.24ms
step:220/2330 train_time:8858ms step_avg:40.27ms
step:221/2330 train_time:8894ms step_avg:40.24ms
step:222/2330 train_time:8939ms step_avg:40.26ms
step:223/2330 train_time:8974ms step_avg:40.24ms
step:224/2330 train_time:9019ms step_avg:40.26ms
step:225/2330 train_time:9055ms step_avg:40.25ms
step:226/2330 train_time:9100ms step_avg:40.26ms
step:227/2330 train_time:9136ms step_avg:40.25ms
step:228/2330 train_time:9181ms step_avg:40.27ms
step:229/2330 train_time:9216ms step_avg:40.25ms
step:230/2330 train_time:9261ms step_avg:40.27ms
step:231/2330 train_time:9297ms step_avg:40.25ms
step:232/2330 train_time:9343ms step_avg:40.27ms
step:233/2330 train_time:9380ms step_avg:40.26ms
step:234/2330 train_time:9427ms step_avg:40.28ms
step:235/2330 train_time:9462ms step_avg:40.26ms
step:236/2330 train_time:9508ms step_avg:40.29ms
step:237/2330 train_time:9543ms step_avg:40.27ms
step:238/2330 train_time:9589ms step_avg:40.29ms
step:239/2330 train_time:9625ms step_avg:40.27ms
step:240/2330 train_time:9669ms step_avg:40.29ms
step:241/2330 train_time:9705ms step_avg:40.27ms
step:242/2330 train_time:9750ms step_avg:40.29ms
step:243/2330 train_time:9787ms step_avg:40.27ms
step:244/2330 train_time:9832ms step_avg:40.29ms
step:245/2330 train_time:9867ms step_avg:40.27ms
step:246/2330 train_time:9912ms step_avg:40.29ms
step:247/2330 train_time:9947ms step_avg:40.27ms
step:248/2330 train_time:9993ms step_avg:40.29ms
step:249/2330 train_time:10029ms step_avg:40.28ms
step:250/2330 train_time:10074ms step_avg:40.30ms
step:250/2330 val_loss:5.4134 train_time:10160ms step_avg:40.64ms
step:251/2330 train_time:10173ms step_avg:40.53ms
step:252/2330 train_time:10185ms step_avg:40.41ms
step:253/2330 train_time:10195ms step_avg:40.30ms
step:254/2330 train_time:10233ms step_avg:40.29ms
step:255/2330 train_time:10268ms step_avg:40.27ms
step:256/2330 train_time:10312ms step_avg:40.28ms
step:257/2330 train_time:10347ms step_avg:40.26ms
step:258/2330 train_time:10391ms step_avg:40.27ms
step:259/2330 train_time:10426ms step_avg:40.25ms
step:260/2330 train_time:10473ms step_avg:40.28ms
step:261/2330 train_time:10514ms step_avg:40.29ms
step:262/2330 train_time:10563ms step_avg:40.32ms
step:263/2330 train_time:10600ms step_avg:40.30ms
step:264/2330 train_time:10646ms step_avg:40.32ms
step:265/2330 train_time:10682ms step_avg:40.31ms
step:266/2330 train_time:10727ms step_avg:40.33ms
step:267/2330 train_time:10762ms step_avg:40.31ms
step:268/2330 train_time:10806ms step_avg:40.32ms
step:269/2330 train_time:10841ms step_avg:40.30ms
step:270/2330 train_time:10887ms step_avg:40.32ms
step:271/2330 train_time:10922ms step_avg:40.30ms
step:272/2330 train_time:10967ms step_avg:40.32ms
step:273/2330 train_time:11002ms step_avg:40.30ms
step:274/2330 train_time:11047ms step_avg:40.32ms
step:275/2330 train_time:11083ms step_avg:40.30ms
step:276/2330 train_time:11128ms step_avg:40.32ms
step:277/2330 train_time:11164ms step_avg:40.30ms
step:278/2330 train_time:11209ms step_avg:40.32ms
step:279/2330 train_time:11244ms step_avg:40.30ms
step:280/2330 train_time:11289ms step_avg:40.32ms
step:281/2330 train_time:11324ms step_avg:40.30ms
step:282/2330 train_time:11369ms step_avg:40.32ms
step:283/2330 train_time:11405ms step_avg:40.30ms
step:284/2330 train_time:11451ms step_avg:40.32ms
step:285/2330 train_time:11488ms step_avg:40.31ms
step:286/2330 train_time:11536ms step_avg:40.34ms
step:287/2330 train_time:11574ms step_avg:40.33ms
step:288/2330 train_time:11620ms step_avg:40.35ms
step:289/2330 train_time:11657ms step_avg:40.33ms
step:290/2330 train_time:11702ms step_avg:40.35ms
step:291/2330 train_time:11737ms step_avg:40.33ms
step:292/2330 train_time:11783ms step_avg:40.35ms
step:293/2330 train_time:11818ms step_avg:40.33ms
step:294/2330 train_time:11863ms step_avg:40.35ms
step:295/2330 train_time:11898ms step_avg:40.33ms
step:296/2330 train_time:11943ms step_avg:40.35ms
step:297/2330 train_time:11978ms step_avg:40.33ms
step:298/2330 train_time:12023ms step_avg:40.34ms
step:299/2330 train_time:12059ms step_avg:40.33ms
step:300/2330 train_time:12104ms step_avg:40.35ms
step:301/2330 train_time:12140ms step_avg:40.33ms
step:302/2330 train_time:12185ms step_avg:40.35ms
step:303/2330 train_time:12220ms step_avg:40.33ms
step:304/2330 train_time:12265ms step_avg:40.35ms
step:305/2330 train_time:12300ms step_avg:40.33ms
step:306/2330 train_time:12346ms step_avg:40.35ms
step:307/2330 train_time:12382ms step_avg:40.33ms
step:308/2330 train_time:12428ms step_avg:40.35ms
step:309/2330 train_time:12464ms step_avg:40.34ms
step:310/2330 train_time:12511ms step_avg:40.36ms
step:311/2330 train_time:12547ms step_avg:40.35ms
step:312/2330 train_time:12594ms step_avg:40.36ms
step:313/2330 train_time:12630ms step_avg:40.35ms
step:314/2330 train_time:12677ms step_avg:40.37ms
step:315/2330 train_time:12713ms step_avg:40.36ms
step:316/2330 train_time:12757ms step_avg:40.37ms
step:317/2330 train_time:12793ms step_avg:40.36ms
step:318/2330 train_time:12838ms step_avg:40.37ms
step:319/2330 train_time:12873ms step_avg:40.36ms
step:320/2330 train_time:12919ms step_avg:40.37ms
step:321/2330 train_time:12954ms step_avg:40.36ms
step:322/2330 train_time:12999ms step_avg:40.37ms
step:323/2330 train_time:13035ms step_avg:40.35ms
step:324/2330 train_time:13080ms step_avg:40.37ms
step:325/2330 train_time:13115ms step_avg:40.36ms
step:326/2330 train_time:13161ms step_avg:40.37ms
step:327/2330 train_time:13196ms step_avg:40.36ms
step:328/2330 train_time:13241ms step_avg:40.37ms
step:329/2330 train_time:13277ms step_avg:40.35ms
step:330/2330 train_time:13322ms step_avg:40.37ms
step:331/2330 train_time:13357ms step_avg:40.35ms
step:332/2330 train_time:13403ms step_avg:40.37ms
step:333/2330 train_time:13439ms step_avg:40.36ms
step:334/2330 train_time:13484ms step_avg:40.37ms
step:335/2330 train_time:13521ms step_avg:40.36ms
step:336/2330 train_time:13566ms step_avg:40.38ms
step:337/2330 train_time:13602ms step_avg:40.36ms
step:338/2330 train_time:13648ms step_avg:40.38ms
step:339/2330 train_time:13684ms step_avg:40.37ms
step:340/2330 train_time:13729ms step_avg:40.38ms
step:341/2330 train_time:13765ms step_avg:40.37ms
step:342/2330 train_time:13810ms step_avg:40.38ms
step:343/2330 train_time:13846ms step_avg:40.37ms
step:344/2330 train_time:13891ms step_avg:40.38ms
step:345/2330 train_time:13927ms step_avg:40.37ms
step:346/2330 train_time:13972ms step_avg:40.38ms
step:347/2330 train_time:14008ms step_avg:40.37ms
step:348/2330 train_time:14054ms step_avg:40.39ms
step:349/2330 train_time:14090ms step_avg:40.37ms
step:350/2330 train_time:14136ms step_avg:40.39ms
step:351/2330 train_time:14172ms step_avg:40.38ms
step:352/2330 train_time:14218ms step_avg:40.39ms
step:353/2330 train_time:14253ms step_avg:40.38ms
step:354/2330 train_time:14299ms step_avg:40.39ms
step:355/2330 train_time:14335ms step_avg:40.38ms
step:356/2330 train_time:14381ms step_avg:40.40ms
step:357/2330 train_time:14416ms step_avg:40.38ms
step:358/2330 train_time:14462ms step_avg:40.40ms
step:359/2330 train_time:14497ms step_avg:40.38ms
step:360/2330 train_time:14542ms step_avg:40.39ms
step:361/2330 train_time:14578ms step_avg:40.38ms
step:362/2330 train_time:14624ms step_avg:40.40ms
step:363/2330 train_time:14659ms step_avg:40.38ms
step:364/2330 train_time:14704ms step_avg:40.40ms
step:365/2330 train_time:14740ms step_avg:40.38ms
step:366/2330 train_time:14784ms step_avg:40.39ms
step:367/2330 train_time:14820ms step_avg:40.38ms
step:368/2330 train_time:14865ms step_avg:40.39ms
step:369/2330 train_time:14901ms step_avg:40.38ms
step:370/2330 train_time:14946ms step_avg:40.39ms
step:371/2330 train_time:14982ms step_avg:40.38ms
step:372/2330 train_time:15028ms step_avg:40.40ms
step:373/2330 train_time:15063ms step_avg:40.38ms
step:374/2330 train_time:15109ms step_avg:40.40ms
step:375/2330 train_time:15144ms step_avg:40.39ms
step:376/2330 train_time:15189ms step_avg:40.40ms
step:377/2330 train_time:15226ms step_avg:40.39ms
step:378/2330 train_time:15271ms step_avg:40.40ms
step:379/2330 train_time:15307ms step_avg:40.39ms
step:380/2330 train_time:15353ms step_avg:40.40ms
step:381/2330 train_time:15389ms step_avg:40.39ms
step:382/2330 train_time:15435ms step_avg:40.41ms
step:383/2330 train_time:15471ms step_avg:40.39ms
step:384/2330 train_time:15518ms step_avg:40.41ms
step:385/2330 train_time:15553ms step_avg:40.40ms
step:386/2330 train_time:15598ms step_avg:40.41ms
step:387/2330 train_time:15633ms step_avg:40.40ms
step:388/2330 train_time:15678ms step_avg:40.41ms
step:389/2330 train_time:15714ms step_avg:40.39ms
step:390/2330 train_time:15759ms step_avg:40.41ms
step:391/2330 train_time:15794ms step_avg:40.39ms
step:392/2330 train_time:15839ms step_avg:40.41ms
step:393/2330 train_time:15874ms step_avg:40.39ms
step:394/2330 train_time:15921ms step_avg:40.41ms
step:395/2330 train_time:15956ms step_avg:40.39ms
step:396/2330 train_time:16000ms step_avg:40.41ms
step:397/2330 train_time:16037ms step_avg:40.39ms
step:398/2330 train_time:16081ms step_avg:40.40ms
step:399/2330 train_time:16117ms step_avg:40.39ms
step:400/2330 train_time:16162ms step_avg:40.41ms
step:401/2330 train_time:16198ms step_avg:40.39ms
step:402/2330 train_time:16243ms step_avg:40.41ms
step:403/2330 train_time:16279ms step_avg:40.40ms
step:404/2330 train_time:16324ms step_avg:40.41ms
step:405/2330 train_time:16360ms step_avg:40.40ms
step:406/2330 train_time:16406ms step_avg:40.41ms
step:407/2330 train_time:16441ms step_avg:40.40ms
step:408/2330 train_time:16487ms step_avg:40.41ms
step:409/2330 train_time:16523ms step_avg:40.40ms
step:410/2330 train_time:16568ms step_avg:40.41ms
step:411/2330 train_time:16604ms step_avg:40.40ms
step:412/2330 train_time:16649ms step_avg:40.41ms
step:413/2330 train_time:16685ms step_avg:40.40ms
step:414/2330 train_time:16729ms step_avg:40.41ms
step:415/2330 train_time:16765ms step_avg:40.40ms
step:416/2330 train_time:16810ms step_avg:40.41ms
step:417/2330 train_time:16846ms step_avg:40.40ms
step:418/2330 train_time:16891ms step_avg:40.41ms
step:419/2330 train_time:16927ms step_avg:40.40ms
step:420/2330 train_time:16973ms step_avg:40.41ms
step:421/2330 train_time:17009ms step_avg:40.40ms
step:422/2330 train_time:17055ms step_avg:40.41ms
step:423/2330 train_time:17090ms step_avg:40.40ms
step:424/2330 train_time:17136ms step_avg:40.42ms
step:425/2330 train_time:17172ms step_avg:40.41ms
step:426/2330 train_time:17217ms step_avg:40.42ms
step:427/2330 train_time:17252ms step_avg:40.40ms
step:428/2330 train_time:17299ms step_avg:40.42ms
step:429/2330 train_time:17334ms step_avg:40.41ms
step:430/2330 train_time:17379ms step_avg:40.42ms
step:431/2330 train_time:17415ms step_avg:40.41ms
step:432/2330 train_time:17460ms step_avg:40.42ms
step:433/2330 train_time:17495ms step_avg:40.40ms
step:434/2330 train_time:17540ms step_avg:40.41ms
step:435/2330 train_time:17575ms step_avg:40.40ms
step:436/2330 train_time:17621ms step_avg:40.41ms
step:437/2330 train_time:17656ms step_avg:40.40ms
step:438/2330 train_time:17702ms step_avg:40.41ms
step:439/2330 train_time:17736ms step_avg:40.40ms
step:440/2330 train_time:17782ms step_avg:40.41ms
step:441/2330 train_time:17818ms step_avg:40.40ms
step:442/2330 train_time:17862ms step_avg:40.41ms
step:443/2330 train_time:17898ms step_avg:40.40ms
step:444/2330 train_time:17943ms step_avg:40.41ms
step:445/2330 train_time:17979ms step_avg:40.40ms
step:446/2330 train_time:18024ms step_avg:40.41ms
step:447/2330 train_time:18059ms step_avg:40.40ms
step:448/2330 train_time:18105ms step_avg:40.41ms
step:449/2330 train_time:18140ms step_avg:40.40ms
step:450/2330 train_time:18185ms step_avg:40.41ms
step:451/2330 train_time:18221ms step_avg:40.40ms
step:452/2330 train_time:18266ms step_avg:40.41ms
step:453/2330 train_time:18302ms step_avg:40.40ms
step:454/2330 train_time:18348ms step_avg:40.41ms
step:455/2330 train_time:18383ms step_avg:40.40ms
step:456/2330 train_time:18429ms step_avg:40.41ms
step:457/2330 train_time:18464ms step_avg:40.40ms
step:458/2330 train_time:18510ms step_avg:40.41ms
step:459/2330 train_time:18546ms step_avg:40.40ms
step:460/2330 train_time:18590ms step_avg:40.41ms
step:461/2330 train_time:18626ms step_avg:40.40ms
step:462/2330 train_time:18671ms step_avg:40.41ms
step:463/2330 train_time:18708ms step_avg:40.41ms
step:464/2330 train_time:18754ms step_avg:40.42ms
step:465/2330 train_time:18790ms step_avg:40.41ms
step:466/2330 train_time:18835ms step_avg:40.42ms
step:467/2330 train_time:18871ms step_avg:40.41ms
step:468/2330 train_time:18917ms step_avg:40.42ms
step:469/2330 train_time:18952ms step_avg:40.41ms
step:470/2330 train_time:18997ms step_avg:40.42ms
step:471/2330 train_time:19032ms step_avg:40.41ms
step:472/2330 train_time:19078ms step_avg:40.42ms
step:473/2330 train_time:19114ms step_avg:40.41ms
step:474/2330 train_time:19159ms step_avg:40.42ms
step:475/2330 train_time:19195ms step_avg:40.41ms
step:476/2330 train_time:19240ms step_avg:40.42ms
step:477/2330 train_time:19275ms step_avg:40.41ms
step:478/2330 train_time:19321ms step_avg:40.42ms
step:479/2330 train_time:19356ms step_avg:40.41ms
step:480/2330 train_time:19401ms step_avg:40.42ms
step:481/2330 train_time:19437ms step_avg:40.41ms
step:482/2330 train_time:19482ms step_avg:40.42ms
step:483/2330 train_time:19516ms step_avg:40.41ms
step:484/2330 train_time:19561ms step_avg:40.41ms
step:485/2330 train_time:19595ms step_avg:40.40ms
step:486/2330 train_time:19641ms step_avg:40.41ms
step:487/2330 train_time:19677ms step_avg:40.40ms
step:488/2330 train_time:19722ms step_avg:40.41ms
step:489/2330 train_time:19757ms step_avg:40.40ms
step:490/2330 train_time:19803ms step_avg:40.41ms
step:491/2330 train_time:19839ms step_avg:40.40ms
step:492/2330 train_time:19884ms step_avg:40.42ms
step:493/2330 train_time:19920ms step_avg:40.41ms
step:494/2330 train_time:19965ms step_avg:40.41ms
step:495/2330 train_time:20001ms step_avg:40.41ms
step:496/2330 train_time:20046ms step_avg:40.41ms
step:497/2330 train_time:20082ms step_avg:40.41ms
step:498/2330 train_time:20128ms step_avg:40.42ms
step:499/2330 train_time:20164ms step_avg:40.41ms
step:500/2330 train_time:20209ms step_avg:40.42ms
step:500/2330 val_loss:5.2854 train_time:20298ms step_avg:40.60ms
step:501/2330 train_time:20312ms step_avg:40.54ms
step:502/2330 train_time:20325ms step_avg:40.49ms
step:503/2330 train_time:20336ms step_avg:40.43ms
step:504/2330 train_time:20372ms step_avg:40.42ms
step:505/2330 train_time:20406ms step_avg:40.41ms
step:506/2330 train_time:20450ms step_avg:40.42ms
step:507/2330 train_time:20485ms step_avg:40.40ms
step:508/2330 train_time:20529ms step_avg:40.41ms
step:509/2330 train_time:20564ms step_avg:40.40ms
step:510/2330 train_time:20612ms step_avg:40.41ms
step:511/2330 train_time:20654ms step_avg:40.42ms
step:512/2330 train_time:20702ms step_avg:40.43ms
step:513/2330 train_time:20737ms step_avg:40.42ms
step:514/2330 train_time:20783ms step_avg:40.43ms
step:515/2330 train_time:20818ms step_avg:40.42ms
step:516/2330 train_time:20862ms step_avg:40.43ms
step:517/2330 train_time:20898ms step_avg:40.42ms
step:518/2330 train_time:20943ms step_avg:40.43ms
step:519/2330 train_time:20978ms step_avg:40.42ms
step:520/2330 train_time:21022ms step_avg:40.43ms
step:521/2330 train_time:21057ms step_avg:40.42ms
step:522/2330 train_time:21102ms step_avg:40.43ms
step:523/2330 train_time:21138ms step_avg:40.42ms
step:524/2330 train_time:21182ms step_avg:40.42ms
step:525/2330 train_time:21218ms step_avg:40.42ms
step:526/2330 train_time:21264ms step_avg:40.43ms
step:527/2330 train_time:21299ms step_avg:40.42ms
step:528/2330 train_time:21344ms step_avg:40.42ms
step:529/2330 train_time:21380ms step_avg:40.41ms
step:530/2330 train_time:21424ms step_avg:40.42ms
step:531/2330 train_time:21459ms step_avg:40.41ms
step:532/2330 train_time:21504ms step_avg:40.42ms
step:533/2330 train_time:21540ms step_avg:40.41ms
step:534/2330 train_time:21587ms step_avg:40.42ms
step:535/2330 train_time:21624ms step_avg:40.42ms
step:536/2330 train_time:21672ms step_avg:40.43ms
step:537/2330 train_time:21709ms step_avg:40.43ms
step:538/2330 train_time:21754ms step_avg:40.44ms
step:539/2330 train_time:21790ms step_avg:40.43ms
step:540/2330 train_time:21836ms step_avg:40.44ms
step:541/2330 train_time:21872ms step_avg:40.43ms
step:542/2330 train_time:21917ms step_avg:40.44ms
step:543/2330 train_time:21951ms step_avg:40.43ms
step:544/2330 train_time:21996ms step_avg:40.43ms
step:545/2330 train_time:22030ms step_avg:40.42ms
step:546/2330 train_time:22076ms step_avg:40.43ms
step:547/2330 train_time:22111ms step_avg:40.42ms
step:548/2330 train_time:22155ms step_avg:40.43ms
step:549/2330 train_time:22191ms step_avg:40.42ms
step:550/2330 train_time:22237ms step_avg:40.43ms
step:551/2330 train_time:22272ms step_avg:40.42ms
step:552/2330 train_time:22317ms step_avg:40.43ms
step:553/2330 train_time:22353ms step_avg:40.42ms
step:554/2330 train_time:22397ms step_avg:40.43ms
step:555/2330 train_time:22433ms step_avg:40.42ms
step:556/2330 train_time:22479ms step_avg:40.43ms
step:557/2330 train_time:22515ms step_avg:40.42ms
step:558/2330 train_time:22560ms step_avg:40.43ms
step:559/2330 train_time:22596ms step_avg:40.42ms
step:560/2330 train_time:22641ms step_avg:40.43ms
step:561/2330 train_time:22678ms step_avg:40.42ms
step:562/2330 train_time:22723ms step_avg:40.43ms
step:563/2330 train_time:22759ms step_avg:40.43ms
step:564/2330 train_time:22805ms step_avg:40.43ms
step:565/2330 train_time:22840ms step_avg:40.43ms
step:566/2330 train_time:22886ms step_avg:40.43ms
step:567/2330 train_time:22922ms step_avg:40.43ms
step:568/2330 train_time:22967ms step_avg:40.43ms
step:569/2330 train_time:23002ms step_avg:40.43ms
step:570/2330 train_time:23048ms step_avg:40.43ms
step:571/2330 train_time:23083ms step_avg:40.42ms
step:572/2330 train_time:23128ms step_avg:40.43ms
step:573/2330 train_time:23163ms step_avg:40.42ms
step:574/2330 train_time:23209ms step_avg:40.43ms
step:575/2330 train_time:23245ms step_avg:40.43ms
step:576/2330 train_time:23290ms step_avg:40.43ms
step:577/2330 train_time:23326ms step_avg:40.43ms
step:578/2330 train_time:23371ms step_avg:40.44ms
step:579/2330 train_time:23406ms step_avg:40.43ms
step:580/2330 train_time:23451ms step_avg:40.43ms
step:581/2330 train_time:23487ms step_avg:40.43ms
step:582/2330 train_time:23534ms step_avg:40.44ms
step:583/2330 train_time:23570ms step_avg:40.43ms
step:584/2330 train_time:23614ms step_avg:40.44ms
step:585/2330 train_time:23650ms step_avg:40.43ms
step:586/2330 train_time:23695ms step_avg:40.43ms
step:587/2330 train_time:23730ms step_avg:40.43ms
step:588/2330 train_time:23776ms step_avg:40.43ms
step:589/2330 train_time:23811ms step_avg:40.43ms
step:590/2330 train_time:23856ms step_avg:40.43ms
step:591/2330 train_time:23891ms step_avg:40.42ms
step:592/2330 train_time:23937ms step_avg:40.43ms
step:593/2330 train_time:23972ms step_avg:40.42ms
step:594/2330 train_time:24017ms step_avg:40.43ms
step:595/2330 train_time:24053ms step_avg:40.43ms
step:596/2330 train_time:24098ms step_avg:40.43ms
step:597/2330 train_time:24134ms step_avg:40.43ms
step:598/2330 train_time:24179ms step_avg:40.43ms
step:599/2330 train_time:24215ms step_avg:40.42ms
step:600/2330 train_time:24260ms step_avg:40.43ms
step:601/2330 train_time:24295ms step_avg:40.42ms
step:602/2330 train_time:24340ms step_avg:40.43ms
step:603/2330 train_time:24376ms step_avg:40.42ms
step:604/2330 train_time:24421ms step_avg:40.43ms
step:605/2330 train_time:24456ms step_avg:40.42ms
step:606/2330 train_time:24502ms step_avg:40.43ms
step:607/2330 train_time:24539ms step_avg:40.43ms
step:608/2330 train_time:24584ms step_avg:40.43ms
step:609/2330 train_time:24619ms step_avg:40.42ms
step:610/2330 train_time:24664ms step_avg:40.43ms
step:611/2330 train_time:24700ms step_avg:40.43ms
step:612/2330 train_time:24745ms step_avg:40.43ms
step:613/2330 train_time:24780ms step_avg:40.42ms
step:614/2330 train_time:24827ms step_avg:40.43ms
step:615/2330 train_time:24863ms step_avg:40.43ms
step:616/2330 train_time:24909ms step_avg:40.44ms
step:617/2330 train_time:24945ms step_avg:40.43ms
step:618/2330 train_time:24990ms step_avg:40.44ms
step:619/2330 train_time:25026ms step_avg:40.43ms
step:620/2330 train_time:25071ms step_avg:40.44ms
step:621/2330 train_time:25107ms step_avg:40.43ms
step:622/2330 train_time:25151ms step_avg:40.44ms
step:623/2330 train_time:25187ms step_avg:40.43ms
step:624/2330 train_time:25232ms step_avg:40.44ms
step:625/2330 train_time:25268ms step_avg:40.43ms
step:626/2330 train_time:25313ms step_avg:40.44ms
step:627/2330 train_time:25350ms step_avg:40.43ms
step:628/2330 train_time:25395ms step_avg:40.44ms
step:629/2330 train_time:25430ms step_avg:40.43ms
step:630/2330 train_time:25476ms step_avg:40.44ms
step:631/2330 train_time:25511ms step_avg:40.43ms
step:632/2330 train_time:25556ms step_avg:40.44ms
step:633/2330 train_time:25591ms step_avg:40.43ms
step:634/2330 train_time:25636ms step_avg:40.44ms
step:635/2330 train_time:25672ms step_avg:40.43ms
step:636/2330 train_time:25717ms step_avg:40.44ms
step:637/2330 train_time:25753ms step_avg:40.43ms
step:638/2330 train_time:25798ms step_avg:40.44ms
step:639/2330 train_time:25833ms step_avg:40.43ms
step:640/2330 train_time:25879ms step_avg:40.44ms
step:641/2330 train_time:25915ms step_avg:40.43ms
step:642/2330 train_time:25960ms step_avg:40.44ms
step:643/2330 train_time:25996ms step_avg:40.43ms
step:644/2330 train_time:26041ms step_avg:40.44ms
step:645/2330 train_time:26077ms step_avg:40.43ms
step:646/2330 train_time:26123ms step_avg:40.44ms
step:647/2330 train_time:26159ms step_avg:40.43ms
step:648/2330 train_time:26204ms step_avg:40.44ms
step:649/2330 train_time:26239ms step_avg:40.43ms
step:650/2330 train_time:26284ms step_avg:40.44ms
step:651/2330 train_time:26320ms step_avg:40.43ms
step:652/2330 train_time:26366ms step_avg:40.44ms
step:653/2330 train_time:26402ms step_avg:40.43ms
step:654/2330 train_time:26448ms step_avg:40.44ms
step:655/2330 train_time:26484ms step_avg:40.43ms
step:656/2330 train_time:26530ms step_avg:40.44ms
step:657/2330 train_time:26566ms step_avg:40.43ms
step:658/2330 train_time:26611ms step_avg:40.44ms
step:659/2330 train_time:26646ms step_avg:40.43ms
step:660/2330 train_time:26691ms step_avg:40.44ms
step:661/2330 train_time:26726ms step_avg:40.43ms
step:662/2330 train_time:26771ms step_avg:40.44ms
step:663/2330 train_time:26808ms step_avg:40.43ms
step:664/2330 train_time:26853ms step_avg:40.44ms
step:665/2330 train_time:26889ms step_avg:40.43ms
step:666/2330 train_time:26934ms step_avg:40.44ms
step:667/2330 train_time:26969ms step_avg:40.43ms
step:668/2330 train_time:27014ms step_avg:40.44ms
step:669/2330 train_time:27050ms step_avg:40.43ms
step:670/2330 train_time:27095ms step_avg:40.44ms
step:671/2330 train_time:27130ms step_avg:40.43ms
step:672/2330 train_time:27176ms step_avg:40.44ms
step:673/2330 train_time:27210ms step_avg:40.43ms
step:674/2330 train_time:27255ms step_avg:40.44ms
step:675/2330 train_time:27291ms step_avg:40.43ms
step:676/2330 train_time:27337ms step_avg:40.44ms
step:677/2330 train_time:27373ms step_avg:40.43ms
step:678/2330 train_time:27417ms step_avg:40.44ms
step:679/2330 train_time:27453ms step_avg:40.43ms
step:680/2330 train_time:27498ms step_avg:40.44ms
step:681/2330 train_time:27534ms step_avg:40.43ms
step:682/2330 train_time:27579ms step_avg:40.44ms
step:683/2330 train_time:27615ms step_avg:40.43ms
step:684/2330 train_time:27660ms step_avg:40.44ms
step:685/2330 train_time:27696ms step_avg:40.43ms
step:686/2330 train_time:27741ms step_avg:40.44ms
step:687/2330 train_time:27777ms step_avg:40.43ms
step:688/2330 train_time:27822ms step_avg:40.44ms
step:689/2330 train_time:27858ms step_avg:40.43ms
step:690/2330 train_time:27902ms step_avg:40.44ms
step:691/2330 train_time:27938ms step_avg:40.43ms
step:692/2330 train_time:27983ms step_avg:40.44ms
step:693/2330 train_time:28019ms step_avg:40.43ms
step:694/2330 train_time:28064ms step_avg:40.44ms
step:695/2330 train_time:28100ms step_avg:40.43ms
step:696/2330 train_time:28145ms step_avg:40.44ms
step:697/2330 train_time:28180ms step_avg:40.43ms
step:698/2330 train_time:28226ms step_avg:40.44ms
step:699/2330 train_time:28262ms step_avg:40.43ms
step:700/2330 train_time:28307ms step_avg:40.44ms
step:701/2330 train_time:28343ms step_avg:40.43ms
step:702/2330 train_time:28388ms step_avg:40.44ms
step:703/2330 train_time:28424ms step_avg:40.43ms
step:704/2330 train_time:28471ms step_avg:40.44ms
step:705/2330 train_time:28507ms step_avg:40.44ms
step:706/2330 train_time:28551ms step_avg:40.44ms
step:707/2330 train_time:28588ms step_avg:40.44ms
step:708/2330 train_time:28634ms step_avg:40.44ms
step:709/2330 train_time:28669ms step_avg:40.44ms
step:710/2330 train_time:28714ms step_avg:40.44ms
step:711/2330 train_time:28750ms step_avg:40.44ms
step:712/2330 train_time:28795ms step_avg:40.44ms
step:713/2330 train_time:28830ms step_avg:40.44ms
step:714/2330 train_time:28875ms step_avg:40.44ms
step:715/2330 train_time:28911ms step_avg:40.43ms
step:716/2330 train_time:28957ms step_avg:40.44ms
step:717/2330 train_time:28992ms step_avg:40.43ms
step:718/2330 train_time:29037ms step_avg:40.44ms
step:719/2330 train_time:29073ms step_avg:40.43ms
step:720/2330 train_time:29117ms step_avg:40.44ms
step:721/2330 train_time:29153ms step_avg:40.43ms
step:722/2330 train_time:29199ms step_avg:40.44ms
step:723/2330 train_time:29235ms step_avg:40.44ms
step:724/2330 train_time:29281ms step_avg:40.44ms
step:725/2330 train_time:29317ms step_avg:40.44ms
step:726/2330 train_time:29362ms step_avg:40.44ms
step:727/2330 train_time:29399ms step_avg:40.44ms
step:728/2330 train_time:29444ms step_avg:40.44ms
step:729/2330 train_time:29479ms step_avg:40.44ms
step:730/2330 train_time:29524ms step_avg:40.44ms
step:731/2330 train_time:29561ms step_avg:40.44ms
step:732/2330 train_time:29606ms step_avg:40.45ms
step:733/2330 train_time:29642ms step_avg:40.44ms
step:734/2330 train_time:29688ms step_avg:40.45ms
step:735/2330 train_time:29724ms step_avg:40.44ms
step:736/2330 train_time:29769ms step_avg:40.45ms
step:737/2330 train_time:29805ms step_avg:40.44ms
step:738/2330 train_time:29850ms step_avg:40.45ms
step:739/2330 train_time:29886ms step_avg:40.44ms
step:740/2330 train_time:29931ms step_avg:40.45ms
step:741/2330 train_time:29967ms step_avg:40.44ms
step:742/2330 train_time:30012ms step_avg:40.45ms
step:743/2330 train_time:30048ms step_avg:40.44ms
step:744/2330 train_time:30093ms step_avg:40.45ms
step:745/2330 train_time:30128ms step_avg:40.44ms
step:746/2330 train_time:30173ms step_avg:40.45ms
step:747/2330 train_time:30210ms step_avg:40.44ms
step:748/2330 train_time:30255ms step_avg:40.45ms
step:749/2330 train_time:30290ms step_avg:40.44ms
step:750/2330 train_time:30335ms step_avg:40.45ms
step:750/2330 val_loss:5.2257 train_time:30423ms step_avg:40.56ms
step:751/2330 train_time:30435ms step_avg:40.53ms
step:752/2330 train_time:30447ms step_avg:40.49ms
step:753/2330 train_time:30458ms step_avg:40.45ms
step:754/2330 train_time:30499ms step_avg:40.45ms
step:755/2330 train_time:30533ms step_avg:40.44ms
step:756/2330 train_time:30577ms step_avg:40.45ms
step:757/2330 train_time:30613ms step_avg:40.44ms
step:758/2330 train_time:30657ms step_avg:40.44ms
step:759/2330 train_time:30692ms step_avg:40.44ms
step:760/2330 train_time:30741ms step_avg:40.45ms
step:761/2330 train_time:30779ms step_avg:40.45ms
step:762/2330 train_time:30826ms step_avg:40.45ms
step:763/2330 train_time:30863ms step_avg:40.45ms
step:764/2330 train_time:30909ms step_avg:40.46ms
step:765/2330 train_time:30947ms step_avg:40.45ms
step:766/2330 train_time:30992ms step_avg:40.46ms
step:767/2330 train_time:31027ms step_avg:40.45ms
step:768/2330 train_time:31072ms step_avg:40.46ms
step:769/2330 train_time:31107ms step_avg:40.45ms
step:770/2330 train_time:31151ms step_avg:40.46ms
step:771/2330 train_time:31187ms step_avg:40.45ms
step:772/2330 train_time:31231ms step_avg:40.46ms
step:773/2330 train_time:31266ms step_avg:40.45ms
step:774/2330 train_time:31311ms step_avg:40.45ms
step:775/2330 train_time:31348ms step_avg:40.45ms
step:776/2330 train_time:31394ms step_avg:40.46ms
step:777/2330 train_time:31431ms step_avg:40.45ms
step:778/2330 train_time:31475ms step_avg:40.46ms
step:779/2330 train_time:31510ms step_avg:40.45ms
step:780/2330 train_time:31554ms step_avg:40.45ms
step:781/2330 train_time:31590ms step_avg:40.45ms
step:782/2330 train_time:31635ms step_avg:40.45ms
step:783/2330 train_time:31671ms step_avg:40.45ms
step:784/2330 train_time:31717ms step_avg:40.46ms
step:785/2330 train_time:31754ms step_avg:40.45ms
step:786/2330 train_time:31801ms step_avg:40.46ms
step:787/2330 train_time:31837ms step_avg:40.45ms
step:788/2330 train_time:31882ms step_avg:40.46ms
step:789/2330 train_time:31918ms step_avg:40.45ms
step:790/2330 train_time:31964ms step_avg:40.46ms
step:791/2330 train_time:31999ms step_avg:40.45ms
step:792/2330 train_time:32044ms step_avg:40.46ms
step:793/2330 train_time:32079ms step_avg:40.45ms
step:794/2330 train_time:32124ms step_avg:40.46ms
step:795/2330 train_time:32160ms step_avg:40.45ms
step:796/2330 train_time:32205ms step_avg:40.46ms
step:797/2330 train_time:32240ms step_avg:40.45ms
step:798/2330 train_time:32285ms step_avg:40.46ms
step:799/2330 train_time:32321ms step_avg:40.45ms
step:800/2330 train_time:32366ms step_avg:40.46ms
step:801/2330 train_time:32401ms step_avg:40.45ms
step:802/2330 train_time:32446ms step_avg:40.46ms
step:803/2330 train_time:32482ms step_avg:40.45ms
step:804/2330 train_time:32527ms step_avg:40.46ms
step:805/2330 train_time:32562ms step_avg:40.45ms
step:806/2330 train_time:32607ms step_avg:40.46ms
step:807/2330 train_time:32643ms step_avg:40.45ms
step:808/2330 train_time:32688ms step_avg:40.46ms
step:809/2330 train_time:32726ms step_avg:40.45ms
step:810/2330 train_time:32773ms step_avg:40.46ms
step:811/2330 train_time:32808ms step_avg:40.45ms
step:812/2330 train_time:32854ms step_avg:40.46ms
step:813/2330 train_time:32891ms step_avg:40.46ms
step:814/2330 train_time:32937ms step_avg:40.46ms
step:815/2330 train_time:32972ms step_avg:40.46ms
step:816/2330 train_time:33017ms step_avg:40.46ms
step:817/2330 train_time:33053ms step_avg:40.46ms
step:818/2330 train_time:33098ms step_avg:40.46ms
step:819/2330 train_time:33133ms step_avg:40.46ms
step:820/2330 train_time:33178ms step_avg:40.46ms
step:821/2330 train_time:33213ms step_avg:40.45ms
step:822/2330 train_time:33258ms step_avg:40.46ms
step:823/2330 train_time:33294ms step_avg:40.45ms
step:824/2330 train_time:33339ms step_avg:40.46ms
step:825/2330 train_time:33374ms step_avg:40.45ms
step:826/2330 train_time:33421ms step_avg:40.46ms
step:827/2330 train_time:33456ms step_avg:40.45ms
step:828/2330 train_time:33501ms step_avg:40.46ms
step:829/2330 train_time:33537ms step_avg:40.45ms
step:830/2330 train_time:33583ms step_avg:40.46ms
step:831/2330 train_time:33619ms step_avg:40.46ms
step:832/2330 train_time:33664ms step_avg:40.46ms
step:833/2330 train_time:33699ms step_avg:40.46ms
step:834/2330 train_time:33744ms step_avg:40.46ms
step:835/2330 train_time:33780ms step_avg:40.46ms
step:836/2330 train_time:33825ms step_avg:40.46ms
step:837/2330 train_time:33862ms step_avg:40.46ms
step:838/2330 train_time:33907ms step_avg:40.46ms
step:839/2330 train_time:33943ms step_avg:40.46ms
step:840/2330 train_time:33988ms step_avg:40.46ms
step:841/2330 train_time:34023ms step_avg:40.46ms
step:842/2330 train_time:34068ms step_avg:40.46ms
step:843/2330 train_time:34104ms step_avg:40.46ms
step:844/2330 train_time:34148ms step_avg:40.46ms
step:845/2330 train_time:34184ms step_avg:40.45ms
step:846/2330 train_time:34230ms step_avg:40.46ms
step:847/2330 train_time:34265ms step_avg:40.45ms
step:848/2330 train_time:34310ms step_avg:40.46ms
step:849/2330 train_time:34347ms step_avg:40.46ms
step:850/2330 train_time:34392ms step_avg:40.46ms
step:851/2330 train_time:34428ms step_avg:40.46ms
step:852/2330 train_time:34473ms step_avg:40.46ms
step:853/2330 train_time:34508ms step_avg:40.45ms
step:854/2330 train_time:34553ms step_avg:40.46ms
step:855/2330 train_time:34588ms step_avg:40.45ms
step:856/2330 train_time:34633ms step_avg:40.46ms
step:857/2330 train_time:34669ms step_avg:40.45ms
step:858/2330 train_time:34715ms step_avg:40.46ms
step:859/2330 train_time:34750ms step_avg:40.45ms
step:860/2330 train_time:34795ms step_avg:40.46ms
step:861/2330 train_time:34830ms step_avg:40.45ms
step:862/2330 train_time:34875ms step_avg:40.46ms
step:863/2330 train_time:34911ms step_avg:40.45ms
step:864/2330 train_time:34956ms step_avg:40.46ms
step:865/2330 train_time:34991ms step_avg:40.45ms
step:866/2330 train_time:35036ms step_avg:40.46ms
step:867/2330 train_time:35071ms step_avg:40.45ms
step:868/2330 train_time:35117ms step_avg:40.46ms
step:869/2330 train_time:35152ms step_avg:40.45ms
step:870/2330 train_time:35198ms step_avg:40.46ms
step:871/2330 train_time:35233ms step_avg:40.45ms
step:872/2330 train_time:35278ms step_avg:40.46ms
step:873/2330 train_time:35314ms step_avg:40.45ms
step:874/2330 train_time:35359ms step_avg:40.46ms
step:875/2330 train_time:35395ms step_avg:40.45ms
step:876/2330 train_time:35440ms step_avg:40.46ms
step:877/2330 train_time:35476ms step_avg:40.45ms
step:878/2330 train_time:35522ms step_avg:40.46ms
step:879/2330 train_time:35558ms step_avg:40.45ms
step:880/2330 train_time:35604ms step_avg:40.46ms
step:881/2330 train_time:35640ms step_avg:40.45ms
step:882/2330 train_time:35685ms step_avg:40.46ms
step:883/2330 train_time:35720ms step_avg:40.45ms
step:884/2330 train_time:35765ms step_avg:40.46ms
step:885/2330 train_time:35801ms step_avg:40.45ms
step:886/2330 train_time:35846ms step_avg:40.46ms
step:887/2330 train_time:35881ms step_avg:40.45ms
step:888/2330 train_time:35927ms step_avg:40.46ms
step:889/2330 train_time:35963ms step_avg:40.45ms
step:890/2330 train_time:36008ms step_avg:40.46ms
step:891/2330 train_time:36044ms step_avg:40.45ms
step:892/2330 train_time:36088ms step_avg:40.46ms
step:893/2330 train_time:36124ms step_avg:40.45ms
step:894/2330 train_time:36170ms step_avg:40.46ms
step:895/2330 train_time:36206ms step_avg:40.45ms
step:896/2330 train_time:36252ms step_avg:40.46ms
step:897/2330 train_time:36288ms step_avg:40.45ms
step:898/2330 train_time:36333ms step_avg:40.46ms
step:899/2330 train_time:36369ms step_avg:40.46ms
step:900/2330 train_time:36415ms step_avg:40.46ms
step:901/2330 train_time:36450ms step_avg:40.46ms
step:902/2330 train_time:36497ms step_avg:40.46ms
step:903/2330 train_time:36532ms step_avg:40.46ms
step:904/2330 train_time:36577ms step_avg:40.46ms
step:905/2330 train_time:36612ms step_avg:40.46ms
step:906/2330 train_time:36657ms step_avg:40.46ms
step:907/2330 train_time:36692ms step_avg:40.45ms
step:908/2330 train_time:36738ms step_avg:40.46ms
step:909/2330 train_time:36772ms step_avg:40.45ms
step:910/2330 train_time:36817ms step_avg:40.46ms
step:911/2330 train_time:36852ms step_avg:40.45ms
step:912/2330 train_time:36898ms step_avg:40.46ms
step:913/2330 train_time:36934ms step_avg:40.45ms
step:914/2330 train_time:36979ms step_avg:40.46ms
step:915/2330 train_time:37014ms step_avg:40.45ms
step:916/2330 train_time:37060ms step_avg:40.46ms
step:917/2330 train_time:37096ms step_avg:40.45ms
step:918/2330 train_time:37140ms step_avg:40.46ms
step:919/2330 train_time:37176ms step_avg:40.45ms
step:920/2330 train_time:37222ms step_avg:40.46ms
step:921/2330 train_time:37257ms step_avg:40.45ms
step:922/2330 train_time:37302ms step_avg:40.46ms
step:923/2330 train_time:37339ms step_avg:40.45ms
step:924/2330 train_time:37384ms step_avg:40.46ms
step:925/2330 train_time:37419ms step_avg:40.45ms
step:926/2330 train_time:37464ms step_avg:40.46ms
step:927/2330 train_time:37500ms step_avg:40.45ms
step:928/2330 train_time:37545ms step_avg:40.46ms
step:929/2330 train_time:37580ms step_avg:40.45ms
step:930/2330 train_time:37626ms step_avg:40.46ms
step:931/2330 train_time:37662ms step_avg:40.45ms
step:932/2330 train_time:37708ms step_avg:40.46ms
step:933/2330 train_time:37744ms step_avg:40.45ms
step:934/2330 train_time:37790ms step_avg:40.46ms
step:935/2330 train_time:37826ms step_avg:40.46ms
step:936/2330 train_time:37871ms step_avg:40.46ms
step:937/2330 train_time:37906ms step_avg:40.46ms
step:938/2330 train_time:37951ms step_avg:40.46ms
step:939/2330 train_time:37986ms step_avg:40.45ms
step:940/2330 train_time:38032ms step_avg:40.46ms
step:941/2330 train_time:38068ms step_avg:40.45ms
step:942/2330 train_time:38113ms step_avg:40.46ms
step:943/2330 train_time:38149ms step_avg:40.45ms
step:944/2330 train_time:38194ms step_avg:40.46ms
step:945/2330 train_time:38230ms step_avg:40.45ms
step:946/2330 train_time:38274ms step_avg:40.46ms
step:947/2330 train_time:38311ms step_avg:40.45ms
step:948/2330 train_time:38356ms step_avg:40.46ms
step:949/2330 train_time:38391ms step_avg:40.45ms
step:950/2330 train_time:38437ms step_avg:40.46ms
step:951/2330 train_time:38472ms step_avg:40.45ms
step:952/2330 train_time:38517ms step_avg:40.46ms
step:953/2330 train_time:38552ms step_avg:40.45ms
step:954/2330 train_time:38597ms step_avg:40.46ms
step:955/2330 train_time:38632ms step_avg:40.45ms
step:956/2330 train_time:38677ms step_avg:40.46ms
step:957/2330 train_time:38712ms step_avg:40.45ms
step:958/2330 train_time:38757ms step_avg:40.46ms
step:959/2330 train_time:38793ms step_avg:40.45ms
step:960/2330 train_time:38838ms step_avg:40.46ms
step:961/2330 train_time:38873ms step_avg:40.45ms
step:962/2330 train_time:38919ms step_avg:40.46ms
step:963/2330 train_time:38955ms step_avg:40.45ms
step:964/2330 train_time:39000ms step_avg:40.46ms
step:965/2330 train_time:39035ms step_avg:40.45ms
step:966/2330 train_time:39082ms step_avg:40.46ms
step:967/2330 train_time:39117ms step_avg:40.45ms
step:968/2330 train_time:39163ms step_avg:40.46ms
step:969/2330 train_time:39199ms step_avg:40.45ms
step:970/2330 train_time:39244ms step_avg:40.46ms
step:971/2330 train_time:39280ms step_avg:40.45ms
step:972/2330 train_time:39326ms step_avg:40.46ms
step:973/2330 train_time:39362ms step_avg:40.45ms
step:974/2330 train_time:39407ms step_avg:40.46ms
step:975/2330 train_time:39444ms step_avg:40.45ms
step:976/2330 train_time:39489ms step_avg:40.46ms
step:977/2330 train_time:39525ms step_avg:40.46ms
step:978/2330 train_time:39569ms step_avg:40.46ms
step:979/2330 train_time:39604ms step_avg:40.45ms
step:980/2330 train_time:39649ms step_avg:40.46ms
step:981/2330 train_time:39685ms step_avg:40.45ms
step:982/2330 train_time:39731ms step_avg:40.46ms
step:983/2330 train_time:39767ms step_avg:40.45ms
step:984/2330 train_time:39811ms step_avg:40.46ms
step:985/2330 train_time:39847ms step_avg:40.45ms
step:986/2330 train_time:39892ms step_avg:40.46ms
step:987/2330 train_time:39929ms step_avg:40.45ms
step:988/2330 train_time:39975ms step_avg:40.46ms
step:989/2330 train_time:40010ms step_avg:40.45ms
step:990/2330 train_time:40055ms step_avg:40.46ms
step:991/2330 train_time:40091ms step_avg:40.46ms
step:992/2330 train_time:40137ms step_avg:40.46ms
step:993/2330 train_time:40172ms step_avg:40.45ms
step:994/2330 train_time:40217ms step_avg:40.46ms
step:995/2330 train_time:40252ms step_avg:40.45ms
step:996/2330 train_time:40296ms step_avg:40.46ms
step:997/2330 train_time:40332ms step_avg:40.45ms
step:998/2330 train_time:40377ms step_avg:40.46ms
step:999/2330 train_time:40413ms step_avg:40.45ms
step:1000/2330 train_time:40457ms step_avg:40.46ms
step:1000/2330 val_loss:5.1886 train_time:40544ms step_avg:40.54ms
step:1001/2330 train_time:40556ms step_avg:40.52ms
step:1002/2330 train_time:40569ms step_avg:40.49ms
step:1003/2330 train_time:40579ms step_avg:40.46ms
step:1004/2330 train_time:40616ms step_avg:40.45ms
step:1005/2330 train_time:40650ms step_avg:40.45ms
step:1006/2330 train_time:40695ms step_avg:40.45ms
step:1007/2330 train_time:40729ms step_avg:40.45ms
step:1008/2330 train_time:40773ms step_avg:40.45ms
step:1009/2330 train_time:40808ms step_avg:40.44ms
step:1010/2330 train_time:40854ms step_avg:40.45ms
step:1011/2330 train_time:40894ms step_avg:40.45ms
step:1012/2330 train_time:40940ms step_avg:40.45ms
step:1013/2330 train_time:40976ms step_avg:40.45ms
step:1014/2330 train_time:41022ms step_avg:40.46ms
step:1015/2330 train_time:41058ms step_avg:40.45ms
step:1016/2330 train_time:41103ms step_avg:40.46ms
step:1017/2330 train_time:41137ms step_avg:40.45ms
step:1018/2330 train_time:41182ms step_avg:40.45ms
step:1019/2330 train_time:41216ms step_avg:40.45ms
step:1020/2330 train_time:41261ms step_avg:40.45ms
step:1021/2330 train_time:41296ms step_avg:40.45ms
step:1022/2330 train_time:41341ms step_avg:40.45ms
step:1023/2330 train_time:41377ms step_avg:40.45ms
step:1024/2330 train_time:41421ms step_avg:40.45ms
step:1025/2330 train_time:41458ms step_avg:40.45ms
step:1026/2330 train_time:41505ms step_avg:40.45ms
step:1027/2330 train_time:41541ms step_avg:40.45ms
step:1028/2330 train_time:41586ms step_avg:40.45ms
step:1029/2330 train_time:41622ms step_avg:40.45ms
step:1030/2330 train_time:41667ms step_avg:40.45ms
step:1031/2330 train_time:41703ms step_avg:40.45ms
step:1032/2330 train_time:41747ms step_avg:40.45ms
step:1033/2330 train_time:41782ms step_avg:40.45ms
step:1034/2330 train_time:41828ms step_avg:40.45ms
step:1035/2330 train_time:41865ms step_avg:40.45ms
step:1036/2330 train_time:41913ms step_avg:40.46ms
step:1037/2330 train_time:41949ms step_avg:40.45ms
step:1038/2330 train_time:41995ms step_avg:40.46ms
step:1039/2330 train_time:42030ms step_avg:40.45ms
step:1040/2330 train_time:42075ms step_avg:40.46ms
step:1041/2330 train_time:42110ms step_avg:40.45ms
step:1042/2330 train_time:42154ms step_avg:40.46ms
step:1043/2330 train_time:42191ms step_avg:40.45ms
step:1044/2330 train_time:42235ms step_avg:40.46ms
step:1045/2330 train_time:42271ms step_avg:40.45ms
step:1046/2330 train_time:42316ms step_avg:40.45ms
step:1047/2330 train_time:42352ms step_avg:40.45ms
step:1048/2330 train_time:42397ms step_avg:40.45ms
step:1049/2330 train_time:42432ms step_avg:40.45ms
step:1050/2330 train_time:42478ms step_avg:40.46ms
step:1051/2330 train_time:42513ms step_avg:40.45ms
step:1052/2330 train_time:42558ms step_avg:40.45ms
step:1053/2330 train_time:42593ms step_avg:40.45ms
step:1054/2330 train_time:42639ms step_avg:40.45ms
step:1055/2330 train_time:42674ms step_avg:40.45ms
step:1056/2330 train_time:42720ms step_avg:40.45ms
step:1057/2330 train_time:42755ms step_avg:40.45ms
step:1058/2330 train_time:42801ms step_avg:40.45ms
step:1059/2330 train_time:42837ms step_avg:40.45ms
step:1060/2330 train_time:42882ms step_avg:40.45ms
step:1061/2330 train_time:42917ms step_avg:40.45ms
step:1062/2330 train_time:42963ms step_avg:40.45ms
step:1063/2330 train_time:42998ms step_avg:40.45ms
step:1064/2330 train_time:43043ms step_avg:40.45ms
step:1065/2330 train_time:43079ms step_avg:40.45ms
step:1066/2330 train_time:43123ms step_avg:40.45ms
step:1067/2330 train_time:43160ms step_avg:40.45ms
step:1068/2330 train_time:43205ms step_avg:40.45ms
step:1069/2330 train_time:43240ms step_avg:40.45ms
step:1070/2330 train_time:43285ms step_avg:40.45ms
step:1071/2330 train_time:43321ms step_avg:40.45ms
step:1072/2330 train_time:43366ms step_avg:40.45ms
step:1073/2330 train_time:43402ms step_avg:40.45ms
step:1074/2330 train_time:43447ms step_avg:40.45ms
step:1075/2330 train_time:43482ms step_avg:40.45ms
step:1076/2330 train_time:43527ms step_avg:40.45ms
step:1077/2330 train_time:43563ms step_avg:40.45ms
step:1078/2330 train_time:43608ms step_avg:40.45ms
step:1079/2330 train_time:43643ms step_avg:40.45ms
step:1080/2330 train_time:43688ms step_avg:40.45ms
step:1081/2330 train_time:43724ms step_avg:40.45ms
step:1082/2330 train_time:43770ms step_avg:40.45ms
step:1083/2330 train_time:43806ms step_avg:40.45ms
step:1084/2330 train_time:43851ms step_avg:40.45ms
step:1085/2330 train_time:43888ms step_avg:40.45ms
step:1086/2330 train_time:43934ms step_avg:40.45ms
step:1087/2330 train_time:43970ms step_avg:40.45ms
step:1088/2330 train_time:44015ms step_avg:40.45ms
step:1089/2330 train_time:44051ms step_avg:40.45ms
step:1090/2330 train_time:44096ms step_avg:40.45ms
step:1091/2330 train_time:44131ms step_avg:40.45ms
step:1092/2330 train_time:44177ms step_avg:40.45ms
step:1093/2330 train_time:44211ms step_avg:40.45ms
step:1094/2330 train_time:44256ms step_avg:40.45ms
step:1095/2330 train_time:44292ms step_avg:40.45ms
step:1096/2330 train_time:44337ms step_avg:40.45ms
step:1097/2330 train_time:44373ms step_avg:40.45ms
step:1098/2330 train_time:44417ms step_avg:40.45ms
step:1099/2330 train_time:44452ms step_avg:40.45ms
step:1100/2330 train_time:44497ms step_avg:40.45ms
step:1101/2330 train_time:44533ms step_avg:40.45ms
step:1102/2330 train_time:44578ms step_avg:40.45ms
step:1103/2330 train_time:44613ms step_avg:40.45ms
step:1104/2330 train_time:44657ms step_avg:40.45ms
step:1105/2330 train_time:44692ms step_avg:40.45ms
step:1106/2330 train_time:44738ms step_avg:40.45ms
step:1107/2330 train_time:44774ms step_avg:40.45ms
step:1108/2330 train_time:44819ms step_avg:40.45ms
step:1109/2330 train_time:44855ms step_avg:40.45ms
step:1110/2330 train_time:44900ms step_avg:40.45ms
step:1111/2330 train_time:44936ms step_avg:40.45ms
step:1112/2330 train_time:44981ms step_avg:40.45ms
step:1113/2330 train_time:45017ms step_avg:40.45ms
step:1114/2330 train_time:45063ms step_avg:40.45ms
step:1115/2330 train_time:45099ms step_avg:40.45ms
step:1116/2330 train_time:45145ms step_avg:40.45ms
step:1117/2330 train_time:45180ms step_avg:40.45ms
step:1118/2330 train_time:45225ms step_avg:40.45ms
step:1119/2330 train_time:45261ms step_avg:40.45ms
step:1120/2330 train_time:45305ms step_avg:40.45ms
step:1121/2330 train_time:45341ms step_avg:40.45ms
step:1122/2330 train_time:45386ms step_avg:40.45ms
step:1123/2330 train_time:45421ms step_avg:40.45ms
step:1124/2330 train_time:45467ms step_avg:40.45ms
step:1125/2330 train_time:45502ms step_avg:40.45ms
step:1126/2330 train_time:45547ms step_avg:40.45ms
step:1127/2330 train_time:45582ms step_avg:40.45ms
step:1128/2330 train_time:45628ms step_avg:40.45ms
step:1129/2330 train_time:45663ms step_avg:40.45ms
step:1130/2330 train_time:45708ms step_avg:40.45ms
step:1131/2330 train_time:45744ms step_avg:40.45ms
step:1132/2330 train_time:45789ms step_avg:40.45ms
step:1133/2330 train_time:45825ms step_avg:40.45ms
step:1134/2330 train_time:45871ms step_avg:40.45ms
step:1135/2330 train_time:45907ms step_avg:40.45ms
step:1136/2330 train_time:45954ms step_avg:40.45ms
step:1137/2330 train_time:45990ms step_avg:40.45ms
step:1138/2330 train_time:46035ms step_avg:40.45ms
step:1139/2330 train_time:46071ms step_avg:40.45ms
step:1140/2330 train_time:46116ms step_avg:40.45ms
step:1141/2330 train_time:46150ms step_avg:40.45ms
step:1142/2330 train_time:46196ms step_avg:40.45ms
step:1143/2330 train_time:46232ms step_avg:40.45ms
step:1144/2330 train_time:46277ms step_avg:40.45ms
step:1145/2330 train_time:46312ms step_avg:40.45ms
step:1146/2330 train_time:46357ms step_avg:40.45ms
step:1147/2330 train_time:46393ms step_avg:40.45ms
step:1148/2330 train_time:46437ms step_avg:40.45ms
step:1149/2330 train_time:46472ms step_avg:40.45ms
step:1150/2330 train_time:46517ms step_avg:40.45ms
step:1151/2330 train_time:46552ms step_avg:40.44ms
step:1152/2330 train_time:46597ms step_avg:40.45ms
step:1153/2330 train_time:46633ms step_avg:40.44ms
step:1154/2330 train_time:46679ms step_avg:40.45ms
step:1155/2330 train_time:46714ms step_avg:40.44ms
step:1156/2330 train_time:46759ms step_avg:40.45ms
step:1157/2330 train_time:46795ms step_avg:40.45ms
step:1158/2330 train_time:46840ms step_avg:40.45ms
step:1159/2330 train_time:46876ms step_avg:40.45ms
step:1160/2330 train_time:46923ms step_avg:40.45ms
step:1161/2330 train_time:46959ms step_avg:40.45ms
step:1162/2330 train_time:47005ms step_avg:40.45ms
step:1163/2330 train_time:47041ms step_avg:40.45ms
step:1164/2330 train_time:47086ms step_avg:40.45ms
step:1165/2330 train_time:47122ms step_avg:40.45ms
step:1166/2330 train_time:47167ms step_avg:40.45ms
step:1167/2330 train_time:47203ms step_avg:40.45ms
step:1168/2330 train_time:47248ms step_avg:40.45ms
step:1169/2330 train_time:47285ms step_avg:40.45ms
step:1170/2330 train_time:47330ms step_avg:40.45ms
step:1171/2330 train_time:47366ms step_avg:40.45ms
step:1172/2330 train_time:47412ms step_avg:40.45ms
step:1173/2330 train_time:47447ms step_avg:40.45ms
step:1174/2330 train_time:47492ms step_avg:40.45ms
step:1175/2330 train_time:47527ms step_avg:40.45ms
step:1176/2330 train_time:47572ms step_avg:40.45ms
step:1177/2330 train_time:47608ms step_avg:40.45ms
step:1178/2330 train_time:47653ms step_avg:40.45ms
step:1179/2330 train_time:47689ms step_avg:40.45ms
step:1180/2330 train_time:47733ms step_avg:40.45ms
step:1181/2330 train_time:47770ms step_avg:40.45ms
step:1182/2330 train_time:47815ms step_avg:40.45ms
step:1183/2330 train_time:47851ms step_avg:40.45ms
step:1184/2330 train_time:47896ms step_avg:40.45ms
step:1185/2330 train_time:47932ms step_avg:40.45ms
step:1186/2330 train_time:47977ms step_avg:40.45ms
step:1187/2330 train_time:48012ms step_avg:40.45ms
step:1188/2330 train_time:48058ms step_avg:40.45ms
step:1189/2330 train_time:48093ms step_avg:40.45ms
step:1190/2330 train_time:48138ms step_avg:40.45ms
step:1191/2330 train_time:48173ms step_avg:40.45ms
step:1192/2330 train_time:48218ms step_avg:40.45ms
step:1193/2330 train_time:48253ms step_avg:40.45ms
step:1194/2330 train_time:48297ms step_avg:40.45ms
step:1195/2330 train_time:48333ms step_avg:40.45ms
step:1196/2330 train_time:48379ms step_avg:40.45ms
step:1197/2330 train_time:48415ms step_avg:40.45ms
step:1198/2330 train_time:48460ms step_avg:40.45ms
step:1199/2330 train_time:48496ms step_avg:40.45ms
step:1200/2330 train_time:48541ms step_avg:40.45ms
step:1201/2330 train_time:48576ms step_avg:40.45ms
step:1202/2330 train_time:48621ms step_avg:40.45ms
step:1203/2330 train_time:48656ms step_avg:40.45ms
step:1204/2330 train_time:48701ms step_avg:40.45ms
step:1205/2330 train_time:48736ms step_avg:40.45ms
step:1206/2330 train_time:48782ms step_avg:40.45ms
step:1207/2330 train_time:48817ms step_avg:40.45ms
step:1208/2330 train_time:48863ms step_avg:40.45ms
step:1209/2330 train_time:48899ms step_avg:40.45ms
step:1210/2330 train_time:48945ms step_avg:40.45ms
step:1211/2330 train_time:48980ms step_avg:40.45ms
step:1212/2330 train_time:49025ms step_avg:40.45ms
step:1213/2330 train_time:49061ms step_avg:40.45ms
step:1214/2330 train_time:49106ms step_avg:40.45ms
step:1215/2330 train_time:49142ms step_avg:40.45ms
step:1216/2330 train_time:49186ms step_avg:40.45ms
step:1217/2330 train_time:49222ms step_avg:40.45ms
step:1218/2330 train_time:49267ms step_avg:40.45ms
step:1219/2330 train_time:49303ms step_avg:40.45ms
step:1220/2330 train_time:49348ms step_avg:40.45ms
step:1221/2330 train_time:49383ms step_avg:40.44ms
step:1222/2330 train_time:49429ms step_avg:40.45ms
step:1223/2330 train_time:49465ms step_avg:40.45ms
step:1224/2330 train_time:49511ms step_avg:40.45ms
step:1225/2330 train_time:49546ms step_avg:40.45ms
step:1226/2330 train_time:49591ms step_avg:40.45ms
step:1227/2330 train_time:49627ms step_avg:40.45ms
step:1228/2330 train_time:49673ms step_avg:40.45ms
step:1229/2330 train_time:49708ms step_avg:40.45ms
step:1230/2330 train_time:49754ms step_avg:40.45ms
step:1231/2330 train_time:49789ms step_avg:40.45ms
step:1232/2330 train_time:49835ms step_avg:40.45ms
step:1233/2330 train_time:49870ms step_avg:40.45ms
step:1234/2330 train_time:49916ms step_avg:40.45ms
step:1235/2330 train_time:49952ms step_avg:40.45ms
step:1236/2330 train_time:49997ms step_avg:40.45ms
step:1237/2330 train_time:50032ms step_avg:40.45ms
step:1238/2330 train_time:50077ms step_avg:40.45ms
step:1239/2330 train_time:50112ms step_avg:40.45ms
step:1240/2330 train_time:50157ms step_avg:40.45ms
step:1241/2330 train_time:50192ms step_avg:40.44ms
step:1242/2330 train_time:50238ms step_avg:40.45ms
step:1243/2330 train_time:50273ms step_avg:40.44ms
step:1244/2330 train_time:50317ms step_avg:40.45ms
step:1245/2330 train_time:50353ms step_avg:40.44ms
step:1246/2330 train_time:50399ms step_avg:40.45ms
step:1247/2330 train_time:50434ms step_avg:40.44ms
step:1248/2330 train_time:50480ms step_avg:40.45ms
step:1249/2330 train_time:50515ms step_avg:40.44ms
step:1250/2330 train_time:50560ms step_avg:40.45ms
step:1250/2330 val_loss:5.1650 train_time:50649ms step_avg:40.52ms
step:1251/2330 train_time:50662ms step_avg:40.50ms
step:1252/2330 train_time:50674ms step_avg:40.47ms
step:1253/2330 train_time:50685ms step_avg:40.45ms
step:1254/2330 train_time:50722ms step_avg:40.45ms
step:1255/2330 train_time:50757ms step_avg:40.44ms
step:1256/2330 train_time:50801ms step_avg:40.45ms
step:1257/2330 train_time:50837ms step_avg:40.44ms
step:1258/2330 train_time:50881ms step_avg:40.45ms
step:1259/2330 train_time:50917ms step_avg:40.44ms
step:1260/2330 train_time:50966ms step_avg:40.45ms
step:1261/2330 train_time:51005ms step_avg:40.45ms
step:1262/2330 train_time:51053ms step_avg:40.45ms
step:1263/2330 train_time:51088ms step_avg:40.45ms
step:1264/2330 train_time:51134ms step_avg:40.45ms
step:1265/2330 train_time:51171ms step_avg:40.45ms
step:1266/2330 train_time:51215ms step_avg:40.45ms
step:1267/2330 train_time:51343ms step_avg:40.52ms
step:1268/2330 train_time:51386ms step_avg:40.53ms
step:1269/2330 train_time:51421ms step_avg:40.52ms
step:1270/2330 train_time:51465ms step_avg:40.52ms
step:1271/2330 train_time:51689ms step_avg:40.67ms
step:1272/2330 train_time:51732ms step_avg:40.67ms
step:1273/2330 train_time:51766ms step_avg:40.66ms
step:1274/2330 train_time:51811ms step_avg:40.67ms
step:1275/2330 train_time:51846ms step_avg:40.66ms
step:1276/2330 train_time:51890ms step_avg:40.67ms
step:1277/2330 train_time:51925ms step_avg:40.66ms
step:1278/2330 train_time:51969ms step_avg:40.66ms
step:1279/2330 train_time:52004ms step_avg:40.66ms
step:1280/2330 train_time:52048ms step_avg:40.66ms
step:1281/2330 train_time:52082ms step_avg:40.66ms
step:1282/2330 train_time:52126ms step_avg:40.66ms
step:1283/2330 train_time:52161ms step_avg:40.66ms
step:1284/2330 train_time:52205ms step_avg:40.66ms
step:1285/2330 train_time:52239ms step_avg:40.65ms
step:1286/2330 train_time:52283ms step_avg:40.66ms
step:1287/2330 train_time:52318ms step_avg:40.65ms
step:1288/2330 train_time:52361ms step_avg:40.65ms
step:1289/2330 train_time:52396ms step_avg:40.65ms
step:1290/2330 train_time:52440ms step_avg:40.65ms
step:1291/2330 train_time:52475ms step_avg:40.65ms
step:1292/2330 train_time:52519ms step_avg:40.65ms
step:1293/2330 train_time:52558ms step_avg:40.65ms
step:1294/2330 train_time:52611ms step_avg:40.66ms
step:1295/2330 train_time:52649ms step_avg:40.66ms
step:1296/2330 train_time:52694ms step_avg:40.66ms
step:1297/2330 train_time:52730ms step_avg:40.66ms
step:1298/2330 train_time:52776ms step_avg:40.66ms
step:1299/2330 train_time:52812ms step_avg:40.66ms
step:1300/2330 train_time:52857ms step_avg:40.66ms
step:1301/2330 train_time:52893ms step_avg:40.66ms
step:1302/2330 train_time:52939ms step_avg:40.66ms
step:1303/2330 train_time:52974ms step_avg:40.66ms
step:1304/2330 train_time:53019ms step_avg:40.66ms
step:1305/2330 train_time:53054ms step_avg:40.65ms
step:1306/2330 train_time:53099ms step_avg:40.66ms
step:1307/2330 train_time:53134ms step_avg:40.65ms
step:1308/2330 train_time:53178ms step_avg:40.66ms
step:1309/2330 train_time:53213ms step_avg:40.65ms
step:1310/2330 train_time:53257ms step_avg:40.65ms
step:1311/2330 train_time:53292ms step_avg:40.65ms
step:1312/2330 train_time:53336ms step_avg:40.65ms
step:1313/2330 train_time:53370ms step_avg:40.65ms
step:1314/2330 train_time:53414ms step_avg:40.65ms
step:1315/2330 train_time:53449ms step_avg:40.65ms
step:1316/2330 train_time:53495ms step_avg:40.65ms
step:1317/2330 train_time:53532ms step_avg:40.65ms
step:1318/2330 train_time:53579ms step_avg:40.65ms
step:1319/2330 train_time:53617ms step_avg:40.65ms
step:1320/2330 train_time:53664ms step_avg:40.65ms
step:1321/2330 train_time:53700ms step_avg:40.65ms
step:1322/2330 train_time:53745ms step_avg:40.65ms
step:1323/2330 train_time:53781ms step_avg:40.65ms
step:1324/2330 train_time:53826ms step_avg:40.65ms
step:1325/2330 train_time:53861ms step_avg:40.65ms
step:1326/2330 train_time:53906ms step_avg:40.65ms
step:1327/2330 train_time:53942ms step_avg:40.65ms
step:1328/2330 train_time:53986ms step_avg:40.65ms
step:1329/2330 train_time:54022ms step_avg:40.65ms
step:1330/2330 train_time:54067ms step_avg:40.65ms
step:1331/2330 train_time:54101ms step_avg:40.65ms
step:1332/2330 train_time:54146ms step_avg:40.65ms
step:1333/2330 train_time:54181ms step_avg:40.65ms
step:1334/2330 train_time:54226ms step_avg:40.65ms
step:1335/2330 train_time:54261ms step_avg:40.64ms
step:1336/2330 train_time:54306ms step_avg:40.65ms
step:1337/2330 train_time:54342ms step_avg:40.64ms
step:1338/2330 train_time:54387ms step_avg:40.65ms
step:1339/2330 train_time:54422ms step_avg:40.64ms
step:1340/2330 train_time:54467ms step_avg:40.65ms
step:1341/2330 train_time:54503ms step_avg:40.64ms
step:1342/2330 train_time:54550ms step_avg:40.65ms
step:1343/2330 train_time:54585ms step_avg:40.64ms
step:1344/2330 train_time:54632ms step_avg:40.65ms
step:1345/2330 train_time:54669ms step_avg:40.65ms
step:1346/2330 train_time:54715ms step_avg:40.65ms
step:1347/2330 train_time:54751ms step_avg:40.65ms
step:1348/2330 train_time:54797ms step_avg:40.65ms
step:1349/2330 train_time:54833ms step_avg:40.65ms
step:1350/2330 train_time:54878ms step_avg:40.65ms
step:1351/2330 train_time:54914ms step_avg:40.65ms
step:1352/2330 train_time:54958ms step_avg:40.65ms
step:1353/2330 train_time:54994ms step_avg:40.65ms
step:1354/2330 train_time:55040ms step_avg:40.65ms
step:1355/2330 train_time:55074ms step_avg:40.65ms
step:1356/2330 train_time:55119ms step_avg:40.65ms
step:1357/2330 train_time:55155ms step_avg:40.64ms
step:1358/2330 train_time:55199ms step_avg:40.65ms
step:1359/2330 train_time:55235ms step_avg:40.64ms
step:1360/2330 train_time:55279ms step_avg:40.65ms
step:1361/2330 train_time:55314ms step_avg:40.64ms
step:1362/2330 train_time:55359ms step_avg:40.65ms
step:1363/2330 train_time:55395ms step_avg:40.64ms
step:1364/2330 train_time:55441ms step_avg:40.65ms
step:1365/2330 train_time:55476ms step_avg:40.64ms
step:1366/2330 train_time:55523ms step_avg:40.65ms
step:1367/2330 train_time:55559ms step_avg:40.64ms
step:1368/2330 train_time:55605ms step_avg:40.65ms
step:1369/2330 train_time:55641ms step_avg:40.64ms
step:1370/2330 train_time:55687ms step_avg:40.65ms
step:1371/2330 train_time:55722ms step_avg:40.64ms
step:1372/2330 train_time:55766ms step_avg:40.65ms
step:1373/2330 train_time:55801ms step_avg:40.64ms
step:1374/2330 train_time:55846ms step_avg:40.65ms
step:1375/2330 train_time:55882ms step_avg:40.64ms
step:1376/2330 train_time:55927ms step_avg:40.64ms
step:1377/2330 train_time:55962ms step_avg:40.64ms
step:1378/2330 train_time:56007ms step_avg:40.64ms
step:1379/2330 train_time:56043ms step_avg:40.64ms
step:1380/2330 train_time:56088ms step_avg:40.64ms
step:1381/2330 train_time:56123ms step_avg:40.64ms
step:1382/2330 train_time:56169ms step_avg:40.64ms
step:1383/2330 train_time:56204ms step_avg:40.64ms
step:1384/2330 train_time:56249ms step_avg:40.64ms
step:1385/2330 train_time:56284ms step_avg:40.64ms
step:1386/2330 train_time:56329ms step_avg:40.64ms
step:1387/2330 train_time:56365ms step_avg:40.64ms
step:1388/2330 train_time:56409ms step_avg:40.64ms
step:1389/2330 train_time:56445ms step_avg:40.64ms
step:1390/2330 train_time:56490ms step_avg:40.64ms
step:1391/2330 train_time:56527ms step_avg:40.64ms
step:1392/2330 train_time:56572ms step_avg:40.64ms
step:1393/2330 train_time:56608ms step_avg:40.64ms
step:1394/2330 train_time:56654ms step_avg:40.64ms
step:1395/2330 train_time:56689ms step_avg:40.64ms
step:1396/2330 train_time:56734ms step_avg:40.64ms
step:1397/2330 train_time:56770ms step_avg:40.64ms
step:1398/2330 train_time:56815ms step_avg:40.64ms
step:1399/2330 train_time:56850ms step_avg:40.64ms
step:1400/2330 train_time:56895ms step_avg:40.64ms
step:1401/2330 train_time:56931ms step_avg:40.64ms
step:1402/2330 train_time:56976ms step_avg:40.64ms
step:1403/2330 train_time:57012ms step_avg:40.64ms
step:1404/2330 train_time:57057ms step_avg:40.64ms
step:1405/2330 train_time:57092ms step_avg:40.63ms
step:1406/2330 train_time:57137ms step_avg:40.64ms
step:1407/2330 train_time:57173ms step_avg:40.63ms
step:1408/2330 train_time:57218ms step_avg:40.64ms
step:1409/2330 train_time:57253ms step_avg:40.63ms
step:1410/2330 train_time:57299ms step_avg:40.64ms
step:1411/2330 train_time:57334ms step_avg:40.63ms
step:1412/2330 train_time:57379ms step_avg:40.64ms
step:1413/2330 train_time:57413ms step_avg:40.63ms
step:1414/2330 train_time:57459ms step_avg:40.64ms
step:1415/2330 train_time:57496ms step_avg:40.63ms
step:1416/2330 train_time:57541ms step_avg:40.64ms
step:1417/2330 train_time:57577ms step_avg:40.63ms
step:1418/2330 train_time:57623ms step_avg:40.64ms
step:1419/2330 train_time:57658ms step_avg:40.63ms
step:1420/2330 train_time:57704ms step_avg:40.64ms
step:1421/2330 train_time:57739ms step_avg:40.63ms
step:1422/2330 train_time:57784ms step_avg:40.64ms
step:1423/2330 train_time:57820ms step_avg:40.63ms
step:1424/2330 train_time:57865ms step_avg:40.64ms
step:1425/2330 train_time:57900ms step_avg:40.63ms
step:1426/2330 train_time:57945ms step_avg:40.63ms
step:1427/2330 train_time:57981ms step_avg:40.63ms
step:1428/2330 train_time:58026ms step_avg:40.63ms
step:1429/2330 train_time:58061ms step_avg:40.63ms
step:1430/2330 train_time:58107ms step_avg:40.63ms
step:1431/2330 train_time:58142ms step_avg:40.63ms
step:1432/2330 train_time:58186ms step_avg:40.63ms
step:1433/2330 train_time:58222ms step_avg:40.63ms
step:1434/2330 train_time:58267ms step_avg:40.63ms
step:1435/2330 train_time:58302ms step_avg:40.63ms
step:1436/2330 train_time:58347ms step_avg:40.63ms
step:1437/2330 train_time:58383ms step_avg:40.63ms
step:1438/2330 train_time:58429ms step_avg:40.63ms
step:1439/2330 train_time:58465ms step_avg:40.63ms
step:1440/2330 train_time:58511ms step_avg:40.63ms
step:1441/2330 train_time:58547ms step_avg:40.63ms
step:1442/2330 train_time:58593ms step_avg:40.63ms
step:1443/2330 train_time:58628ms step_avg:40.63ms
step:1444/2330 train_time:58674ms step_avg:40.63ms
step:1445/2330 train_time:58710ms step_avg:40.63ms
step:1446/2330 train_time:58754ms step_avg:40.63ms
step:1447/2330 train_time:58789ms step_avg:40.63ms
step:1448/2330 train_time:58834ms step_avg:40.63ms
step:1449/2330 train_time:58870ms step_avg:40.63ms
step:1450/2330 train_time:58915ms step_avg:40.63ms
step:1451/2330 train_time:58951ms step_avg:40.63ms
step:1452/2330 train_time:58995ms step_avg:40.63ms
step:1453/2330 train_time:59031ms step_avg:40.63ms
step:1454/2330 train_time:59076ms step_avg:40.63ms
step:1455/2330 train_time:59112ms step_avg:40.63ms
step:1456/2330 train_time:59156ms step_avg:40.63ms
step:1457/2330 train_time:59192ms step_avg:40.63ms
step:1458/2330 train_time:59238ms step_avg:40.63ms
step:1459/2330 train_time:59273ms step_avg:40.63ms
step:1460/2330 train_time:59318ms step_avg:40.63ms
step:1461/2330 train_time:59354ms step_avg:40.63ms
step:1462/2330 train_time:59399ms step_avg:40.63ms
step:1463/2330 train_time:59435ms step_avg:40.63ms
step:1464/2330 train_time:59481ms step_avg:40.63ms
step:1465/2330 train_time:59518ms step_avg:40.63ms
step:1466/2330 train_time:59564ms step_avg:40.63ms
step:1467/2330 train_time:59599ms step_avg:40.63ms
step:1468/2330 train_time:59644ms step_avg:40.63ms
step:1469/2330 train_time:59679ms step_avg:40.63ms
step:1470/2330 train_time:59724ms step_avg:40.63ms
step:1471/2330 train_time:59759ms step_avg:40.62ms
step:1472/2330 train_time:59805ms step_avg:40.63ms
step:1473/2330 train_time:59840ms step_avg:40.62ms
step:1474/2330 train_time:59885ms step_avg:40.63ms
step:1475/2330 train_time:59920ms step_avg:40.62ms
step:1476/2330 train_time:59965ms step_avg:40.63ms
step:1477/2330 train_time:60001ms step_avg:40.62ms
step:1478/2330 train_time:60046ms step_avg:40.63ms
step:1479/2330 train_time:60082ms step_avg:40.62ms
step:1480/2330 train_time:60127ms step_avg:40.63ms
step:1481/2330 train_time:60163ms step_avg:40.62ms
step:1482/2330 train_time:60207ms step_avg:40.63ms
step:1483/2330 train_time:60244ms step_avg:40.62ms
step:1484/2330 train_time:60288ms step_avg:40.63ms
step:1485/2330 train_time:60324ms step_avg:40.62ms
step:1486/2330 train_time:60369ms step_avg:40.62ms
step:1487/2330 train_time:60405ms step_avg:40.62ms
step:1488/2330 train_time:60450ms step_avg:40.63ms
step:1489/2330 train_time:60487ms step_avg:40.62ms
step:1490/2330 train_time:60533ms step_avg:40.63ms
step:1491/2330 train_time:60569ms step_avg:40.62ms
step:1492/2330 train_time:60614ms step_avg:40.63ms
step:1493/2330 train_time:60649ms step_avg:40.62ms
step:1494/2330 train_time:60694ms step_avg:40.63ms
step:1495/2330 train_time:60730ms step_avg:40.62ms
step:1496/2330 train_time:60775ms step_avg:40.63ms
step:1497/2330 train_time:60811ms step_avg:40.62ms
step:1498/2330 train_time:60856ms step_avg:40.62ms
step:1499/2330 train_time:60892ms step_avg:40.62ms
step:1500/2330 train_time:60938ms step_avg:40.63ms
step:1500/2330 val_loss:5.1287 train_time:61026ms step_avg:40.68ms
step:1501/2330 train_time:61039ms step_avg:40.67ms
step:1502/2330 train_time:61051ms step_avg:40.65ms
step:1503/2330 train_time:61062ms step_avg:40.63ms
step:1504/2330 train_time:61101ms step_avg:40.63ms
step:1505/2330 train_time:61135ms step_avg:40.62ms
step:1506/2330 train_time:61179ms step_avg:40.62ms
step:1507/2330 train_time:61213ms step_avg:40.62ms
step:1508/2330 train_time:61258ms step_avg:40.62ms
step:1509/2330 train_time:61293ms step_avg:40.62ms
step:1510/2330 train_time:61341ms step_avg:40.62ms
step:1511/2330 train_time:61381ms step_avg:40.62ms
step:1512/2330 train_time:61429ms step_avg:40.63ms
step:1513/2330 train_time:61466ms step_avg:40.63ms
step:1514/2330 train_time:61511ms step_avg:40.63ms
step:1515/2330 train_time:61547ms step_avg:40.63ms
step:1516/2330 train_time:61592ms step_avg:40.63ms
step:1517/2330 train_time:61755ms step_avg:40.71ms
step:1518/2330 train_time:61798ms step_avg:40.71ms
step:1519/2330 train_time:61833ms step_avg:40.71ms
step:1520/2330 train_time:61877ms step_avg:40.71ms
step:1521/2330 train_time:61911ms step_avg:40.70ms
step:1522/2330 train_time:61955ms step_avg:40.71ms
step:1523/2330 train_time:61989ms step_avg:40.70ms
step:1524/2330 train_time:62033ms step_avg:40.70ms
step:1525/2330 train_time:62067ms step_avg:40.70ms
step:1526/2330 train_time:62111ms step_avg:40.70ms
step:1527/2330 train_time:62146ms step_avg:40.70ms
step:1528/2330 train_time:62190ms step_avg:40.70ms
step:1529/2330 train_time:62226ms step_avg:40.70ms
step:1530/2330 train_time:62269ms step_avg:40.70ms
step:1531/2330 train_time:62303ms step_avg:40.69ms
step:1532/2330 train_time:62347ms step_avg:40.70ms
step:1533/2330 train_time:62382ms step_avg:40.69ms
step:1534/2330 train_time:62426ms step_avg:40.69ms
step:1535/2330 train_time:62461ms step_avg:40.69ms
step:1536/2330 train_time:62505ms step_avg:40.69ms
step:1537/2330 train_time:62539ms step_avg:40.69ms
step:1538/2330 train_time:62584ms step_avg:40.69ms
step:1539/2330 train_time:62625ms step_avg:40.69ms
step:1540/2330 train_time:62675ms step_avg:40.70ms
step:1541/2330 train_time:62713ms step_avg:40.70ms
step:1542/2330 train_time:62761ms step_avg:40.70ms
step:1543/2330 train_time:62798ms step_avg:40.70ms
step:1544/2330 train_time:62843ms step_avg:40.70ms
step:1545/2330 train_time:62878ms step_avg:40.70ms
step:1546/2330 train_time:62923ms step_avg:40.70ms
step:1547/2330 train_time:62959ms step_avg:40.70ms
step:1548/2330 train_time:63003ms step_avg:40.70ms
step:1549/2330 train_time:63038ms step_avg:40.70ms
step:1550/2330 train_time:63083ms step_avg:40.70ms
step:1551/2330 train_time:63118ms step_avg:40.69ms
step:1552/2330 train_time:63162ms step_avg:40.70ms
step:1553/2330 train_time:63197ms step_avg:40.69ms
step:1554/2330 train_time:63241ms step_avg:40.70ms
step:1555/2330 train_time:63276ms step_avg:40.69ms
step:1556/2330 train_time:63320ms step_avg:40.69ms
step:1557/2330 train_time:63355ms step_avg:40.69ms
step:1558/2330 train_time:63399ms step_avg:40.69ms
step:1559/2330 train_time:63434ms step_avg:40.69ms
step:1560/2330 train_time:63478ms step_avg:40.69ms
step:1561/2330 train_time:63514ms step_avg:40.69ms
step:1562/2330 train_time:63563ms step_avg:40.69ms
step:1563/2330 train_time:63600ms step_avg:40.69ms
step:1564/2330 train_time:63646ms step_avg:40.69ms
step:1565/2330 train_time:63682ms step_avg:40.69ms
step:1566/2330 train_time:63727ms step_avg:40.69ms
step:1567/2330 train_time:63763ms step_avg:40.69ms
step:1568/2330 train_time:63809ms step_avg:40.69ms
step:1569/2330 train_time:63846ms step_avg:40.69ms
step:1570/2330 train_time:63891ms step_avg:40.70ms
step:1571/2330 train_time:63927ms step_avg:40.69ms
step:1572/2330 train_time:63972ms step_avg:40.69ms
step:1573/2330 train_time:64008ms step_avg:40.69ms
step:1574/2330 train_time:64053ms step_avg:40.69ms
step:1575/2330 train_time:64088ms step_avg:40.69ms
step:1576/2330 train_time:64132ms step_avg:40.69ms
step:1577/2330 train_time:64168ms step_avg:40.69ms
step:1578/2330 train_time:64213ms step_avg:40.69ms
step:1579/2330 train_time:64248ms step_avg:40.69ms
step:1580/2330 train_time:64292ms step_avg:40.69ms
step:1581/2330 train_time:64328ms step_avg:40.69ms
step:1582/2330 train_time:64372ms step_avg:40.69ms
step:1583/2330 train_time:64408ms step_avg:40.69ms
step:1584/2330 train_time:64452ms step_avg:40.69ms
step:1585/2330 train_time:64488ms step_avg:40.69ms
step:1586/2330 train_time:64533ms step_avg:40.69ms
step:1587/2330 train_time:64569ms step_avg:40.69ms
step:1588/2330 train_time:64614ms step_avg:40.69ms
step:1589/2330 train_time:64650ms step_avg:40.69ms
step:1590/2330 train_time:64697ms step_avg:40.69ms
step:1591/2330 train_time:64733ms step_avg:40.69ms
step:1592/2330 train_time:64779ms step_avg:40.69ms
step:1593/2330 train_time:64815ms step_avg:40.69ms
step:1594/2330 train_time:64862ms step_avg:40.69ms
step:1595/2330 train_time:64898ms step_avg:40.69ms
step:1596/2330 train_time:64942ms step_avg:40.69ms
step:1597/2330 train_time:64977ms step_avg:40.69ms
step:1598/2330 train_time:65022ms step_avg:40.69ms
step:1599/2330 train_time:65057ms step_avg:40.69ms
step:1600/2330 train_time:65103ms step_avg:40.69ms
step:1601/2330 train_time:65138ms step_avg:40.69ms
step:1602/2330 train_time:65184ms step_avg:40.69ms
step:1603/2330 train_time:65219ms step_avg:40.69ms
step:1604/2330 train_time:65265ms step_avg:40.69ms
step:1605/2330 train_time:65300ms step_avg:40.69ms
step:1606/2330 train_time:65344ms step_avg:40.69ms
step:1607/2330 train_time:65379ms step_avg:40.68ms
step:1608/2330 train_time:65424ms step_avg:40.69ms
step:1609/2330 train_time:65459ms step_avg:40.68ms
step:1610/2330 train_time:65504ms step_avg:40.69ms
step:1611/2330 train_time:65539ms step_avg:40.68ms
step:1612/2330 train_time:65585ms step_avg:40.69ms
step:1613/2330 train_time:65620ms step_avg:40.68ms
step:1614/2330 train_time:65665ms step_avg:40.68ms
step:1615/2330 train_time:65702ms step_avg:40.68ms
step:1616/2330 train_time:65746ms step_avg:40.68ms
step:1617/2330 train_time:65782ms step_avg:40.68ms
step:1618/2330 train_time:65827ms step_avg:40.68ms
step:1619/2330 train_time:65863ms step_avg:40.68ms
step:1620/2330 train_time:65909ms step_avg:40.68ms
step:1621/2330 train_time:65944ms step_avg:40.68ms
step:1622/2330 train_time:65989ms step_avg:40.68ms
step:1623/2330 train_time:66024ms step_avg:40.68ms
step:1624/2330 train_time:66070ms step_avg:40.68ms
step:1625/2330 train_time:66106ms step_avg:40.68ms
step:1626/2330 train_time:66151ms step_avg:40.68ms
step:1627/2330 train_time:66187ms step_avg:40.68ms
step:1628/2330 train_time:66232ms step_avg:40.68ms
step:1629/2330 train_time:66267ms step_avg:40.68ms
step:1630/2330 train_time:66312ms step_avg:40.68ms
step:1631/2330 train_time:66346ms step_avg:40.68ms
step:1632/2330 train_time:66391ms step_avg:40.68ms
step:1633/2330 train_time:66426ms step_avg:40.68ms
step:1634/2330 train_time:66472ms step_avg:40.68ms
step:1635/2330 train_time:66508ms step_avg:40.68ms
step:1636/2330 train_time:66552ms step_avg:40.68ms
step:1637/2330 train_time:66587ms step_avg:40.68ms
step:1638/2330 train_time:66632ms step_avg:40.68ms
step:1639/2330 train_time:66668ms step_avg:40.68ms
step:1640/2330 train_time:66712ms step_avg:40.68ms
step:1641/2330 train_time:66748ms step_avg:40.68ms
step:1642/2330 train_time:66793ms step_avg:40.68ms
step:1643/2330 train_time:66829ms step_avg:40.68ms
step:1644/2330 train_time:66875ms step_avg:40.68ms
step:1645/2330 train_time:66911ms step_avg:40.68ms
step:1646/2330 train_time:66958ms step_avg:40.68ms
step:1647/2330 train_time:66995ms step_avg:40.68ms
step:1648/2330 train_time:67040ms step_avg:40.68ms
step:1649/2330 train_time:67076ms step_avg:40.68ms
step:1650/2330 train_time:67121ms step_avg:40.68ms
step:1651/2330 train_time:67157ms step_avg:40.68ms
step:1652/2330 train_time:67202ms step_avg:40.68ms
step:1653/2330 train_time:67237ms step_avg:40.68ms
step:1654/2330 train_time:67283ms step_avg:40.68ms
step:1655/2330 train_time:67318ms step_avg:40.68ms
step:1656/2330 train_time:67363ms step_avg:40.68ms
step:1657/2330 train_time:67398ms step_avg:40.67ms
step:1658/2330 train_time:67443ms step_avg:40.68ms
step:1659/2330 train_time:67478ms step_avg:40.67ms
step:1660/2330 train_time:67524ms step_avg:40.68ms
step:1661/2330 train_time:67559ms step_avg:40.67ms
step:1662/2330 train_time:67604ms step_avg:40.68ms
step:1663/2330 train_time:67639ms step_avg:40.67ms
step:1664/2330 train_time:67685ms step_avg:40.68ms
step:1665/2330 train_time:67720ms step_avg:40.67ms
step:1666/2330 train_time:67766ms step_avg:40.68ms
step:1667/2330 train_time:67801ms step_avg:40.67ms
step:1668/2330 train_time:67846ms step_avg:40.68ms
step:1669/2330 train_time:67881ms step_avg:40.67ms
step:1670/2330 train_time:67927ms step_avg:40.67ms
step:1671/2330 train_time:67963ms step_avg:40.67ms
step:1672/2330 train_time:68008ms step_avg:40.67ms
step:1673/2330 train_time:68044ms step_avg:40.67ms
step:1674/2330 train_time:68090ms step_avg:40.68ms
step:1675/2330 train_time:68126ms step_avg:40.67ms
step:1676/2330 train_time:68170ms step_avg:40.67ms
step:1677/2330 train_time:68206ms step_avg:40.67ms
step:1678/2330 train_time:68251ms step_avg:40.67ms
step:1679/2330 train_time:68286ms step_avg:40.67ms
step:1680/2330 train_time:68331ms step_avg:40.67ms
step:1681/2330 train_time:68366ms step_avg:40.67ms
step:1682/2330 train_time:68411ms step_avg:40.67ms
step:1683/2330 train_time:68447ms step_avg:40.67ms
step:1684/2330 train_time:68492ms step_avg:40.67ms
step:1685/2330 train_time:68528ms step_avg:40.67ms
step:1686/2330 train_time:68573ms step_avg:40.67ms
step:1687/2330 train_time:68609ms step_avg:40.67ms
step:1688/2330 train_time:68653ms step_avg:40.67ms
step:1689/2330 train_time:68689ms step_avg:40.67ms
step:1690/2330 train_time:68734ms step_avg:40.67ms
step:1691/2330 train_time:68770ms step_avg:40.67ms
step:1692/2330 train_time:68815ms step_avg:40.67ms
step:1693/2330 train_time:68851ms step_avg:40.67ms
step:1694/2330 train_time:68896ms step_avg:40.67ms
step:1695/2330 train_time:68932ms step_avg:40.67ms
step:1696/2330 train_time:68978ms step_avg:40.67ms
step:1697/2330 train_time:69014ms step_avg:40.67ms
step:1698/2330 train_time:69059ms step_avg:40.67ms
step:1699/2330 train_time:69095ms step_avg:40.67ms
step:1700/2330 train_time:69140ms step_avg:40.67ms
step:1701/2330 train_time:69176ms step_avg:40.67ms
step:1702/2330 train_time:69220ms step_avg:40.67ms
step:1703/2330 train_time:69256ms step_avg:40.67ms
step:1704/2330 train_time:69301ms step_avg:40.67ms
step:1705/2330 train_time:69337ms step_avg:40.67ms
step:1706/2330 train_time:69382ms step_avg:40.67ms
step:1707/2330 train_time:69418ms step_avg:40.67ms
step:1708/2330 train_time:69463ms step_avg:40.67ms
step:1709/2330 train_time:69499ms step_avg:40.67ms
step:1710/2330 train_time:69544ms step_avg:40.67ms
step:1711/2330 train_time:69579ms step_avg:40.67ms
step:1712/2330 train_time:69624ms step_avg:40.67ms
step:1713/2330 train_time:69660ms step_avg:40.67ms
step:1714/2330 train_time:69705ms step_avg:40.67ms
step:1715/2330 train_time:69740ms step_avg:40.66ms
step:1716/2330 train_time:69785ms step_avg:40.67ms
step:1717/2330 train_time:69820ms step_avg:40.66ms
step:1718/2330 train_time:69866ms step_avg:40.67ms
step:1719/2330 train_time:69901ms step_avg:40.66ms
step:1720/2330 train_time:69946ms step_avg:40.67ms
step:1721/2330 train_time:69982ms step_avg:40.66ms
step:1722/2330 train_time:70028ms step_avg:40.67ms
step:1723/2330 train_time:70064ms step_avg:40.66ms
step:1724/2330 train_time:70110ms step_avg:40.67ms
step:1725/2330 train_time:70144ms step_avg:40.66ms
step:1726/2330 train_time:70189ms step_avg:40.67ms
step:1727/2330 train_time:70225ms step_avg:40.66ms
step:1728/2330 train_time:70270ms step_avg:40.67ms
step:1729/2330 train_time:70306ms step_avg:40.66ms
step:1730/2330 train_time:70351ms step_avg:40.67ms
step:1731/2330 train_time:70388ms step_avg:40.66ms
step:1732/2330 train_time:70433ms step_avg:40.67ms
step:1733/2330 train_time:70469ms step_avg:40.66ms
step:1734/2330 train_time:70514ms step_avg:40.67ms
step:1735/2330 train_time:70550ms step_avg:40.66ms
step:1736/2330 train_time:70596ms step_avg:40.67ms
step:1737/2330 train_time:70631ms step_avg:40.66ms
step:1738/2330 train_time:70677ms step_avg:40.67ms
step:1739/2330 train_time:70712ms step_avg:40.66ms
step:1740/2330 train_time:70758ms step_avg:40.67ms
step:1741/2330 train_time:70793ms step_avg:40.66ms
step:1742/2330 train_time:70837ms step_avg:40.66ms
step:1743/2330 train_time:70872ms step_avg:40.66ms
step:1744/2330 train_time:70918ms step_avg:40.66ms
step:1745/2330 train_time:70954ms step_avg:40.66ms
step:1746/2330 train_time:71000ms step_avg:40.66ms
step:1747/2330 train_time:71036ms step_avg:40.66ms
step:1748/2330 train_time:71081ms step_avg:40.66ms
step:1749/2330 train_time:71117ms step_avg:40.66ms
step:1750/2330 train_time:71163ms step_avg:40.66ms
step:1750/2330 val_loss:5.0957 train_time:71251ms step_avg:40.71ms
step:1751/2330 train_time:71264ms step_avg:40.70ms
step:1752/2330 train_time:71276ms step_avg:40.68ms
step:1753/2330 train_time:71287ms step_avg:40.67ms
step:1754/2330 train_time:71324ms step_avg:40.66ms
step:1755/2330 train_time:71358ms step_avg:40.66ms
step:1756/2330 train_time:71402ms step_avg:40.66ms
step:1757/2330 train_time:71437ms step_avg:40.66ms
step:1758/2330 train_time:71480ms step_avg:40.66ms
step:1759/2330 train_time:71515ms step_avg:40.66ms
step:1760/2330 train_time:71559ms step_avg:40.66ms
step:1761/2330 train_time:71594ms step_avg:40.66ms
step:1762/2330 train_time:71644ms step_avg:40.66ms
step:1763/2330 train_time:71682ms step_avg:40.66ms
step:1764/2330 train_time:71727ms step_avg:40.66ms
step:1765/2330 train_time:71762ms step_avg:40.66ms
step:1766/2330 train_time:71807ms step_avg:40.66ms
step:1767/2330 train_time:71842ms step_avg:40.66ms
step:1768/2330 train_time:71886ms step_avg:40.66ms
step:1769/2330 train_time:71920ms step_avg:40.66ms
step:1770/2330 train_time:71964ms step_avg:40.66ms
step:1771/2330 train_time:71999ms step_avg:40.65ms
step:1772/2330 train_time:72043ms step_avg:40.66ms
step:1773/2330 train_time:72078ms step_avg:40.65ms
step:1774/2330 train_time:72122ms step_avg:40.66ms
step:1775/2330 train_time:72159ms step_avg:40.65ms
step:1776/2330 train_time:72208ms step_avg:40.66ms
step:1777/2330 train_time:72247ms step_avg:40.66ms
step:1778/2330 train_time:72295ms step_avg:40.66ms
step:1779/2330 train_time:72332ms step_avg:40.66ms
step:1780/2330 train_time:72377ms step_avg:40.66ms
step:1781/2330 train_time:72411ms step_avg:40.66ms
step:1782/2330 train_time:72456ms step_avg:40.66ms
step:1783/2330 train_time:72491ms step_avg:40.66ms
step:1784/2330 train_time:72536ms step_avg:40.66ms
step:1785/2330 train_time:72572ms step_avg:40.66ms
step:1786/2330 train_time:72618ms step_avg:40.66ms
step:1787/2330 train_time:72653ms step_avg:40.66ms
step:1788/2330 train_time:72698ms step_avg:40.66ms
step:1789/2330 train_time:72733ms step_avg:40.66ms
step:1790/2330 train_time:72778ms step_avg:40.66ms
step:1791/2330 train_time:72813ms step_avg:40.65ms
step:1792/2330 train_time:72858ms step_avg:40.66ms
step:1793/2330 train_time:72893ms step_avg:40.65ms
step:1794/2330 train_time:72937ms step_avg:40.66ms
step:1795/2330 train_time:72972ms step_avg:40.65ms
step:1796/2330 train_time:73016ms step_avg:40.65ms
step:1797/2330 train_time:73052ms step_avg:40.65ms
step:1798/2330 train_time:73097ms step_avg:40.65ms
step:1799/2330 train_time:73133ms step_avg:40.65ms
step:1800/2330 train_time:73179ms step_avg:40.66ms
step:1801/2330 train_time:73215ms step_avg:40.65ms
step:1802/2330 train_time:73260ms step_avg:40.66ms
step:1803/2330 train_time:73296ms step_avg:40.65ms
step:1804/2330 train_time:73341ms step_avg:40.65ms
step:1805/2330 train_time:73376ms step_avg:40.65ms
step:1806/2330 train_time:73420ms step_avg:40.65ms
step:1807/2330 train_time:73456ms step_avg:40.65ms
step:1808/2330 train_time:73500ms step_avg:40.65ms
step:1809/2330 train_time:73536ms step_avg:40.65ms
step:1810/2330 train_time:73581ms step_avg:40.65ms
step:1811/2330 train_time:73616ms step_avg:40.65ms
step:1812/2330 train_time:73661ms step_avg:40.65ms
step:1813/2330 train_time:73696ms step_avg:40.65ms
step:1814/2330 train_time:73741ms step_avg:40.65ms
step:1815/2330 train_time:73777ms step_avg:40.65ms
step:1816/2330 train_time:73821ms step_avg:40.65ms
step:1817/2330 train_time:73856ms step_avg:40.65ms
step:1818/2330 train_time:73900ms step_avg:40.65ms
step:1819/2330 train_time:73936ms step_avg:40.65ms
step:1820/2330 train_time:73979ms step_avg:40.65ms
step:1821/2330 train_time:74015ms step_avg:40.65ms
step:1822/2330 train_time:74059ms step_avg:40.65ms
step:1823/2330 train_time:74096ms step_avg:40.64ms
step:1824/2330 train_time:74141ms step_avg:40.65ms
step:1825/2330 train_time:74176ms step_avg:40.64ms
step:1826/2330 train_time:74222ms step_avg:40.65ms
step:1827/2330 train_time:74257ms step_avg:40.64ms
step:1828/2330 train_time:74303ms step_avg:40.65ms
step:1829/2330 train_time:74338ms step_avg:40.64ms
step:1830/2330 train_time:74383ms step_avg:40.65ms
step:1831/2330 train_time:74418ms step_avg:40.64ms
step:1832/2330 train_time:74463ms step_avg:40.65ms
step:1833/2330 train_time:74498ms step_avg:40.64ms
step:1834/2330 train_time:74544ms step_avg:40.65ms
step:1835/2330 train_time:74580ms step_avg:40.64ms
step:1836/2330 train_time:74625ms step_avg:40.65ms
step:1837/2330 train_time:74660ms step_avg:40.64ms
step:1838/2330 train_time:74705ms step_avg:40.64ms
step:1839/2330 train_time:74740ms step_avg:40.64ms
step:1840/2330 train_time:74784ms step_avg:40.64ms
step:1841/2330 train_time:74819ms step_avg:40.64ms
step:1842/2330 train_time:74864ms step_avg:40.64ms
step:1843/2330 train_time:74899ms step_avg:40.64ms
step:1844/2330 train_time:74944ms step_avg:40.64ms
step:1845/2330 train_time:74980ms step_avg:40.64ms
step:1846/2330 train_time:75025ms step_avg:40.64ms
step:1847/2330 train_time:75061ms step_avg:40.64ms
step:1848/2330 train_time:75106ms step_avg:40.64ms
step:1849/2330 train_time:75142ms step_avg:40.64ms
step:1850/2330 train_time:75187ms step_avg:40.64ms
step:1851/2330 train_time:75222ms step_avg:40.64ms
step:1852/2330 train_time:75267ms step_avg:40.64ms
step:1853/2330 train_time:75303ms step_avg:40.64ms
step:1854/2330 train_time:75347ms step_avg:40.64ms
step:1855/2330 train_time:75383ms step_avg:40.64ms
step:1856/2330 train_time:75428ms step_avg:40.64ms
step:1857/2330 train_time:75464ms step_avg:40.64ms
step:1858/2330 train_time:75510ms step_avg:40.64ms
step:1859/2330 train_time:75546ms step_avg:40.64ms
step:1860/2330 train_time:75592ms step_avg:40.64ms
step:1861/2330 train_time:75628ms step_avg:40.64ms
step:1862/2330 train_time:75673ms step_avg:40.64ms
step:1863/2330 train_time:75708ms step_avg:40.64ms
step:1864/2330 train_time:75753ms step_avg:40.64ms
step:1865/2330 train_time:75789ms step_avg:40.64ms
step:1866/2330 train_time:75834ms step_avg:40.64ms
step:1867/2330 train_time:75870ms step_avg:40.64ms
step:1868/2330 train_time:75914ms step_avg:40.64ms
step:1869/2330 train_time:75950ms step_avg:40.64ms
step:1870/2330 train_time:75995ms step_avg:40.64ms
step:1871/2330 train_time:76031ms step_avg:40.64ms
step:1872/2330 train_time:76076ms step_avg:40.64ms
step:1873/2330 train_time:76111ms step_avg:40.64ms
step:1874/2330 train_time:76156ms step_avg:40.64ms
step:1875/2330 train_time:76191ms step_avg:40.64ms
step:1876/2330 train_time:76237ms step_avg:40.64ms
step:1877/2330 train_time:76272ms step_avg:40.64ms
step:1878/2330 train_time:76318ms step_avg:40.64ms
step:1879/2330 train_time:76353ms step_avg:40.64ms
step:1880/2330 train_time:76398ms step_avg:40.64ms
step:1881/2330 train_time:76434ms step_avg:40.63ms
step:1882/2330 train_time:76478ms step_avg:40.64ms
step:1883/2330 train_time:76514ms step_avg:40.63ms
step:1884/2330 train_time:76559ms step_avg:40.64ms
step:1885/2330 train_time:76594ms step_avg:40.63ms
step:1886/2330 train_time:76638ms step_avg:40.64ms
step:1887/2330 train_time:76674ms step_avg:40.63ms
step:1888/2330 train_time:76719ms step_avg:40.64ms
step:1889/2330 train_time:76755ms step_avg:40.63ms
step:1890/2330 train_time:76800ms step_avg:40.64ms
step:1891/2330 train_time:76835ms step_avg:40.63ms
step:1892/2330 train_time:76880ms step_avg:40.63ms
step:1893/2330 train_time:76915ms step_avg:40.63ms
step:1894/2330 train_time:76960ms step_avg:40.63ms
step:1895/2330 train_time:76996ms step_avg:40.63ms
step:1896/2330 train_time:77040ms step_avg:40.63ms
step:1897/2330 train_time:77075ms step_avg:40.63ms
step:1898/2330 train_time:77120ms step_avg:40.63ms
step:1899/2330 train_time:77155ms step_avg:40.63ms
step:1900/2330 train_time:77200ms step_avg:40.63ms
step:1901/2330 train_time:77236ms step_avg:40.63ms
step:1902/2330 train_time:77280ms step_avg:40.63ms
step:1903/2330 train_time:77315ms step_avg:40.63ms
step:1904/2330 train_time:77360ms step_avg:40.63ms
step:1905/2330 train_time:77395ms step_avg:40.63ms
step:1906/2330 train_time:77440ms step_avg:40.63ms
step:1907/2330 train_time:77475ms step_avg:40.63ms
step:1908/2330 train_time:77520ms step_avg:40.63ms
step:1909/2330 train_time:77555ms step_avg:40.63ms
step:1910/2330 train_time:77600ms step_avg:40.63ms
step:1911/2330 train_time:77636ms step_avg:40.63ms
step:1912/2330 train_time:77681ms step_avg:40.63ms
step:1913/2330 train_time:77716ms step_avg:40.63ms
step:1914/2330 train_time:77761ms step_avg:40.63ms
step:1915/2330 train_time:77797ms step_avg:40.63ms
step:1916/2330 train_time:77842ms step_avg:40.63ms
step:1917/2330 train_time:77877ms step_avg:40.62ms
step:1918/2330 train_time:77923ms step_avg:40.63ms
step:1919/2330 train_time:77958ms step_avg:40.62ms
step:1920/2330 train_time:78003ms step_avg:40.63ms
step:1921/2330 train_time:78037ms step_avg:40.62ms
step:1922/2330 train_time:78083ms step_avg:40.63ms
step:1923/2330 train_time:78118ms step_avg:40.62ms
step:1924/2330 train_time:78163ms step_avg:40.63ms
step:1925/2330 train_time:78198ms step_avg:40.62ms
step:1926/2330 train_time:78243ms step_avg:40.62ms
step:1927/2330 train_time:78278ms step_avg:40.62ms
step:1928/2330 train_time:78323ms step_avg:40.62ms
step:1929/2330 train_time:78359ms step_avg:40.62ms
step:1930/2330 train_time:78403ms step_avg:40.62ms
step:1931/2330 train_time:78439ms step_avg:40.62ms
step:1932/2330 train_time:78483ms step_avg:40.62ms
step:1933/2330 train_time:78519ms step_avg:40.62ms
step:1934/2330 train_time:78563ms step_avg:40.62ms
step:1935/2330 train_time:78599ms step_avg:40.62ms
step:1936/2330 train_time:78644ms step_avg:40.62ms
step:1937/2330 train_time:78679ms step_avg:40.62ms
step:1938/2330 train_time:78725ms step_avg:40.62ms
step:1939/2330 train_time:78759ms step_avg:40.62ms
step:1940/2330 train_time:78804ms step_avg:40.62ms
step:1941/2330 train_time:78840ms step_avg:40.62ms
step:1942/2330 train_time:78885ms step_avg:40.62ms
step:1943/2330 train_time:78920ms step_avg:40.62ms
step:1944/2330 train_time:78964ms step_avg:40.62ms
step:1945/2330 train_time:78999ms step_avg:40.62ms
step:1946/2330 train_time:79044ms step_avg:40.62ms
step:1947/2330 train_time:79080ms step_avg:40.62ms
step:1948/2330 train_time:79124ms step_avg:40.62ms
step:1949/2330 train_time:79159ms step_avg:40.62ms
step:1950/2330 train_time:79204ms step_avg:40.62ms
step:1951/2330 train_time:79240ms step_avg:40.61ms
step:1952/2330 train_time:79286ms step_avg:40.62ms
step:1953/2330 train_time:79322ms step_avg:40.62ms
step:1954/2330 train_time:79367ms step_avg:40.62ms
step:1955/2330 train_time:79402ms step_avg:40.61ms
step:1956/2330 train_time:79447ms step_avg:40.62ms
step:1957/2330 train_time:79483ms step_avg:40.61ms
step:1958/2330 train_time:79528ms step_avg:40.62ms
step:1959/2330 train_time:79564ms step_avg:40.61ms
step:1960/2330 train_time:79608ms step_avg:40.62ms
step:1961/2330 train_time:79644ms step_avg:40.61ms
step:1962/2330 train_time:79689ms step_avg:40.62ms
step:1963/2330 train_time:79724ms step_avg:40.61ms
step:1964/2330 train_time:79769ms step_avg:40.62ms
step:1965/2330 train_time:79805ms step_avg:40.61ms
step:1966/2330 train_time:79850ms step_avg:40.62ms
step:1967/2330 train_time:79886ms step_avg:40.61ms
step:1968/2330 train_time:79932ms step_avg:40.62ms
step:1969/2330 train_time:79968ms step_avg:40.61ms
step:1970/2330 train_time:80013ms step_avg:40.62ms
step:1971/2330 train_time:80049ms step_avg:40.61ms
step:1972/2330 train_time:80095ms step_avg:40.62ms
step:1973/2330 train_time:80130ms step_avg:40.61ms
step:1974/2330 train_time:80175ms step_avg:40.62ms
step:1975/2330 train_time:80211ms step_avg:40.61ms
step:1976/2330 train_time:80256ms step_avg:40.62ms
step:1977/2330 train_time:80292ms step_avg:40.61ms
step:1978/2330 train_time:80337ms step_avg:40.62ms
step:1979/2330 train_time:80372ms step_avg:40.61ms
step:1980/2330 train_time:80417ms step_avg:40.61ms
step:1981/2330 train_time:80452ms step_avg:40.61ms
step:1982/2330 train_time:80497ms step_avg:40.61ms
step:1983/2330 train_time:80533ms step_avg:40.61ms
step:1984/2330 train_time:80578ms step_avg:40.61ms
step:1985/2330 train_time:80614ms step_avg:40.61ms
step:1986/2330 train_time:80658ms step_avg:40.61ms
step:1987/2330 train_time:80694ms step_avg:40.61ms
step:1988/2330 train_time:80739ms step_avg:40.61ms
step:1989/2330 train_time:80774ms step_avg:40.61ms
step:1990/2330 train_time:80819ms step_avg:40.61ms
step:1991/2330 train_time:80854ms step_avg:40.61ms
step:1992/2330 train_time:80899ms step_avg:40.61ms
step:1993/2330 train_time:80934ms step_avg:40.61ms
step:1994/2330 train_time:80979ms step_avg:40.61ms
step:1995/2330 train_time:81014ms step_avg:40.61ms
step:1996/2330 train_time:81059ms step_avg:40.61ms
step:1997/2330 train_time:81094ms step_avg:40.61ms
step:1998/2330 train_time:81140ms step_avg:40.61ms
step:1999/2330 train_time:81175ms step_avg:40.61ms
step:2000/2330 train_time:81220ms step_avg:40.61ms
step:2000/2330 val_loss:5.0664 train_time:81307ms step_avg:40.65ms
step:2001/2330 train_time:81319ms step_avg:40.64ms
step:2002/2330 train_time:81333ms step_avg:40.63ms
step:2003/2330 train_time:81343ms step_avg:40.61ms
step:2004/2330 train_time:81380ms step_avg:40.61ms
step:2005/2330 train_time:81414ms step_avg:40.61ms
step:2006/2330 train_time:81458ms step_avg:40.61ms
step:2007/2330 train_time:81493ms step_avg:40.60ms
step:2008/2330 train_time:81539ms step_avg:40.61ms
step:2009/2330 train_time:81574ms step_avg:40.60ms
step:2010/2330 train_time:81621ms step_avg:40.61ms
step:2011/2330 train_time:81660ms step_avg:40.61ms
step:2012/2330 train_time:81707ms step_avg:40.61ms
step:2013/2330 train_time:81744ms step_avg:40.61ms
step:2014/2330 train_time:81791ms step_avg:40.61ms
step:2015/2330 train_time:81826ms step_avg:40.61ms
step:2016/2330 train_time:81870ms step_avg:40.61ms
step:2017/2330 train_time:81905ms step_avg:40.61ms
step:2018/2330 train_time:81949ms step_avg:40.61ms
step:2019/2330 train_time:81984ms step_avg:40.61ms
step:2020/2330 train_time:82030ms step_avg:40.61ms
step:2021/2330 train_time:82064ms step_avg:40.61ms
step:2022/2330 train_time:82109ms step_avg:40.61ms
step:2023/2330 train_time:82144ms step_avg:40.61ms
step:2024/2330 train_time:82188ms step_avg:40.61ms
step:2025/2330 train_time:82224ms step_avg:40.60ms
step:2026/2330 train_time:82268ms step_avg:40.61ms
step:2027/2330 train_time:82305ms step_avg:40.60ms
step:2028/2330 train_time:82351ms step_avg:40.61ms
step:2029/2330 train_time:82386ms step_avg:40.60ms
step:2030/2330 train_time:82431ms step_avg:40.61ms
step:2031/2330 train_time:82467ms step_avg:40.60ms
step:2032/2330 train_time:82511ms step_avg:40.61ms
step:2033/2330 train_time:82547ms step_avg:40.60ms
step:2034/2330 train_time:82595ms step_avg:40.61ms
step:2035/2330 train_time:82631ms step_avg:40.60ms
step:2036/2330 train_time:82676ms step_avg:40.61ms
step:2037/2330 train_time:82711ms step_avg:40.60ms
step:2038/2330 train_time:82756ms step_avg:40.61ms
step:2039/2330 train_time:82792ms step_avg:40.60ms
step:2040/2330 train_time:82836ms step_avg:40.61ms
step:2041/2330 train_time:82871ms step_avg:40.60ms
step:2042/2330 train_time:82916ms step_avg:40.61ms
step:2043/2330 train_time:82951ms step_avg:40.60ms
step:2044/2330 train_time:82995ms step_avg:40.60ms
step:2045/2330 train_time:83030ms step_avg:40.60ms
step:2046/2330 train_time:83075ms step_avg:40.60ms
step:2047/2330 train_time:83110ms step_avg:40.60ms
step:2048/2330 train_time:83155ms step_avg:40.60ms
step:2049/2330 train_time:83191ms step_avg:40.60ms
step:2050/2330 train_time:83235ms step_avg:40.60ms
step:2051/2330 train_time:83270ms step_avg:40.60ms
step:2052/2330 train_time:83315ms step_avg:40.60ms
step:2053/2330 train_time:83350ms step_avg:40.60ms
step:2054/2330 train_time:83394ms step_avg:40.60ms
step:2055/2330 train_time:83429ms step_avg:40.60ms
step:2056/2330 train_time:83474ms step_avg:40.60ms
step:2057/2330 train_time:83510ms step_avg:40.60ms
step:2058/2330 train_time:83555ms step_avg:40.60ms
step:2059/2330 train_time:83591ms step_avg:40.60ms
step:2060/2330 train_time:83637ms step_avg:40.60ms
step:2061/2330 train_time:83671ms step_avg:40.60ms
step:2062/2330 train_time:83716ms step_avg:40.60ms
step:2063/2330 train_time:83751ms step_avg:40.60ms
step:2064/2330 train_time:83796ms step_avg:40.60ms
step:2065/2330 train_time:83831ms step_avg:40.60ms
step:2066/2330 train_time:83875ms step_avg:40.60ms
step:2067/2330 train_time:83910ms step_avg:40.60ms
step:2068/2330 train_time:83956ms step_avg:40.60ms
step:2069/2330 train_time:83991ms step_avg:40.59ms
step:2070/2330 train_time:84035ms step_avg:40.60ms
step:2071/2330 train_time:84070ms step_avg:40.59ms
step:2072/2330 train_time:84115ms step_avg:40.60ms
step:2073/2330 train_time:84150ms step_avg:40.59ms
step:2074/2330 train_time:84196ms step_avg:40.60ms
step:2075/2330 train_time:84232ms step_avg:40.59ms
step:2076/2330 train_time:84276ms step_avg:40.60ms
step:2077/2330 train_time:84311ms step_avg:40.59ms
step:2078/2330 train_time:84356ms step_avg:40.59ms
step:2079/2330 train_time:84391ms step_avg:40.59ms
step:2080/2330 train_time:84436ms step_avg:40.59ms
step:2081/2330 train_time:84472ms step_avg:40.59ms
step:2082/2330 train_time:84516ms step_avg:40.59ms
step:2083/2330 train_time:84552ms step_avg:40.59ms
step:2084/2330 train_time:84596ms step_avg:40.59ms
step:2085/2330 train_time:84632ms step_avg:40.59ms
step:2086/2330 train_time:84677ms step_avg:40.59ms
step:2087/2330 train_time:84712ms step_avg:40.59ms
step:2088/2330 train_time:84757ms step_avg:40.59ms
step:2089/2330 train_time:84792ms step_avg:40.59ms
step:2090/2330 train_time:84838ms step_avg:40.59ms
step:2091/2330 train_time:84873ms step_avg:40.59ms
step:2092/2330 train_time:84918ms step_avg:40.59ms
step:2093/2330 train_time:84954ms step_avg:40.59ms
step:2094/2330 train_time:84998ms step_avg:40.59ms
step:2095/2330 train_time:85033ms step_avg:40.59ms
step:2096/2330 train_time:85078ms step_avg:40.59ms
step:2097/2330 train_time:85114ms step_avg:40.59ms
step:2098/2330 train_time:85159ms step_avg:40.59ms
step:2099/2330 train_time:85193ms step_avg:40.59ms
step:2100/2330 train_time:85238ms step_avg:40.59ms
step:2101/2330 train_time:85274ms step_avg:40.59ms
step:2102/2330 train_time:85319ms step_avg:40.59ms
step:2103/2330 train_time:85354ms step_avg:40.59ms
step:2104/2330 train_time:85399ms step_avg:40.59ms
step:2105/2330 train_time:85435ms step_avg:40.59ms
step:2106/2330 train_time:85480ms step_avg:40.59ms
step:2107/2330 train_time:85515ms step_avg:40.59ms
step:2108/2330 train_time:85560ms step_avg:40.59ms
step:2109/2330 train_time:85596ms step_avg:40.59ms
step:2110/2330 train_time:85641ms step_avg:40.59ms
step:2111/2330 train_time:85676ms step_avg:40.59ms
step:2112/2330 train_time:85721ms step_avg:40.59ms
step:2113/2330 train_time:85756ms step_avg:40.58ms
step:2114/2330 train_time:85801ms step_avg:40.59ms
step:2115/2330 train_time:85836ms step_avg:40.58ms
step:2116/2330 train_time:85881ms step_avg:40.59ms
step:2117/2330 train_time:85917ms step_avg:40.58ms
step:2118/2330 train_time:85963ms step_avg:40.59ms
step:2119/2330 train_time:85998ms step_avg:40.58ms
step:2120/2330 train_time:86043ms step_avg:40.59ms
step:2121/2330 train_time:86078ms step_avg:40.58ms
step:2122/2330 train_time:86123ms step_avg:40.59ms
step:2123/2330 train_time:86159ms step_avg:40.58ms
step:2124/2330 train_time:86203ms step_avg:40.59ms
step:2125/2330 train_time:86239ms step_avg:40.58ms
step:2126/2330 train_time:86283ms step_avg:40.58ms
step:2127/2330 train_time:86319ms step_avg:40.58ms
step:2128/2330 train_time:86364ms step_avg:40.58ms
step:2129/2330 train_time:86399ms step_avg:40.58ms
step:2130/2330 train_time:86444ms step_avg:40.58ms
step:2131/2330 train_time:86480ms step_avg:40.58ms
step:2132/2330 train_time:86525ms step_avg:40.58ms
step:2133/2330 train_time:86560ms step_avg:40.58ms
step:2134/2330 train_time:86605ms step_avg:40.58ms
step:2135/2330 train_time:86640ms step_avg:40.58ms
step:2136/2330 train_time:86686ms step_avg:40.58ms
step:2137/2330 train_time:86722ms step_avg:40.58ms
step:2138/2330 train_time:86767ms step_avg:40.58ms
step:2139/2330 train_time:86802ms step_avg:40.58ms
step:2140/2330 train_time:86848ms step_avg:40.58ms
step:2141/2330 train_time:86883ms step_avg:40.58ms
step:2142/2330 train_time:86928ms step_avg:40.58ms
step:2143/2330 train_time:86964ms step_avg:40.58ms
step:2144/2330 train_time:87009ms step_avg:40.58ms
step:2145/2330 train_time:87045ms step_avg:40.58ms
step:2146/2330 train_time:87089ms step_avg:40.58ms
step:2147/2330 train_time:87124ms step_avg:40.58ms
step:2148/2330 train_time:87169ms step_avg:40.58ms
step:2149/2330 train_time:87204ms step_avg:40.58ms
step:2150/2330 train_time:87249ms step_avg:40.58ms
step:2151/2330 train_time:87286ms step_avg:40.58ms
step:2152/2330 train_time:87330ms step_avg:40.58ms
step:2153/2330 train_time:87366ms step_avg:40.58ms
step:2154/2330 train_time:87411ms step_avg:40.58ms
step:2155/2330 train_time:87446ms step_avg:40.58ms
step:2156/2330 train_time:87491ms step_avg:40.58ms
step:2157/2330 train_time:87526ms step_avg:40.58ms
step:2158/2330 train_time:87571ms step_avg:40.58ms
step:2159/2330 train_time:87607ms step_avg:40.58ms
step:2160/2330 train_time:87652ms step_avg:40.58ms
step:2161/2330 train_time:87687ms step_avg:40.58ms
step:2162/2330 train_time:87733ms step_avg:40.58ms
step:2163/2330 train_time:87768ms step_avg:40.58ms
step:2164/2330 train_time:87812ms step_avg:40.58ms
step:2165/2330 train_time:87848ms step_avg:40.58ms
step:2166/2330 train_time:87893ms step_avg:40.58ms
step:2167/2330 train_time:87929ms step_avg:40.58ms
step:2168/2330 train_time:87974ms step_avg:40.58ms
step:2169/2330 train_time:88010ms step_avg:40.58ms
step:2170/2330 train_time:88055ms step_avg:40.58ms
step:2171/2330 train_time:88090ms step_avg:40.58ms
step:2172/2330 train_time:88135ms step_avg:40.58ms
step:2173/2330 train_time:88171ms step_avg:40.58ms
step:2174/2330 train_time:88215ms step_avg:40.58ms
step:2175/2330 train_time:88251ms step_avg:40.58ms
step:2176/2330 train_time:88296ms step_avg:40.58ms
step:2177/2330 train_time:88332ms step_avg:40.57ms
step:2178/2330 train_time:88376ms step_avg:40.58ms
step:2179/2330 train_time:88411ms step_avg:40.57ms
step:2180/2330 train_time:88456ms step_avg:40.58ms
step:2181/2330 train_time:88491ms step_avg:40.57ms
step:2182/2330 train_time:88536ms step_avg:40.58ms
step:2183/2330 train_time:88571ms step_avg:40.57ms
step:2184/2330 train_time:88615ms step_avg:40.57ms
step:2185/2330 train_time:88650ms step_avg:40.57ms
step:2186/2330 train_time:88695ms step_avg:40.57ms
step:2187/2330 train_time:88730ms step_avg:40.57ms
step:2188/2330 train_time:88774ms step_avg:40.57ms
step:2189/2330 train_time:88810ms step_avg:40.57ms
step:2190/2330 train_time:88854ms step_avg:40.57ms
step:2191/2330 train_time:88889ms step_avg:40.57ms
step:2192/2330 train_time:88935ms step_avg:40.57ms
step:2193/2330 train_time:88970ms step_avg:40.57ms
step:2194/2330 train_time:89015ms step_avg:40.57ms
step:2195/2330 train_time:89050ms step_avg:40.57ms
step:2196/2330 train_time:89095ms step_avg:40.57ms
step:2197/2330 train_time:89130ms step_avg:40.57ms
step:2198/2330 train_time:89175ms step_avg:40.57ms
step:2199/2330 train_time:89210ms step_avg:40.57ms
step:2200/2330 train_time:89255ms step_avg:40.57ms
step:2201/2330 train_time:89291ms step_avg:40.57ms
step:2202/2330 train_time:89336ms step_avg:40.57ms
step:2203/2330 train_time:89371ms step_avg:40.57ms
step:2204/2330 train_time:89416ms step_avg:40.57ms
step:2205/2330 train_time:89451ms step_avg:40.57ms
step:2206/2330 train_time:89496ms step_avg:40.57ms
step:2207/2330 train_time:89531ms step_avg:40.57ms
step:2208/2330 train_time:89575ms step_avg:40.57ms
step:2209/2330 train_time:89610ms step_avg:40.57ms
step:2210/2330 train_time:89655ms step_avg:40.57ms
step:2211/2330 train_time:89690ms step_avg:40.57ms
step:2212/2330 train_time:89735ms step_avg:40.57ms
step:2213/2330 train_time:89770ms step_avg:40.56ms
step:2214/2330 train_time:89815ms step_avg:40.57ms
step:2215/2330 train_time:89850ms step_avg:40.56ms
step:2216/2330 train_time:89895ms step_avg:40.57ms
step:2217/2330 train_time:89930ms step_avg:40.56ms
step:2218/2330 train_time:89975ms step_avg:40.57ms
step:2219/2330 train_time:90010ms step_avg:40.56ms
step:2220/2330 train_time:90054ms step_avg:40.57ms
step:2221/2330 train_time:90090ms step_avg:40.56ms
step:2222/2330 train_time:90134ms step_avg:40.56ms
step:2223/2330 train_time:90169ms step_avg:40.56ms
step:2224/2330 train_time:90215ms step_avg:40.56ms
step:2225/2330 train_time:90250ms step_avg:40.56ms
step:2226/2330 train_time:90295ms step_avg:40.56ms
step:2227/2330 train_time:90331ms step_avg:40.56ms
step:2228/2330 train_time:90376ms step_avg:40.56ms
step:2229/2330 train_time:90411ms step_avg:40.56ms
step:2230/2330 train_time:90456ms step_avg:40.56ms
step:2231/2330 train_time:90492ms step_avg:40.56ms
step:2232/2330 train_time:90536ms step_avg:40.56ms
step:2233/2330 train_time:90571ms step_avg:40.56ms
step:2234/2330 train_time:90615ms step_avg:40.56ms
step:2235/2330 train_time:90651ms step_avg:40.56ms
step:2236/2330 train_time:90695ms step_avg:40.56ms
step:2237/2330 train_time:90730ms step_avg:40.56ms
step:2238/2330 train_time:90775ms step_avg:40.56ms
step:2239/2330 train_time:90810ms step_avg:40.56ms
step:2240/2330 train_time:90855ms step_avg:40.56ms
step:2241/2330 train_time:90890ms step_avg:40.56ms
step:2242/2330 train_time:90935ms step_avg:40.56ms
step:2243/2330 train_time:90971ms step_avg:40.56ms
step:2244/2330 train_time:91015ms step_avg:40.56ms
step:2245/2330 train_time:91050ms step_avg:40.56ms
step:2246/2330 train_time:91095ms step_avg:40.56ms
step:2247/2330 train_time:91130ms step_avg:40.56ms
step:2248/2330 train_time:91175ms step_avg:40.56ms
step:2249/2330 train_time:91209ms step_avg:40.56ms
step:2250/2330 train_time:91255ms step_avg:40.56ms
step:2250/2330 val_loss:5.0412 train_time:91342ms step_avg:40.60ms
step:2251/2330 train_time:91355ms step_avg:40.58ms
step:2252/2330 train_time:91367ms step_avg:40.57ms
step:2253/2330 train_time:91376ms step_avg:40.56ms
step:2254/2330 train_time:91415ms step_avg:40.56ms
step:2255/2330 train_time:91450ms step_avg:40.55ms
step:2256/2330 train_time:91494ms step_avg:40.56ms
step:2257/2330 train_time:91528ms step_avg:40.55ms
step:2258/2330 train_time:91572ms step_avg:40.55ms
step:2259/2330 train_time:91607ms step_avg:40.55ms
step:2260/2330 train_time:91652ms step_avg:40.55ms
step:2261/2330 train_time:91694ms step_avg:40.55ms
step:2262/2330 train_time:91740ms step_avg:40.56ms
step:2263/2330 train_time:91776ms step_avg:40.56ms
step:2264/2330 train_time:91822ms step_avg:40.56ms
step:2265/2330 train_time:91858ms step_avg:40.56ms
step:2266/2330 train_time:91903ms step_avg:40.56ms
step:2267/2330 train_time:91939ms step_avg:40.56ms
step:2268/2330 train_time:91983ms step_avg:40.56ms
step:2269/2330 train_time:92018ms step_avg:40.55ms
step:2270/2330 train_time:92063ms step_avg:40.56ms
step:2271/2330 train_time:92098ms step_avg:40.55ms
step:2272/2330 train_time:92143ms step_avg:40.56ms
step:2273/2330 train_time:92177ms step_avg:40.55ms
step:2274/2330 train_time:92222ms step_avg:40.55ms
step:2275/2330 train_time:92257ms step_avg:40.55ms
step:2276/2330 train_time:92302ms step_avg:40.55ms
step:2277/2330 train_time:92338ms step_avg:40.55ms
step:2278/2330 train_time:92383ms step_avg:40.55ms
step:2279/2330 train_time:92418ms step_avg:40.55ms
step:2280/2330 train_time:92463ms step_avg:40.55ms
step:2281/2330 train_time:92498ms step_avg:40.55ms
step:2282/2330 train_time:92543ms step_avg:40.55ms
step:2283/2330 train_time:92578ms step_avg:40.55ms
step:2284/2330 train_time:92624ms step_avg:40.55ms
step:2285/2330 train_time:92661ms step_avg:40.55ms
step:2286/2330 train_time:92707ms step_avg:40.55ms
step:2287/2330 train_time:92744ms step_avg:40.55ms
step:2288/2330 train_time:92791ms step_avg:40.56ms
step:2289/2330 train_time:92829ms step_avg:40.55ms
step:2290/2330 train_time:92874ms step_avg:40.56ms
step:2291/2330 train_time:92910ms step_avg:40.55ms
step:2292/2330 train_time:92955ms step_avg:40.56ms
step:2293/2330 train_time:92991ms step_avg:40.55ms
step:2294/2330 train_time:93035ms step_avg:40.56ms
step:2295/2330 train_time:93070ms step_avg:40.55ms
step:2296/2330 train_time:93114ms step_avg:40.56ms
step:2297/2330 train_time:93150ms step_avg:40.55ms
step:2298/2330 train_time:93194ms step_avg:40.55ms
step:2299/2330 train_time:93230ms step_avg:40.55ms
step:2300/2330 train_time:93275ms step_avg:40.55ms
step:2301/2330 train_time:93310ms step_avg:40.55ms
step:2302/2330 train_time:93355ms step_avg:40.55ms
step:2303/2330 train_time:93391ms step_avg:40.55ms
step:2304/2330 train_time:93435ms step_avg:40.55ms
step:2305/2330 train_time:93470ms step_avg:40.55ms
step:2306/2330 train_time:93516ms step_avg:40.55ms
step:2307/2330 train_time:93552ms step_avg:40.55ms
step:2308/2330 train_time:93597ms step_avg:40.55ms
step:2309/2330 train_time:93632ms step_avg:40.55ms
step:2310/2330 train_time:93677ms step_avg:40.55ms
step:2311/2330 train_time:93713ms step_avg:40.55ms
step:2312/2330 train_time:93759ms step_avg:40.55ms
step:2313/2330 train_time:93794ms step_avg:40.55ms
step:2314/2330 train_time:93840ms step_avg:40.55ms
step:2315/2330 train_time:93875ms step_avg:40.55ms
step:2316/2330 train_time:93920ms step_avg:40.55ms
step:2317/2330 train_time:93955ms step_avg:40.55ms
step:2318/2330 train_time:94000ms step_avg:40.55ms
step:2319/2330 train_time:94036ms step_avg:40.55ms
step:2320/2330 train_time:94080ms step_avg:40.55ms
step:2321/2330 train_time:94115ms step_avg:40.55ms
step:2322/2330 train_time:94160ms step_avg:40.55ms
step:2323/2330 train_time:94196ms step_avg:40.55ms
step:2324/2330 train_time:94240ms step_avg:40.55ms
step:2325/2330 train_time:94275ms step_avg:40.55ms
step:2326/2330 train_time:94320ms step_avg:40.55ms
step:2327/2330 train_time:94355ms step_avg:40.55ms
step:2328/2330 train_time:94400ms step_avg:40.55ms
step:2329/2330 train_time:94435ms step_avg:40.55ms
step:2330/2330 train_time:94481ms step_avg:40.55ms
step:2330/2330 val_loss:5.0344 train_time:94568ms step_avg:40.59ms
peak memory allocated: 29226 MiB reserved: 38888 MiB
