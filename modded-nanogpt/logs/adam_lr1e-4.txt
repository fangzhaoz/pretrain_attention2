import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "adam_lr1e-4"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=1e-1, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.Adam(
    hidden_matrix_params + gate_params,
    lr=1e-4,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 00:00:34 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   31C    P0             111W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   29C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   28C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   27C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   29C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   28C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   31C    P0             114W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:83ms step_avg:83.06ms
step:2/2330 train_time:178ms step_avg:88.86ms
step:3/2330 train_time:198ms step_avg:65.84ms
step:4/2330 train_time:219ms step_avg:54.70ms
step:5/2330 train_time:274ms step_avg:54.80ms
step:6/2330 train_time:333ms step_avg:55.52ms
step:7/2330 train_time:389ms step_avg:55.61ms
step:8/2330 train_time:450ms step_avg:56.22ms
step:9/2330 train_time:506ms step_avg:56.22ms
step:10/2330 train_time:566ms step_avg:56.62ms
step:11/2330 train_time:623ms step_avg:56.60ms
step:12/2330 train_time:684ms step_avg:56.99ms
step:13/2330 train_time:740ms step_avg:56.92ms
step:14/2330 train_time:801ms step_avg:57.20ms
step:15/2330 train_time:857ms step_avg:57.15ms
step:16/2330 train_time:918ms step_avg:57.38ms
step:17/2330 train_time:975ms step_avg:57.33ms
step:18/2330 train_time:1036ms step_avg:57.58ms
step:19/2330 train_time:1095ms step_avg:57.64ms
step:20/2330 train_time:1160ms step_avg:58.01ms
step:21/2330 train_time:1219ms step_avg:58.03ms
step:22/2330 train_time:1285ms step_avg:58.40ms
step:23/2330 train_time:1342ms step_avg:58.35ms
step:24/2330 train_time:1403ms step_avg:58.47ms
step:25/2330 train_time:1461ms step_avg:58.45ms
step:26/2330 train_time:1522ms step_avg:58.56ms
step:27/2330 train_time:1580ms step_avg:58.52ms
step:28/2330 train_time:1641ms step_avg:58.59ms
step:29/2330 train_time:1697ms step_avg:58.52ms
step:30/2330 train_time:1758ms step_avg:58.60ms
step:31/2330 train_time:1814ms step_avg:58.52ms
step:32/2330 train_time:1877ms step_avg:58.64ms
step:33/2330 train_time:1933ms step_avg:58.58ms
step:34/2330 train_time:1995ms step_avg:58.67ms
step:35/2330 train_time:2052ms step_avg:58.62ms
step:36/2330 train_time:2115ms step_avg:58.75ms
step:37/2330 train_time:2173ms step_avg:58.73ms
step:38/2330 train_time:2237ms step_avg:58.87ms
step:39/2330 train_time:2295ms step_avg:58.83ms
step:40/2330 train_time:2358ms step_avg:58.94ms
step:41/2330 train_time:2415ms step_avg:58.91ms
step:42/2330 train_time:2478ms step_avg:58.99ms
step:43/2330 train_time:2535ms step_avg:58.96ms
step:44/2330 train_time:2597ms step_avg:59.02ms
step:45/2330 train_time:2654ms step_avg:58.98ms
step:46/2330 train_time:2716ms step_avg:59.05ms
step:47/2330 train_time:2774ms step_avg:59.02ms
step:48/2330 train_time:2836ms step_avg:59.08ms
step:49/2330 train_time:2893ms step_avg:59.04ms
step:50/2330 train_time:2954ms step_avg:59.08ms
step:51/2330 train_time:3011ms step_avg:59.03ms
step:52/2330 train_time:3073ms step_avg:59.09ms
step:53/2330 train_time:3131ms step_avg:59.07ms
step:54/2330 train_time:3194ms step_avg:59.15ms
step:55/2330 train_time:3251ms step_avg:59.11ms
step:56/2330 train_time:3314ms step_avg:59.17ms
step:57/2330 train_time:3370ms step_avg:59.12ms
step:58/2330 train_time:3433ms step_avg:59.20ms
step:59/2330 train_time:3490ms step_avg:59.15ms
step:60/2330 train_time:3553ms step_avg:59.22ms
step:61/2330 train_time:3609ms step_avg:59.17ms
step:62/2330 train_time:3672ms step_avg:59.23ms
step:63/2330 train_time:3729ms step_avg:59.18ms
step:64/2330 train_time:3792ms step_avg:59.24ms
step:65/2330 train_time:3848ms step_avg:59.20ms
step:66/2330 train_time:3910ms step_avg:59.24ms
step:67/2330 train_time:3966ms step_avg:59.20ms
step:68/2330 train_time:4028ms step_avg:59.23ms
step:69/2330 train_time:4085ms step_avg:59.20ms
step:70/2330 train_time:4146ms step_avg:59.23ms
step:71/2330 train_time:4202ms step_avg:59.19ms
step:72/2330 train_time:4264ms step_avg:59.23ms
step:73/2330 train_time:4321ms step_avg:59.19ms
step:74/2330 train_time:4384ms step_avg:59.24ms
step:75/2330 train_time:4441ms step_avg:59.21ms
step:76/2330 train_time:4502ms step_avg:59.24ms
step:77/2330 train_time:4560ms step_avg:59.23ms
step:78/2330 train_time:4622ms step_avg:59.26ms
step:79/2330 train_time:4679ms step_avg:59.23ms
step:80/2330 train_time:4741ms step_avg:59.26ms
step:81/2330 train_time:4798ms step_avg:59.24ms
step:82/2330 train_time:4860ms step_avg:59.27ms
step:83/2330 train_time:4918ms step_avg:59.25ms
step:84/2330 train_time:4979ms step_avg:59.28ms
step:85/2330 train_time:5038ms step_avg:59.27ms
step:86/2330 train_time:5099ms step_avg:59.29ms
step:87/2330 train_time:5156ms step_avg:59.26ms
step:88/2330 train_time:5218ms step_avg:59.29ms
step:89/2330 train_time:5275ms step_avg:59.27ms
step:90/2330 train_time:5337ms step_avg:59.29ms
step:91/2330 train_time:5394ms step_avg:59.27ms
step:92/2330 train_time:5456ms step_avg:59.30ms
step:93/2330 train_time:5513ms step_avg:59.28ms
step:94/2330 train_time:5575ms step_avg:59.31ms
step:95/2330 train_time:5632ms step_avg:59.29ms
step:96/2330 train_time:5694ms step_avg:59.31ms
step:97/2330 train_time:5750ms step_avg:59.28ms
step:98/2330 train_time:5813ms step_avg:59.32ms
step:99/2330 train_time:5870ms step_avg:59.29ms
step:100/2330 train_time:5931ms step_avg:59.31ms
step:101/2330 train_time:5988ms step_avg:59.29ms
step:102/2330 train_time:6051ms step_avg:59.33ms
step:103/2330 train_time:6107ms step_avg:59.30ms
step:104/2330 train_time:6170ms step_avg:59.33ms
step:105/2330 train_time:6227ms step_avg:59.31ms
step:106/2330 train_time:6289ms step_avg:59.33ms
step:107/2330 train_time:6345ms step_avg:59.30ms
step:108/2330 train_time:6407ms step_avg:59.32ms
step:109/2330 train_time:6463ms step_avg:59.30ms
step:110/2330 train_time:6526ms step_avg:59.32ms
step:111/2330 train_time:6583ms step_avg:59.30ms
step:112/2330 train_time:6646ms step_avg:59.34ms
step:113/2330 train_time:6703ms step_avg:59.32ms
step:114/2330 train_time:6765ms step_avg:59.34ms
step:115/2330 train_time:6822ms step_avg:59.32ms
step:116/2330 train_time:6884ms step_avg:59.35ms
step:117/2330 train_time:6941ms step_avg:59.32ms
step:118/2330 train_time:7004ms step_avg:59.35ms
step:119/2330 train_time:7061ms step_avg:59.34ms
step:120/2330 train_time:7123ms step_avg:59.36ms
step:121/2330 train_time:7181ms step_avg:59.35ms
step:122/2330 train_time:7242ms step_avg:59.36ms
step:123/2330 train_time:7299ms step_avg:59.34ms
step:124/2330 train_time:7361ms step_avg:59.36ms
step:125/2330 train_time:7418ms step_avg:59.34ms
step:126/2330 train_time:7480ms step_avg:59.36ms
step:127/2330 train_time:7537ms step_avg:59.35ms
step:128/2330 train_time:7600ms step_avg:59.38ms
step:129/2330 train_time:7658ms step_avg:59.36ms
step:130/2330 train_time:7721ms step_avg:59.39ms
step:131/2330 train_time:7778ms step_avg:59.38ms
step:132/2330 train_time:7840ms step_avg:59.39ms
step:133/2330 train_time:7897ms step_avg:59.37ms
step:134/2330 train_time:7960ms step_avg:59.40ms
step:135/2330 train_time:8017ms step_avg:59.38ms
step:136/2330 train_time:8079ms step_avg:59.41ms
step:137/2330 train_time:8137ms step_avg:59.40ms
step:138/2330 train_time:8199ms step_avg:59.41ms
step:139/2330 train_time:8256ms step_avg:59.39ms
step:140/2330 train_time:8318ms step_avg:59.42ms
step:141/2330 train_time:8376ms step_avg:59.40ms
step:142/2330 train_time:8437ms step_avg:59.42ms
step:143/2330 train_time:8495ms step_avg:59.40ms
step:144/2330 train_time:8557ms step_avg:59.42ms
step:145/2330 train_time:8614ms step_avg:59.41ms
step:146/2330 train_time:8678ms step_avg:59.44ms
step:147/2330 train_time:8736ms step_avg:59.43ms
step:148/2330 train_time:8797ms step_avg:59.44ms
step:149/2330 train_time:8854ms step_avg:59.42ms
step:150/2330 train_time:8916ms step_avg:59.44ms
step:151/2330 train_time:8973ms step_avg:59.43ms
step:152/2330 train_time:9035ms step_avg:59.44ms
step:153/2330 train_time:9092ms step_avg:59.43ms
step:154/2330 train_time:9154ms step_avg:59.44ms
step:155/2330 train_time:9212ms step_avg:59.43ms
step:156/2330 train_time:9273ms step_avg:59.44ms
step:157/2330 train_time:9330ms step_avg:59.42ms
step:158/2330 train_time:9393ms step_avg:59.45ms
step:159/2330 train_time:9450ms step_avg:59.43ms
step:160/2330 train_time:9513ms step_avg:59.45ms
step:161/2330 train_time:9569ms step_avg:59.44ms
step:162/2330 train_time:9632ms step_avg:59.46ms
step:163/2330 train_time:9688ms step_avg:59.44ms
step:164/2330 train_time:9751ms step_avg:59.46ms
step:165/2330 train_time:9808ms step_avg:59.44ms
step:166/2330 train_time:9870ms step_avg:59.46ms
step:167/2330 train_time:9926ms step_avg:59.44ms
step:168/2330 train_time:9988ms step_avg:59.45ms
step:169/2330 train_time:10044ms step_avg:59.43ms
step:170/2330 train_time:10105ms step_avg:59.44ms
step:171/2330 train_time:10162ms step_avg:59.43ms
step:172/2330 train_time:10223ms step_avg:59.44ms
step:173/2330 train_time:10281ms step_avg:59.43ms
step:174/2330 train_time:10342ms step_avg:59.44ms
step:175/2330 train_time:10399ms step_avg:59.43ms
step:176/2330 train_time:10462ms step_avg:59.44ms
step:177/2330 train_time:10519ms step_avg:59.43ms
step:178/2330 train_time:10581ms step_avg:59.44ms
step:179/2330 train_time:10638ms step_avg:59.43ms
step:180/2330 train_time:10700ms step_avg:59.45ms
step:181/2330 train_time:10758ms step_avg:59.44ms
step:182/2330 train_time:10820ms step_avg:59.45ms
step:183/2330 train_time:10877ms step_avg:59.44ms
step:184/2330 train_time:10939ms step_avg:59.45ms
step:185/2330 train_time:10996ms step_avg:59.44ms
step:186/2330 train_time:11057ms step_avg:59.45ms
step:187/2330 train_time:11115ms step_avg:59.44ms
step:188/2330 train_time:11177ms step_avg:59.45ms
step:189/2330 train_time:11234ms step_avg:59.44ms
step:190/2330 train_time:11295ms step_avg:59.45ms
step:191/2330 train_time:11352ms step_avg:59.43ms
step:192/2330 train_time:11415ms step_avg:59.45ms
step:193/2330 train_time:11472ms step_avg:59.44ms
step:194/2330 train_time:11534ms step_avg:59.45ms
step:195/2330 train_time:11591ms step_avg:59.44ms
step:196/2330 train_time:11653ms step_avg:59.45ms
step:197/2330 train_time:11710ms step_avg:59.44ms
step:198/2330 train_time:11772ms step_avg:59.46ms
step:199/2330 train_time:11829ms step_avg:59.44ms
step:200/2330 train_time:11893ms step_avg:59.46ms
step:201/2330 train_time:11949ms step_avg:59.45ms
step:202/2330 train_time:12012ms step_avg:59.47ms
step:203/2330 train_time:12068ms step_avg:59.45ms
step:204/2330 train_time:12132ms step_avg:59.47ms
step:205/2330 train_time:12189ms step_avg:59.46ms
step:206/2330 train_time:12252ms step_avg:59.47ms
step:207/2330 train_time:12307ms step_avg:59.46ms
step:208/2330 train_time:12370ms step_avg:59.47ms
step:209/2330 train_time:12426ms step_avg:59.46ms
step:210/2330 train_time:12489ms step_avg:59.47ms
step:211/2330 train_time:12545ms step_avg:59.46ms
step:212/2330 train_time:12606ms step_avg:59.46ms
step:213/2330 train_time:12663ms step_avg:59.45ms
step:214/2330 train_time:12725ms step_avg:59.46ms
step:215/2330 train_time:12782ms step_avg:59.45ms
step:216/2330 train_time:12844ms step_avg:59.46ms
step:217/2330 train_time:12901ms step_avg:59.45ms
step:218/2330 train_time:12963ms step_avg:59.46ms
step:219/2330 train_time:13020ms step_avg:59.45ms
step:220/2330 train_time:13082ms step_avg:59.46ms
step:221/2330 train_time:13139ms step_avg:59.45ms
step:222/2330 train_time:13201ms step_avg:59.46ms
step:223/2330 train_time:13259ms step_avg:59.46ms
step:224/2330 train_time:13320ms step_avg:59.46ms
step:225/2330 train_time:13378ms step_avg:59.46ms
step:226/2330 train_time:13439ms step_avg:59.47ms
step:227/2330 train_time:13496ms step_avg:59.45ms
step:228/2330 train_time:13558ms step_avg:59.46ms
step:229/2330 train_time:13615ms step_avg:59.45ms
step:230/2330 train_time:13677ms step_avg:59.47ms
step:231/2330 train_time:13735ms step_avg:59.46ms
step:232/2330 train_time:13797ms step_avg:59.47ms
step:233/2330 train_time:13854ms step_avg:59.46ms
step:234/2330 train_time:13915ms step_avg:59.47ms
step:235/2330 train_time:13972ms step_avg:59.46ms
step:236/2330 train_time:14035ms step_avg:59.47ms
step:237/2330 train_time:14092ms step_avg:59.46ms
step:238/2330 train_time:14154ms step_avg:59.47ms
step:239/2330 train_time:14211ms step_avg:59.46ms
step:240/2330 train_time:14274ms step_avg:59.47ms
step:241/2330 train_time:14331ms step_avg:59.46ms
step:242/2330 train_time:14393ms step_avg:59.48ms
step:243/2330 train_time:14450ms step_avg:59.46ms
step:244/2330 train_time:14512ms step_avg:59.47ms
step:245/2330 train_time:14569ms step_avg:59.46ms
step:246/2330 train_time:14631ms step_avg:59.48ms
step:247/2330 train_time:14687ms step_avg:59.46ms
step:248/2330 train_time:14750ms step_avg:59.48ms
step:249/2330 train_time:14807ms step_avg:59.46ms
step:250/2330 train_time:14869ms step_avg:59.48ms
step:250/2330 val_loss:5.0764 train_time:14947ms step_avg:59.79ms
step:251/2330 train_time:14967ms step_avg:59.63ms
step:252/2330 train_time:14989ms step_avg:59.48ms
step:253/2330 train_time:15046ms step_avg:59.47ms
step:254/2330 train_time:15115ms step_avg:59.51ms
step:255/2330 train_time:15171ms step_avg:59.49ms
step:256/2330 train_time:15238ms step_avg:59.52ms
step:257/2330 train_time:15294ms step_avg:59.51ms
step:258/2330 train_time:15356ms step_avg:59.52ms
step:259/2330 train_time:15412ms step_avg:59.51ms
step:260/2330 train_time:15475ms step_avg:59.52ms
step:261/2330 train_time:15531ms step_avg:59.50ms
step:262/2330 train_time:15592ms step_avg:59.51ms
step:263/2330 train_time:15648ms step_avg:59.50ms
step:264/2330 train_time:15709ms step_avg:59.50ms
step:265/2330 train_time:15765ms step_avg:59.49ms
step:266/2330 train_time:15826ms step_avg:59.50ms
step:267/2330 train_time:15883ms step_avg:59.49ms
step:268/2330 train_time:15944ms step_avg:59.49ms
step:269/2330 train_time:16003ms step_avg:59.49ms
step:270/2330 train_time:16065ms step_avg:59.50ms
step:271/2330 train_time:16123ms step_avg:59.49ms
step:272/2330 train_time:16185ms step_avg:59.51ms
step:273/2330 train_time:16242ms step_avg:59.49ms
step:274/2330 train_time:16305ms step_avg:59.51ms
step:275/2330 train_time:16361ms step_avg:59.50ms
step:276/2330 train_time:16423ms step_avg:59.50ms
step:277/2330 train_time:16480ms step_avg:59.49ms
step:278/2330 train_time:16542ms step_avg:59.50ms
step:279/2330 train_time:16598ms step_avg:59.49ms
step:280/2330 train_time:16661ms step_avg:59.50ms
step:281/2330 train_time:16718ms step_avg:59.50ms
step:282/2330 train_time:16779ms step_avg:59.50ms
step:283/2330 train_time:16836ms step_avg:59.49ms
step:284/2330 train_time:16897ms step_avg:59.50ms
step:285/2330 train_time:16955ms step_avg:59.49ms
step:286/2330 train_time:17018ms step_avg:59.50ms
step:287/2330 train_time:17076ms step_avg:59.50ms
step:288/2330 train_time:17138ms step_avg:59.51ms
step:289/2330 train_time:17196ms step_avg:59.50ms
step:290/2330 train_time:17259ms step_avg:59.51ms
step:291/2330 train_time:17317ms step_avg:59.51ms
step:292/2330 train_time:17379ms step_avg:59.52ms
step:293/2330 train_time:17436ms step_avg:59.51ms
step:294/2330 train_time:17499ms step_avg:59.52ms
step:295/2330 train_time:17556ms step_avg:59.51ms
step:296/2330 train_time:17617ms step_avg:59.52ms
step:297/2330 train_time:17673ms step_avg:59.51ms
step:298/2330 train_time:17735ms step_avg:59.51ms
step:299/2330 train_time:17791ms step_avg:59.50ms
step:300/2330 train_time:17853ms step_avg:59.51ms
step:301/2330 train_time:17910ms step_avg:59.50ms
step:302/2330 train_time:17972ms step_avg:59.51ms
step:303/2330 train_time:18029ms step_avg:59.50ms
step:304/2330 train_time:18091ms step_avg:59.51ms
step:305/2330 train_time:18148ms step_avg:59.50ms
step:306/2330 train_time:18211ms step_avg:59.51ms
step:307/2330 train_time:18268ms step_avg:59.51ms
step:308/2330 train_time:18330ms step_avg:59.51ms
step:309/2330 train_time:18386ms step_avg:59.50ms
step:310/2330 train_time:18448ms step_avg:59.51ms
step:311/2330 train_time:18505ms step_avg:59.50ms
step:312/2330 train_time:18567ms step_avg:59.51ms
step:313/2330 train_time:18623ms step_avg:59.50ms
step:314/2330 train_time:18685ms step_avg:59.51ms
step:315/2330 train_time:18742ms step_avg:59.50ms
step:316/2330 train_time:18804ms step_avg:59.50ms
step:317/2330 train_time:18861ms step_avg:59.50ms
step:318/2330 train_time:18923ms step_avg:59.51ms
step:319/2330 train_time:18980ms step_avg:59.50ms
step:320/2330 train_time:19043ms step_avg:59.51ms
step:321/2330 train_time:19100ms step_avg:59.50ms
step:322/2330 train_time:19164ms step_avg:59.51ms
step:323/2330 train_time:19221ms step_avg:59.51ms
step:324/2330 train_time:19283ms step_avg:59.52ms
step:325/2330 train_time:19341ms step_avg:59.51ms
step:326/2330 train_time:19403ms step_avg:59.52ms
step:327/2330 train_time:19461ms step_avg:59.51ms
step:328/2330 train_time:19523ms step_avg:59.52ms
step:329/2330 train_time:19580ms step_avg:59.51ms
step:330/2330 train_time:19641ms step_avg:59.52ms
step:331/2330 train_time:19698ms step_avg:59.51ms
step:332/2330 train_time:19760ms step_avg:59.52ms
step:333/2330 train_time:19818ms step_avg:59.51ms
step:334/2330 train_time:19880ms step_avg:59.52ms
step:335/2330 train_time:19937ms step_avg:59.51ms
step:336/2330 train_time:19999ms step_avg:59.52ms
step:337/2330 train_time:20056ms step_avg:59.51ms
step:338/2330 train_time:20119ms step_avg:59.52ms
step:339/2330 train_time:20177ms step_avg:59.52ms
step:340/2330 train_time:20238ms step_avg:59.52ms
step:341/2330 train_time:20296ms step_avg:59.52ms
step:342/2330 train_time:20359ms step_avg:59.53ms
step:343/2330 train_time:20417ms step_avg:59.52ms
step:344/2330 train_time:20478ms step_avg:59.53ms
step:345/2330 train_time:20536ms step_avg:59.53ms
step:346/2330 train_time:20597ms step_avg:59.53ms
step:347/2330 train_time:20654ms step_avg:59.52ms
step:348/2330 train_time:20715ms step_avg:59.53ms
step:349/2330 train_time:20772ms step_avg:59.52ms
step:350/2330 train_time:20834ms step_avg:59.53ms
step:351/2330 train_time:20891ms step_avg:59.52ms
step:352/2330 train_time:20953ms step_avg:59.53ms
step:353/2330 train_time:21010ms step_avg:59.52ms
step:354/2330 train_time:21074ms step_avg:59.53ms
step:355/2330 train_time:21131ms step_avg:59.52ms
step:356/2330 train_time:21193ms step_avg:59.53ms
step:357/2330 train_time:21249ms step_avg:59.52ms
step:358/2330 train_time:21312ms step_avg:59.53ms
step:359/2330 train_time:21369ms step_avg:59.52ms
step:360/2330 train_time:21432ms step_avg:59.53ms
step:361/2330 train_time:21488ms step_avg:59.52ms
step:362/2330 train_time:21551ms step_avg:59.53ms
step:363/2330 train_time:21607ms step_avg:59.52ms
step:364/2330 train_time:21669ms step_avg:59.53ms
step:365/2330 train_time:21725ms step_avg:59.52ms
step:366/2330 train_time:21788ms step_avg:59.53ms
step:367/2330 train_time:21844ms step_avg:59.52ms
step:368/2330 train_time:21905ms step_avg:59.53ms
step:369/2330 train_time:21962ms step_avg:59.52ms
step:370/2330 train_time:22025ms step_avg:59.53ms
step:371/2330 train_time:22081ms step_avg:59.52ms
step:372/2330 train_time:22144ms step_avg:59.53ms
step:373/2330 train_time:22201ms step_avg:59.52ms
step:374/2330 train_time:22263ms step_avg:59.53ms
step:375/2330 train_time:22321ms step_avg:59.52ms
step:376/2330 train_time:22382ms step_avg:59.53ms
step:377/2330 train_time:22440ms step_avg:59.52ms
step:378/2330 train_time:22502ms step_avg:59.53ms
step:379/2330 train_time:22559ms step_avg:59.52ms
step:380/2330 train_time:22622ms step_avg:59.53ms
step:381/2330 train_time:22680ms step_avg:59.53ms
step:382/2330 train_time:22741ms step_avg:59.53ms
step:383/2330 train_time:22798ms step_avg:59.52ms
step:384/2330 train_time:22860ms step_avg:59.53ms
step:385/2330 train_time:22917ms step_avg:59.53ms
step:386/2330 train_time:22982ms step_avg:59.54ms
step:387/2330 train_time:23039ms step_avg:59.53ms
step:388/2330 train_time:23101ms step_avg:59.54ms
step:389/2330 train_time:23157ms step_avg:59.53ms
step:390/2330 train_time:23220ms step_avg:59.54ms
step:391/2330 train_time:23277ms step_avg:59.53ms
step:392/2330 train_time:23339ms step_avg:59.54ms
step:393/2330 train_time:23397ms step_avg:59.53ms
step:394/2330 train_time:23460ms step_avg:59.54ms
step:395/2330 train_time:23517ms step_avg:59.54ms
step:396/2330 train_time:23578ms step_avg:59.54ms
step:397/2330 train_time:23636ms step_avg:59.54ms
step:398/2330 train_time:23698ms step_avg:59.54ms
step:399/2330 train_time:23756ms step_avg:59.54ms
step:400/2330 train_time:23818ms step_avg:59.54ms
step:401/2330 train_time:23875ms step_avg:59.54ms
step:402/2330 train_time:23936ms step_avg:59.54ms
step:403/2330 train_time:23994ms step_avg:59.54ms
step:404/2330 train_time:24055ms step_avg:59.54ms
step:405/2330 train_time:24112ms step_avg:59.54ms
step:406/2330 train_time:24175ms step_avg:59.54ms
step:407/2330 train_time:24232ms step_avg:59.54ms
step:408/2330 train_time:24293ms step_avg:59.54ms
step:409/2330 train_time:24350ms step_avg:59.54ms
step:410/2330 train_time:24413ms step_avg:59.54ms
step:411/2330 train_time:24471ms step_avg:59.54ms
step:412/2330 train_time:24534ms step_avg:59.55ms
step:413/2330 train_time:24590ms step_avg:59.54ms
step:414/2330 train_time:24652ms step_avg:59.55ms
step:415/2330 train_time:24709ms step_avg:59.54ms
step:416/2330 train_time:24772ms step_avg:59.55ms
step:417/2330 train_time:24829ms step_avg:59.54ms
step:418/2330 train_time:24892ms step_avg:59.55ms
step:419/2330 train_time:24948ms step_avg:59.54ms
step:420/2330 train_time:25011ms step_avg:59.55ms
step:421/2330 train_time:25068ms step_avg:59.54ms
step:422/2330 train_time:25130ms step_avg:59.55ms
step:423/2330 train_time:25186ms step_avg:59.54ms
step:424/2330 train_time:25247ms step_avg:59.55ms
step:425/2330 train_time:25304ms step_avg:59.54ms
step:426/2330 train_time:25365ms step_avg:59.54ms
step:427/2330 train_time:25422ms step_avg:59.54ms
step:428/2330 train_time:25484ms step_avg:59.54ms
step:429/2330 train_time:25541ms step_avg:59.54ms
step:430/2330 train_time:25604ms step_avg:59.54ms
step:431/2330 train_time:25662ms step_avg:59.54ms
step:432/2330 train_time:25723ms step_avg:59.54ms
step:433/2330 train_time:25780ms step_avg:59.54ms
step:434/2330 train_time:25841ms step_avg:59.54ms
step:435/2330 train_time:25899ms step_avg:59.54ms
step:436/2330 train_time:25962ms step_avg:59.55ms
step:437/2330 train_time:26019ms step_avg:59.54ms
step:438/2330 train_time:26081ms step_avg:59.55ms
step:439/2330 train_time:26139ms step_avg:59.54ms
step:440/2330 train_time:26201ms step_avg:59.55ms
step:441/2330 train_time:26258ms step_avg:59.54ms
step:442/2330 train_time:26320ms step_avg:59.55ms
step:443/2330 train_time:26377ms step_avg:59.54ms
step:444/2330 train_time:26439ms step_avg:59.55ms
step:445/2330 train_time:26496ms step_avg:59.54ms
step:446/2330 train_time:26558ms step_avg:59.55ms
step:447/2330 train_time:26615ms step_avg:59.54ms
step:448/2330 train_time:26679ms step_avg:59.55ms
step:449/2330 train_time:26736ms step_avg:59.55ms
step:450/2330 train_time:26798ms step_avg:59.55ms
step:451/2330 train_time:26856ms step_avg:59.55ms
step:452/2330 train_time:26918ms step_avg:59.55ms
step:453/2330 train_time:26975ms step_avg:59.55ms
step:454/2330 train_time:27037ms step_avg:59.55ms
step:455/2330 train_time:27094ms step_avg:59.55ms
step:456/2330 train_time:27157ms step_avg:59.55ms
step:457/2330 train_time:27213ms step_avg:59.55ms
step:458/2330 train_time:27276ms step_avg:59.56ms
step:459/2330 train_time:27333ms step_avg:59.55ms
step:460/2330 train_time:27395ms step_avg:59.55ms
step:461/2330 train_time:27452ms step_avg:59.55ms
step:462/2330 train_time:27514ms step_avg:59.55ms
step:463/2330 train_time:27572ms step_avg:59.55ms
step:464/2330 train_time:27634ms step_avg:59.56ms
step:465/2330 train_time:27690ms step_avg:59.55ms
step:466/2330 train_time:27753ms step_avg:59.56ms
step:467/2330 train_time:27809ms step_avg:59.55ms
step:468/2330 train_time:27874ms step_avg:59.56ms
step:469/2330 train_time:27930ms step_avg:59.55ms
step:470/2330 train_time:27993ms step_avg:59.56ms
step:471/2330 train_time:28049ms step_avg:59.55ms
step:472/2330 train_time:28112ms step_avg:59.56ms
step:473/2330 train_time:28169ms step_avg:59.55ms
step:474/2330 train_time:28231ms step_avg:59.56ms
step:475/2330 train_time:28287ms step_avg:59.55ms
step:476/2330 train_time:28349ms step_avg:59.56ms
step:477/2330 train_time:28405ms step_avg:59.55ms
step:478/2330 train_time:28467ms step_avg:59.55ms
step:479/2330 train_time:28524ms step_avg:59.55ms
step:480/2330 train_time:28585ms step_avg:59.55ms
step:481/2330 train_time:28642ms step_avg:59.55ms
step:482/2330 train_time:28704ms step_avg:59.55ms
step:483/2330 train_time:28762ms step_avg:59.55ms
step:484/2330 train_time:28824ms step_avg:59.55ms
step:485/2330 train_time:28881ms step_avg:59.55ms
step:486/2330 train_time:28942ms step_avg:59.55ms
step:487/2330 train_time:28999ms step_avg:59.55ms
step:488/2330 train_time:29062ms step_avg:59.55ms
step:489/2330 train_time:29119ms step_avg:59.55ms
step:490/2330 train_time:29180ms step_avg:59.55ms
step:491/2330 train_time:29239ms step_avg:59.55ms
step:492/2330 train_time:29300ms step_avg:59.55ms
step:493/2330 train_time:29357ms step_avg:59.55ms
step:494/2330 train_time:29420ms step_avg:59.55ms
step:495/2330 train_time:29478ms step_avg:59.55ms
step:496/2330 train_time:29539ms step_avg:59.55ms
step:497/2330 train_time:29596ms step_avg:59.55ms
step:498/2330 train_time:29659ms step_avg:59.56ms
step:499/2330 train_time:29716ms step_avg:59.55ms
step:500/2330 train_time:29777ms step_avg:59.55ms
step:500/2330 val_loss:4.6879 train_time:29856ms step_avg:59.71ms
step:501/2330 train_time:29876ms step_avg:59.63ms
step:502/2330 train_time:29901ms step_avg:59.56ms
step:503/2330 train_time:29958ms step_avg:59.56ms
step:504/2330 train_time:30022ms step_avg:59.57ms
step:505/2330 train_time:30080ms step_avg:59.56ms
step:506/2330 train_time:30142ms step_avg:59.57ms
step:507/2330 train_time:30198ms step_avg:59.56ms
step:508/2330 train_time:30261ms step_avg:59.57ms
step:509/2330 train_time:30317ms step_avg:59.56ms
step:510/2330 train_time:30379ms step_avg:59.57ms
step:511/2330 train_time:30435ms step_avg:59.56ms
step:512/2330 train_time:30497ms step_avg:59.56ms
step:513/2330 train_time:30553ms step_avg:59.56ms
step:514/2330 train_time:30615ms step_avg:59.56ms
step:515/2330 train_time:30671ms step_avg:59.56ms
step:516/2330 train_time:30733ms step_avg:59.56ms
step:517/2330 train_time:30790ms step_avg:59.56ms
step:518/2330 train_time:30852ms step_avg:59.56ms
step:519/2330 train_time:30910ms step_avg:59.56ms
step:520/2330 train_time:30972ms step_avg:59.56ms
step:521/2330 train_time:31029ms step_avg:59.56ms
step:522/2330 train_time:31092ms step_avg:59.56ms
step:523/2330 train_time:31149ms step_avg:59.56ms
step:524/2330 train_time:31211ms step_avg:59.56ms
step:525/2330 train_time:31268ms step_avg:59.56ms
step:526/2330 train_time:31330ms step_avg:59.56ms
step:527/2330 train_time:31387ms step_avg:59.56ms
step:528/2330 train_time:31448ms step_avg:59.56ms
step:529/2330 train_time:31505ms step_avg:59.56ms
step:530/2330 train_time:31568ms step_avg:59.56ms
step:531/2330 train_time:31624ms step_avg:59.56ms
step:532/2330 train_time:31686ms step_avg:59.56ms
step:533/2330 train_time:31743ms step_avg:59.56ms
step:534/2330 train_time:31806ms step_avg:59.56ms
step:535/2330 train_time:31863ms step_avg:59.56ms
step:536/2330 train_time:31926ms step_avg:59.56ms
step:537/2330 train_time:31983ms step_avg:59.56ms
step:538/2330 train_time:32046ms step_avg:59.57ms
step:539/2330 train_time:32104ms step_avg:59.56ms
step:540/2330 train_time:32167ms step_avg:59.57ms
step:541/2330 train_time:32224ms step_avg:59.56ms
step:542/2330 train_time:32286ms step_avg:59.57ms
step:543/2330 train_time:32343ms step_avg:59.56ms
step:544/2330 train_time:32405ms step_avg:59.57ms
step:545/2330 train_time:32462ms step_avg:59.56ms
step:546/2330 train_time:32524ms step_avg:59.57ms
step:547/2330 train_time:32580ms step_avg:59.56ms
step:548/2330 train_time:32642ms step_avg:59.57ms
step:549/2330 train_time:32699ms step_avg:59.56ms
step:550/2330 train_time:32762ms step_avg:59.57ms
step:551/2330 train_time:32820ms step_avg:59.56ms
step:552/2330 train_time:32882ms step_avg:59.57ms
step:553/2330 train_time:32940ms step_avg:59.57ms
step:554/2330 train_time:33003ms step_avg:59.57ms
step:555/2330 train_time:33061ms step_avg:59.57ms
step:556/2330 train_time:33124ms step_avg:59.58ms
step:557/2330 train_time:33181ms step_avg:59.57ms
step:558/2330 train_time:33243ms step_avg:59.58ms
step:559/2330 train_time:33300ms step_avg:59.57ms
step:560/2330 train_time:33362ms step_avg:59.58ms
step:561/2330 train_time:33420ms step_avg:59.57ms
step:562/2330 train_time:33481ms step_avg:59.57ms
step:563/2330 train_time:33538ms step_avg:59.57ms
step:564/2330 train_time:33600ms step_avg:59.57ms
step:565/2330 train_time:33656ms step_avg:59.57ms
step:566/2330 train_time:33720ms step_avg:59.58ms
step:567/2330 train_time:33777ms step_avg:59.57ms
step:568/2330 train_time:33839ms step_avg:59.58ms
step:569/2330 train_time:33896ms step_avg:59.57ms
step:570/2330 train_time:33959ms step_avg:59.58ms
step:571/2330 train_time:34016ms step_avg:59.57ms
step:572/2330 train_time:34079ms step_avg:59.58ms
step:573/2330 train_time:34135ms step_avg:59.57ms
step:574/2330 train_time:34199ms step_avg:59.58ms
step:575/2330 train_time:34255ms step_avg:59.57ms
step:576/2330 train_time:34319ms step_avg:59.58ms
step:577/2330 train_time:34376ms step_avg:59.58ms
step:578/2330 train_time:34437ms step_avg:59.58ms
step:579/2330 train_time:34494ms step_avg:59.57ms
step:580/2330 train_time:34556ms step_avg:59.58ms
step:581/2330 train_time:34613ms step_avg:59.57ms
step:582/2330 train_time:34675ms step_avg:59.58ms
step:583/2330 train_time:34732ms step_avg:59.57ms
step:584/2330 train_time:34794ms step_avg:59.58ms
step:585/2330 train_time:34851ms step_avg:59.57ms
step:586/2330 train_time:34913ms step_avg:59.58ms
step:587/2330 train_time:34970ms step_avg:59.57ms
step:588/2330 train_time:35031ms step_avg:59.58ms
step:589/2330 train_time:35088ms step_avg:59.57ms
step:590/2330 train_time:35150ms step_avg:59.58ms
step:591/2330 train_time:35207ms step_avg:59.57ms
step:592/2330 train_time:35270ms step_avg:59.58ms
step:593/2330 train_time:35327ms step_avg:59.57ms
step:594/2330 train_time:35388ms step_avg:59.58ms
step:595/2330 train_time:35445ms step_avg:59.57ms
step:596/2330 train_time:35508ms step_avg:59.58ms
step:597/2330 train_time:35565ms step_avg:59.57ms
step:598/2330 train_time:35627ms step_avg:59.58ms
step:599/2330 train_time:35684ms step_avg:59.57ms
step:600/2330 train_time:35747ms step_avg:59.58ms
step:601/2330 train_time:35805ms step_avg:59.58ms
step:602/2330 train_time:35867ms step_avg:59.58ms
step:603/2330 train_time:35924ms step_avg:59.58ms
step:604/2330 train_time:35986ms step_avg:59.58ms
step:605/2330 train_time:36043ms step_avg:59.57ms
step:606/2330 train_time:36106ms step_avg:59.58ms
step:607/2330 train_time:36163ms step_avg:59.58ms
step:608/2330 train_time:36226ms step_avg:59.58ms
step:609/2330 train_time:36284ms step_avg:59.58ms
step:610/2330 train_time:36346ms step_avg:59.58ms
step:611/2330 train_time:36403ms step_avg:59.58ms
step:612/2330 train_time:36464ms step_avg:59.58ms
step:613/2330 train_time:36522ms step_avg:59.58ms
step:614/2330 train_time:36584ms step_avg:59.58ms
step:615/2330 train_time:36642ms step_avg:59.58ms
step:616/2330 train_time:36704ms step_avg:59.59ms
step:617/2330 train_time:36762ms step_avg:59.58ms
step:618/2330 train_time:36824ms step_avg:59.59ms
step:619/2330 train_time:36881ms step_avg:59.58ms
step:620/2330 train_time:36944ms step_avg:59.59ms
step:621/2330 train_time:37001ms step_avg:59.58ms
step:622/2330 train_time:37065ms step_avg:59.59ms
step:623/2330 train_time:37122ms step_avg:59.59ms
step:624/2330 train_time:37183ms step_avg:59.59ms
step:625/2330 train_time:37240ms step_avg:59.58ms
step:626/2330 train_time:37302ms step_avg:59.59ms
step:627/2330 train_time:37360ms step_avg:59.58ms
step:628/2330 train_time:37422ms step_avg:59.59ms
step:629/2330 train_time:37480ms step_avg:59.59ms
step:630/2330 train_time:37543ms step_avg:59.59ms
step:631/2330 train_time:37600ms step_avg:59.59ms
step:632/2330 train_time:37662ms step_avg:59.59ms
step:633/2330 train_time:37719ms step_avg:59.59ms
step:634/2330 train_time:37781ms step_avg:59.59ms
step:635/2330 train_time:37838ms step_avg:59.59ms
step:636/2330 train_time:37900ms step_avg:59.59ms
step:637/2330 train_time:37957ms step_avg:59.59ms
step:638/2330 train_time:38020ms step_avg:59.59ms
step:639/2330 train_time:38077ms step_avg:59.59ms
step:640/2330 train_time:38140ms step_avg:59.59ms
step:641/2330 train_time:38196ms step_avg:59.59ms
step:642/2330 train_time:38259ms step_avg:59.59ms
step:643/2330 train_time:38316ms step_avg:59.59ms
step:644/2330 train_time:38379ms step_avg:59.59ms
step:645/2330 train_time:38435ms step_avg:59.59ms
step:646/2330 train_time:38498ms step_avg:59.59ms
step:647/2330 train_time:38555ms step_avg:59.59ms
step:648/2330 train_time:38617ms step_avg:59.59ms
step:649/2330 train_time:38674ms step_avg:59.59ms
step:650/2330 train_time:38736ms step_avg:59.59ms
step:651/2330 train_time:38793ms step_avg:59.59ms
step:652/2330 train_time:38854ms step_avg:59.59ms
step:653/2330 train_time:38911ms step_avg:59.59ms
step:654/2330 train_time:38974ms step_avg:59.59ms
step:655/2330 train_time:39031ms step_avg:59.59ms
step:656/2330 train_time:39093ms step_avg:59.59ms
step:657/2330 train_time:39150ms step_avg:59.59ms
step:658/2330 train_time:39212ms step_avg:59.59ms
step:659/2330 train_time:39269ms step_avg:59.59ms
step:660/2330 train_time:39330ms step_avg:59.59ms
step:661/2330 train_time:39387ms step_avg:59.59ms
step:662/2330 train_time:39450ms step_avg:59.59ms
step:663/2330 train_time:39507ms step_avg:59.59ms
step:664/2330 train_time:39569ms step_avg:59.59ms
step:665/2330 train_time:39626ms step_avg:59.59ms
step:666/2330 train_time:39688ms step_avg:59.59ms
step:667/2330 train_time:39746ms step_avg:59.59ms
step:668/2330 train_time:39808ms step_avg:59.59ms
step:669/2330 train_time:39865ms step_avg:59.59ms
step:670/2330 train_time:39927ms step_avg:59.59ms
step:671/2330 train_time:39984ms step_avg:59.59ms
step:672/2330 train_time:40046ms step_avg:59.59ms
step:673/2330 train_time:40104ms step_avg:59.59ms
step:674/2330 train_time:40166ms step_avg:59.59ms
step:675/2330 train_time:40224ms step_avg:59.59ms
step:676/2330 train_time:40285ms step_avg:59.59ms
step:677/2330 train_time:40342ms step_avg:59.59ms
step:678/2330 train_time:40405ms step_avg:59.59ms
step:679/2330 train_time:40462ms step_avg:59.59ms
step:680/2330 train_time:40525ms step_avg:59.60ms
step:681/2330 train_time:40582ms step_avg:59.59ms
step:682/2330 train_time:40644ms step_avg:59.60ms
step:683/2330 train_time:40702ms step_avg:59.59ms
step:684/2330 train_time:40763ms step_avg:59.60ms
step:685/2330 train_time:40821ms step_avg:59.59ms
step:686/2330 train_time:40882ms step_avg:59.60ms
step:687/2330 train_time:40940ms step_avg:59.59ms
step:688/2330 train_time:41002ms step_avg:59.60ms
step:689/2330 train_time:41059ms step_avg:59.59ms
step:690/2330 train_time:41121ms step_avg:59.60ms
step:691/2330 train_time:41178ms step_avg:59.59ms
step:692/2330 train_time:41240ms step_avg:59.60ms
step:693/2330 train_time:41297ms step_avg:59.59ms
step:694/2330 train_time:41360ms step_avg:59.60ms
step:695/2330 train_time:41417ms step_avg:59.59ms
step:696/2330 train_time:41480ms step_avg:59.60ms
step:697/2330 train_time:41536ms step_avg:59.59ms
step:698/2330 train_time:41600ms step_avg:59.60ms
step:699/2330 train_time:41657ms step_avg:59.60ms
step:700/2330 train_time:41719ms step_avg:59.60ms
step:701/2330 train_time:41776ms step_avg:59.59ms
step:702/2330 train_time:41839ms step_avg:59.60ms
step:703/2330 train_time:41896ms step_avg:59.60ms
step:704/2330 train_time:41958ms step_avg:59.60ms
step:705/2330 train_time:42014ms step_avg:59.59ms
step:706/2330 train_time:42078ms step_avg:59.60ms
step:707/2330 train_time:42134ms step_avg:59.60ms
step:708/2330 train_time:42197ms step_avg:59.60ms
step:709/2330 train_time:42254ms step_avg:59.60ms
step:710/2330 train_time:42316ms step_avg:59.60ms
step:711/2330 train_time:42373ms step_avg:59.60ms
step:712/2330 train_time:42435ms step_avg:59.60ms
step:713/2330 train_time:42492ms step_avg:59.60ms
step:714/2330 train_time:42553ms step_avg:59.60ms
step:715/2330 train_time:42610ms step_avg:59.59ms
step:716/2330 train_time:42671ms step_avg:59.60ms
step:717/2330 train_time:42728ms step_avg:59.59ms
step:718/2330 train_time:42790ms step_avg:59.60ms
step:719/2330 train_time:42847ms step_avg:59.59ms
step:720/2330 train_time:42910ms step_avg:59.60ms
step:721/2330 train_time:42967ms step_avg:59.59ms
step:722/2330 train_time:43030ms step_avg:59.60ms
step:723/2330 train_time:43087ms step_avg:59.59ms
step:724/2330 train_time:43149ms step_avg:59.60ms
step:725/2330 train_time:43206ms step_avg:59.59ms
step:726/2330 train_time:43268ms step_avg:59.60ms
step:727/2330 train_time:43325ms step_avg:59.59ms
step:728/2330 train_time:43387ms step_avg:59.60ms
step:729/2330 train_time:43444ms step_avg:59.59ms
step:730/2330 train_time:43507ms step_avg:59.60ms
step:731/2330 train_time:43564ms step_avg:59.59ms
step:732/2330 train_time:43625ms step_avg:59.60ms
step:733/2330 train_time:43683ms step_avg:59.60ms
step:734/2330 train_time:43744ms step_avg:59.60ms
step:735/2330 train_time:43802ms step_avg:59.59ms
step:736/2330 train_time:43864ms step_avg:59.60ms
step:737/2330 train_time:43921ms step_avg:59.59ms
step:738/2330 train_time:43983ms step_avg:59.60ms
step:739/2330 train_time:44041ms step_avg:59.59ms
step:740/2330 train_time:44103ms step_avg:59.60ms
step:741/2330 train_time:44162ms step_avg:59.60ms
step:742/2330 train_time:44223ms step_avg:59.60ms
step:743/2330 train_time:44281ms step_avg:59.60ms
step:744/2330 train_time:44342ms step_avg:59.60ms
step:745/2330 train_time:44399ms step_avg:59.60ms
step:746/2330 train_time:44461ms step_avg:59.60ms
step:747/2330 train_time:44518ms step_avg:59.60ms
step:748/2330 train_time:44580ms step_avg:59.60ms
step:749/2330 train_time:44636ms step_avg:59.59ms
step:750/2330 train_time:44699ms step_avg:59.60ms
step:750/2330 val_loss:4.3875 train_time:44779ms step_avg:59.70ms
step:751/2330 train_time:44798ms step_avg:59.65ms
step:752/2330 train_time:44821ms step_avg:59.60ms
step:753/2330 train_time:44879ms step_avg:59.60ms
step:754/2330 train_time:44947ms step_avg:59.61ms
step:755/2330 train_time:45004ms step_avg:59.61ms
step:756/2330 train_time:45067ms step_avg:59.61ms
step:757/2330 train_time:45124ms step_avg:59.61ms
step:758/2330 train_time:45185ms step_avg:59.61ms
step:759/2330 train_time:45241ms step_avg:59.61ms
step:760/2330 train_time:45303ms step_avg:59.61ms
step:761/2330 train_time:45360ms step_avg:59.61ms
step:762/2330 train_time:45420ms step_avg:59.61ms
step:763/2330 train_time:45476ms step_avg:59.60ms
step:764/2330 train_time:45537ms step_avg:59.60ms
step:765/2330 train_time:45594ms step_avg:59.60ms
step:766/2330 train_time:45655ms step_avg:59.60ms
step:767/2330 train_time:45712ms step_avg:59.60ms
step:768/2330 train_time:45775ms step_avg:59.60ms
step:769/2330 train_time:45834ms step_avg:59.60ms
step:770/2330 train_time:45897ms step_avg:59.61ms
step:771/2330 train_time:45956ms step_avg:59.61ms
step:772/2330 train_time:46020ms step_avg:59.61ms
step:773/2330 train_time:46078ms step_avg:59.61ms
step:774/2330 train_time:46140ms step_avg:59.61ms
step:775/2330 train_time:46198ms step_avg:59.61ms
step:776/2330 train_time:46260ms step_avg:59.61ms
step:777/2330 train_time:46317ms step_avg:59.61ms
step:778/2330 train_time:46380ms step_avg:59.61ms
step:779/2330 train_time:46437ms step_avg:59.61ms
step:780/2330 train_time:46499ms step_avg:59.61ms
step:781/2330 train_time:46556ms step_avg:59.61ms
step:782/2330 train_time:46617ms step_avg:59.61ms
step:783/2330 train_time:46674ms step_avg:59.61ms
step:784/2330 train_time:46736ms step_avg:59.61ms
step:785/2330 train_time:46794ms step_avg:59.61ms
step:786/2330 train_time:46857ms step_avg:59.61ms
step:787/2330 train_time:46915ms step_avg:59.61ms
step:788/2330 train_time:46978ms step_avg:59.62ms
step:789/2330 train_time:47036ms step_avg:59.62ms
step:790/2330 train_time:47099ms step_avg:59.62ms
step:791/2330 train_time:47158ms step_avg:59.62ms
step:792/2330 train_time:47219ms step_avg:59.62ms
step:793/2330 train_time:47276ms step_avg:59.62ms
step:794/2330 train_time:47338ms step_avg:59.62ms
step:795/2330 train_time:47396ms step_avg:59.62ms
step:796/2330 train_time:47458ms step_avg:59.62ms
step:797/2330 train_time:47515ms step_avg:59.62ms
step:798/2330 train_time:47577ms step_avg:59.62ms
step:799/2330 train_time:47634ms step_avg:59.62ms
step:800/2330 train_time:47696ms step_avg:59.62ms
step:801/2330 train_time:47754ms step_avg:59.62ms
step:802/2330 train_time:47816ms step_avg:59.62ms
step:803/2330 train_time:47874ms step_avg:59.62ms
step:804/2330 train_time:47936ms step_avg:59.62ms
step:805/2330 train_time:47994ms step_avg:59.62ms
step:806/2330 train_time:48058ms step_avg:59.63ms
step:807/2330 train_time:48116ms step_avg:59.62ms
step:808/2330 train_time:48178ms step_avg:59.63ms
step:809/2330 train_time:48236ms step_avg:59.62ms
step:810/2330 train_time:48299ms step_avg:59.63ms
step:811/2330 train_time:48357ms step_avg:59.63ms
step:812/2330 train_time:48419ms step_avg:59.63ms
step:813/2330 train_time:48476ms step_avg:59.63ms
step:814/2330 train_time:48538ms step_avg:59.63ms
step:815/2330 train_time:48595ms step_avg:59.63ms
step:816/2330 train_time:48658ms step_avg:59.63ms
step:817/2330 train_time:48715ms step_avg:59.63ms
step:818/2330 train_time:48776ms step_avg:59.63ms
step:819/2330 train_time:48834ms step_avg:59.63ms
step:820/2330 train_time:48897ms step_avg:59.63ms
step:821/2330 train_time:48956ms step_avg:59.63ms
step:822/2330 train_time:49018ms step_avg:59.63ms
step:823/2330 train_time:49075ms step_avg:59.63ms
step:824/2330 train_time:49138ms step_avg:59.63ms
step:825/2330 train_time:49196ms step_avg:59.63ms
step:826/2330 train_time:49258ms step_avg:59.63ms
step:827/2330 train_time:49316ms step_avg:59.63ms
step:828/2330 train_time:49378ms step_avg:59.64ms
step:829/2330 train_time:49436ms step_avg:59.63ms
step:830/2330 train_time:49498ms step_avg:59.64ms
step:831/2330 train_time:49556ms step_avg:59.63ms
step:832/2330 train_time:49617ms step_avg:59.64ms
step:833/2330 train_time:49674ms step_avg:59.63ms
step:834/2330 train_time:49736ms step_avg:59.64ms
step:835/2330 train_time:49794ms step_avg:59.63ms
step:836/2330 train_time:49856ms step_avg:59.64ms
step:837/2330 train_time:49914ms step_avg:59.63ms
step:838/2330 train_time:49977ms step_avg:59.64ms
step:839/2330 train_time:50035ms step_avg:59.64ms
step:840/2330 train_time:50097ms step_avg:59.64ms
step:841/2330 train_time:50154ms step_avg:59.64ms
step:842/2330 train_time:50217ms step_avg:59.64ms
step:843/2330 train_time:50275ms step_avg:59.64ms
step:844/2330 train_time:50338ms step_avg:59.64ms
step:845/2330 train_time:50396ms step_avg:59.64ms
step:846/2330 train_time:50459ms step_avg:59.64ms
step:847/2330 train_time:50516ms step_avg:59.64ms
step:848/2330 train_time:50578ms step_avg:59.64ms
step:849/2330 train_time:50635ms step_avg:59.64ms
step:850/2330 train_time:50698ms step_avg:59.64ms
step:851/2330 train_time:50755ms step_avg:59.64ms
step:852/2330 train_time:50817ms step_avg:59.64ms
step:853/2330 train_time:50874ms step_avg:59.64ms
step:854/2330 train_time:50937ms step_avg:59.65ms
step:855/2330 train_time:50994ms step_avg:59.64ms
step:856/2330 train_time:51057ms step_avg:59.65ms
step:857/2330 train_time:51116ms step_avg:59.64ms
step:858/2330 train_time:51177ms step_avg:59.65ms
step:859/2330 train_time:51235ms step_avg:59.64ms
step:860/2330 train_time:51297ms step_avg:59.65ms
step:861/2330 train_time:51356ms step_avg:59.65ms
step:862/2330 train_time:51417ms step_avg:59.65ms
step:863/2330 train_time:51474ms step_avg:59.65ms
step:864/2330 train_time:51537ms step_avg:59.65ms
step:865/2330 train_time:51594ms step_avg:59.65ms
step:866/2330 train_time:51657ms step_avg:59.65ms
step:867/2330 train_time:51715ms step_avg:59.65ms
step:868/2330 train_time:51776ms step_avg:59.65ms
step:869/2330 train_time:51834ms step_avg:59.65ms
step:870/2330 train_time:51897ms step_avg:59.65ms
step:871/2330 train_time:51955ms step_avg:59.65ms
step:872/2330 train_time:52017ms step_avg:59.65ms
step:873/2330 train_time:52074ms step_avg:59.65ms
step:874/2330 train_time:52137ms step_avg:59.65ms
step:875/2330 train_time:52195ms step_avg:59.65ms
step:876/2330 train_time:52257ms step_avg:59.65ms
step:877/2330 train_time:52314ms step_avg:59.65ms
step:878/2330 train_time:52378ms step_avg:59.66ms
step:879/2330 train_time:52435ms step_avg:59.65ms
step:880/2330 train_time:52498ms step_avg:59.66ms
step:881/2330 train_time:52556ms step_avg:59.65ms
step:882/2330 train_time:52618ms step_avg:59.66ms
step:883/2330 train_time:52675ms step_avg:59.65ms
step:884/2330 train_time:52736ms step_avg:59.66ms
step:885/2330 train_time:52794ms step_avg:59.65ms
step:886/2330 train_time:52857ms step_avg:59.66ms
step:887/2330 train_time:52915ms step_avg:59.66ms
step:888/2330 train_time:52977ms step_avg:59.66ms
step:889/2330 train_time:53035ms step_avg:59.66ms
step:890/2330 train_time:53097ms step_avg:59.66ms
step:891/2330 train_time:53155ms step_avg:59.66ms
step:892/2330 train_time:53217ms step_avg:59.66ms
step:893/2330 train_time:53275ms step_avg:59.66ms
step:894/2330 train_time:53338ms step_avg:59.66ms
step:895/2330 train_time:53396ms step_avg:59.66ms
step:896/2330 train_time:53458ms step_avg:59.66ms
step:897/2330 train_time:53516ms step_avg:59.66ms
step:898/2330 train_time:53577ms step_avg:59.66ms
step:899/2330 train_time:53634ms step_avg:59.66ms
step:900/2330 train_time:53696ms step_avg:59.66ms
step:901/2330 train_time:53754ms step_avg:59.66ms
step:902/2330 train_time:53816ms step_avg:59.66ms
step:903/2330 train_time:53874ms step_avg:59.66ms
step:904/2330 train_time:53937ms step_avg:59.66ms
step:905/2330 train_time:53994ms step_avg:59.66ms
step:906/2330 train_time:54057ms step_avg:59.67ms
step:907/2330 train_time:54116ms step_avg:59.66ms
step:908/2330 train_time:54177ms step_avg:59.67ms
step:909/2330 train_time:54235ms step_avg:59.66ms
step:910/2330 train_time:54297ms step_avg:59.67ms
step:911/2330 train_time:54355ms step_avg:59.67ms
step:912/2330 train_time:54417ms step_avg:59.67ms
step:913/2330 train_time:54475ms step_avg:59.67ms
step:914/2330 train_time:54538ms step_avg:59.67ms
step:915/2330 train_time:54596ms step_avg:59.67ms
step:916/2330 train_time:54658ms step_avg:59.67ms
step:917/2330 train_time:54715ms step_avg:59.67ms
step:918/2330 train_time:54777ms step_avg:59.67ms
step:919/2330 train_time:54835ms step_avg:59.67ms
step:920/2330 train_time:54897ms step_avg:59.67ms
step:921/2330 train_time:54954ms step_avg:59.67ms
step:922/2330 train_time:55016ms step_avg:59.67ms
step:923/2330 train_time:55073ms step_avg:59.67ms
step:924/2330 train_time:55137ms step_avg:59.67ms
step:925/2330 train_time:55195ms step_avg:59.67ms
step:926/2330 train_time:55257ms step_avg:59.67ms
step:927/2330 train_time:55315ms step_avg:59.67ms
step:928/2330 train_time:55377ms step_avg:59.67ms
step:929/2330 train_time:55435ms step_avg:59.67ms
step:930/2330 train_time:55497ms step_avg:59.67ms
step:931/2330 train_time:55556ms step_avg:59.67ms
step:932/2330 train_time:55617ms step_avg:59.68ms
step:933/2330 train_time:55674ms step_avg:59.67ms
step:934/2330 train_time:55738ms step_avg:59.68ms
step:935/2330 train_time:55796ms step_avg:59.67ms
step:936/2330 train_time:55857ms step_avg:59.68ms
step:937/2330 train_time:55916ms step_avg:59.68ms
step:938/2330 train_time:55976ms step_avg:59.68ms
step:939/2330 train_time:56034ms step_avg:59.67ms
step:940/2330 train_time:56097ms step_avg:59.68ms
step:941/2330 train_time:56155ms step_avg:59.68ms
step:942/2330 train_time:56216ms step_avg:59.68ms
step:943/2330 train_time:56274ms step_avg:59.68ms
step:944/2330 train_time:56337ms step_avg:59.68ms
step:945/2330 train_time:56395ms step_avg:59.68ms
step:946/2330 train_time:56457ms step_avg:59.68ms
step:947/2330 train_time:56514ms step_avg:59.68ms
step:948/2330 train_time:56577ms step_avg:59.68ms
step:949/2330 train_time:56634ms step_avg:59.68ms
step:950/2330 train_time:56697ms step_avg:59.68ms
step:951/2330 train_time:56755ms step_avg:59.68ms
step:952/2330 train_time:56817ms step_avg:59.68ms
step:953/2330 train_time:56875ms step_avg:59.68ms
step:954/2330 train_time:56936ms step_avg:59.68ms
step:955/2330 train_time:56994ms step_avg:59.68ms
step:956/2330 train_time:57057ms step_avg:59.68ms
step:957/2330 train_time:57115ms step_avg:59.68ms
step:958/2330 train_time:57177ms step_avg:59.68ms
step:959/2330 train_time:57234ms step_avg:59.68ms
step:960/2330 train_time:57297ms step_avg:59.68ms
step:961/2330 train_time:57354ms step_avg:59.68ms
step:962/2330 train_time:57417ms step_avg:59.68ms
step:963/2330 train_time:57475ms step_avg:59.68ms
step:964/2330 train_time:57538ms step_avg:59.69ms
step:965/2330 train_time:57595ms step_avg:59.68ms
step:966/2330 train_time:57658ms step_avg:59.69ms
step:967/2330 train_time:57715ms step_avg:59.68ms
step:968/2330 train_time:57777ms step_avg:59.69ms
step:969/2330 train_time:57835ms step_avg:59.69ms
step:970/2330 train_time:57896ms step_avg:59.69ms
step:971/2330 train_time:57954ms step_avg:59.68ms
step:972/2330 train_time:58016ms step_avg:59.69ms
step:973/2330 train_time:58074ms step_avg:59.69ms
step:974/2330 train_time:58136ms step_avg:59.69ms
step:975/2330 train_time:58194ms step_avg:59.69ms
step:976/2330 train_time:58256ms step_avg:59.69ms
step:977/2330 train_time:58314ms step_avg:59.69ms
step:978/2330 train_time:58376ms step_avg:59.69ms
step:979/2330 train_time:58434ms step_avg:59.69ms
step:980/2330 train_time:58498ms step_avg:59.69ms
step:981/2330 train_time:58556ms step_avg:59.69ms
step:982/2330 train_time:58617ms step_avg:59.69ms
step:983/2330 train_time:58674ms step_avg:59.69ms
step:984/2330 train_time:58736ms step_avg:59.69ms
step:985/2330 train_time:58794ms step_avg:59.69ms
step:986/2330 train_time:58856ms step_avg:59.69ms
step:987/2330 train_time:58914ms step_avg:59.69ms
step:988/2330 train_time:58977ms step_avg:59.69ms
step:989/2330 train_time:59034ms step_avg:59.69ms
step:990/2330 train_time:59097ms step_avg:59.69ms
step:991/2330 train_time:59155ms step_avg:59.69ms
step:992/2330 train_time:59217ms step_avg:59.69ms
step:993/2330 train_time:59275ms step_avg:59.69ms
step:994/2330 train_time:59338ms step_avg:59.70ms
step:995/2330 train_time:59396ms step_avg:59.69ms
step:996/2330 train_time:59458ms step_avg:59.70ms
step:997/2330 train_time:59516ms step_avg:59.69ms
step:998/2330 train_time:59578ms step_avg:59.70ms
step:999/2330 train_time:59636ms step_avg:59.70ms
step:1000/2330 train_time:59697ms step_avg:59.70ms
step:1000/2330 val_loss:4.2105 train_time:59776ms step_avg:59.78ms
step:1001/2330 train_time:59797ms step_avg:59.74ms
step:1002/2330 train_time:59819ms step_avg:59.70ms
step:1003/2330 train_time:59875ms step_avg:59.70ms
step:1004/2330 train_time:59945ms step_avg:59.71ms
step:1005/2330 train_time:60002ms step_avg:59.70ms
step:1006/2330 train_time:60069ms step_avg:59.71ms
step:1007/2330 train_time:60126ms step_avg:59.71ms
step:1008/2330 train_time:60188ms step_avg:59.71ms
step:1009/2330 train_time:60246ms step_avg:59.71ms
step:1010/2330 train_time:60307ms step_avg:59.71ms
step:1011/2330 train_time:60364ms step_avg:59.71ms
step:1012/2330 train_time:60425ms step_avg:59.71ms
step:1013/2330 train_time:60482ms step_avg:59.71ms
step:1014/2330 train_time:60543ms step_avg:59.71ms
step:1015/2330 train_time:60601ms step_avg:59.71ms
step:1016/2330 train_time:60662ms step_avg:59.71ms
step:1017/2330 train_time:60721ms step_avg:59.71ms
step:1018/2330 train_time:60785ms step_avg:59.71ms
step:1019/2330 train_time:60843ms step_avg:59.71ms
step:1020/2330 train_time:60906ms step_avg:59.71ms
step:1021/2330 train_time:60964ms step_avg:59.71ms
step:1022/2330 train_time:61028ms step_avg:59.71ms
step:1023/2330 train_time:61085ms step_avg:59.71ms
step:1024/2330 train_time:61148ms step_avg:59.71ms
step:1025/2330 train_time:61205ms step_avg:59.71ms
step:1026/2330 train_time:61267ms step_avg:59.71ms
step:1027/2330 train_time:61324ms step_avg:59.71ms
step:1028/2330 train_time:61385ms step_avg:59.71ms
step:1029/2330 train_time:61442ms step_avg:59.71ms
step:1030/2330 train_time:61503ms step_avg:59.71ms
step:1031/2330 train_time:61561ms step_avg:59.71ms
step:1032/2330 train_time:61622ms step_avg:59.71ms
step:1033/2330 train_time:61680ms step_avg:59.71ms
step:1034/2330 train_time:61742ms step_avg:59.71ms
step:1035/2330 train_time:61800ms step_avg:59.71ms
step:1036/2330 train_time:61864ms step_avg:59.71ms
step:1037/2330 train_time:61922ms step_avg:59.71ms
step:1038/2330 train_time:61985ms step_avg:59.72ms
step:1039/2330 train_time:62043ms step_avg:59.71ms
step:1040/2330 train_time:62106ms step_avg:59.72ms
step:1041/2330 train_time:62163ms step_avg:59.71ms
step:1042/2330 train_time:62225ms step_avg:59.72ms
step:1043/2330 train_time:62281ms step_avg:59.71ms
step:1044/2330 train_time:62343ms step_avg:59.72ms
step:1045/2330 train_time:62400ms step_avg:59.71ms
step:1046/2330 train_time:62463ms step_avg:59.72ms
step:1047/2330 train_time:62520ms step_avg:59.71ms
step:1048/2330 train_time:62581ms step_avg:59.72ms
step:1049/2330 train_time:62639ms step_avg:59.71ms
step:1050/2330 train_time:62701ms step_avg:59.71ms
step:1051/2330 train_time:62759ms step_avg:59.71ms
step:1052/2330 train_time:62822ms step_avg:59.72ms
step:1053/2330 train_time:62879ms step_avg:59.71ms
step:1054/2330 train_time:62942ms step_avg:59.72ms
step:1055/2330 train_time:63001ms step_avg:59.72ms
step:1056/2330 train_time:63064ms step_avg:59.72ms
step:1057/2330 train_time:63122ms step_avg:59.72ms
step:1058/2330 train_time:63184ms step_avg:59.72ms
step:1059/2330 train_time:63241ms step_avg:59.72ms
step:1060/2330 train_time:63304ms step_avg:59.72ms
step:1061/2330 train_time:63361ms step_avg:59.72ms
step:1062/2330 train_time:63423ms step_avg:59.72ms
step:1063/2330 train_time:63481ms step_avg:59.72ms
step:1064/2330 train_time:63542ms step_avg:59.72ms
step:1065/2330 train_time:63599ms step_avg:59.72ms
step:1066/2330 train_time:63662ms step_avg:59.72ms
step:1067/2330 train_time:63719ms step_avg:59.72ms
step:1068/2330 train_time:63782ms step_avg:59.72ms
step:1069/2330 train_time:63840ms step_avg:59.72ms
step:1070/2330 train_time:63903ms step_avg:59.72ms
step:1071/2330 train_time:63961ms step_avg:59.72ms
step:1072/2330 train_time:64023ms step_avg:59.72ms
step:1073/2330 train_time:64081ms step_avg:59.72ms
step:1074/2330 train_time:64144ms step_avg:59.72ms
step:1075/2330 train_time:64202ms step_avg:59.72ms
step:1076/2330 train_time:64265ms step_avg:59.73ms
step:1077/2330 train_time:64322ms step_avg:59.72ms
step:1078/2330 train_time:64383ms step_avg:59.72ms
step:1079/2330 train_time:64440ms step_avg:59.72ms
step:1080/2330 train_time:64503ms step_avg:59.72ms
step:1081/2330 train_time:64560ms step_avg:59.72ms
step:1082/2330 train_time:64622ms step_avg:59.72ms
step:1083/2330 train_time:64680ms step_avg:59.72ms
step:1084/2330 train_time:64742ms step_avg:59.73ms
step:1085/2330 train_time:64800ms step_avg:59.72ms
step:1086/2330 train_time:64862ms step_avg:59.73ms
step:1087/2330 train_time:64920ms step_avg:59.72ms
step:1088/2330 train_time:64983ms step_avg:59.73ms
step:1089/2330 train_time:65040ms step_avg:59.72ms
step:1090/2330 train_time:65104ms step_avg:59.73ms
step:1091/2330 train_time:65162ms step_avg:59.73ms
step:1092/2330 train_time:65225ms step_avg:59.73ms
step:1093/2330 train_time:65283ms step_avg:59.73ms
step:1094/2330 train_time:65344ms step_avg:59.73ms
step:1095/2330 train_time:65402ms step_avg:59.73ms
step:1096/2330 train_time:65463ms step_avg:59.73ms
step:1097/2330 train_time:65521ms step_avg:59.73ms
step:1098/2330 train_time:65583ms step_avg:59.73ms
step:1099/2330 train_time:65640ms step_avg:59.73ms
step:1100/2330 train_time:65703ms step_avg:59.73ms
step:1101/2330 train_time:65760ms step_avg:59.73ms
step:1102/2330 train_time:65823ms step_avg:59.73ms
step:1103/2330 train_time:65881ms step_avg:59.73ms
step:1104/2330 train_time:65943ms step_avg:59.73ms
step:1105/2330 train_time:66001ms step_avg:59.73ms
step:1106/2330 train_time:66064ms step_avg:59.73ms
step:1107/2330 train_time:66122ms step_avg:59.73ms
step:1108/2330 train_time:66183ms step_avg:59.73ms
step:1109/2330 train_time:66241ms step_avg:59.73ms
step:1110/2330 train_time:66305ms step_avg:59.73ms
step:1111/2330 train_time:66363ms step_avg:59.73ms
step:1112/2330 train_time:66425ms step_avg:59.73ms
step:1113/2330 train_time:66482ms step_avg:59.73ms
step:1114/2330 train_time:66544ms step_avg:59.73ms
step:1115/2330 train_time:66601ms step_avg:59.73ms
step:1116/2330 train_time:66663ms step_avg:59.73ms
step:1117/2330 train_time:66721ms step_avg:59.73ms
step:1118/2330 train_time:66782ms step_avg:59.73ms
step:1119/2330 train_time:66840ms step_avg:59.73ms
step:1120/2330 train_time:66903ms step_avg:59.73ms
step:1121/2330 train_time:66961ms step_avg:59.73ms
step:1122/2330 train_time:67023ms step_avg:59.74ms
step:1123/2330 train_time:67080ms step_avg:59.73ms
step:1124/2330 train_time:67144ms step_avg:59.74ms
step:1125/2330 train_time:67202ms step_avg:59.74ms
step:1126/2330 train_time:67265ms step_avg:59.74ms
step:1127/2330 train_time:67323ms step_avg:59.74ms
step:1128/2330 train_time:67385ms step_avg:59.74ms
step:1129/2330 train_time:67443ms step_avg:59.74ms
step:1130/2330 train_time:67504ms step_avg:59.74ms
step:1131/2330 train_time:67562ms step_avg:59.74ms
step:1132/2330 train_time:67624ms step_avg:59.74ms
step:1133/2330 train_time:67681ms step_avg:59.74ms
step:1134/2330 train_time:67743ms step_avg:59.74ms
step:1135/2330 train_time:67800ms step_avg:59.74ms
step:1136/2330 train_time:67863ms step_avg:59.74ms
step:1137/2330 train_time:67921ms step_avg:59.74ms
step:1138/2330 train_time:67983ms step_avg:59.74ms
step:1139/2330 train_time:68040ms step_avg:59.74ms
step:1140/2330 train_time:68104ms step_avg:59.74ms
step:1141/2330 train_time:68162ms step_avg:59.74ms
step:1142/2330 train_time:68224ms step_avg:59.74ms
step:1143/2330 train_time:68281ms step_avg:59.74ms
step:1144/2330 train_time:68344ms step_avg:59.74ms
step:1145/2330 train_time:68401ms step_avg:59.74ms
step:1146/2330 train_time:68465ms step_avg:59.74ms
step:1147/2330 train_time:68522ms step_avg:59.74ms
step:1148/2330 train_time:68584ms step_avg:59.74ms
step:1149/2330 train_time:68641ms step_avg:59.74ms
step:1150/2330 train_time:68704ms step_avg:59.74ms
step:1151/2330 train_time:68762ms step_avg:59.74ms
step:1152/2330 train_time:68823ms step_avg:59.74ms
step:1153/2330 train_time:68881ms step_avg:59.74ms
step:1154/2330 train_time:68944ms step_avg:59.74ms
step:1155/2330 train_time:69002ms step_avg:59.74ms
step:1156/2330 train_time:69064ms step_avg:59.74ms
step:1157/2330 train_time:69122ms step_avg:59.74ms
step:1158/2330 train_time:69185ms step_avg:59.74ms
step:1159/2330 train_time:69242ms step_avg:59.74ms
step:1160/2330 train_time:69304ms step_avg:59.75ms
step:1161/2330 train_time:69362ms step_avg:59.74ms
step:1162/2330 train_time:69424ms step_avg:59.75ms
step:1163/2330 train_time:69482ms step_avg:59.74ms
step:1164/2330 train_time:69544ms step_avg:59.75ms
step:1165/2330 train_time:69602ms step_avg:59.74ms
step:1166/2330 train_time:69664ms step_avg:59.75ms
step:1167/2330 train_time:69722ms step_avg:59.74ms
step:1168/2330 train_time:69784ms step_avg:59.75ms
step:1169/2330 train_time:69841ms step_avg:59.74ms
step:1170/2330 train_time:69904ms step_avg:59.75ms
step:1171/2330 train_time:69962ms step_avg:59.75ms
step:1172/2330 train_time:70025ms step_avg:59.75ms
step:1173/2330 train_time:70082ms step_avg:59.75ms
step:1174/2330 train_time:70145ms step_avg:59.75ms
step:1175/2330 train_time:70203ms step_avg:59.75ms
step:1176/2330 train_time:70265ms step_avg:59.75ms
step:1177/2330 train_time:70322ms step_avg:59.75ms
step:1178/2330 train_time:70384ms step_avg:59.75ms
step:1179/2330 train_time:70441ms step_avg:59.75ms
step:1180/2330 train_time:70504ms step_avg:59.75ms
step:1181/2330 train_time:70562ms step_avg:59.75ms
step:1182/2330 train_time:70624ms step_avg:59.75ms
step:1183/2330 train_time:70680ms step_avg:59.75ms
step:1184/2330 train_time:70743ms step_avg:59.75ms
step:1185/2330 train_time:70801ms step_avg:59.75ms
step:1186/2330 train_time:70865ms step_avg:59.75ms
step:1187/2330 train_time:70922ms step_avg:59.75ms
step:1188/2330 train_time:70985ms step_avg:59.75ms
step:1189/2330 train_time:71042ms step_avg:59.75ms
step:1190/2330 train_time:71105ms step_avg:59.75ms
step:1191/2330 train_time:71162ms step_avg:59.75ms
step:1192/2330 train_time:71225ms step_avg:59.75ms
step:1193/2330 train_time:71283ms step_avg:59.75ms
step:1194/2330 train_time:71345ms step_avg:59.75ms
step:1195/2330 train_time:71403ms step_avg:59.75ms
step:1196/2330 train_time:71464ms step_avg:59.75ms
step:1197/2330 train_time:71522ms step_avg:59.75ms
step:1198/2330 train_time:71583ms step_avg:59.75ms
step:1199/2330 train_time:71641ms step_avg:59.75ms
step:1200/2330 train_time:71703ms step_avg:59.75ms
step:1201/2330 train_time:71760ms step_avg:59.75ms
step:1202/2330 train_time:71823ms step_avg:59.75ms
step:1203/2330 train_time:71881ms step_avg:59.75ms
step:1204/2330 train_time:71943ms step_avg:59.75ms
step:1205/2330 train_time:72001ms step_avg:59.75ms
step:1206/2330 train_time:72064ms step_avg:59.75ms
step:1207/2330 train_time:72122ms step_avg:59.75ms
step:1208/2330 train_time:72183ms step_avg:59.75ms
step:1209/2330 train_time:72241ms step_avg:59.75ms
step:1210/2330 train_time:72304ms step_avg:59.76ms
step:1211/2330 train_time:72363ms step_avg:59.75ms
step:1212/2330 train_time:72424ms step_avg:59.76ms
step:1213/2330 train_time:72481ms step_avg:59.75ms
step:1214/2330 train_time:72543ms step_avg:59.76ms
step:1215/2330 train_time:72601ms step_avg:59.75ms
step:1216/2330 train_time:72663ms step_avg:59.76ms
step:1217/2330 train_time:72721ms step_avg:59.75ms
step:1218/2330 train_time:72783ms step_avg:59.76ms
step:1219/2330 train_time:72841ms step_avg:59.75ms
step:1220/2330 train_time:72904ms step_avg:59.76ms
step:1221/2330 train_time:72961ms step_avg:59.76ms
step:1222/2330 train_time:73024ms step_avg:59.76ms
step:1223/2330 train_time:73081ms step_avg:59.76ms
step:1224/2330 train_time:73145ms step_avg:59.76ms
step:1225/2330 train_time:73202ms step_avg:59.76ms
step:1226/2330 train_time:73265ms step_avg:59.76ms
step:1227/2330 train_time:73323ms step_avg:59.76ms
step:1228/2330 train_time:73385ms step_avg:59.76ms
step:1229/2330 train_time:73442ms step_avg:59.76ms
step:1230/2330 train_time:73504ms step_avg:59.76ms
step:1231/2330 train_time:73562ms step_avg:59.76ms
step:1232/2330 train_time:73623ms step_avg:59.76ms
step:1233/2330 train_time:73681ms step_avg:59.76ms
step:1234/2330 train_time:73744ms step_avg:59.76ms
step:1235/2330 train_time:73802ms step_avg:59.76ms
step:1236/2330 train_time:73864ms step_avg:59.76ms
step:1237/2330 train_time:73921ms step_avg:59.76ms
step:1238/2330 train_time:73984ms step_avg:59.76ms
step:1239/2330 train_time:74041ms step_avg:59.76ms
step:1240/2330 train_time:74104ms step_avg:59.76ms
step:1241/2330 train_time:74162ms step_avg:59.76ms
step:1242/2330 train_time:74224ms step_avg:59.76ms
step:1243/2330 train_time:74281ms step_avg:59.76ms
step:1244/2330 train_time:74344ms step_avg:59.76ms
step:1245/2330 train_time:74402ms step_avg:59.76ms
step:1246/2330 train_time:74465ms step_avg:59.76ms
step:1247/2330 train_time:74522ms step_avg:59.76ms
step:1248/2330 train_time:74583ms step_avg:59.76ms
step:1249/2330 train_time:74641ms step_avg:59.76ms
step:1250/2330 train_time:74704ms step_avg:59.76ms
step:1250/2330 val_loss:4.1028 train_time:74782ms step_avg:59.83ms
step:1251/2330 train_time:74803ms step_avg:59.79ms
step:1252/2330 train_time:74825ms step_avg:59.76ms
step:1253/2330 train_time:74886ms step_avg:59.77ms
step:1254/2330 train_time:74952ms step_avg:59.77ms
step:1255/2330 train_time:75010ms step_avg:59.77ms
step:1256/2330 train_time:75073ms step_avg:59.77ms
step:1257/2330 train_time:75130ms step_avg:59.77ms
step:1258/2330 train_time:75192ms step_avg:59.77ms
step:1259/2330 train_time:75249ms step_avg:59.77ms
step:1260/2330 train_time:75311ms step_avg:59.77ms
step:1261/2330 train_time:75368ms step_avg:59.77ms
step:1262/2330 train_time:75430ms step_avg:59.77ms
step:1263/2330 train_time:75488ms step_avg:59.77ms
step:1264/2330 train_time:75549ms step_avg:59.77ms
step:1265/2330 train_time:75607ms step_avg:59.77ms
step:1266/2330 train_time:75668ms step_avg:59.77ms
step:1267/2330 train_time:75726ms step_avg:59.77ms
step:1268/2330 train_time:75790ms step_avg:59.77ms
step:1269/2330 train_time:75850ms step_avg:59.77ms
step:1270/2330 train_time:75913ms step_avg:59.77ms
step:1271/2330 train_time:75972ms step_avg:59.77ms
step:1272/2330 train_time:76035ms step_avg:59.78ms
step:1273/2330 train_time:76093ms step_avg:59.77ms
step:1274/2330 train_time:76155ms step_avg:59.78ms
step:1275/2330 train_time:76212ms step_avg:59.77ms
step:1276/2330 train_time:76273ms step_avg:59.77ms
step:1277/2330 train_time:76330ms step_avg:59.77ms
step:1278/2330 train_time:76392ms step_avg:59.77ms
step:1279/2330 train_time:76449ms step_avg:59.77ms
step:1280/2330 train_time:76511ms step_avg:59.77ms
step:1281/2330 train_time:76568ms step_avg:59.77ms
step:1282/2330 train_time:76631ms step_avg:59.77ms
step:1283/2330 train_time:76688ms step_avg:59.77ms
step:1284/2330 train_time:76751ms step_avg:59.77ms
step:1285/2330 train_time:76809ms step_avg:59.77ms
step:1286/2330 train_time:76872ms step_avg:59.78ms
step:1287/2330 train_time:76930ms step_avg:59.77ms
step:1288/2330 train_time:76994ms step_avg:59.78ms
step:1289/2330 train_time:77052ms step_avg:59.78ms
step:1290/2330 train_time:77115ms step_avg:59.78ms
step:1291/2330 train_time:77172ms step_avg:59.78ms
step:1292/2330 train_time:77235ms step_avg:59.78ms
step:1293/2330 train_time:77292ms step_avg:59.78ms
step:1294/2330 train_time:77353ms step_avg:59.78ms
step:1295/2330 train_time:77410ms step_avg:59.78ms
step:1296/2330 train_time:77472ms step_avg:59.78ms
step:1297/2330 train_time:77530ms step_avg:59.78ms
step:1298/2330 train_time:77591ms step_avg:59.78ms
step:1299/2330 train_time:77648ms step_avg:59.77ms
step:1300/2330 train_time:77710ms step_avg:59.78ms
step:1301/2330 train_time:77768ms step_avg:59.78ms
step:1302/2330 train_time:77832ms step_avg:59.78ms
step:1303/2330 train_time:77890ms step_avg:59.78ms
step:1304/2330 train_time:77953ms step_avg:59.78ms
step:1305/2330 train_time:78011ms step_avg:59.78ms
step:1306/2330 train_time:78073ms step_avg:59.78ms
step:1307/2330 train_time:78131ms step_avg:59.78ms
step:1308/2330 train_time:78193ms step_avg:59.78ms
step:1309/2330 train_time:78250ms step_avg:59.78ms
step:1310/2330 train_time:78312ms step_avg:59.78ms
step:1311/2330 train_time:78370ms step_avg:59.78ms
step:1312/2330 train_time:78432ms step_avg:59.78ms
step:1313/2330 train_time:78490ms step_avg:59.78ms
step:1314/2330 train_time:78551ms step_avg:59.78ms
step:1315/2330 train_time:78608ms step_avg:59.78ms
step:1316/2330 train_time:78669ms step_avg:59.78ms
step:1317/2330 train_time:78727ms step_avg:59.78ms
step:1318/2330 train_time:78790ms step_avg:59.78ms
step:1319/2330 train_time:78848ms step_avg:59.78ms
step:1320/2330 train_time:78910ms step_avg:59.78ms
step:1321/2330 train_time:78969ms step_avg:59.78ms
step:1322/2330 train_time:79033ms step_avg:59.78ms
step:1323/2330 train_time:79091ms step_avg:59.78ms
step:1324/2330 train_time:79153ms step_avg:59.78ms
step:1325/2330 train_time:79211ms step_avg:59.78ms
step:1326/2330 train_time:79272ms step_avg:59.78ms
step:1327/2330 train_time:79330ms step_avg:59.78ms
step:1328/2330 train_time:79393ms step_avg:59.78ms
step:1329/2330 train_time:79450ms step_avg:59.78ms
step:1330/2330 train_time:79511ms step_avg:59.78ms
step:1331/2330 train_time:79569ms step_avg:59.78ms
step:1332/2330 train_time:79630ms step_avg:59.78ms
step:1333/2330 train_time:79689ms step_avg:59.78ms
step:1334/2330 train_time:79750ms step_avg:59.78ms
step:1335/2330 train_time:79808ms step_avg:59.78ms
step:1336/2330 train_time:79871ms step_avg:59.78ms
step:1337/2330 train_time:79929ms step_avg:59.78ms
step:1338/2330 train_time:79993ms step_avg:59.79ms
step:1339/2330 train_time:80051ms step_avg:59.78ms
step:1340/2330 train_time:80113ms step_avg:59.79ms
step:1341/2330 train_time:80171ms step_avg:59.78ms
step:1342/2330 train_time:80233ms step_avg:59.79ms
step:1343/2330 train_time:80291ms step_avg:59.78ms
step:1344/2330 train_time:80352ms step_avg:59.79ms
step:1345/2330 train_time:80409ms step_avg:59.78ms
step:1346/2330 train_time:80471ms step_avg:59.79ms
step:1347/2330 train_time:80529ms step_avg:59.78ms
step:1348/2330 train_time:80591ms step_avg:59.79ms
step:1349/2330 train_time:80649ms step_avg:59.78ms
step:1350/2330 train_time:80711ms step_avg:59.79ms
step:1351/2330 train_time:80768ms step_avg:59.78ms
step:1352/2330 train_time:80831ms step_avg:59.79ms
step:1353/2330 train_time:80889ms step_avg:59.78ms
step:1354/2330 train_time:80952ms step_avg:59.79ms
step:1355/2330 train_time:81010ms step_avg:59.79ms
step:1356/2330 train_time:81072ms step_avg:59.79ms
step:1357/2330 train_time:81130ms step_avg:59.79ms
step:1358/2330 train_time:81192ms step_avg:59.79ms
step:1359/2330 train_time:81249ms step_avg:59.79ms
step:1360/2330 train_time:81311ms step_avg:59.79ms
step:1361/2330 train_time:81369ms step_avg:59.79ms
step:1362/2330 train_time:81431ms step_avg:59.79ms
step:1363/2330 train_time:81489ms step_avg:59.79ms
step:1364/2330 train_time:81551ms step_avg:59.79ms
step:1365/2330 train_time:81608ms step_avg:59.79ms
step:1366/2330 train_time:81671ms step_avg:59.79ms
step:1367/2330 train_time:81728ms step_avg:59.79ms
step:1368/2330 train_time:81791ms step_avg:59.79ms
step:1369/2330 train_time:81849ms step_avg:59.79ms
step:1370/2330 train_time:81911ms step_avg:59.79ms
step:1371/2330 train_time:81969ms step_avg:59.79ms
step:1372/2330 train_time:82032ms step_avg:59.79ms
step:1373/2330 train_time:82090ms step_avg:59.79ms
step:1374/2330 train_time:82152ms step_avg:59.79ms
step:1375/2330 train_time:82209ms step_avg:59.79ms
step:1376/2330 train_time:82271ms step_avg:59.79ms
step:1377/2330 train_time:82329ms step_avg:59.79ms
step:1378/2330 train_time:82391ms step_avg:59.79ms
step:1379/2330 train_time:82449ms step_avg:59.79ms
step:1380/2330 train_time:82510ms step_avg:59.79ms
step:1381/2330 train_time:82568ms step_avg:59.79ms
step:1382/2330 train_time:82631ms step_avg:59.79ms
step:1383/2330 train_time:82688ms step_avg:59.79ms
step:1384/2330 train_time:82750ms step_avg:59.79ms
step:1385/2330 train_time:82808ms step_avg:59.79ms
step:1386/2330 train_time:82870ms step_avg:59.79ms
step:1387/2330 train_time:82928ms step_avg:59.79ms
step:1388/2330 train_time:82992ms step_avg:59.79ms
step:1389/2330 train_time:83049ms step_avg:59.79ms
step:1390/2330 train_time:83111ms step_avg:59.79ms
step:1391/2330 train_time:83169ms step_avg:59.79ms
step:1392/2330 train_time:83231ms step_avg:59.79ms
step:1393/2330 train_time:83289ms step_avg:59.79ms
step:1394/2330 train_time:83351ms step_avg:59.79ms
step:1395/2330 train_time:83408ms step_avg:59.79ms
step:1396/2330 train_time:83471ms step_avg:59.79ms
step:1397/2330 train_time:83529ms step_avg:59.79ms
step:1398/2330 train_time:83591ms step_avg:59.79ms
step:1399/2330 train_time:83649ms step_avg:59.79ms
step:1400/2330 train_time:83711ms step_avg:59.79ms
step:1401/2330 train_time:83768ms step_avg:59.79ms
step:1402/2330 train_time:83830ms step_avg:59.79ms
step:1403/2330 train_time:83888ms step_avg:59.79ms
step:1404/2330 train_time:83951ms step_avg:59.79ms
step:1405/2330 train_time:84008ms step_avg:59.79ms
step:1406/2330 train_time:84071ms step_avg:59.79ms
step:1407/2330 train_time:84129ms step_avg:59.79ms
step:1408/2330 train_time:84192ms step_avg:59.80ms
step:1409/2330 train_time:84250ms step_avg:59.79ms
step:1410/2330 train_time:84311ms step_avg:59.80ms
step:1411/2330 train_time:84369ms step_avg:59.79ms
step:1412/2330 train_time:84432ms step_avg:59.80ms
step:1413/2330 train_time:84490ms step_avg:59.79ms
step:1414/2330 train_time:84551ms step_avg:59.80ms
step:1415/2330 train_time:84609ms step_avg:59.79ms
step:1416/2330 train_time:84671ms step_avg:59.80ms
step:1417/2330 train_time:84728ms step_avg:59.79ms
step:1418/2330 train_time:84791ms step_avg:59.80ms
step:1419/2330 train_time:84848ms step_avg:59.79ms
step:1420/2330 train_time:84911ms step_avg:59.80ms
step:1421/2330 train_time:84969ms step_avg:59.80ms
step:1422/2330 train_time:85031ms step_avg:59.80ms
step:1423/2330 train_time:85089ms step_avg:59.80ms
step:1424/2330 train_time:85152ms step_avg:59.80ms
step:1425/2330 train_time:85209ms step_avg:59.80ms
step:1426/2330 train_time:85271ms step_avg:59.80ms
step:1427/2330 train_time:85329ms step_avg:59.80ms
step:1428/2330 train_time:85392ms step_avg:59.80ms
step:1429/2330 train_time:85450ms step_avg:59.80ms
step:1430/2330 train_time:85512ms step_avg:59.80ms
step:1431/2330 train_time:85570ms step_avg:59.80ms
step:1432/2330 train_time:85633ms step_avg:59.80ms
step:1433/2330 train_time:85690ms step_avg:59.80ms
step:1434/2330 train_time:85752ms step_avg:59.80ms
step:1435/2330 train_time:85809ms step_avg:59.80ms
step:1436/2330 train_time:85872ms step_avg:59.80ms
step:1437/2330 train_time:85930ms step_avg:59.80ms
step:1438/2330 train_time:85992ms step_avg:59.80ms
step:1439/2330 train_time:86050ms step_avg:59.80ms
step:1440/2330 train_time:86111ms step_avg:59.80ms
step:1441/2330 train_time:86169ms step_avg:59.80ms
step:1442/2330 train_time:86231ms step_avg:59.80ms
step:1443/2330 train_time:86289ms step_avg:59.80ms
step:1444/2330 train_time:86351ms step_avg:59.80ms
step:1445/2330 train_time:86409ms step_avg:59.80ms
step:1446/2330 train_time:86470ms step_avg:59.80ms
step:1447/2330 train_time:86528ms step_avg:59.80ms
step:1448/2330 train_time:86591ms step_avg:59.80ms
step:1449/2330 train_time:86649ms step_avg:59.80ms
step:1450/2330 train_time:86711ms step_avg:59.80ms
step:1451/2330 train_time:86769ms step_avg:59.80ms
step:1452/2330 train_time:86831ms step_avg:59.80ms
step:1453/2330 train_time:86889ms step_avg:59.80ms
step:1454/2330 train_time:86951ms step_avg:59.80ms
step:1455/2330 train_time:87008ms step_avg:59.80ms
step:1456/2330 train_time:87071ms step_avg:59.80ms
step:1457/2330 train_time:87129ms step_avg:59.80ms
step:1458/2330 train_time:87191ms step_avg:59.80ms
step:1459/2330 train_time:87249ms step_avg:59.80ms
step:1460/2330 train_time:87311ms step_avg:59.80ms
step:1461/2330 train_time:87369ms step_avg:59.80ms
step:1462/2330 train_time:87431ms step_avg:59.80ms
step:1463/2330 train_time:87490ms step_avg:59.80ms
step:1464/2330 train_time:87551ms step_avg:59.80ms
step:1465/2330 train_time:87609ms step_avg:59.80ms
step:1466/2330 train_time:87672ms step_avg:59.80ms
step:1467/2330 train_time:87729ms step_avg:59.80ms
step:1468/2330 train_time:87792ms step_avg:59.80ms
step:1469/2330 train_time:87850ms step_avg:59.80ms
step:1470/2330 train_time:87911ms step_avg:59.80ms
step:1471/2330 train_time:87969ms step_avg:59.80ms
step:1472/2330 train_time:88031ms step_avg:59.80ms
step:1473/2330 train_time:88088ms step_avg:59.80ms
step:1474/2330 train_time:88150ms step_avg:59.80ms
step:1475/2330 train_time:88208ms step_avg:59.80ms
step:1476/2330 train_time:88270ms step_avg:59.80ms
step:1477/2330 train_time:88327ms step_avg:59.80ms
step:1478/2330 train_time:88390ms step_avg:59.80ms
step:1479/2330 train_time:88448ms step_avg:59.80ms
step:1480/2330 train_time:88510ms step_avg:59.80ms
step:1481/2330 train_time:88568ms step_avg:59.80ms
step:1482/2330 train_time:88630ms step_avg:59.80ms
step:1483/2330 train_time:88688ms step_avg:59.80ms
step:1484/2330 train_time:88750ms step_avg:59.80ms
step:1485/2330 train_time:88808ms step_avg:59.80ms
step:1486/2330 train_time:88870ms step_avg:59.81ms
step:1487/2330 train_time:88928ms step_avg:59.80ms
step:1488/2330 train_time:88991ms step_avg:59.81ms
step:1489/2330 train_time:89048ms step_avg:59.80ms
step:1490/2330 train_time:89110ms step_avg:59.81ms
step:1491/2330 train_time:89168ms step_avg:59.80ms
step:1492/2330 train_time:89230ms step_avg:59.81ms
step:1493/2330 train_time:89288ms step_avg:59.80ms
step:1494/2330 train_time:89350ms step_avg:59.81ms
step:1495/2330 train_time:89408ms step_avg:59.80ms
step:1496/2330 train_time:89471ms step_avg:59.81ms
step:1497/2330 train_time:89529ms step_avg:59.81ms
step:1498/2330 train_time:89591ms step_avg:59.81ms
step:1499/2330 train_time:89649ms step_avg:59.81ms
step:1500/2330 train_time:89711ms step_avg:59.81ms
step:1500/2330 val_loss:4.0044 train_time:89791ms step_avg:59.86ms
step:1501/2330 train_time:89812ms step_avg:59.83ms
step:1502/2330 train_time:89834ms step_avg:59.81ms
step:1503/2330 train_time:89894ms step_avg:59.81ms
step:1504/2330 train_time:89962ms step_avg:59.82ms
step:1505/2330 train_time:90020ms step_avg:59.81ms
step:1506/2330 train_time:90087ms step_avg:59.82ms
step:1507/2330 train_time:90144ms step_avg:59.82ms
step:1508/2330 train_time:90206ms step_avg:59.82ms
step:1509/2330 train_time:90263ms step_avg:59.82ms
step:1510/2330 train_time:90326ms step_avg:59.82ms
step:1511/2330 train_time:90382ms step_avg:59.82ms
step:1512/2330 train_time:90444ms step_avg:59.82ms
step:1513/2330 train_time:90501ms step_avg:59.82ms
step:1514/2330 train_time:90563ms step_avg:59.82ms
step:1515/2330 train_time:90621ms step_avg:59.82ms
step:1516/2330 train_time:90682ms step_avg:59.82ms
step:1517/2330 train_time:90741ms step_avg:59.82ms
step:1518/2330 train_time:90804ms step_avg:59.82ms
step:1519/2330 train_time:90863ms step_avg:59.82ms
step:1520/2330 train_time:90927ms step_avg:59.82ms
step:1521/2330 train_time:90985ms step_avg:59.82ms
step:1522/2330 train_time:91049ms step_avg:59.82ms
step:1523/2330 train_time:91106ms step_avg:59.82ms
step:1524/2330 train_time:91169ms step_avg:59.82ms
step:1525/2330 train_time:91226ms step_avg:59.82ms
step:1526/2330 train_time:91288ms step_avg:59.82ms
step:1527/2330 train_time:91345ms step_avg:59.82ms
step:1528/2330 train_time:91406ms step_avg:59.82ms
step:1529/2330 train_time:91464ms step_avg:59.82ms
step:1530/2330 train_time:91525ms step_avg:59.82ms
step:1531/2330 train_time:91583ms step_avg:59.82ms
step:1532/2330 train_time:91645ms step_avg:59.82ms
step:1533/2330 train_time:91702ms step_avg:59.82ms
step:1534/2330 train_time:91765ms step_avg:59.82ms
step:1535/2330 train_time:91824ms step_avg:59.82ms
step:1536/2330 train_time:91887ms step_avg:59.82ms
step:1537/2330 train_time:91946ms step_avg:59.82ms
step:1538/2330 train_time:92009ms step_avg:59.82ms
step:1539/2330 train_time:92067ms step_avg:59.82ms
step:1540/2330 train_time:92131ms step_avg:59.83ms
step:1541/2330 train_time:92189ms step_avg:59.82ms
step:1542/2330 train_time:92252ms step_avg:59.83ms
step:1543/2330 train_time:92309ms step_avg:59.82ms
step:1544/2330 train_time:92372ms step_avg:59.83ms
step:1545/2330 train_time:92430ms step_avg:59.83ms
step:1546/2330 train_time:92492ms step_avg:59.83ms
step:1547/2330 train_time:92549ms step_avg:59.82ms
step:1548/2330 train_time:92613ms step_avg:59.83ms
step:1549/2330 train_time:92670ms step_avg:59.83ms
step:1550/2330 train_time:92734ms step_avg:59.83ms
step:1551/2330 train_time:92791ms step_avg:59.83ms
step:1552/2330 train_time:92856ms step_avg:59.83ms
step:1553/2330 train_time:92913ms step_avg:59.83ms
step:1554/2330 train_time:92979ms step_avg:59.83ms
step:1555/2330 train_time:93037ms step_avg:59.83ms
step:1556/2330 train_time:93102ms step_avg:59.83ms
step:1557/2330 train_time:93160ms step_avg:59.83ms
step:1558/2330 train_time:93225ms step_avg:59.84ms
step:1559/2330 train_time:93283ms step_avg:59.84ms
step:1560/2330 train_time:93347ms step_avg:59.84ms
step:1561/2330 train_time:93405ms step_avg:59.84ms
step:1562/2330 train_time:93467ms step_avg:59.84ms
step:1563/2330 train_time:93525ms step_avg:59.84ms
step:1564/2330 train_time:93586ms step_avg:59.84ms
step:1565/2330 train_time:93643ms step_avg:59.84ms
step:1566/2330 train_time:93707ms step_avg:59.84ms
step:1567/2330 train_time:93765ms step_avg:59.84ms
step:1568/2330 train_time:93828ms step_avg:59.84ms
step:1569/2330 train_time:93885ms step_avg:59.84ms
step:1570/2330 train_time:93948ms step_avg:59.84ms
step:1571/2330 train_time:94006ms step_avg:59.84ms
step:1572/2330 train_time:94070ms step_avg:59.84ms
step:1573/2330 train_time:94128ms step_avg:59.84ms
step:1574/2330 train_time:94191ms step_avg:59.84ms
step:1575/2330 train_time:94248ms step_avg:59.84ms
step:1576/2330 train_time:94312ms step_avg:59.84ms
step:1577/2330 train_time:94370ms step_avg:59.84ms
step:1578/2330 train_time:94433ms step_avg:59.84ms
step:1579/2330 train_time:94490ms step_avg:59.84ms
step:1580/2330 train_time:94554ms step_avg:59.84ms
step:1581/2330 train_time:94611ms step_avg:59.84ms
step:1582/2330 train_time:94675ms step_avg:59.85ms
step:1583/2330 train_time:94733ms step_avg:59.84ms
step:1584/2330 train_time:94797ms step_avg:59.85ms
step:1585/2330 train_time:94854ms step_avg:59.84ms
step:1586/2330 train_time:94918ms step_avg:59.85ms
step:1587/2330 train_time:94975ms step_avg:59.85ms
step:1588/2330 train_time:95041ms step_avg:59.85ms
step:1589/2330 train_time:95098ms step_avg:59.85ms
step:1590/2330 train_time:95163ms step_avg:59.85ms
step:1591/2330 train_time:95221ms step_avg:59.85ms
step:1592/2330 train_time:95285ms step_avg:59.85ms
step:1593/2330 train_time:95343ms step_avg:59.85ms
step:1594/2330 train_time:95406ms step_avg:59.85ms
step:1595/2330 train_time:95464ms step_avg:59.85ms
step:1596/2330 train_time:95527ms step_avg:59.85ms
step:1597/2330 train_time:95584ms step_avg:59.85ms
step:1598/2330 train_time:95647ms step_avg:59.85ms
step:1599/2330 train_time:95704ms step_avg:59.85ms
step:1600/2330 train_time:95768ms step_avg:59.85ms
step:1601/2330 train_time:95826ms step_avg:59.85ms
step:1602/2330 train_time:95888ms step_avg:59.86ms
step:1603/2330 train_time:95946ms step_avg:59.85ms
step:1604/2330 train_time:96009ms step_avg:59.86ms
step:1605/2330 train_time:96067ms step_avg:59.85ms
step:1606/2330 train_time:96129ms step_avg:59.86ms
step:1607/2330 train_time:96187ms step_avg:59.85ms
step:1608/2330 train_time:96250ms step_avg:59.86ms
step:1609/2330 train_time:96308ms step_avg:59.86ms
step:1610/2330 train_time:96371ms step_avg:59.86ms
step:1611/2330 train_time:96429ms step_avg:59.86ms
step:1612/2330 train_time:96491ms step_avg:59.86ms
step:1613/2330 train_time:96548ms step_avg:59.86ms
step:1614/2330 train_time:96612ms step_avg:59.86ms
step:1615/2330 train_time:96669ms step_avg:59.86ms
step:1616/2330 train_time:96733ms step_avg:59.86ms
step:1617/2330 train_time:96791ms step_avg:59.86ms
step:1618/2330 train_time:96853ms step_avg:59.86ms
step:1619/2330 train_time:96910ms step_avg:59.86ms
step:1620/2330 train_time:96975ms step_avg:59.86ms
step:1621/2330 train_time:97033ms step_avg:59.86ms
step:1622/2330 train_time:97096ms step_avg:59.86ms
step:1623/2330 train_time:97154ms step_avg:59.86ms
step:1624/2330 train_time:97218ms step_avg:59.86ms
step:1625/2330 train_time:97276ms step_avg:59.86ms
step:1626/2330 train_time:97341ms step_avg:59.87ms
step:1627/2330 train_time:97399ms step_avg:59.86ms
step:1628/2330 train_time:97462ms step_avg:59.87ms
step:1629/2330 train_time:97521ms step_avg:59.87ms
step:1630/2330 train_time:97584ms step_avg:59.87ms
step:1631/2330 train_time:97641ms step_avg:59.87ms
step:1632/2330 train_time:97704ms step_avg:59.87ms
step:1633/2330 train_time:97762ms step_avg:59.87ms
step:1634/2330 train_time:97825ms step_avg:59.87ms
step:1635/2330 train_time:97883ms step_avg:59.87ms
step:1636/2330 train_time:97945ms step_avg:59.87ms
step:1637/2330 train_time:98003ms step_avg:59.87ms
step:1638/2330 train_time:98066ms step_avg:59.87ms
step:1639/2330 train_time:98124ms step_avg:59.87ms
step:1640/2330 train_time:98188ms step_avg:59.87ms
step:1641/2330 train_time:98246ms step_avg:59.87ms
step:1642/2330 train_time:98309ms step_avg:59.87ms
step:1643/2330 train_time:98367ms step_avg:59.87ms
step:1644/2330 train_time:98429ms step_avg:59.87ms
step:1645/2330 train_time:98487ms step_avg:59.87ms
step:1646/2330 train_time:98548ms step_avg:59.87ms
step:1647/2330 train_time:98606ms step_avg:59.87ms
step:1648/2330 train_time:98669ms step_avg:59.87ms
step:1649/2330 train_time:98727ms step_avg:59.87ms
step:1650/2330 train_time:98789ms step_avg:59.87ms
step:1651/2330 train_time:98846ms step_avg:59.87ms
step:1652/2330 train_time:98910ms step_avg:59.87ms
step:1653/2330 train_time:98967ms step_avg:59.87ms
step:1654/2330 train_time:99030ms step_avg:59.87ms
step:1655/2330 train_time:99088ms step_avg:59.87ms
step:1656/2330 train_time:99150ms step_avg:59.87ms
step:1657/2330 train_time:99208ms step_avg:59.87ms
step:1658/2330 train_time:99270ms step_avg:59.87ms
step:1659/2330 train_time:99327ms step_avg:59.87ms
step:1660/2330 train_time:99391ms step_avg:59.87ms
step:1661/2330 train_time:99447ms step_avg:59.87ms
step:1662/2330 train_time:99511ms step_avg:59.87ms
step:1663/2330 train_time:99568ms step_avg:59.87ms
step:1664/2330 train_time:99631ms step_avg:59.87ms
step:1665/2330 train_time:99689ms step_avg:59.87ms
step:1666/2330 train_time:99752ms step_avg:59.87ms
step:1667/2330 train_time:99809ms step_avg:59.87ms
step:1668/2330 train_time:99873ms step_avg:59.88ms
step:1669/2330 train_time:99930ms step_avg:59.87ms
step:1670/2330 train_time:99993ms step_avg:59.88ms
step:1671/2330 train_time:100050ms step_avg:59.87ms
step:1672/2330 train_time:100115ms step_avg:59.88ms
step:1673/2330 train_time:100173ms step_avg:59.88ms
step:1674/2330 train_time:100236ms step_avg:59.88ms
step:1675/2330 train_time:100295ms step_avg:59.88ms
step:1676/2330 train_time:100359ms step_avg:59.88ms
step:1677/2330 train_time:100416ms step_avg:59.88ms
step:1678/2330 train_time:100480ms step_avg:59.88ms
step:1679/2330 train_time:100538ms step_avg:59.88ms
step:1680/2330 train_time:100601ms step_avg:59.88ms
step:1681/2330 train_time:100660ms step_avg:59.88ms
step:1682/2330 train_time:100723ms step_avg:59.88ms
step:1683/2330 train_time:100781ms step_avg:59.88ms
step:1684/2330 train_time:100844ms step_avg:59.88ms
step:1685/2330 train_time:100902ms step_avg:59.88ms
step:1686/2330 train_time:100966ms step_avg:59.88ms
step:1687/2330 train_time:101024ms step_avg:59.88ms
step:1688/2330 train_time:101086ms step_avg:59.89ms
step:1689/2330 train_time:101144ms step_avg:59.88ms
step:1690/2330 train_time:101208ms step_avg:59.89ms
step:1691/2330 train_time:101265ms step_avg:59.88ms
step:1692/2330 train_time:101328ms step_avg:59.89ms
step:1693/2330 train_time:101386ms step_avg:59.89ms
step:1694/2330 train_time:101449ms step_avg:59.89ms
step:1695/2330 train_time:101506ms step_avg:59.89ms
step:1696/2330 train_time:101569ms step_avg:59.89ms
step:1697/2330 train_time:101627ms step_avg:59.89ms
step:1698/2330 train_time:101690ms step_avg:59.89ms
step:1699/2330 train_time:101747ms step_avg:59.89ms
step:1700/2330 train_time:101810ms step_avg:59.89ms
step:1701/2330 train_time:101868ms step_avg:59.89ms
step:1702/2330 train_time:101931ms step_avg:59.89ms
step:1703/2330 train_time:101989ms step_avg:59.89ms
step:1704/2330 train_time:102051ms step_avg:59.89ms
step:1705/2330 train_time:102108ms step_avg:59.89ms
step:1706/2330 train_time:102173ms step_avg:59.89ms
step:1707/2330 train_time:102230ms step_avg:59.89ms
step:1708/2330 train_time:102294ms step_avg:59.89ms
step:1709/2330 train_time:102350ms step_avg:59.89ms
step:1710/2330 train_time:102415ms step_avg:59.89ms
step:1711/2330 train_time:102473ms step_avg:59.89ms
step:1712/2330 train_time:102537ms step_avg:59.89ms
step:1713/2330 train_time:102594ms step_avg:59.89ms
step:1714/2330 train_time:102658ms step_avg:59.89ms
step:1715/2330 train_time:102716ms step_avg:59.89ms
step:1716/2330 train_time:102781ms step_avg:59.90ms
step:1717/2330 train_time:102840ms step_avg:59.90ms
step:1718/2330 train_time:102903ms step_avg:59.90ms
step:1719/2330 train_time:102961ms step_avg:59.90ms
step:1720/2330 train_time:103024ms step_avg:59.90ms
step:1721/2330 train_time:103082ms step_avg:59.90ms
step:1722/2330 train_time:103145ms step_avg:59.90ms
step:1723/2330 train_time:103203ms step_avg:59.90ms
step:1724/2330 train_time:103267ms step_avg:59.90ms
step:1725/2330 train_time:103324ms step_avg:59.90ms
step:1726/2330 train_time:103388ms step_avg:59.90ms
step:1727/2330 train_time:103445ms step_avg:59.90ms
step:1728/2330 train_time:103508ms step_avg:59.90ms
step:1729/2330 train_time:103566ms step_avg:59.90ms
step:1730/2330 train_time:103628ms step_avg:59.90ms
step:1731/2330 train_time:103685ms step_avg:59.90ms
step:1732/2330 train_time:103749ms step_avg:59.90ms
step:1733/2330 train_time:103806ms step_avg:59.90ms
step:1734/2330 train_time:103869ms step_avg:59.90ms
step:1735/2330 train_time:103927ms step_avg:59.90ms
step:1736/2330 train_time:103989ms step_avg:59.90ms
step:1737/2330 train_time:104047ms step_avg:59.90ms
step:1738/2330 train_time:104110ms step_avg:59.90ms
step:1739/2330 train_time:104167ms step_avg:59.90ms
step:1740/2330 train_time:104232ms step_avg:59.90ms
step:1741/2330 train_time:104289ms step_avg:59.90ms
step:1742/2330 train_time:104353ms step_avg:59.90ms
step:1743/2330 train_time:104410ms step_avg:59.90ms
step:1744/2330 train_time:104475ms step_avg:59.91ms
step:1745/2330 train_time:104532ms step_avg:59.90ms
step:1746/2330 train_time:104597ms step_avg:59.91ms
step:1747/2330 train_time:104654ms step_avg:59.91ms
step:1748/2330 train_time:104718ms step_avg:59.91ms
step:1749/2330 train_time:104777ms step_avg:59.91ms
step:1750/2330 train_time:104840ms step_avg:59.91ms
step:1750/2330 val_loss:3.9191 train_time:104921ms step_avg:59.96ms
step:1751/2330 train_time:104943ms step_avg:59.93ms
step:1752/2330 train_time:104965ms step_avg:59.91ms
step:1753/2330 train_time:105022ms step_avg:59.91ms
step:1754/2330 train_time:105090ms step_avg:59.91ms
step:1755/2330 train_time:105147ms step_avg:59.91ms
step:1756/2330 train_time:105215ms step_avg:59.92ms
step:1757/2330 train_time:105272ms step_avg:59.92ms
step:1758/2330 train_time:105337ms step_avg:59.92ms
step:1759/2330 train_time:105394ms step_avg:59.92ms
step:1760/2330 train_time:105458ms step_avg:59.92ms
step:1761/2330 train_time:105515ms step_avg:59.92ms
step:1762/2330 train_time:105577ms step_avg:59.92ms
step:1763/2330 train_time:105634ms step_avg:59.92ms
step:1764/2330 train_time:105698ms step_avg:59.92ms
step:1765/2330 train_time:105755ms step_avg:59.92ms
step:1766/2330 train_time:105820ms step_avg:59.92ms
step:1767/2330 train_time:105881ms step_avg:59.92ms
step:1768/2330 train_time:105944ms step_avg:59.92ms
step:1769/2330 train_time:106003ms step_avg:59.92ms
step:1770/2330 train_time:106066ms step_avg:59.92ms
step:1771/2330 train_time:106125ms step_avg:59.92ms
step:1772/2330 train_time:106188ms step_avg:59.93ms
step:1773/2330 train_time:106245ms step_avg:59.92ms
step:1774/2330 train_time:106311ms step_avg:59.93ms
step:1775/2330 train_time:106368ms step_avg:59.93ms
step:1776/2330 train_time:106431ms step_avg:59.93ms
step:1777/2330 train_time:106489ms step_avg:59.93ms
step:1778/2330 train_time:106553ms step_avg:59.93ms
step:1779/2330 train_time:106610ms step_avg:59.93ms
step:1780/2330 train_time:106673ms step_avg:59.93ms
step:1781/2330 train_time:106730ms step_avg:59.93ms
step:1782/2330 train_time:106794ms step_avg:59.93ms
step:1783/2330 train_time:106852ms step_avg:59.93ms
step:1784/2330 train_time:106917ms step_avg:59.93ms
step:1785/2330 train_time:106975ms step_avg:59.93ms
step:1786/2330 train_time:107040ms step_avg:59.93ms
step:1787/2330 train_time:107099ms step_avg:59.93ms
step:1788/2330 train_time:107162ms step_avg:59.93ms
step:1789/2330 train_time:107220ms step_avg:59.93ms
step:1790/2330 train_time:107284ms step_avg:59.94ms
step:1791/2330 train_time:107342ms step_avg:59.93ms
step:1792/2330 train_time:107404ms step_avg:59.94ms
step:1793/2330 train_time:107462ms step_avg:59.93ms
step:1794/2330 train_time:107524ms step_avg:59.94ms
step:1795/2330 train_time:107582ms step_avg:59.93ms
step:1796/2330 train_time:107645ms step_avg:59.94ms
step:1797/2330 train_time:107702ms step_avg:59.93ms
step:1798/2330 train_time:107764ms step_avg:59.94ms
step:1799/2330 train_time:107821ms step_avg:59.93ms
step:1800/2330 train_time:107884ms step_avg:59.94ms
step:1801/2330 train_time:107942ms step_avg:59.93ms
step:1802/2330 train_time:108005ms step_avg:59.94ms
step:1803/2330 train_time:108063ms step_avg:59.94ms
step:1804/2330 train_time:108128ms step_avg:59.94ms
step:1805/2330 train_time:108186ms step_avg:59.94ms
step:1806/2330 train_time:108250ms step_avg:59.94ms
step:1807/2330 train_time:108308ms step_avg:59.94ms
step:1808/2330 train_time:108371ms step_avg:59.94ms
step:1809/2330 train_time:108428ms step_avg:59.94ms
step:1810/2330 train_time:108492ms step_avg:59.94ms
step:1811/2330 train_time:108550ms step_avg:59.94ms
step:1812/2330 train_time:108615ms step_avg:59.94ms
step:1813/2330 train_time:108673ms step_avg:59.94ms
step:1814/2330 train_time:108736ms step_avg:59.94ms
step:1815/2330 train_time:108795ms step_avg:59.94ms
step:1816/2330 train_time:108858ms step_avg:59.94ms
step:1817/2330 train_time:108916ms step_avg:59.94ms
step:1818/2330 train_time:108980ms step_avg:59.94ms
step:1819/2330 train_time:109038ms step_avg:59.94ms
step:1820/2330 train_time:109101ms step_avg:59.95ms
step:1821/2330 train_time:109160ms step_avg:59.94ms
step:1822/2330 train_time:109223ms step_avg:59.95ms
step:1823/2330 train_time:109282ms step_avg:59.95ms
step:1824/2330 train_time:109343ms step_avg:59.95ms
step:1825/2330 train_time:109401ms step_avg:59.95ms
step:1826/2330 train_time:109464ms step_avg:59.95ms
step:1827/2330 train_time:109521ms step_avg:59.95ms
step:1828/2330 train_time:109585ms step_avg:59.95ms
step:1829/2330 train_time:109642ms step_avg:59.95ms
step:1830/2330 train_time:109705ms step_avg:59.95ms
step:1831/2330 train_time:109762ms step_avg:59.95ms
step:1832/2330 train_time:109825ms step_avg:59.95ms
step:1833/2330 train_time:109882ms step_avg:59.95ms
step:1834/2330 train_time:109945ms step_avg:59.95ms
step:1835/2330 train_time:110002ms step_avg:59.95ms
step:1836/2330 train_time:110066ms step_avg:59.95ms
step:1837/2330 train_time:110124ms step_avg:59.95ms
step:1838/2330 train_time:110188ms step_avg:59.95ms
step:1839/2330 train_time:110246ms step_avg:59.95ms
step:1840/2330 train_time:110308ms step_avg:59.95ms
step:1841/2330 train_time:110366ms step_avg:59.95ms
step:1842/2330 train_time:110428ms step_avg:59.95ms
step:1843/2330 train_time:110486ms step_avg:59.95ms
step:1844/2330 train_time:110549ms step_avg:59.95ms
step:1845/2330 train_time:110606ms step_avg:59.95ms
step:1846/2330 train_time:110671ms step_avg:59.95ms
step:1847/2330 train_time:110728ms step_avg:59.95ms
step:1848/2330 train_time:110792ms step_avg:59.95ms
step:1849/2330 train_time:110850ms step_avg:59.95ms
step:1850/2330 train_time:110913ms step_avg:59.95ms
step:1851/2330 train_time:110971ms step_avg:59.95ms
step:1852/2330 train_time:111035ms step_avg:59.95ms
step:1853/2330 train_time:111092ms step_avg:59.95ms
step:1854/2330 train_time:111157ms step_avg:59.96ms
step:1855/2330 train_time:111215ms step_avg:59.95ms
step:1856/2330 train_time:111280ms step_avg:59.96ms
step:1857/2330 train_time:111339ms step_avg:59.96ms
step:1858/2330 train_time:111402ms step_avg:59.96ms
step:1859/2330 train_time:111461ms step_avg:59.96ms
step:1860/2330 train_time:111522ms step_avg:59.96ms
step:1861/2330 train_time:111580ms step_avg:59.96ms
step:1862/2330 train_time:111644ms step_avg:59.96ms
step:1863/2330 train_time:111702ms step_avg:59.96ms
step:1864/2330 train_time:111765ms step_avg:59.96ms
step:1865/2330 train_time:111822ms step_avg:59.96ms
step:1866/2330 train_time:111885ms step_avg:59.96ms
step:1867/2330 train_time:111943ms step_avg:59.96ms
step:1868/2330 train_time:112005ms step_avg:59.96ms
step:1869/2330 train_time:112062ms step_avg:59.96ms
step:1870/2330 train_time:112125ms step_avg:59.96ms
step:1871/2330 train_time:112183ms step_avg:59.96ms
step:1872/2330 train_time:112246ms step_avg:59.96ms
step:1873/2330 train_time:112304ms step_avg:59.96ms
step:1874/2330 train_time:112367ms step_avg:59.96ms
step:1875/2330 train_time:112424ms step_avg:59.96ms
step:1876/2330 train_time:112487ms step_avg:59.96ms
step:1877/2330 train_time:112544ms step_avg:59.96ms
step:1878/2330 train_time:112607ms step_avg:59.96ms
step:1879/2330 train_time:112665ms step_avg:59.96ms
step:1880/2330 train_time:112727ms step_avg:59.96ms
step:1881/2330 train_time:112785ms step_avg:59.96ms
step:1882/2330 train_time:112848ms step_avg:59.96ms
step:1883/2330 train_time:112905ms step_avg:59.96ms
step:1884/2330 train_time:112967ms step_avg:59.96ms
step:1885/2330 train_time:113025ms step_avg:59.96ms
step:1886/2330 train_time:113088ms step_avg:59.96ms
step:1887/2330 train_time:113145ms step_avg:59.96ms
step:1888/2330 train_time:113208ms step_avg:59.96ms
step:1889/2330 train_time:113266ms step_avg:59.96ms
step:1890/2330 train_time:113329ms step_avg:59.96ms
step:1891/2330 train_time:113387ms step_avg:59.96ms
step:1892/2330 train_time:113450ms step_avg:59.96ms
step:1893/2330 train_time:113508ms step_avg:59.96ms
step:1894/2330 train_time:113571ms step_avg:59.96ms
step:1895/2330 train_time:113628ms step_avg:59.96ms
step:1896/2330 train_time:113692ms step_avg:59.96ms
step:1897/2330 train_time:113749ms step_avg:59.96ms
step:1898/2330 train_time:113812ms step_avg:59.96ms
step:1899/2330 train_time:113870ms step_avg:59.96ms
step:1900/2330 train_time:113934ms step_avg:59.97ms
step:1901/2330 train_time:113992ms step_avg:59.96ms
step:1902/2330 train_time:114055ms step_avg:59.97ms
step:1903/2330 train_time:114113ms step_avg:59.96ms
step:1904/2330 train_time:114176ms step_avg:59.97ms
step:1905/2330 train_time:114234ms step_avg:59.97ms
step:1906/2330 train_time:114297ms step_avg:59.97ms
step:1907/2330 train_time:114356ms step_avg:59.97ms
step:1908/2330 train_time:114420ms step_avg:59.97ms
step:1909/2330 train_time:114478ms step_avg:59.97ms
step:1910/2330 train_time:114540ms step_avg:59.97ms
step:1911/2330 train_time:114599ms step_avg:59.97ms
step:1912/2330 train_time:114662ms step_avg:59.97ms
step:1913/2330 train_time:114720ms step_avg:59.97ms
step:1914/2330 train_time:114783ms step_avg:59.97ms
step:1915/2330 train_time:114841ms step_avg:59.97ms
step:1916/2330 train_time:114904ms step_avg:59.97ms
step:1917/2330 train_time:114962ms step_avg:59.97ms
step:1918/2330 train_time:115024ms step_avg:59.97ms
step:1919/2330 train_time:115082ms step_avg:59.97ms
step:1920/2330 train_time:115145ms step_avg:59.97ms
step:1921/2330 train_time:115203ms step_avg:59.97ms
step:1922/2330 train_time:115265ms step_avg:59.97ms
step:1923/2330 train_time:115322ms step_avg:59.97ms
step:1924/2330 train_time:115386ms step_avg:59.97ms
step:1925/2330 train_time:115444ms step_avg:59.97ms
step:1926/2330 train_time:115507ms step_avg:59.97ms
step:1927/2330 train_time:115565ms step_avg:59.97ms
step:1928/2330 train_time:115627ms step_avg:59.97ms
step:1929/2330 train_time:115686ms step_avg:59.97ms
step:1930/2330 train_time:115747ms step_avg:59.97ms
step:1931/2330 train_time:115804ms step_avg:59.97ms
step:1932/2330 train_time:115868ms step_avg:59.97ms
step:1933/2330 train_time:115925ms step_avg:59.97ms
step:1934/2330 train_time:115988ms step_avg:59.97ms
step:1935/2330 train_time:116044ms step_avg:59.97ms
step:1936/2330 train_time:116108ms step_avg:59.97ms
step:1937/2330 train_time:116165ms step_avg:59.97ms
step:1938/2330 train_time:116227ms step_avg:59.97ms
step:1939/2330 train_time:116285ms step_avg:59.97ms
step:1940/2330 train_time:116348ms step_avg:59.97ms
step:1941/2330 train_time:116406ms step_avg:59.97ms
step:1942/2330 train_time:116468ms step_avg:59.97ms
step:1943/2330 train_time:116525ms step_avg:59.97ms
step:1944/2330 train_time:116589ms step_avg:59.97ms
step:1945/2330 train_time:116647ms step_avg:59.97ms
step:1946/2330 train_time:116710ms step_avg:59.97ms
step:1947/2330 train_time:116767ms step_avg:59.97ms
step:1948/2330 train_time:116831ms step_avg:59.97ms
step:1949/2330 train_time:116889ms step_avg:59.97ms
step:1950/2330 train_time:116952ms step_avg:59.98ms
step:1951/2330 train_time:117009ms step_avg:59.97ms
step:1952/2330 train_time:117074ms step_avg:59.98ms
step:1953/2330 train_time:117131ms step_avg:59.98ms
step:1954/2330 train_time:117194ms step_avg:59.98ms
step:1955/2330 train_time:117253ms step_avg:59.98ms
step:1956/2330 train_time:117318ms step_avg:59.98ms
step:1957/2330 train_time:117376ms step_avg:59.98ms
step:1958/2330 train_time:117440ms step_avg:59.98ms
step:1959/2330 train_time:117497ms step_avg:59.98ms
step:1960/2330 train_time:117561ms step_avg:59.98ms
step:1961/2330 train_time:117619ms step_avg:59.98ms
step:1962/2330 train_time:117681ms step_avg:59.98ms
step:1963/2330 train_time:117740ms step_avg:59.98ms
step:1964/2330 train_time:117803ms step_avg:59.98ms
step:1965/2330 train_time:117862ms step_avg:59.98ms
step:1966/2330 train_time:117923ms step_avg:59.98ms
step:1967/2330 train_time:117981ms step_avg:59.98ms
step:1968/2330 train_time:118044ms step_avg:59.98ms
step:1969/2330 train_time:118102ms step_avg:59.98ms
step:1970/2330 train_time:118164ms step_avg:59.98ms
step:1971/2330 train_time:118222ms step_avg:59.98ms
step:1972/2330 train_time:118285ms step_avg:59.98ms
step:1973/2330 train_time:118342ms step_avg:59.98ms
step:1974/2330 train_time:118405ms step_avg:59.98ms
step:1975/2330 train_time:118462ms step_avg:59.98ms
step:1976/2330 train_time:118525ms step_avg:59.98ms
step:1977/2330 train_time:118582ms step_avg:59.98ms
step:1978/2330 train_time:118646ms step_avg:59.98ms
step:1979/2330 train_time:118703ms step_avg:59.98ms
step:1980/2330 train_time:118767ms step_avg:59.98ms
step:1981/2330 train_time:118824ms step_avg:59.98ms
step:1982/2330 train_time:118887ms step_avg:59.98ms
step:1983/2330 train_time:118945ms step_avg:59.98ms
step:1984/2330 train_time:119007ms step_avg:59.98ms
step:1985/2330 train_time:119065ms step_avg:59.98ms
step:1986/2330 train_time:119127ms step_avg:59.98ms
step:1987/2330 train_time:119184ms step_avg:59.98ms
step:1988/2330 train_time:119247ms step_avg:59.98ms
step:1989/2330 train_time:119304ms step_avg:59.98ms
step:1990/2330 train_time:119367ms step_avg:59.98ms
step:1991/2330 train_time:119424ms step_avg:59.98ms
step:1992/2330 train_time:119488ms step_avg:59.98ms
step:1993/2330 train_time:119545ms step_avg:59.98ms
step:1994/2330 train_time:119608ms step_avg:59.98ms
step:1995/2330 train_time:119665ms step_avg:59.98ms
step:1996/2330 train_time:119729ms step_avg:59.98ms
step:1997/2330 train_time:119787ms step_avg:59.98ms
step:1998/2330 train_time:119850ms step_avg:59.98ms
step:1999/2330 train_time:119907ms step_avg:59.98ms
step:2000/2330 train_time:119972ms step_avg:59.99ms
step:2000/2330 val_loss:3.8523 train_time:120052ms step_avg:60.03ms
step:2001/2330 train_time:120072ms step_avg:60.01ms
step:2002/2330 train_time:120095ms step_avg:59.99ms
step:2003/2330 train_time:120154ms step_avg:59.99ms
step:2004/2330 train_time:120221ms step_avg:59.99ms
step:2005/2330 train_time:120278ms step_avg:59.99ms
step:2006/2330 train_time:120343ms step_avg:59.99ms
step:2007/2330 train_time:120401ms step_avg:59.99ms
step:2008/2330 train_time:120463ms step_avg:59.99ms
step:2009/2330 train_time:120521ms step_avg:59.99ms
step:2010/2330 train_time:120583ms step_avg:59.99ms
step:2011/2330 train_time:120640ms step_avg:59.99ms
step:2012/2330 train_time:120702ms step_avg:59.99ms
step:2013/2330 train_time:120759ms step_avg:59.99ms
step:2014/2330 train_time:120823ms step_avg:59.99ms
step:2015/2330 train_time:120880ms step_avg:59.99ms
step:2016/2330 train_time:120944ms step_avg:59.99ms
step:2017/2330 train_time:121003ms step_avg:59.99ms
step:2018/2330 train_time:121067ms step_avg:59.99ms
step:2019/2330 train_time:121127ms step_avg:59.99ms
step:2020/2330 train_time:121189ms step_avg:59.99ms
step:2021/2330 train_time:121247ms step_avg:59.99ms
step:2022/2330 train_time:121311ms step_avg:60.00ms
step:2023/2330 train_time:121369ms step_avg:59.99ms
step:2024/2330 train_time:121431ms step_avg:60.00ms
step:2025/2330 train_time:121488ms step_avg:59.99ms
step:2026/2330 train_time:121551ms step_avg:60.00ms
step:2027/2330 train_time:121608ms step_avg:59.99ms
step:2028/2330 train_time:121669ms step_avg:59.99ms
step:2029/2330 train_time:121727ms step_avg:59.99ms
step:2030/2330 train_time:121789ms step_avg:59.99ms
step:2031/2330 train_time:121846ms step_avg:59.99ms
step:2032/2330 train_time:121909ms step_avg:59.99ms
step:2033/2330 train_time:121966ms step_avg:59.99ms
step:2034/2330 train_time:122029ms step_avg:59.99ms
step:2035/2330 train_time:122087ms step_avg:59.99ms
step:2036/2330 train_time:122150ms step_avg:60.00ms
step:2037/2330 train_time:122209ms step_avg:59.99ms
step:2038/2330 train_time:122272ms step_avg:60.00ms
step:2039/2330 train_time:122331ms step_avg:60.00ms
step:2040/2330 train_time:122393ms step_avg:60.00ms
step:2041/2330 train_time:122451ms step_avg:60.00ms
step:2042/2330 train_time:122514ms step_avg:60.00ms
step:2043/2330 train_time:122571ms step_avg:60.00ms
step:2044/2330 train_time:122634ms step_avg:60.00ms
step:2045/2330 train_time:122691ms step_avg:60.00ms
step:2046/2330 train_time:122755ms step_avg:60.00ms
step:2047/2330 train_time:122813ms step_avg:60.00ms
step:2048/2330 train_time:122875ms step_avg:60.00ms
step:2049/2330 train_time:122932ms step_avg:60.00ms
step:2050/2330 train_time:122996ms step_avg:60.00ms
step:2051/2330 train_time:123053ms step_avg:60.00ms
step:2052/2330 train_time:123118ms step_avg:60.00ms
step:2053/2330 train_time:123176ms step_avg:60.00ms
step:2054/2330 train_time:123241ms step_avg:60.00ms
step:2055/2330 train_time:123299ms step_avg:60.00ms
step:2056/2330 train_time:123364ms step_avg:60.00ms
step:2057/2330 train_time:123423ms step_avg:60.00ms
step:2058/2330 train_time:123485ms step_avg:60.00ms
step:2059/2330 train_time:123543ms step_avg:60.00ms
step:2060/2330 train_time:123606ms step_avg:60.00ms
step:2061/2330 train_time:123664ms step_avg:60.00ms
step:2062/2330 train_time:123727ms step_avg:60.00ms
step:2063/2330 train_time:123785ms step_avg:60.00ms
step:2064/2330 train_time:123847ms step_avg:60.00ms
step:2065/2330 train_time:123905ms step_avg:60.00ms
step:2066/2330 train_time:123968ms step_avg:60.00ms
step:2067/2330 train_time:124026ms step_avg:60.00ms
step:2068/2330 train_time:124089ms step_avg:60.00ms
step:2069/2330 train_time:124147ms step_avg:60.00ms
step:2070/2330 train_time:124211ms step_avg:60.01ms
step:2071/2330 train_time:124268ms step_avg:60.00ms
step:2072/2330 train_time:124332ms step_avg:60.01ms
step:2073/2330 train_time:124390ms step_avg:60.00ms
step:2074/2330 train_time:124452ms step_avg:60.01ms
step:2075/2330 train_time:124510ms step_avg:60.00ms
step:2076/2330 train_time:124573ms step_avg:60.01ms
step:2077/2330 train_time:124631ms step_avg:60.01ms
step:2078/2330 train_time:124693ms step_avg:60.01ms
step:2079/2330 train_time:124751ms step_avg:60.01ms
step:2080/2330 train_time:124813ms step_avg:60.01ms
step:2081/2330 train_time:124870ms step_avg:60.00ms
step:2082/2330 train_time:124934ms step_avg:60.01ms
step:2083/2330 train_time:124991ms step_avg:60.01ms
step:2084/2330 train_time:125054ms step_avg:60.01ms
step:2085/2330 train_time:125112ms step_avg:60.01ms
step:2086/2330 train_time:125175ms step_avg:60.01ms
step:2087/2330 train_time:125232ms step_avg:60.01ms
step:2088/2330 train_time:125296ms step_avg:60.01ms
step:2089/2330 train_time:125354ms step_avg:60.01ms
step:2090/2330 train_time:125417ms step_avg:60.01ms
step:2091/2330 train_time:125475ms step_avg:60.01ms
step:2092/2330 train_time:125538ms step_avg:60.01ms
step:2093/2330 train_time:125597ms step_avg:60.01ms
step:2094/2330 train_time:125660ms step_avg:60.01ms
step:2095/2330 train_time:125718ms step_avg:60.01ms
step:2096/2330 train_time:125781ms step_avg:60.01ms
step:2097/2330 train_time:125839ms step_avg:60.01ms
step:2098/2330 train_time:125903ms step_avg:60.01ms
step:2099/2330 train_time:125962ms step_avg:60.01ms
step:2100/2330 train_time:126026ms step_avg:60.01ms
step:2101/2330 train_time:126084ms step_avg:60.01ms
step:2102/2330 train_time:126147ms step_avg:60.01ms
step:2103/2330 train_time:126205ms step_avg:60.01ms
step:2104/2330 train_time:126268ms step_avg:60.01ms
step:2105/2330 train_time:126327ms step_avg:60.01ms
step:2106/2330 train_time:126388ms step_avg:60.01ms
step:2107/2330 train_time:126446ms step_avg:60.01ms
step:2108/2330 train_time:126509ms step_avg:60.01ms
step:2109/2330 train_time:126567ms step_avg:60.01ms
step:2110/2330 train_time:126629ms step_avg:60.01ms
step:2111/2330 train_time:126687ms step_avg:60.01ms
step:2112/2330 train_time:126749ms step_avg:60.01ms
step:2113/2330 train_time:126806ms step_avg:60.01ms
step:2114/2330 train_time:126869ms step_avg:60.01ms
step:2115/2330 train_time:126927ms step_avg:60.01ms
step:2116/2330 train_time:126990ms step_avg:60.01ms
step:2117/2330 train_time:127047ms step_avg:60.01ms
step:2118/2330 train_time:127110ms step_avg:60.01ms
step:2119/2330 train_time:127167ms step_avg:60.01ms
step:2120/2330 train_time:127230ms step_avg:60.01ms
step:2121/2330 train_time:127287ms step_avg:60.01ms
step:2122/2330 train_time:127350ms step_avg:60.01ms
step:2123/2330 train_time:127409ms step_avg:60.01ms
step:2124/2330 train_time:127471ms step_avg:60.01ms
step:2125/2330 train_time:127528ms step_avg:60.01ms
step:2126/2330 train_time:127590ms step_avg:60.01ms
step:2127/2330 train_time:127647ms step_avg:60.01ms
step:2128/2330 train_time:127711ms step_avg:60.01ms
step:2129/2330 train_time:127768ms step_avg:60.01ms
step:2130/2330 train_time:127831ms step_avg:60.01ms
step:2131/2330 train_time:127888ms step_avg:60.01ms
step:2132/2330 train_time:127951ms step_avg:60.01ms
step:2133/2330 train_time:128009ms step_avg:60.01ms
step:2134/2330 train_time:128073ms step_avg:60.02ms
step:2135/2330 train_time:128130ms step_avg:60.01ms
step:2136/2330 train_time:128192ms step_avg:60.02ms
step:2137/2330 train_time:128250ms step_avg:60.01ms
step:2138/2330 train_time:128313ms step_avg:60.02ms
step:2139/2330 train_time:128371ms step_avg:60.01ms
step:2140/2330 train_time:128434ms step_avg:60.02ms
step:2141/2330 train_time:128491ms step_avg:60.01ms
step:2142/2330 train_time:128555ms step_avg:60.02ms
step:2143/2330 train_time:128612ms step_avg:60.02ms
step:2144/2330 train_time:128675ms step_avg:60.02ms
step:2145/2330 train_time:128733ms step_avg:60.02ms
step:2146/2330 train_time:128796ms step_avg:60.02ms
step:2147/2330 train_time:128853ms step_avg:60.02ms
step:2148/2330 train_time:128918ms step_avg:60.02ms
step:2149/2330 train_time:128976ms step_avg:60.02ms
step:2150/2330 train_time:129039ms step_avg:60.02ms
step:2151/2330 train_time:129098ms step_avg:60.02ms
step:2152/2330 train_time:129161ms step_avg:60.02ms
step:2153/2330 train_time:129219ms step_avg:60.02ms
step:2154/2330 train_time:129283ms step_avg:60.02ms
step:2155/2330 train_time:129341ms step_avg:60.02ms
step:2156/2330 train_time:129404ms step_avg:60.02ms
step:2157/2330 train_time:129462ms step_avg:60.02ms
step:2158/2330 train_time:129526ms step_avg:60.02ms
step:2159/2330 train_time:129584ms step_avg:60.02ms
step:2160/2330 train_time:129647ms step_avg:60.02ms
step:2161/2330 train_time:129705ms step_avg:60.02ms
step:2162/2330 train_time:129768ms step_avg:60.02ms
step:2163/2330 train_time:129826ms step_avg:60.02ms
step:2164/2330 train_time:129888ms step_avg:60.02ms
step:2165/2330 train_time:129946ms step_avg:60.02ms
step:2166/2330 train_time:130009ms step_avg:60.02ms
step:2167/2330 train_time:130067ms step_avg:60.02ms
step:2168/2330 train_time:130129ms step_avg:60.02ms
step:2169/2330 train_time:130187ms step_avg:60.02ms
step:2170/2330 train_time:130249ms step_avg:60.02ms
step:2171/2330 train_time:130307ms step_avg:60.02ms
step:2172/2330 train_time:130370ms step_avg:60.02ms
step:2173/2330 train_time:130428ms step_avg:60.02ms
step:2174/2330 train_time:130490ms step_avg:60.02ms
step:2175/2330 train_time:130548ms step_avg:60.02ms
step:2176/2330 train_time:130611ms step_avg:60.02ms
step:2177/2330 train_time:130668ms step_avg:60.02ms
step:2178/2330 train_time:130732ms step_avg:60.02ms
step:2179/2330 train_time:130790ms step_avg:60.02ms
step:2180/2330 train_time:130853ms step_avg:60.02ms
step:2181/2330 train_time:130910ms step_avg:60.02ms
step:2182/2330 train_time:130973ms step_avg:60.02ms
step:2183/2330 train_time:131031ms step_avg:60.02ms
step:2184/2330 train_time:131093ms step_avg:60.02ms
step:2185/2330 train_time:131150ms step_avg:60.02ms
step:2186/2330 train_time:131213ms step_avg:60.02ms
step:2187/2330 train_time:131271ms step_avg:60.02ms
step:2188/2330 train_time:131335ms step_avg:60.02ms
step:2189/2330 train_time:131392ms step_avg:60.02ms
step:2190/2330 train_time:131456ms step_avg:60.03ms
step:2191/2330 train_time:131513ms step_avg:60.02ms
step:2192/2330 train_time:131578ms step_avg:60.03ms
step:2193/2330 train_time:131636ms step_avg:60.03ms
step:2194/2330 train_time:131699ms step_avg:60.03ms
step:2195/2330 train_time:131757ms step_avg:60.03ms
step:2196/2330 train_time:131820ms step_avg:60.03ms
step:2197/2330 train_time:131879ms step_avg:60.03ms
step:2198/2330 train_time:131942ms step_avg:60.03ms
step:2199/2330 train_time:132001ms step_avg:60.03ms
step:2200/2330 train_time:132063ms step_avg:60.03ms
step:2201/2330 train_time:132122ms step_avg:60.03ms
step:2202/2330 train_time:132184ms step_avg:60.03ms
step:2203/2330 train_time:132243ms step_avg:60.03ms
step:2204/2330 train_time:132305ms step_avg:60.03ms
step:2205/2330 train_time:132364ms step_avg:60.03ms
step:2206/2330 train_time:132427ms step_avg:60.03ms
step:2207/2330 train_time:132485ms step_avg:60.03ms
step:2208/2330 train_time:132547ms step_avg:60.03ms
step:2209/2330 train_time:132606ms step_avg:60.03ms
step:2210/2330 train_time:132668ms step_avg:60.03ms
step:2211/2330 train_time:132726ms step_avg:60.03ms
step:2212/2330 train_time:132788ms step_avg:60.03ms
step:2213/2330 train_time:132846ms step_avg:60.03ms
step:2214/2330 train_time:132909ms step_avg:60.03ms
step:2215/2330 train_time:132967ms step_avg:60.03ms
step:2216/2330 train_time:133029ms step_avg:60.03ms
step:2217/2330 train_time:133087ms step_avg:60.03ms
step:2218/2330 train_time:133149ms step_avg:60.03ms
step:2219/2330 train_time:133206ms step_avg:60.03ms
step:2220/2330 train_time:133269ms step_avg:60.03ms
step:2221/2330 train_time:133328ms step_avg:60.03ms
step:2222/2330 train_time:133389ms step_avg:60.03ms
step:2223/2330 train_time:133447ms step_avg:60.03ms
step:2224/2330 train_time:133510ms step_avg:60.03ms
step:2225/2330 train_time:133568ms step_avg:60.03ms
step:2226/2330 train_time:133631ms step_avg:60.03ms
step:2227/2330 train_time:133689ms step_avg:60.03ms
step:2228/2330 train_time:133751ms step_avg:60.03ms
step:2229/2330 train_time:133809ms step_avg:60.03ms
step:2230/2330 train_time:133871ms step_avg:60.03ms
step:2231/2330 train_time:133929ms step_avg:60.03ms
step:2232/2330 train_time:133992ms step_avg:60.03ms
step:2233/2330 train_time:134049ms step_avg:60.03ms
step:2234/2330 train_time:134112ms step_avg:60.03ms
step:2235/2330 train_time:134170ms step_avg:60.03ms
step:2236/2330 train_time:134232ms step_avg:60.03ms
step:2237/2330 train_time:134290ms step_avg:60.03ms
step:2238/2330 train_time:134352ms step_avg:60.03ms
step:2239/2330 train_time:134410ms step_avg:60.03ms
step:2240/2330 train_time:134473ms step_avg:60.03ms
step:2241/2330 train_time:134532ms step_avg:60.03ms
step:2242/2330 train_time:134594ms step_avg:60.03ms
step:2243/2330 train_time:134651ms step_avg:60.03ms
step:2244/2330 train_time:134714ms step_avg:60.03ms
step:2245/2330 train_time:134772ms step_avg:60.03ms
step:2246/2330 train_time:134835ms step_avg:60.03ms
step:2247/2330 train_time:134892ms step_avg:60.03ms
step:2248/2330 train_time:134956ms step_avg:60.03ms
step:2249/2330 train_time:135013ms step_avg:60.03ms
step:2250/2330 train_time:135078ms step_avg:60.03ms
step:2250/2330 val_loss:3.8049 train_time:135160ms step_avg:60.07ms
step:2251/2330 train_time:135180ms step_avg:60.05ms
step:2252/2330 train_time:135203ms step_avg:60.04ms
step:2253/2330 train_time:135263ms step_avg:60.04ms
step:2254/2330 train_time:135329ms step_avg:60.04ms
step:2255/2330 train_time:135387ms step_avg:60.04ms
step:2256/2330 train_time:135453ms step_avg:60.04ms
step:2257/2330 train_time:135510ms step_avg:60.04ms
step:2258/2330 train_time:135573ms step_avg:60.04ms
step:2259/2330 train_time:135630ms step_avg:60.04ms
step:2260/2330 train_time:135693ms step_avg:60.04ms
step:2261/2330 train_time:135751ms step_avg:60.04ms
step:2262/2330 train_time:135813ms step_avg:60.04ms
step:2263/2330 train_time:135871ms step_avg:60.04ms
step:2264/2330 train_time:135934ms step_avg:60.04ms
step:2265/2330 train_time:135992ms step_avg:60.04ms
step:2266/2330 train_time:136055ms step_avg:60.04ms
step:2267/2330 train_time:136115ms step_avg:60.04ms
step:2268/2330 train_time:136178ms step_avg:60.04ms
step:2269/2330 train_time:136238ms step_avg:60.04ms
step:2270/2330 train_time:136301ms step_avg:60.04ms
step:2271/2330 train_time:136359ms step_avg:60.04ms
step:2272/2330 train_time:136422ms step_avg:60.04ms
step:2273/2330 train_time:136479ms step_avg:60.04ms
step:2274/2330 train_time:136542ms step_avg:60.04ms
step:2275/2330 train_time:136599ms step_avg:60.04ms
step:2276/2330 train_time:136661ms step_avg:60.04ms
step:2277/2330 train_time:136719ms step_avg:60.04ms
step:2278/2330 train_time:136781ms step_avg:60.04ms
step:2279/2330 train_time:136838ms step_avg:60.04ms
step:2280/2330 train_time:136900ms step_avg:60.04ms
step:2281/2330 train_time:136958ms step_avg:60.04ms
step:2282/2330 train_time:137020ms step_avg:60.04ms
step:2283/2330 train_time:137077ms step_avg:60.04ms
step:2284/2330 train_time:137141ms step_avg:60.04ms
step:2285/2330 train_time:137199ms step_avg:60.04ms
step:2286/2330 train_time:137263ms step_avg:60.04ms
step:2287/2330 train_time:137321ms step_avg:60.04ms
step:2288/2330 train_time:137384ms step_avg:60.05ms
step:2289/2330 train_time:137442ms step_avg:60.04ms
step:2290/2330 train_time:137504ms step_avg:60.05ms
step:2291/2330 train_time:137562ms step_avg:60.04ms
step:2292/2330 train_time:137624ms step_avg:60.05ms
step:2293/2330 train_time:137681ms step_avg:60.04ms
step:2294/2330 train_time:137745ms step_avg:60.05ms
step:2295/2330 train_time:137803ms step_avg:60.04ms
step:2296/2330 train_time:137867ms step_avg:60.05ms
step:2297/2330 train_time:137924ms step_avg:60.05ms
step:2298/2330 train_time:137988ms step_avg:60.05ms
step:2299/2330 train_time:138046ms step_avg:60.05ms
step:2300/2330 train_time:138110ms step_avg:60.05ms
step:2301/2330 train_time:138169ms step_avg:60.05ms
step:2302/2330 train_time:138232ms step_avg:60.05ms
step:2303/2330 train_time:138291ms step_avg:60.05ms
step:2304/2330 train_time:138354ms step_avg:60.05ms
step:2305/2330 train_time:138412ms step_avg:60.05ms
step:2306/2330 train_time:138475ms step_avg:60.05ms
step:2307/2330 train_time:138533ms step_avg:60.05ms
step:2308/2330 train_time:138596ms step_avg:60.05ms
step:2309/2330 train_time:138654ms step_avg:60.05ms
step:2310/2330 train_time:138716ms step_avg:60.05ms
step:2311/2330 train_time:138774ms step_avg:60.05ms
step:2312/2330 train_time:138837ms step_avg:60.05ms
step:2313/2330 train_time:138895ms step_avg:60.05ms
step:2314/2330 train_time:138959ms step_avg:60.05ms
step:2315/2330 train_time:139016ms step_avg:60.05ms
step:2316/2330 train_time:139079ms step_avg:60.05ms
step:2317/2330 train_time:139137ms step_avg:60.05ms
step:2318/2330 train_time:139200ms step_avg:60.05ms
step:2319/2330 train_time:139257ms step_avg:60.05ms
step:2320/2330 train_time:139321ms step_avg:60.05ms
step:2321/2330 train_time:139378ms step_avg:60.05ms
step:2322/2330 train_time:139441ms step_avg:60.05ms
step:2323/2330 train_time:139498ms step_avg:60.05ms
step:2324/2330 train_time:139561ms step_avg:60.05ms
step:2325/2330 train_time:139619ms step_avg:60.05ms
step:2326/2330 train_time:139681ms step_avg:60.05ms
step:2327/2330 train_time:139740ms step_avg:60.05ms
step:2328/2330 train_time:139801ms step_avg:60.05ms
step:2329/2330 train_time:139859ms step_avg:60.05ms
step:2330/2330 train_time:139921ms step_avg:60.05ms
step:2330/2330 val_loss:3.7909 train_time:140001ms step_avg:60.09ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
