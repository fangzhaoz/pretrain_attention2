import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "adam_lr3e-3"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=1e-1, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.Adam(
    hidden_matrix_params + gate_params,
    lr=3e-3,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 00:24:25 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   29C    P0             110W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   28C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   27C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   27C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   28C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   27C    P0             108W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:86ms step_avg:86.46ms
step:2/2330 train_time:207ms step_avg:103.42ms
step:3/2330 train_time:227ms step_avg:75.54ms
step:4/2330 train_time:248ms step_avg:62.03ms
step:5/2330 train_time:303ms step_avg:60.65ms
step:6/2330 train_time:362ms step_avg:60.35ms
step:7/2330 train_time:417ms step_avg:59.61ms
step:8/2330 train_time:477ms step_avg:59.63ms
step:9/2330 train_time:532ms step_avg:59.16ms
step:10/2330 train_time:591ms step_avg:59.15ms
step:11/2330 train_time:647ms step_avg:58.80ms
step:12/2330 train_time:706ms step_avg:58.82ms
step:13/2330 train_time:761ms step_avg:58.54ms
step:14/2330 train_time:820ms step_avg:58.57ms
step:15/2330 train_time:875ms step_avg:58.33ms
step:16/2330 train_time:934ms step_avg:58.37ms
step:17/2330 train_time:989ms step_avg:58.17ms
step:18/2330 train_time:1048ms step_avg:58.24ms
step:19/2330 train_time:1104ms step_avg:58.10ms
step:20/2330 train_time:1164ms step_avg:58.19ms
step:21/2330 train_time:1219ms step_avg:58.05ms
step:22/2330 train_time:1282ms step_avg:58.26ms
step:23/2330 train_time:1337ms step_avg:58.14ms
step:24/2330 train_time:1399ms step_avg:58.29ms
step:25/2330 train_time:1455ms step_avg:58.18ms
step:26/2330 train_time:1515ms step_avg:58.28ms
step:27/2330 train_time:1571ms step_avg:58.19ms
step:28/2330 train_time:1632ms step_avg:58.28ms
step:29/2330 train_time:1687ms step_avg:58.18ms
step:30/2330 train_time:1747ms step_avg:58.22ms
step:31/2330 train_time:1802ms step_avg:58.13ms
step:32/2330 train_time:1862ms step_avg:58.20ms
step:33/2330 train_time:1918ms step_avg:58.12ms
step:34/2330 train_time:1979ms step_avg:58.20ms
step:35/2330 train_time:2034ms step_avg:58.12ms
step:36/2330 train_time:2095ms step_avg:58.20ms
step:37/2330 train_time:2152ms step_avg:58.16ms
step:38/2330 train_time:2212ms step_avg:58.22ms
step:39/2330 train_time:2269ms step_avg:58.18ms
step:40/2330 train_time:2329ms step_avg:58.22ms
step:41/2330 train_time:2384ms step_avg:58.15ms
step:42/2330 train_time:2445ms step_avg:58.21ms
step:43/2330 train_time:2500ms step_avg:58.14ms
step:44/2330 train_time:2562ms step_avg:58.22ms
step:45/2330 train_time:2617ms step_avg:58.16ms
step:46/2330 train_time:2679ms step_avg:58.24ms
step:47/2330 train_time:2735ms step_avg:58.18ms
step:48/2330 train_time:2796ms step_avg:58.25ms
step:49/2330 train_time:2852ms step_avg:58.20ms
step:50/2330 train_time:2913ms step_avg:58.26ms
step:51/2330 train_time:2969ms step_avg:58.21ms
step:52/2330 train_time:3028ms step_avg:58.24ms
step:53/2330 train_time:3084ms step_avg:58.19ms
step:54/2330 train_time:3144ms step_avg:58.22ms
step:55/2330 train_time:3200ms step_avg:58.17ms
step:56/2330 train_time:3261ms step_avg:58.23ms
step:57/2330 train_time:3317ms step_avg:58.19ms
step:58/2330 train_time:3378ms step_avg:58.24ms
step:59/2330 train_time:3434ms step_avg:58.20ms
step:60/2330 train_time:3495ms step_avg:58.25ms
step:61/2330 train_time:3551ms step_avg:58.22ms
step:62/2330 train_time:3611ms step_avg:58.24ms
step:63/2330 train_time:3667ms step_avg:58.20ms
step:64/2330 train_time:3727ms step_avg:58.24ms
step:65/2330 train_time:3783ms step_avg:58.20ms
step:66/2330 train_time:3843ms step_avg:58.23ms
step:67/2330 train_time:3899ms step_avg:58.20ms
step:68/2330 train_time:3961ms step_avg:58.25ms
step:69/2330 train_time:4017ms step_avg:58.21ms
step:70/2330 train_time:4077ms step_avg:58.25ms
step:71/2330 train_time:4133ms step_avg:58.22ms
step:72/2330 train_time:4194ms step_avg:58.25ms
step:73/2330 train_time:4251ms step_avg:58.23ms
step:74/2330 train_time:4311ms step_avg:58.25ms
step:75/2330 train_time:4367ms step_avg:58.23ms
step:76/2330 train_time:4427ms step_avg:58.25ms
step:77/2330 train_time:4482ms step_avg:58.21ms
step:78/2330 train_time:4543ms step_avg:58.24ms
step:79/2330 train_time:4599ms step_avg:58.21ms
step:80/2330 train_time:4660ms step_avg:58.25ms
step:81/2330 train_time:4716ms step_avg:58.22ms
step:82/2330 train_time:4777ms step_avg:58.26ms
step:83/2330 train_time:4833ms step_avg:58.23ms
step:84/2330 train_time:4894ms step_avg:58.26ms
step:85/2330 train_time:4950ms step_avg:58.23ms
step:86/2330 train_time:5010ms step_avg:58.25ms
step:87/2330 train_time:5065ms step_avg:58.22ms
step:88/2330 train_time:5126ms step_avg:58.25ms
step:89/2330 train_time:5182ms step_avg:58.22ms
step:90/2330 train_time:5243ms step_avg:58.25ms
step:91/2330 train_time:5299ms step_avg:58.23ms
step:92/2330 train_time:5361ms step_avg:58.28ms
step:93/2330 train_time:5417ms step_avg:58.25ms
step:94/2330 train_time:5478ms step_avg:58.27ms
step:95/2330 train_time:5534ms step_avg:58.25ms
step:96/2330 train_time:5595ms step_avg:58.28ms
step:97/2330 train_time:5651ms step_avg:58.26ms
step:98/2330 train_time:5711ms step_avg:58.28ms
step:99/2330 train_time:5767ms step_avg:58.25ms
step:100/2330 train_time:5828ms step_avg:58.28ms
step:101/2330 train_time:5884ms step_avg:58.25ms
step:102/2330 train_time:5944ms step_avg:58.27ms
step:103/2330 train_time:6000ms step_avg:58.25ms
step:104/2330 train_time:6061ms step_avg:58.27ms
step:105/2330 train_time:6116ms step_avg:58.25ms
step:106/2330 train_time:6178ms step_avg:58.29ms
step:107/2330 train_time:6234ms step_avg:58.26ms
step:108/2330 train_time:6295ms step_avg:58.29ms
step:109/2330 train_time:6351ms step_avg:58.26ms
step:110/2330 train_time:6411ms step_avg:58.28ms
step:111/2330 train_time:6467ms step_avg:58.26ms
step:112/2330 train_time:6528ms step_avg:58.28ms
step:113/2330 train_time:6583ms step_avg:58.26ms
step:114/2330 train_time:6644ms step_avg:58.28ms
step:115/2330 train_time:6699ms step_avg:58.26ms
step:116/2330 train_time:6761ms step_avg:58.28ms
step:117/2330 train_time:6816ms step_avg:58.26ms
step:118/2330 train_time:6878ms step_avg:58.29ms
step:119/2330 train_time:6934ms step_avg:58.27ms
step:120/2330 train_time:6995ms step_avg:58.29ms
step:121/2330 train_time:7051ms step_avg:58.28ms
step:122/2330 train_time:7111ms step_avg:58.29ms
step:123/2330 train_time:7167ms step_avg:58.27ms
step:124/2330 train_time:7227ms step_avg:58.28ms
step:125/2330 train_time:7283ms step_avg:58.26ms
step:126/2330 train_time:7344ms step_avg:58.28ms
step:127/2330 train_time:7399ms step_avg:58.26ms
step:128/2330 train_time:7460ms step_avg:58.28ms
step:129/2330 train_time:7516ms step_avg:58.26ms
step:130/2330 train_time:7578ms step_avg:58.29ms
step:131/2330 train_time:7634ms step_avg:58.27ms
step:132/2330 train_time:7695ms step_avg:58.29ms
step:133/2330 train_time:7751ms step_avg:58.28ms
step:134/2330 train_time:7812ms step_avg:58.30ms
step:135/2330 train_time:7868ms step_avg:58.28ms
step:136/2330 train_time:7928ms step_avg:58.29ms
step:137/2330 train_time:7984ms step_avg:58.28ms
step:138/2330 train_time:8044ms step_avg:58.29ms
step:139/2330 train_time:8100ms step_avg:58.27ms
step:140/2330 train_time:8161ms step_avg:58.29ms
step:141/2330 train_time:8217ms step_avg:58.27ms
step:142/2330 train_time:8278ms step_avg:58.29ms
step:143/2330 train_time:8334ms step_avg:58.28ms
step:144/2330 train_time:8395ms step_avg:58.30ms
step:145/2330 train_time:8451ms step_avg:58.28ms
step:146/2330 train_time:8511ms step_avg:58.30ms
step:147/2330 train_time:8567ms step_avg:58.28ms
step:148/2330 train_time:8628ms step_avg:58.30ms
step:149/2330 train_time:8684ms step_avg:58.28ms
step:150/2330 train_time:8745ms step_avg:58.30ms
step:151/2330 train_time:8801ms step_avg:58.28ms
step:152/2330 train_time:8861ms step_avg:58.30ms
step:153/2330 train_time:8917ms step_avg:58.28ms
step:154/2330 train_time:8978ms step_avg:58.30ms
step:155/2330 train_time:9035ms step_avg:58.29ms
step:156/2330 train_time:9095ms step_avg:58.30ms
step:157/2330 train_time:9151ms step_avg:58.29ms
step:158/2330 train_time:9211ms step_avg:58.30ms
step:159/2330 train_time:9268ms step_avg:58.29ms
step:160/2330 train_time:9328ms step_avg:58.30ms
step:161/2330 train_time:9383ms step_avg:58.28ms
step:162/2330 train_time:9444ms step_avg:58.29ms
step:163/2330 train_time:9500ms step_avg:58.28ms
step:164/2330 train_time:9561ms step_avg:58.30ms
step:165/2330 train_time:9617ms step_avg:58.28ms
step:166/2330 train_time:9679ms step_avg:58.31ms
step:167/2330 train_time:9734ms step_avg:58.29ms
step:168/2330 train_time:9796ms step_avg:58.31ms
step:169/2330 train_time:9852ms step_avg:58.30ms
step:170/2330 train_time:9913ms step_avg:58.31ms
step:171/2330 train_time:9969ms step_avg:58.30ms
step:172/2330 train_time:10029ms step_avg:58.31ms
step:173/2330 train_time:10085ms step_avg:58.30ms
step:174/2330 train_time:10145ms step_avg:58.30ms
step:175/2330 train_time:10201ms step_avg:58.29ms
step:176/2330 train_time:10261ms step_avg:58.30ms
step:177/2330 train_time:10317ms step_avg:58.29ms
step:178/2330 train_time:10378ms step_avg:58.30ms
step:179/2330 train_time:10435ms step_avg:58.29ms
step:180/2330 train_time:10495ms step_avg:58.30ms
step:181/2330 train_time:10551ms step_avg:58.29ms
step:182/2330 train_time:10612ms step_avg:58.31ms
step:183/2330 train_time:10669ms step_avg:58.30ms
step:184/2330 train_time:10729ms step_avg:58.31ms
step:185/2330 train_time:10784ms step_avg:58.29ms
step:186/2330 train_time:10845ms step_avg:58.31ms
step:187/2330 train_time:10901ms step_avg:58.29ms
step:188/2330 train_time:10961ms step_avg:58.30ms
step:189/2330 train_time:11017ms step_avg:58.29ms
step:190/2330 train_time:11078ms step_avg:58.31ms
step:191/2330 train_time:11134ms step_avg:58.29ms
step:192/2330 train_time:11196ms step_avg:58.31ms
step:193/2330 train_time:11253ms step_avg:58.31ms
step:194/2330 train_time:11313ms step_avg:58.31ms
step:195/2330 train_time:11369ms step_avg:58.30ms
step:196/2330 train_time:11429ms step_avg:58.31ms
step:197/2330 train_time:11486ms step_avg:58.30ms
step:198/2330 train_time:11545ms step_avg:58.31ms
step:199/2330 train_time:11601ms step_avg:58.30ms
step:200/2330 train_time:11661ms step_avg:58.31ms
step:201/2330 train_time:11717ms step_avg:58.30ms
step:202/2330 train_time:11779ms step_avg:58.31ms
step:203/2330 train_time:11835ms step_avg:58.30ms
step:204/2330 train_time:11897ms step_avg:58.32ms
step:205/2330 train_time:11952ms step_avg:58.30ms
step:206/2330 train_time:12013ms step_avg:58.32ms
step:207/2330 train_time:12069ms step_avg:58.31ms
step:208/2330 train_time:12130ms step_avg:58.32ms
step:209/2330 train_time:12186ms step_avg:58.31ms
step:210/2330 train_time:12246ms step_avg:58.32ms
step:211/2330 train_time:12302ms step_avg:58.30ms
step:212/2330 train_time:12362ms step_avg:58.31ms
step:213/2330 train_time:12418ms step_avg:58.30ms
step:214/2330 train_time:12480ms step_avg:58.32ms
step:215/2330 train_time:12536ms step_avg:58.31ms
step:216/2330 train_time:12596ms step_avg:58.32ms
step:217/2330 train_time:12653ms step_avg:58.31ms
step:218/2330 train_time:12714ms step_avg:58.32ms
step:219/2330 train_time:12770ms step_avg:58.31ms
step:220/2330 train_time:12830ms step_avg:58.32ms
step:221/2330 train_time:12886ms step_avg:58.31ms
step:222/2330 train_time:12946ms step_avg:58.31ms
step:223/2330 train_time:13002ms step_avg:58.30ms
step:224/2330 train_time:13063ms step_avg:58.32ms
step:225/2330 train_time:13118ms step_avg:58.30ms
step:226/2330 train_time:13180ms step_avg:58.32ms
step:227/2330 train_time:13236ms step_avg:58.31ms
step:228/2330 train_time:13297ms step_avg:58.32ms
step:229/2330 train_time:13354ms step_avg:58.31ms
step:230/2330 train_time:13414ms step_avg:58.32ms
step:231/2330 train_time:13470ms step_avg:58.31ms
step:232/2330 train_time:13530ms step_avg:58.32ms
step:233/2330 train_time:13586ms step_avg:58.31ms
step:234/2330 train_time:13647ms step_avg:58.32ms
step:235/2330 train_time:13703ms step_avg:58.31ms
step:236/2330 train_time:13763ms step_avg:58.32ms
step:237/2330 train_time:13819ms step_avg:58.31ms
step:238/2330 train_time:13879ms step_avg:58.32ms
step:239/2330 train_time:13935ms step_avg:58.31ms
step:240/2330 train_time:13996ms step_avg:58.32ms
step:241/2330 train_time:14052ms step_avg:58.31ms
step:242/2330 train_time:14113ms step_avg:58.32ms
step:243/2330 train_time:14169ms step_avg:58.31ms
step:244/2330 train_time:14230ms step_avg:58.32ms
step:245/2330 train_time:14285ms step_avg:58.31ms
step:246/2330 train_time:14346ms step_avg:58.32ms
step:247/2330 train_time:14402ms step_avg:58.31ms
step:248/2330 train_time:14464ms step_avg:58.32ms
step:249/2330 train_time:14520ms step_avg:58.31ms
step:250/2330 train_time:14580ms step_avg:58.32ms
step:250/2330 val_loss:5.0153 train_time:14658ms step_avg:58.63ms
step:251/2330 train_time:14678ms step_avg:58.48ms
step:252/2330 train_time:14700ms step_avg:58.33ms
step:253/2330 train_time:14756ms step_avg:58.32ms
step:254/2330 train_time:14823ms step_avg:58.36ms
step:255/2330 train_time:14880ms step_avg:58.35ms
step:256/2330 train_time:14943ms step_avg:58.37ms
step:257/2330 train_time:14999ms step_avg:58.36ms
step:258/2330 train_time:15060ms step_avg:58.37ms
step:259/2330 train_time:15116ms step_avg:58.36ms
step:260/2330 train_time:15176ms step_avg:58.37ms
step:261/2330 train_time:15233ms step_avg:58.36ms
step:262/2330 train_time:15292ms step_avg:58.37ms
step:263/2330 train_time:15348ms step_avg:58.36ms
step:264/2330 train_time:15407ms step_avg:58.36ms
step:265/2330 train_time:15462ms step_avg:58.35ms
step:266/2330 train_time:15522ms step_avg:58.35ms
step:267/2330 train_time:15578ms step_avg:58.34ms
step:268/2330 train_time:15638ms step_avg:58.35ms
step:269/2330 train_time:15694ms step_avg:58.34ms
step:270/2330 train_time:15755ms step_avg:58.35ms
step:271/2330 train_time:15812ms step_avg:58.35ms
step:272/2330 train_time:15875ms step_avg:58.36ms
step:273/2330 train_time:15931ms step_avg:58.36ms
step:274/2330 train_time:15993ms step_avg:58.37ms
step:275/2330 train_time:16049ms step_avg:58.36ms
step:276/2330 train_time:16110ms step_avg:58.37ms
step:277/2330 train_time:16166ms step_avg:58.36ms
step:278/2330 train_time:16226ms step_avg:58.37ms
step:279/2330 train_time:16281ms step_avg:58.36ms
step:280/2330 train_time:16342ms step_avg:58.36ms
step:281/2330 train_time:16398ms step_avg:58.35ms
step:282/2330 train_time:16458ms step_avg:58.36ms
step:283/2330 train_time:16514ms step_avg:58.35ms
step:284/2330 train_time:16574ms step_avg:58.36ms
step:285/2330 train_time:16630ms step_avg:58.35ms
step:286/2330 train_time:16690ms step_avg:58.36ms
step:287/2330 train_time:16746ms step_avg:58.35ms
step:288/2330 train_time:16808ms step_avg:58.36ms
step:289/2330 train_time:16863ms step_avg:58.35ms
step:290/2330 train_time:16925ms step_avg:58.36ms
step:291/2330 train_time:16981ms step_avg:58.35ms
step:292/2330 train_time:17042ms step_avg:58.36ms
step:293/2330 train_time:17097ms step_avg:58.35ms
step:294/2330 train_time:17158ms step_avg:58.36ms
step:295/2330 train_time:17214ms step_avg:58.35ms
step:296/2330 train_time:17276ms step_avg:58.37ms
step:297/2330 train_time:17332ms step_avg:58.36ms
step:298/2330 train_time:17393ms step_avg:58.36ms
step:299/2330 train_time:17449ms step_avg:58.36ms
step:300/2330 train_time:17509ms step_avg:58.36ms
step:301/2330 train_time:17565ms step_avg:58.35ms
step:302/2330 train_time:17625ms step_avg:58.36ms
step:303/2330 train_time:17680ms step_avg:58.35ms
step:304/2330 train_time:17741ms step_avg:58.36ms
step:305/2330 train_time:17797ms step_avg:58.35ms
step:306/2330 train_time:17858ms step_avg:58.36ms
step:307/2330 train_time:17913ms step_avg:58.35ms
step:308/2330 train_time:17976ms step_avg:58.36ms
step:309/2330 train_time:18032ms step_avg:58.36ms
step:310/2330 train_time:18094ms step_avg:58.37ms
step:311/2330 train_time:18150ms step_avg:58.36ms
step:312/2330 train_time:18212ms step_avg:58.37ms
step:313/2330 train_time:18268ms step_avg:58.36ms
step:314/2330 train_time:18328ms step_avg:58.37ms
step:315/2330 train_time:18384ms step_avg:58.36ms
step:316/2330 train_time:18445ms step_avg:58.37ms
step:317/2330 train_time:18510ms step_avg:58.39ms
step:318/2330 train_time:18561ms step_avg:58.37ms
step:319/2330 train_time:18617ms step_avg:58.36ms
step:320/2330 train_time:18677ms step_avg:58.37ms
step:321/2330 train_time:18734ms step_avg:58.36ms
step:322/2330 train_time:18795ms step_avg:58.37ms
step:323/2330 train_time:18851ms step_avg:58.36ms
step:324/2330 train_time:18911ms step_avg:58.37ms
step:325/2330 train_time:18967ms step_avg:58.36ms
step:326/2330 train_time:19028ms step_avg:58.37ms
step:327/2330 train_time:19084ms step_avg:58.36ms
step:328/2330 train_time:19145ms step_avg:58.37ms
step:329/2330 train_time:19201ms step_avg:58.36ms
step:330/2330 train_time:19261ms step_avg:58.37ms
step:331/2330 train_time:19317ms step_avg:58.36ms
step:332/2330 train_time:19379ms step_avg:58.37ms
step:333/2330 train_time:19435ms step_avg:58.36ms
step:334/2330 train_time:19497ms step_avg:58.37ms
step:335/2330 train_time:19553ms step_avg:58.37ms
step:336/2330 train_time:19613ms step_avg:58.37ms
step:337/2330 train_time:19670ms step_avg:58.37ms
step:338/2330 train_time:19730ms step_avg:58.37ms
step:339/2330 train_time:19786ms step_avg:58.37ms
step:340/2330 train_time:19846ms step_avg:58.37ms
step:341/2330 train_time:19902ms step_avg:58.36ms
step:342/2330 train_time:19962ms step_avg:58.37ms
step:343/2330 train_time:20018ms step_avg:58.36ms
step:344/2330 train_time:20080ms step_avg:58.37ms
step:345/2330 train_time:20136ms step_avg:58.37ms
step:346/2330 train_time:20197ms step_avg:58.37ms
step:347/2330 train_time:20253ms step_avg:58.37ms
step:348/2330 train_time:20314ms step_avg:58.37ms
step:349/2330 train_time:20371ms step_avg:58.37ms
step:350/2330 train_time:20432ms step_avg:58.38ms
step:351/2330 train_time:20489ms step_avg:58.37ms
step:352/2330 train_time:20550ms step_avg:58.38ms
step:353/2330 train_time:20606ms step_avg:58.37ms
step:354/2330 train_time:20666ms step_avg:58.38ms
step:355/2330 train_time:20722ms step_avg:58.37ms
step:356/2330 train_time:20782ms step_avg:58.38ms
step:357/2330 train_time:20838ms step_avg:58.37ms
step:358/2330 train_time:20899ms step_avg:58.38ms
step:359/2330 train_time:20955ms step_avg:58.37ms
step:360/2330 train_time:21017ms step_avg:58.38ms
step:361/2330 train_time:21074ms step_avg:58.38ms
step:362/2330 train_time:21134ms step_avg:58.38ms
step:363/2330 train_time:21191ms step_avg:58.38ms
step:364/2330 train_time:21251ms step_avg:58.38ms
step:365/2330 train_time:21307ms step_avg:58.38ms
step:366/2330 train_time:21367ms step_avg:58.38ms
step:367/2330 train_time:21423ms step_avg:58.37ms
step:368/2330 train_time:21484ms step_avg:58.38ms
step:369/2330 train_time:21540ms step_avg:58.37ms
step:370/2330 train_time:21600ms step_avg:58.38ms
step:371/2330 train_time:21656ms step_avg:58.37ms
step:372/2330 train_time:21717ms step_avg:58.38ms
step:373/2330 train_time:21773ms step_avg:58.37ms
step:374/2330 train_time:21836ms step_avg:58.38ms
step:375/2330 train_time:21892ms step_avg:58.38ms
step:376/2330 train_time:21955ms step_avg:58.39ms
step:377/2330 train_time:22011ms step_avg:58.38ms
step:378/2330 train_time:22072ms step_avg:58.39ms
step:379/2330 train_time:22128ms step_avg:58.39ms
step:380/2330 train_time:22189ms step_avg:58.39ms
step:381/2330 train_time:22245ms step_avg:58.39ms
step:382/2330 train_time:22306ms step_avg:58.39ms
step:383/2330 train_time:22362ms step_avg:58.39ms
step:384/2330 train_time:22422ms step_avg:58.39ms
step:385/2330 train_time:22479ms step_avg:58.39ms
step:386/2330 train_time:22539ms step_avg:58.39ms
step:387/2330 train_time:22595ms step_avg:58.39ms
step:388/2330 train_time:22656ms step_avg:58.39ms
step:389/2330 train_time:22712ms step_avg:58.38ms
step:390/2330 train_time:22773ms step_avg:58.39ms
step:391/2330 train_time:22829ms step_avg:58.39ms
step:392/2330 train_time:22890ms step_avg:58.39ms
step:393/2330 train_time:22946ms step_avg:58.39ms
step:394/2330 train_time:23007ms step_avg:58.39ms
step:395/2330 train_time:23062ms step_avg:58.39ms
step:396/2330 train_time:23124ms step_avg:58.39ms
step:397/2330 train_time:23180ms step_avg:58.39ms
step:398/2330 train_time:23240ms step_avg:58.39ms
step:399/2330 train_time:23296ms step_avg:58.39ms
step:400/2330 train_time:23358ms step_avg:58.39ms
step:401/2330 train_time:23414ms step_avg:58.39ms
step:402/2330 train_time:23475ms step_avg:58.39ms
step:403/2330 train_time:23531ms step_avg:58.39ms
step:404/2330 train_time:23592ms step_avg:58.40ms
step:405/2330 train_time:23648ms step_avg:58.39ms
step:406/2330 train_time:23709ms step_avg:58.40ms
step:407/2330 train_time:23764ms step_avg:58.39ms
step:408/2330 train_time:23824ms step_avg:58.39ms
step:409/2330 train_time:23880ms step_avg:58.39ms
step:410/2330 train_time:23941ms step_avg:58.39ms
step:411/2330 train_time:23998ms step_avg:58.39ms
step:412/2330 train_time:24059ms step_avg:58.39ms
step:413/2330 train_time:24114ms step_avg:58.39ms
step:414/2330 train_time:24177ms step_avg:58.40ms
step:415/2330 train_time:24234ms step_avg:58.39ms
step:416/2330 train_time:24294ms step_avg:58.40ms
step:417/2330 train_time:24351ms step_avg:58.40ms
step:418/2330 train_time:24410ms step_avg:58.40ms
step:419/2330 train_time:24466ms step_avg:58.39ms
step:420/2330 train_time:24526ms step_avg:58.40ms
step:421/2330 train_time:24583ms step_avg:58.39ms
step:422/2330 train_time:24642ms step_avg:58.39ms
step:423/2330 train_time:24699ms step_avg:58.39ms
step:424/2330 train_time:24759ms step_avg:58.39ms
step:425/2330 train_time:24815ms step_avg:58.39ms
step:426/2330 train_time:24877ms step_avg:58.40ms
step:427/2330 train_time:24933ms step_avg:58.39ms
step:428/2330 train_time:24995ms step_avg:58.40ms
step:429/2330 train_time:25051ms step_avg:58.39ms
step:430/2330 train_time:25112ms step_avg:58.40ms
step:431/2330 train_time:25168ms step_avg:58.39ms
step:432/2330 train_time:25229ms step_avg:58.40ms
step:433/2330 train_time:25286ms step_avg:58.40ms
step:434/2330 train_time:25346ms step_avg:58.40ms
step:435/2330 train_time:25402ms step_avg:58.39ms
step:436/2330 train_time:25463ms step_avg:58.40ms
step:437/2330 train_time:25519ms step_avg:58.40ms
step:438/2330 train_time:25579ms step_avg:58.40ms
step:439/2330 train_time:25635ms step_avg:58.39ms
step:440/2330 train_time:25697ms step_avg:58.40ms
step:441/2330 train_time:25753ms step_avg:58.40ms
step:442/2330 train_time:25815ms step_avg:58.40ms
step:443/2330 train_time:25871ms step_avg:58.40ms
step:444/2330 train_time:25932ms step_avg:58.40ms
step:445/2330 train_time:25988ms step_avg:58.40ms
step:446/2330 train_time:26049ms step_avg:58.40ms
step:447/2330 train_time:26104ms step_avg:58.40ms
step:448/2330 train_time:26165ms step_avg:58.40ms
step:449/2330 train_time:26221ms step_avg:58.40ms
step:450/2330 train_time:26283ms step_avg:58.41ms
step:451/2330 train_time:26339ms step_avg:58.40ms
step:452/2330 train_time:26400ms step_avg:58.41ms
step:453/2330 train_time:26455ms step_avg:58.40ms
step:454/2330 train_time:26517ms step_avg:58.41ms
step:455/2330 train_time:26573ms step_avg:58.40ms
step:456/2330 train_time:26633ms step_avg:58.41ms
step:457/2330 train_time:26690ms step_avg:58.40ms
step:458/2330 train_time:26752ms step_avg:58.41ms
step:459/2330 train_time:26808ms step_avg:58.41ms
step:460/2330 train_time:26869ms step_avg:58.41ms
step:461/2330 train_time:26925ms step_avg:58.40ms
step:462/2330 train_time:26985ms step_avg:58.41ms
step:463/2330 train_time:27042ms step_avg:58.40ms
step:464/2330 train_time:27101ms step_avg:58.41ms
step:465/2330 train_time:27157ms step_avg:58.40ms
step:466/2330 train_time:27219ms step_avg:58.41ms
step:467/2330 train_time:27275ms step_avg:58.40ms
step:468/2330 train_time:27337ms step_avg:58.41ms
step:469/2330 train_time:27393ms step_avg:58.41ms
step:470/2330 train_time:27454ms step_avg:58.41ms
step:471/2330 train_time:27511ms step_avg:58.41ms
step:472/2330 train_time:27571ms step_avg:58.41ms
step:473/2330 train_time:27627ms step_avg:58.41ms
step:474/2330 train_time:27687ms step_avg:58.41ms
step:475/2330 train_time:27743ms step_avg:58.41ms
step:476/2330 train_time:27805ms step_avg:58.41ms
step:477/2330 train_time:27861ms step_avg:58.41ms
step:478/2330 train_time:27922ms step_avg:58.41ms
step:479/2330 train_time:27978ms step_avg:58.41ms
step:480/2330 train_time:28039ms step_avg:58.41ms
step:481/2330 train_time:28095ms step_avg:58.41ms
step:482/2330 train_time:28157ms step_avg:58.42ms
step:483/2330 train_time:28213ms step_avg:58.41ms
step:484/2330 train_time:28274ms step_avg:58.42ms
step:485/2330 train_time:28330ms step_avg:58.41ms
step:486/2330 train_time:28391ms step_avg:58.42ms
step:487/2330 train_time:28447ms step_avg:58.41ms
step:488/2330 train_time:28507ms step_avg:58.42ms
step:489/2330 train_time:28563ms step_avg:58.41ms
step:490/2330 train_time:28624ms step_avg:58.42ms
step:491/2330 train_time:28680ms step_avg:58.41ms
step:492/2330 train_time:28742ms step_avg:58.42ms
step:493/2330 train_time:28798ms step_avg:58.41ms
step:494/2330 train_time:28859ms step_avg:58.42ms
step:495/2330 train_time:28915ms step_avg:58.41ms
step:496/2330 train_time:28976ms step_avg:58.42ms
step:497/2330 train_time:29032ms step_avg:58.41ms
step:498/2330 train_time:29095ms step_avg:58.42ms
step:499/2330 train_time:29151ms step_avg:58.42ms
step:500/2330 train_time:29212ms step_avg:58.42ms
step:500/2330 val_loss:4.4947 train_time:29289ms step_avg:58.58ms
step:501/2330 train_time:29310ms step_avg:58.50ms
step:502/2330 train_time:29332ms step_avg:58.43ms
step:503/2330 train_time:29389ms step_avg:58.43ms
step:504/2330 train_time:29452ms step_avg:58.44ms
step:505/2330 train_time:29508ms step_avg:58.43ms
step:506/2330 train_time:29571ms step_avg:58.44ms
step:507/2330 train_time:29627ms step_avg:58.44ms
step:508/2330 train_time:29688ms step_avg:58.44ms
step:509/2330 train_time:29744ms step_avg:58.44ms
step:510/2330 train_time:29804ms step_avg:58.44ms
step:511/2330 train_time:29860ms step_avg:58.43ms
step:512/2330 train_time:29920ms step_avg:58.44ms
step:513/2330 train_time:29976ms step_avg:58.43ms
step:514/2330 train_time:30036ms step_avg:58.44ms
step:515/2330 train_time:30092ms step_avg:58.43ms
step:516/2330 train_time:30153ms step_avg:58.44ms
step:517/2330 train_time:30209ms step_avg:58.43ms
step:518/2330 train_time:30269ms step_avg:58.43ms
step:519/2330 train_time:30325ms step_avg:58.43ms
step:520/2330 train_time:30387ms step_avg:58.44ms
step:521/2330 train_time:30443ms step_avg:58.43ms
step:522/2330 train_time:30507ms step_avg:58.44ms
step:523/2330 train_time:30563ms step_avg:58.44ms
step:524/2330 train_time:30625ms step_avg:58.44ms
step:525/2330 train_time:30681ms step_avg:58.44ms
step:526/2330 train_time:30742ms step_avg:58.44ms
step:527/2330 train_time:30798ms step_avg:58.44ms
step:528/2330 train_time:30858ms step_avg:58.44ms
step:529/2330 train_time:30914ms step_avg:58.44ms
step:530/2330 train_time:30975ms step_avg:58.44ms
step:531/2330 train_time:31031ms step_avg:58.44ms
step:532/2330 train_time:31091ms step_avg:58.44ms
step:533/2330 train_time:31146ms step_avg:58.44ms
step:534/2330 train_time:31207ms step_avg:58.44ms
step:535/2330 train_time:31262ms step_avg:58.43ms
step:536/2330 train_time:31324ms step_avg:58.44ms
step:537/2330 train_time:31381ms step_avg:58.44ms
step:538/2330 train_time:31444ms step_avg:58.45ms
step:539/2330 train_time:31501ms step_avg:58.44ms
step:540/2330 train_time:31563ms step_avg:58.45ms
step:541/2330 train_time:31619ms step_avg:58.45ms
step:542/2330 train_time:31680ms step_avg:58.45ms
step:543/2330 train_time:31737ms step_avg:58.45ms
step:544/2330 train_time:31797ms step_avg:58.45ms
step:545/2330 train_time:31854ms step_avg:58.45ms
step:546/2330 train_time:31914ms step_avg:58.45ms
step:547/2330 train_time:31970ms step_avg:58.45ms
step:548/2330 train_time:32030ms step_avg:58.45ms
step:549/2330 train_time:32087ms step_avg:58.45ms
step:550/2330 train_time:32146ms step_avg:58.45ms
step:551/2330 train_time:32202ms step_avg:58.44ms
step:552/2330 train_time:32264ms step_avg:58.45ms
step:553/2330 train_time:32320ms step_avg:58.44ms
step:554/2330 train_time:32381ms step_avg:58.45ms
step:555/2330 train_time:32438ms step_avg:58.45ms
step:556/2330 train_time:32499ms step_avg:58.45ms
step:557/2330 train_time:32556ms step_avg:58.45ms
step:558/2330 train_time:32617ms step_avg:58.45ms
step:559/2330 train_time:32673ms step_avg:58.45ms
step:560/2330 train_time:32734ms step_avg:58.45ms
step:561/2330 train_time:32790ms step_avg:58.45ms
step:562/2330 train_time:32851ms step_avg:58.45ms
step:563/2330 train_time:32907ms step_avg:58.45ms
step:564/2330 train_time:32967ms step_avg:58.45ms
step:565/2330 train_time:33023ms step_avg:58.45ms
step:566/2330 train_time:33085ms step_avg:58.45ms
step:567/2330 train_time:33141ms step_avg:58.45ms
step:568/2330 train_time:33201ms step_avg:58.45ms
step:569/2330 train_time:33257ms step_avg:58.45ms
step:570/2330 train_time:33318ms step_avg:58.45ms
step:571/2330 train_time:33374ms step_avg:58.45ms
step:572/2330 train_time:33435ms step_avg:58.45ms
step:573/2330 train_time:33491ms step_avg:58.45ms
step:574/2330 train_time:33552ms step_avg:58.45ms
step:575/2330 train_time:33608ms step_avg:58.45ms
step:576/2330 train_time:33669ms step_avg:58.45ms
step:577/2330 train_time:33726ms step_avg:58.45ms
step:578/2330 train_time:33786ms step_avg:58.45ms
step:579/2330 train_time:33843ms step_avg:58.45ms
step:580/2330 train_time:33904ms step_avg:58.46ms
step:581/2330 train_time:33961ms step_avg:58.45ms
step:582/2330 train_time:34021ms step_avg:58.46ms
step:583/2330 train_time:34078ms step_avg:58.45ms
step:584/2330 train_time:34139ms step_avg:58.46ms
step:585/2330 train_time:34195ms step_avg:58.45ms
step:586/2330 train_time:34255ms step_avg:58.46ms
step:587/2330 train_time:34311ms step_avg:58.45ms
step:588/2330 train_time:34372ms step_avg:58.46ms
step:589/2330 train_time:34428ms step_avg:58.45ms
step:590/2330 train_time:34489ms step_avg:58.46ms
step:591/2330 train_time:34545ms step_avg:58.45ms
step:592/2330 train_time:34606ms step_avg:58.46ms
step:593/2330 train_time:34662ms step_avg:58.45ms
step:594/2330 train_time:34725ms step_avg:58.46ms
step:595/2330 train_time:34781ms step_avg:58.46ms
step:596/2330 train_time:34843ms step_avg:58.46ms
step:597/2330 train_time:34899ms step_avg:58.46ms
step:598/2330 train_time:34960ms step_avg:58.46ms
step:599/2330 train_time:35017ms step_avg:58.46ms
step:600/2330 train_time:35079ms step_avg:58.46ms
step:601/2330 train_time:35135ms step_avg:58.46ms
step:602/2330 train_time:35195ms step_avg:58.46ms
step:603/2330 train_time:35252ms step_avg:58.46ms
step:604/2330 train_time:35312ms step_avg:58.46ms
step:605/2330 train_time:35368ms step_avg:58.46ms
step:606/2330 train_time:35429ms step_avg:58.46ms
step:607/2330 train_time:35485ms step_avg:58.46ms
step:608/2330 train_time:35545ms step_avg:58.46ms
step:609/2330 train_time:35601ms step_avg:58.46ms
step:610/2330 train_time:35663ms step_avg:58.46ms
step:611/2330 train_time:35720ms step_avg:58.46ms
step:612/2330 train_time:35780ms step_avg:58.46ms
step:613/2330 train_time:35836ms step_avg:58.46ms
step:614/2330 train_time:35897ms step_avg:58.46ms
step:615/2330 train_time:35954ms step_avg:58.46ms
step:616/2330 train_time:36015ms step_avg:58.47ms
step:617/2330 train_time:36071ms step_avg:58.46ms
step:618/2330 train_time:36131ms step_avg:58.46ms
step:619/2330 train_time:36187ms step_avg:58.46ms
step:620/2330 train_time:36247ms step_avg:58.46ms
step:621/2330 train_time:36303ms step_avg:58.46ms
step:622/2330 train_time:36365ms step_avg:58.46ms
step:623/2330 train_time:36421ms step_avg:58.46ms
step:624/2330 train_time:36483ms step_avg:58.47ms
step:625/2330 train_time:36540ms step_avg:58.46ms
step:626/2330 train_time:36601ms step_avg:58.47ms
step:627/2330 train_time:36657ms step_avg:58.46ms
step:628/2330 train_time:36718ms step_avg:58.47ms
step:629/2330 train_time:36775ms step_avg:58.47ms
step:630/2330 train_time:36835ms step_avg:58.47ms
step:631/2330 train_time:36892ms step_avg:58.47ms
step:632/2330 train_time:36953ms step_avg:58.47ms
step:633/2330 train_time:37009ms step_avg:58.47ms
step:634/2330 train_time:37070ms step_avg:58.47ms
step:635/2330 train_time:37126ms step_avg:58.47ms
step:636/2330 train_time:37187ms step_avg:58.47ms
step:637/2330 train_time:37243ms step_avg:58.47ms
step:638/2330 train_time:37305ms step_avg:58.47ms
step:639/2330 train_time:37361ms step_avg:58.47ms
step:640/2330 train_time:37422ms step_avg:58.47ms
step:641/2330 train_time:37477ms step_avg:58.47ms
step:642/2330 train_time:37539ms step_avg:58.47ms
step:643/2330 train_time:37595ms step_avg:58.47ms
step:644/2330 train_time:37657ms step_avg:58.47ms
step:645/2330 train_time:37713ms step_avg:58.47ms
step:646/2330 train_time:37774ms step_avg:58.47ms
step:647/2330 train_time:37831ms step_avg:58.47ms
step:648/2330 train_time:37891ms step_avg:58.47ms
step:649/2330 train_time:37948ms step_avg:58.47ms
step:650/2330 train_time:38008ms step_avg:58.47ms
step:651/2330 train_time:38065ms step_avg:58.47ms
step:652/2330 train_time:38125ms step_avg:58.47ms
step:653/2330 train_time:38182ms step_avg:58.47ms
step:654/2330 train_time:38243ms step_avg:58.47ms
step:655/2330 train_time:38299ms step_avg:58.47ms
step:656/2330 train_time:38360ms step_avg:58.48ms
step:657/2330 train_time:38416ms step_avg:58.47ms
step:658/2330 train_time:38478ms step_avg:58.48ms
step:659/2330 train_time:38534ms step_avg:58.47ms
step:660/2330 train_time:38595ms step_avg:58.48ms
step:661/2330 train_time:38651ms step_avg:58.47ms
step:662/2330 train_time:38712ms step_avg:58.48ms
step:663/2330 train_time:38768ms step_avg:58.47ms
step:664/2330 train_time:38829ms step_avg:58.48ms
step:665/2330 train_time:38886ms step_avg:58.47ms
step:666/2330 train_time:38946ms step_avg:58.48ms
step:667/2330 train_time:39002ms step_avg:58.47ms
step:668/2330 train_time:39064ms step_avg:58.48ms
step:669/2330 train_time:39121ms step_avg:58.48ms
step:670/2330 train_time:39182ms step_avg:58.48ms
step:671/2330 train_time:39238ms step_avg:58.48ms
step:672/2330 train_time:39299ms step_avg:58.48ms
step:673/2330 train_time:39355ms step_avg:58.48ms
step:674/2330 train_time:39415ms step_avg:58.48ms
step:675/2330 train_time:39471ms step_avg:58.48ms
step:676/2330 train_time:39533ms step_avg:58.48ms
step:677/2330 train_time:39589ms step_avg:58.48ms
step:678/2330 train_time:39649ms step_avg:58.48ms
step:679/2330 train_time:39705ms step_avg:58.48ms
step:680/2330 train_time:39766ms step_avg:58.48ms
step:681/2330 train_time:39822ms step_avg:58.48ms
step:682/2330 train_time:39884ms step_avg:58.48ms
step:683/2330 train_time:39941ms step_avg:58.48ms
step:684/2330 train_time:40001ms step_avg:58.48ms
step:685/2330 train_time:40058ms step_avg:58.48ms
step:686/2330 train_time:40119ms step_avg:58.48ms
step:687/2330 train_time:40176ms step_avg:58.48ms
step:688/2330 train_time:40236ms step_avg:58.48ms
step:689/2330 train_time:40292ms step_avg:58.48ms
step:690/2330 train_time:40354ms step_avg:58.48ms
step:691/2330 train_time:40410ms step_avg:58.48ms
step:692/2330 train_time:40471ms step_avg:58.48ms
step:693/2330 train_time:40527ms step_avg:58.48ms
step:694/2330 train_time:40588ms step_avg:58.48ms
step:695/2330 train_time:40644ms step_avg:58.48ms
step:696/2330 train_time:40705ms step_avg:58.48ms
step:697/2330 train_time:40761ms step_avg:58.48ms
step:698/2330 train_time:40823ms step_avg:58.49ms
step:699/2330 train_time:40880ms step_avg:58.48ms
step:700/2330 train_time:40941ms step_avg:58.49ms
step:701/2330 train_time:40997ms step_avg:58.48ms
step:702/2330 train_time:41058ms step_avg:58.49ms
step:703/2330 train_time:41114ms step_avg:58.48ms
step:704/2330 train_time:41175ms step_avg:58.49ms
step:705/2330 train_time:41232ms step_avg:58.48ms
step:706/2330 train_time:41293ms step_avg:58.49ms
step:707/2330 train_time:41348ms step_avg:58.48ms
step:708/2330 train_time:41410ms step_avg:58.49ms
step:709/2330 train_time:41466ms step_avg:58.48ms
step:710/2330 train_time:41527ms step_avg:58.49ms
step:711/2330 train_time:41583ms step_avg:58.49ms
step:712/2330 train_time:41644ms step_avg:58.49ms
step:713/2330 train_time:41700ms step_avg:58.49ms
step:714/2330 train_time:41761ms step_avg:58.49ms
step:715/2330 train_time:41818ms step_avg:58.49ms
step:716/2330 train_time:41878ms step_avg:58.49ms
step:717/2330 train_time:41934ms step_avg:58.49ms
step:718/2330 train_time:41996ms step_avg:58.49ms
step:719/2330 train_time:42052ms step_avg:58.49ms
step:720/2330 train_time:42112ms step_avg:58.49ms
step:721/2330 train_time:42169ms step_avg:58.49ms
step:722/2330 train_time:42230ms step_avg:58.49ms
step:723/2330 train_time:42286ms step_avg:58.49ms
step:724/2330 train_time:42347ms step_avg:58.49ms
step:725/2330 train_time:42403ms step_avg:58.49ms
step:726/2330 train_time:42464ms step_avg:58.49ms
step:727/2330 train_time:42521ms step_avg:58.49ms
step:728/2330 train_time:42583ms step_avg:58.49ms
step:729/2330 train_time:42639ms step_avg:58.49ms
step:730/2330 train_time:42701ms step_avg:58.49ms
step:731/2330 train_time:42757ms step_avg:58.49ms
step:732/2330 train_time:42817ms step_avg:58.49ms
step:733/2330 train_time:42874ms step_avg:58.49ms
step:734/2330 train_time:42935ms step_avg:58.49ms
step:735/2330 train_time:42991ms step_avg:58.49ms
step:736/2330 train_time:43052ms step_avg:58.49ms
step:737/2330 train_time:43108ms step_avg:58.49ms
step:738/2330 train_time:43169ms step_avg:58.50ms
step:739/2330 train_time:43226ms step_avg:58.49ms
step:740/2330 train_time:43287ms step_avg:58.50ms
step:741/2330 train_time:43343ms step_avg:58.49ms
step:742/2330 train_time:43404ms step_avg:58.50ms
step:743/2330 train_time:43461ms step_avg:58.49ms
step:744/2330 train_time:43521ms step_avg:58.50ms
step:745/2330 train_time:43578ms step_avg:58.49ms
step:746/2330 train_time:43639ms step_avg:58.50ms
step:747/2330 train_time:43696ms step_avg:58.50ms
step:748/2330 train_time:43756ms step_avg:58.50ms
step:749/2330 train_time:43812ms step_avg:58.49ms
step:750/2330 train_time:43873ms step_avg:58.50ms
step:750/2330 val_loss:4.2823 train_time:43951ms step_avg:58.60ms
step:751/2330 train_time:43971ms step_avg:58.55ms
step:752/2330 train_time:43994ms step_avg:58.50ms
step:753/2330 train_time:44049ms step_avg:58.50ms
step:754/2330 train_time:44116ms step_avg:58.51ms
step:755/2330 train_time:44172ms step_avg:58.51ms
step:756/2330 train_time:44232ms step_avg:58.51ms
step:757/2330 train_time:44288ms step_avg:58.50ms
step:758/2330 train_time:44351ms step_avg:58.51ms
step:759/2330 train_time:44407ms step_avg:58.51ms
step:760/2330 train_time:44469ms step_avg:58.51ms
step:761/2330 train_time:44524ms step_avg:58.51ms
step:762/2330 train_time:44585ms step_avg:58.51ms
step:763/2330 train_time:44641ms step_avg:58.51ms
step:764/2330 train_time:44702ms step_avg:58.51ms
step:765/2330 train_time:44758ms step_avg:58.51ms
step:766/2330 train_time:44818ms step_avg:58.51ms
step:767/2330 train_time:44874ms step_avg:58.51ms
step:768/2330 train_time:44935ms step_avg:58.51ms
step:769/2330 train_time:44993ms step_avg:58.51ms
step:770/2330 train_time:45055ms step_avg:58.51ms
step:771/2330 train_time:45113ms step_avg:58.51ms
step:772/2330 train_time:45174ms step_avg:58.52ms
step:773/2330 train_time:45232ms step_avg:58.52ms
step:774/2330 train_time:45293ms step_avg:58.52ms
step:775/2330 train_time:45351ms step_avg:58.52ms
step:776/2330 train_time:45412ms step_avg:58.52ms
step:777/2330 train_time:45469ms step_avg:58.52ms
step:778/2330 train_time:45531ms step_avg:58.52ms
step:779/2330 train_time:45587ms step_avg:58.52ms
step:780/2330 train_time:45649ms step_avg:58.52ms
step:781/2330 train_time:45705ms step_avg:58.52ms
step:782/2330 train_time:45767ms step_avg:58.53ms
step:783/2330 train_time:45824ms step_avg:58.52ms
step:784/2330 train_time:45886ms step_avg:58.53ms
step:785/2330 train_time:45943ms step_avg:58.53ms
step:786/2330 train_time:46006ms step_avg:58.53ms
step:787/2330 train_time:46064ms step_avg:58.53ms
step:788/2330 train_time:46126ms step_avg:58.54ms
step:789/2330 train_time:46184ms step_avg:58.54ms
step:790/2330 train_time:46246ms step_avg:58.54ms
step:791/2330 train_time:46304ms step_avg:58.54ms
step:792/2330 train_time:46366ms step_avg:58.54ms
step:793/2330 train_time:46423ms step_avg:58.54ms
step:794/2330 train_time:46485ms step_avg:58.55ms
step:795/2330 train_time:46542ms step_avg:58.54ms
step:796/2330 train_time:46603ms step_avg:58.55ms
step:797/2330 train_time:46660ms step_avg:58.54ms
step:798/2330 train_time:46722ms step_avg:58.55ms
step:799/2330 train_time:46779ms step_avg:58.55ms
step:800/2330 train_time:46840ms step_avg:58.55ms
step:801/2330 train_time:46898ms step_avg:58.55ms
step:802/2330 train_time:46960ms step_avg:58.55ms
step:803/2330 train_time:47017ms step_avg:58.55ms
step:804/2330 train_time:47079ms step_avg:58.56ms
step:805/2330 train_time:47136ms step_avg:58.55ms
step:806/2330 train_time:47199ms step_avg:58.56ms
step:807/2330 train_time:47256ms step_avg:58.56ms
step:808/2330 train_time:47318ms step_avg:58.56ms
step:809/2330 train_time:47376ms step_avg:58.56ms
step:810/2330 train_time:47437ms step_avg:58.56ms
step:811/2330 train_time:47493ms step_avg:58.56ms
step:812/2330 train_time:47555ms step_avg:58.57ms
step:813/2330 train_time:47612ms step_avg:58.56ms
step:814/2330 train_time:47673ms step_avg:58.57ms
step:815/2330 train_time:47730ms step_avg:58.56ms
step:816/2330 train_time:47792ms step_avg:58.57ms
step:817/2330 train_time:47848ms step_avg:58.57ms
step:818/2330 train_time:47910ms step_avg:58.57ms
step:819/2330 train_time:47966ms step_avg:58.57ms
step:820/2330 train_time:48029ms step_avg:58.57ms
step:821/2330 train_time:48086ms step_avg:58.57ms
step:822/2330 train_time:48149ms step_avg:58.58ms
step:823/2330 train_time:48205ms step_avg:58.57ms
step:824/2330 train_time:48269ms step_avg:58.58ms
step:825/2330 train_time:48327ms step_avg:58.58ms
step:826/2330 train_time:48389ms step_avg:58.58ms
step:827/2330 train_time:48446ms step_avg:58.58ms
step:828/2330 train_time:48509ms step_avg:58.59ms
step:829/2330 train_time:48566ms step_avg:58.58ms
step:830/2330 train_time:48627ms step_avg:58.59ms
step:831/2330 train_time:48685ms step_avg:58.59ms
step:832/2330 train_time:48746ms step_avg:58.59ms
step:833/2330 train_time:48803ms step_avg:58.59ms
step:834/2330 train_time:48864ms step_avg:58.59ms
step:835/2330 train_time:48921ms step_avg:58.59ms
step:836/2330 train_time:48983ms step_avg:58.59ms
step:837/2330 train_time:49040ms step_avg:58.59ms
step:838/2330 train_time:49102ms step_avg:58.59ms
step:839/2330 train_time:49159ms step_avg:58.59ms
step:840/2330 train_time:49221ms step_avg:58.60ms
step:841/2330 train_time:49279ms step_avg:58.60ms
step:842/2330 train_time:49342ms step_avg:58.60ms
step:843/2330 train_time:49399ms step_avg:58.60ms
step:844/2330 train_time:49463ms step_avg:58.61ms
step:845/2330 train_time:49521ms step_avg:58.60ms
step:846/2330 train_time:49584ms step_avg:58.61ms
step:847/2330 train_time:49641ms step_avg:58.61ms
step:848/2330 train_time:49704ms step_avg:58.61ms
step:849/2330 train_time:49761ms step_avg:58.61ms
step:850/2330 train_time:49822ms step_avg:58.61ms
step:851/2330 train_time:49880ms step_avg:58.61ms
step:852/2330 train_time:49940ms step_avg:58.62ms
step:853/2330 train_time:49998ms step_avg:58.61ms
step:854/2330 train_time:50059ms step_avg:58.62ms
step:855/2330 train_time:50115ms step_avg:58.61ms
step:856/2330 train_time:50177ms step_avg:58.62ms
step:857/2330 train_time:50234ms step_avg:58.62ms
step:858/2330 train_time:50295ms step_avg:58.62ms
step:859/2330 train_time:50352ms step_avg:58.62ms
step:860/2330 train_time:50413ms step_avg:58.62ms
step:861/2330 train_time:50470ms step_avg:58.62ms
step:862/2330 train_time:50532ms step_avg:58.62ms
step:863/2330 train_time:50589ms step_avg:58.62ms
step:864/2330 train_time:50652ms step_avg:58.62ms
step:865/2330 train_time:50709ms step_avg:58.62ms
step:866/2330 train_time:50770ms step_avg:58.63ms
step:867/2330 train_time:50826ms step_avg:58.62ms
step:868/2330 train_time:50888ms step_avg:58.63ms
step:869/2330 train_time:50945ms step_avg:58.62ms
step:870/2330 train_time:51007ms step_avg:58.63ms
step:871/2330 train_time:51064ms step_avg:58.63ms
step:872/2330 train_time:51125ms step_avg:58.63ms
step:873/2330 train_time:51182ms step_avg:58.63ms
step:874/2330 train_time:51246ms step_avg:58.63ms
step:875/2330 train_time:51303ms step_avg:58.63ms
step:876/2330 train_time:51365ms step_avg:58.64ms
step:877/2330 train_time:51423ms step_avg:58.64ms
step:878/2330 train_time:51485ms step_avg:58.64ms
step:879/2330 train_time:51543ms step_avg:58.64ms
step:880/2330 train_time:51604ms step_avg:58.64ms
step:881/2330 train_time:51662ms step_avg:58.64ms
step:882/2330 train_time:51724ms step_avg:58.64ms
step:883/2330 train_time:51781ms step_avg:58.64ms
step:884/2330 train_time:51843ms step_avg:58.65ms
step:885/2330 train_time:51900ms step_avg:58.64ms
step:886/2330 train_time:51962ms step_avg:58.65ms
step:887/2330 train_time:52019ms step_avg:58.65ms
step:888/2330 train_time:52081ms step_avg:58.65ms
step:889/2330 train_time:52138ms step_avg:58.65ms
step:890/2330 train_time:52201ms step_avg:58.65ms
step:891/2330 train_time:52258ms step_avg:58.65ms
step:892/2330 train_time:52319ms step_avg:58.65ms
step:893/2330 train_time:52377ms step_avg:58.65ms
step:894/2330 train_time:52438ms step_avg:58.66ms
step:895/2330 train_time:52496ms step_avg:58.65ms
step:896/2330 train_time:52557ms step_avg:58.66ms
step:897/2330 train_time:52614ms step_avg:58.66ms
step:898/2330 train_time:52675ms step_avg:58.66ms
step:899/2330 train_time:52732ms step_avg:58.66ms
step:900/2330 train_time:52793ms step_avg:58.66ms
step:901/2330 train_time:52850ms step_avg:58.66ms
step:902/2330 train_time:52913ms step_avg:58.66ms
step:903/2330 train_time:52969ms step_avg:58.66ms
step:904/2330 train_time:53031ms step_avg:58.66ms
step:905/2330 train_time:53087ms step_avg:58.66ms
step:906/2330 train_time:53150ms step_avg:58.66ms
step:907/2330 train_time:53206ms step_avg:58.66ms
step:908/2330 train_time:53270ms step_avg:58.67ms
step:909/2330 train_time:53327ms step_avg:58.67ms
step:910/2330 train_time:53390ms step_avg:58.67ms
step:911/2330 train_time:53446ms step_avg:58.67ms
step:912/2330 train_time:53509ms step_avg:58.67ms
step:913/2330 train_time:53565ms step_avg:58.67ms
step:914/2330 train_time:53629ms step_avg:58.67ms
step:915/2330 train_time:53686ms step_avg:58.67ms
step:916/2330 train_time:53749ms step_avg:58.68ms
step:917/2330 train_time:53806ms step_avg:58.68ms
step:918/2330 train_time:53868ms step_avg:58.68ms
step:919/2330 train_time:53925ms step_avg:58.68ms
step:920/2330 train_time:53987ms step_avg:58.68ms
step:921/2330 train_time:54044ms step_avg:58.68ms
step:922/2330 train_time:54106ms step_avg:58.68ms
step:923/2330 train_time:54163ms step_avg:58.68ms
step:924/2330 train_time:54224ms step_avg:58.68ms
step:925/2330 train_time:54282ms step_avg:58.68ms
step:926/2330 train_time:54344ms step_avg:58.69ms
step:927/2330 train_time:54401ms step_avg:58.69ms
step:928/2330 train_time:54463ms step_avg:58.69ms
step:929/2330 train_time:54521ms step_avg:58.69ms
step:930/2330 train_time:54583ms step_avg:58.69ms
step:931/2330 train_time:54641ms step_avg:58.69ms
step:932/2330 train_time:54703ms step_avg:58.69ms
step:933/2330 train_time:54761ms step_avg:58.69ms
step:934/2330 train_time:54822ms step_avg:58.70ms
step:935/2330 train_time:54880ms step_avg:58.69ms
step:936/2330 train_time:54942ms step_avg:58.70ms
step:937/2330 train_time:55000ms step_avg:58.70ms
step:938/2330 train_time:55060ms step_avg:58.70ms
step:939/2330 train_time:55118ms step_avg:58.70ms
step:940/2330 train_time:55179ms step_avg:58.70ms
step:941/2330 train_time:55236ms step_avg:58.70ms
step:942/2330 train_time:55298ms step_avg:58.70ms
step:943/2330 train_time:55354ms step_avg:58.70ms
step:944/2330 train_time:55416ms step_avg:58.70ms
step:945/2330 train_time:55473ms step_avg:58.70ms
step:946/2330 train_time:55534ms step_avg:58.70ms
step:947/2330 train_time:55591ms step_avg:58.70ms
step:948/2330 train_time:55653ms step_avg:58.71ms
step:949/2330 train_time:55709ms step_avg:58.70ms
step:950/2330 train_time:55772ms step_avg:58.71ms
step:951/2330 train_time:55829ms step_avg:58.71ms
step:952/2330 train_time:55891ms step_avg:58.71ms
step:953/2330 train_time:55947ms step_avg:58.71ms
step:954/2330 train_time:56009ms step_avg:58.71ms
step:955/2330 train_time:56066ms step_avg:58.71ms
step:956/2330 train_time:56130ms step_avg:58.71ms
step:957/2330 train_time:56186ms step_avg:58.71ms
step:958/2330 train_time:56249ms step_avg:58.71ms
step:959/2330 train_time:56306ms step_avg:58.71ms
step:960/2330 train_time:56369ms step_avg:58.72ms
step:961/2330 train_time:56426ms step_avg:58.72ms
step:962/2330 train_time:56488ms step_avg:58.72ms
step:963/2330 train_time:56545ms step_avg:58.72ms
step:964/2330 train_time:56608ms step_avg:58.72ms
step:965/2330 train_time:56665ms step_avg:58.72ms
step:966/2330 train_time:56727ms step_avg:58.72ms
step:967/2330 train_time:56784ms step_avg:58.72ms
step:968/2330 train_time:56846ms step_avg:58.72ms
step:969/2330 train_time:56902ms step_avg:58.72ms
step:970/2330 train_time:56964ms step_avg:58.73ms
step:971/2330 train_time:57022ms step_avg:58.73ms
step:972/2330 train_time:57083ms step_avg:58.73ms
step:973/2330 train_time:57141ms step_avg:58.73ms
step:974/2330 train_time:57202ms step_avg:58.73ms
step:975/2330 train_time:57259ms step_avg:58.73ms
step:976/2330 train_time:57321ms step_avg:58.73ms
step:977/2330 train_time:57379ms step_avg:58.73ms
step:978/2330 train_time:57440ms step_avg:58.73ms
step:979/2330 train_time:57498ms step_avg:58.73ms
step:980/2330 train_time:57560ms step_avg:58.73ms
step:981/2330 train_time:57617ms step_avg:58.73ms
step:982/2330 train_time:57680ms step_avg:58.74ms
step:983/2330 train_time:57738ms step_avg:58.74ms
step:984/2330 train_time:57800ms step_avg:58.74ms
step:985/2330 train_time:57857ms step_avg:58.74ms
step:986/2330 train_time:57918ms step_avg:58.74ms
step:987/2330 train_time:57976ms step_avg:58.74ms
step:988/2330 train_time:58036ms step_avg:58.74ms
step:989/2330 train_time:58093ms step_avg:58.74ms
step:990/2330 train_time:58154ms step_avg:58.74ms
step:991/2330 train_time:58211ms step_avg:58.74ms
step:992/2330 train_time:58272ms step_avg:58.74ms
step:993/2330 train_time:58329ms step_avg:58.74ms
step:994/2330 train_time:58390ms step_avg:58.74ms
step:995/2330 train_time:58446ms step_avg:58.74ms
step:996/2330 train_time:58509ms step_avg:58.74ms
step:997/2330 train_time:58566ms step_avg:58.74ms
step:998/2330 train_time:58628ms step_avg:58.75ms
step:999/2330 train_time:58685ms step_avg:58.74ms
step:1000/2330 train_time:58748ms step_avg:58.75ms
step:1000/2330 val_loss:4.1266 train_time:58827ms step_avg:58.83ms
step:1001/2330 train_time:58848ms step_avg:58.79ms
step:1002/2330 train_time:58869ms step_avg:58.75ms
step:1003/2330 train_time:58926ms step_avg:58.75ms
step:1004/2330 train_time:58992ms step_avg:58.76ms
step:1005/2330 train_time:59049ms step_avg:58.75ms
step:1006/2330 train_time:59113ms step_avg:58.76ms
step:1007/2330 train_time:59170ms step_avg:58.76ms
step:1008/2330 train_time:59232ms step_avg:58.76ms
step:1009/2330 train_time:59288ms step_avg:58.76ms
step:1010/2330 train_time:59350ms step_avg:58.76ms
step:1011/2330 train_time:59407ms step_avg:58.76ms
step:1012/2330 train_time:59469ms step_avg:58.76ms
step:1013/2330 train_time:59525ms step_avg:58.76ms
step:1014/2330 train_time:59586ms step_avg:58.76ms
step:1015/2330 train_time:59643ms step_avg:58.76ms
step:1016/2330 train_time:59704ms step_avg:58.76ms
step:1017/2330 train_time:59764ms step_avg:58.76ms
step:1018/2330 train_time:59826ms step_avg:58.77ms
step:1019/2330 train_time:59883ms step_avg:58.77ms
step:1020/2330 train_time:59944ms step_avg:58.77ms
step:1021/2330 train_time:60001ms step_avg:58.77ms
step:1022/2330 train_time:60064ms step_avg:58.77ms
step:1023/2330 train_time:60120ms step_avg:58.77ms
step:1024/2330 train_time:60182ms step_avg:58.77ms
step:1025/2330 train_time:60238ms step_avg:58.77ms
step:1026/2330 train_time:60300ms step_avg:58.77ms
step:1027/2330 train_time:60357ms step_avg:58.77ms
step:1028/2330 train_time:60418ms step_avg:58.77ms
step:1029/2330 train_time:60475ms step_avg:58.77ms
step:1030/2330 train_time:60537ms step_avg:58.77ms
step:1031/2330 train_time:60593ms step_avg:58.77ms
step:1032/2330 train_time:60657ms step_avg:58.78ms
step:1033/2330 train_time:60714ms step_avg:58.77ms
step:1034/2330 train_time:60776ms step_avg:58.78ms
step:1035/2330 train_time:60833ms step_avg:58.78ms
step:1036/2330 train_time:60897ms step_avg:58.78ms
step:1037/2330 train_time:60954ms step_avg:58.78ms
step:1038/2330 train_time:61018ms step_avg:58.78ms
step:1039/2330 train_time:61075ms step_avg:58.78ms
step:1040/2330 train_time:61137ms step_avg:58.79ms
step:1041/2330 train_time:61194ms step_avg:58.78ms
step:1042/2330 train_time:61256ms step_avg:58.79ms
step:1043/2330 train_time:61313ms step_avg:58.79ms
step:1044/2330 train_time:61374ms step_avg:58.79ms
step:1045/2330 train_time:61431ms step_avg:58.79ms
step:1046/2330 train_time:61494ms step_avg:58.79ms
step:1047/2330 train_time:61552ms step_avg:58.79ms
step:1048/2330 train_time:61613ms step_avg:58.79ms
step:1049/2330 train_time:61670ms step_avg:58.79ms
step:1050/2330 train_time:61733ms step_avg:58.79ms
step:1051/2330 train_time:61790ms step_avg:58.79ms
step:1052/2330 train_time:61852ms step_avg:58.80ms
step:1053/2330 train_time:61910ms step_avg:58.79ms
step:1054/2330 train_time:61973ms step_avg:58.80ms
step:1055/2330 train_time:62030ms step_avg:58.80ms
step:1056/2330 train_time:62093ms step_avg:58.80ms
step:1057/2330 train_time:62150ms step_avg:58.80ms
step:1058/2330 train_time:62212ms step_avg:58.80ms
step:1059/2330 train_time:62269ms step_avg:58.80ms
step:1060/2330 train_time:62331ms step_avg:58.80ms
step:1061/2330 train_time:62387ms step_avg:58.80ms
step:1062/2330 train_time:62449ms step_avg:58.80ms
step:1063/2330 train_time:62507ms step_avg:58.80ms
step:1064/2330 train_time:62568ms step_avg:58.80ms
step:1065/2330 train_time:62625ms step_avg:58.80ms
step:1066/2330 train_time:62686ms step_avg:58.81ms
step:1067/2330 train_time:62743ms step_avg:58.80ms
step:1068/2330 train_time:62804ms step_avg:58.81ms
step:1069/2330 train_time:62861ms step_avg:58.80ms
step:1070/2330 train_time:62923ms step_avg:58.81ms
step:1071/2330 train_time:62979ms step_avg:58.80ms
step:1072/2330 train_time:63040ms step_avg:58.81ms
step:1073/2330 train_time:63097ms step_avg:58.80ms
step:1074/2330 train_time:63159ms step_avg:58.81ms
step:1075/2330 train_time:63216ms step_avg:58.81ms
step:1076/2330 train_time:63279ms step_avg:58.81ms
step:1077/2330 train_time:63335ms step_avg:58.81ms
step:1078/2330 train_time:63398ms step_avg:58.81ms
step:1079/2330 train_time:63454ms step_avg:58.81ms
step:1080/2330 train_time:63516ms step_avg:58.81ms
step:1081/2330 train_time:63573ms step_avg:58.81ms
step:1082/2330 train_time:63635ms step_avg:58.81ms
step:1083/2330 train_time:63693ms step_avg:58.81ms
step:1084/2330 train_time:63754ms step_avg:58.81ms
step:1085/2330 train_time:63812ms step_avg:58.81ms
step:1086/2330 train_time:63874ms step_avg:58.82ms
step:1087/2330 train_time:63932ms step_avg:58.81ms
step:1088/2330 train_time:63993ms step_avg:58.82ms
step:1089/2330 train_time:64051ms step_avg:58.82ms
step:1090/2330 train_time:64113ms step_avg:58.82ms
step:1091/2330 train_time:64170ms step_avg:58.82ms
step:1092/2330 train_time:64233ms step_avg:58.82ms
step:1093/2330 train_time:64290ms step_avg:58.82ms
step:1094/2330 train_time:64352ms step_avg:58.82ms
step:1095/2330 train_time:64409ms step_avg:58.82ms
step:1096/2330 train_time:64471ms step_avg:58.82ms
step:1097/2330 train_time:64529ms step_avg:58.82ms
step:1098/2330 train_time:64590ms step_avg:58.83ms
step:1099/2330 train_time:64648ms step_avg:58.82ms
step:1100/2330 train_time:64709ms step_avg:58.83ms
step:1101/2330 train_time:64767ms step_avg:58.83ms
step:1102/2330 train_time:64829ms step_avg:58.83ms
step:1103/2330 train_time:64887ms step_avg:58.83ms
step:1104/2330 train_time:64948ms step_avg:58.83ms
step:1105/2330 train_time:65007ms step_avg:58.83ms
step:1106/2330 train_time:65068ms step_avg:58.83ms
step:1107/2330 train_time:65126ms step_avg:58.83ms
step:1108/2330 train_time:65187ms step_avg:58.83ms
step:1109/2330 train_time:65244ms step_avg:58.83ms
step:1110/2330 train_time:65305ms step_avg:58.83ms
step:1111/2330 train_time:65362ms step_avg:58.83ms
step:1112/2330 train_time:65423ms step_avg:58.83ms
step:1113/2330 train_time:65480ms step_avg:58.83ms
step:1114/2330 train_time:65542ms step_avg:58.83ms
step:1115/2330 train_time:65598ms step_avg:58.83ms
step:1116/2330 train_time:65661ms step_avg:58.84ms
step:1117/2330 train_time:65717ms step_avg:58.83ms
step:1118/2330 train_time:65780ms step_avg:58.84ms
step:1119/2330 train_time:65837ms step_avg:58.84ms
step:1120/2330 train_time:65899ms step_avg:58.84ms
step:1121/2330 train_time:65955ms step_avg:58.84ms
step:1122/2330 train_time:66018ms step_avg:58.84ms
step:1123/2330 train_time:66075ms step_avg:58.84ms
step:1124/2330 train_time:66137ms step_avg:58.84ms
step:1125/2330 train_time:66195ms step_avg:58.84ms
step:1126/2330 train_time:66257ms step_avg:58.84ms
step:1127/2330 train_time:66314ms step_avg:58.84ms
step:1128/2330 train_time:66376ms step_avg:58.84ms
step:1129/2330 train_time:66434ms step_avg:58.84ms
step:1130/2330 train_time:66494ms step_avg:58.84ms
step:1131/2330 train_time:66552ms step_avg:58.84ms
step:1132/2330 train_time:66614ms step_avg:58.85ms
step:1133/2330 train_time:66672ms step_avg:58.85ms
step:1134/2330 train_time:66733ms step_avg:58.85ms
step:1135/2330 train_time:66791ms step_avg:58.85ms
step:1136/2330 train_time:66852ms step_avg:58.85ms
step:1137/2330 train_time:66910ms step_avg:58.85ms
step:1138/2330 train_time:66971ms step_avg:58.85ms
step:1139/2330 train_time:67028ms step_avg:58.85ms
step:1140/2330 train_time:67090ms step_avg:58.85ms
step:1141/2330 train_time:67147ms step_avg:58.85ms
step:1142/2330 train_time:67208ms step_avg:58.85ms
step:1143/2330 train_time:67266ms step_avg:58.85ms
step:1144/2330 train_time:67327ms step_avg:58.85ms
step:1145/2330 train_time:67384ms step_avg:58.85ms
step:1146/2330 train_time:67446ms step_avg:58.85ms
step:1147/2330 train_time:67502ms step_avg:58.85ms
step:1148/2330 train_time:67565ms step_avg:58.85ms
step:1149/2330 train_time:67622ms step_avg:58.85ms
step:1150/2330 train_time:67683ms step_avg:58.85ms
step:1151/2330 train_time:67740ms step_avg:58.85ms
step:1152/2330 train_time:67802ms step_avg:58.86ms
step:1153/2330 train_time:67858ms step_avg:58.85ms
step:1154/2330 train_time:67921ms step_avg:58.86ms
step:1155/2330 train_time:67977ms step_avg:58.85ms
step:1156/2330 train_time:68040ms step_avg:58.86ms
step:1157/2330 train_time:68096ms step_avg:58.86ms
step:1158/2330 train_time:68158ms step_avg:58.86ms
step:1159/2330 train_time:68215ms step_avg:58.86ms
step:1160/2330 train_time:68277ms step_avg:58.86ms
step:1161/2330 train_time:68333ms step_avg:58.86ms
step:1162/2330 train_time:68396ms step_avg:58.86ms
step:1163/2330 train_time:68453ms step_avg:58.86ms
step:1164/2330 train_time:68515ms step_avg:58.86ms
step:1165/2330 train_time:68573ms step_avg:58.86ms
step:1166/2330 train_time:68633ms step_avg:58.86ms
step:1167/2330 train_time:68690ms step_avg:58.86ms
step:1168/2330 train_time:68753ms step_avg:58.86ms
step:1169/2330 train_time:68811ms step_avg:58.86ms
step:1170/2330 train_time:68872ms step_avg:58.86ms
step:1171/2330 train_time:68929ms step_avg:58.86ms
step:1172/2330 train_time:68991ms step_avg:58.87ms
step:1173/2330 train_time:69048ms step_avg:58.86ms
step:1174/2330 train_time:69110ms step_avg:58.87ms
step:1175/2330 train_time:69167ms step_avg:58.87ms
step:1176/2330 train_time:69229ms step_avg:58.87ms
step:1177/2330 train_time:69286ms step_avg:58.87ms
step:1178/2330 train_time:69347ms step_avg:58.87ms
step:1179/2330 train_time:69405ms step_avg:58.87ms
step:1180/2330 train_time:69466ms step_avg:58.87ms
step:1181/2330 train_time:69523ms step_avg:58.87ms
step:1182/2330 train_time:69583ms step_avg:58.87ms
step:1183/2330 train_time:69640ms step_avg:58.87ms
step:1184/2330 train_time:69702ms step_avg:58.87ms
step:1185/2330 train_time:69759ms step_avg:58.87ms
step:1186/2330 train_time:69821ms step_avg:58.87ms
step:1187/2330 train_time:69877ms step_avg:58.87ms
step:1188/2330 train_time:69940ms step_avg:58.87ms
step:1189/2330 train_time:69996ms step_avg:58.87ms
step:1190/2330 train_time:70058ms step_avg:58.87ms
step:1191/2330 train_time:70115ms step_avg:58.87ms
step:1192/2330 train_time:70178ms step_avg:58.87ms
step:1193/2330 train_time:70235ms step_avg:58.87ms
step:1194/2330 train_time:70297ms step_avg:58.88ms
step:1195/2330 train_time:70354ms step_avg:58.87ms
step:1196/2330 train_time:70416ms step_avg:58.88ms
step:1197/2330 train_time:70473ms step_avg:58.87ms
step:1198/2330 train_time:70534ms step_avg:58.88ms
step:1199/2330 train_time:70591ms step_avg:58.88ms
step:1200/2330 train_time:70653ms step_avg:58.88ms
step:1201/2330 train_time:70710ms step_avg:58.88ms
step:1202/2330 train_time:70773ms step_avg:58.88ms
step:1203/2330 train_time:70830ms step_avg:58.88ms
step:1204/2330 train_time:70891ms step_avg:58.88ms
step:1205/2330 train_time:70949ms step_avg:58.88ms
step:1206/2330 train_time:71011ms step_avg:58.88ms
step:1207/2330 train_time:71068ms step_avg:58.88ms
step:1208/2330 train_time:71128ms step_avg:58.88ms
step:1209/2330 train_time:71186ms step_avg:58.88ms
step:1210/2330 train_time:71247ms step_avg:58.88ms
step:1211/2330 train_time:71304ms step_avg:58.88ms
step:1212/2330 train_time:71366ms step_avg:58.88ms
step:1213/2330 train_time:71422ms step_avg:58.88ms
step:1214/2330 train_time:71483ms step_avg:58.88ms
step:1215/2330 train_time:71540ms step_avg:58.88ms
step:1216/2330 train_time:71602ms step_avg:58.88ms
step:1217/2330 train_time:71659ms step_avg:58.88ms
step:1218/2330 train_time:71720ms step_avg:58.88ms
step:1219/2330 train_time:71777ms step_avg:58.88ms
step:1220/2330 train_time:71839ms step_avg:58.88ms
step:1221/2330 train_time:71896ms step_avg:58.88ms
step:1222/2330 train_time:71957ms step_avg:58.88ms
step:1223/2330 train_time:72014ms step_avg:58.88ms
step:1224/2330 train_time:72077ms step_avg:58.89ms
step:1225/2330 train_time:72133ms step_avg:58.88ms
step:1226/2330 train_time:72197ms step_avg:58.89ms
step:1227/2330 train_time:72254ms step_avg:58.89ms
step:1228/2330 train_time:72316ms step_avg:58.89ms
step:1229/2330 train_time:72373ms step_avg:58.89ms
step:1230/2330 train_time:72435ms step_avg:58.89ms
step:1231/2330 train_time:72492ms step_avg:58.89ms
step:1232/2330 train_time:72555ms step_avg:58.89ms
step:1233/2330 train_time:72612ms step_avg:58.89ms
step:1234/2330 train_time:72673ms step_avg:58.89ms
step:1235/2330 train_time:72730ms step_avg:58.89ms
step:1236/2330 train_time:72791ms step_avg:58.89ms
step:1237/2330 train_time:72849ms step_avg:58.89ms
step:1238/2330 train_time:72911ms step_avg:58.89ms
step:1239/2330 train_time:72969ms step_avg:58.89ms
step:1240/2330 train_time:73030ms step_avg:58.89ms
step:1241/2330 train_time:73087ms step_avg:58.89ms
step:1242/2330 train_time:73148ms step_avg:58.90ms
step:1243/2330 train_time:73206ms step_avg:58.89ms
step:1244/2330 train_time:73267ms step_avg:58.90ms
step:1245/2330 train_time:73325ms step_avg:58.90ms
step:1246/2330 train_time:73386ms step_avg:58.90ms
step:1247/2330 train_time:73443ms step_avg:58.90ms
step:1248/2330 train_time:73504ms step_avg:58.90ms
step:1249/2330 train_time:73562ms step_avg:58.90ms
step:1250/2330 train_time:73623ms step_avg:58.90ms
step:1250/2330 val_loss:4.0442 train_time:73701ms step_avg:58.96ms
step:1251/2330 train_time:73721ms step_avg:58.93ms
step:1252/2330 train_time:73744ms step_avg:58.90ms
step:1253/2330 train_time:73802ms step_avg:58.90ms
step:1254/2330 train_time:73868ms step_avg:58.91ms
step:1255/2330 train_time:73925ms step_avg:58.90ms
step:1256/2330 train_time:73988ms step_avg:58.91ms
step:1257/2330 train_time:74046ms step_avg:58.91ms
step:1258/2330 train_time:74107ms step_avg:58.91ms
step:1259/2330 train_time:74163ms step_avg:58.91ms
step:1260/2330 train_time:74225ms step_avg:58.91ms
step:1261/2330 train_time:74281ms step_avg:58.91ms
step:1262/2330 train_time:74342ms step_avg:58.91ms
step:1263/2330 train_time:74399ms step_avg:58.91ms
step:1264/2330 train_time:74460ms step_avg:58.91ms
step:1265/2330 train_time:74516ms step_avg:58.91ms
step:1266/2330 train_time:74577ms step_avg:58.91ms
step:1267/2330 train_time:74634ms step_avg:58.91ms
step:1268/2330 train_time:74696ms step_avg:58.91ms
step:1269/2330 train_time:74754ms step_avg:58.91ms
step:1270/2330 train_time:74815ms step_avg:58.91ms
step:1271/2330 train_time:74872ms step_avg:58.91ms
step:1272/2330 train_time:74935ms step_avg:58.91ms
step:1273/2330 train_time:74992ms step_avg:58.91ms
step:1274/2330 train_time:75054ms step_avg:58.91ms
step:1275/2330 train_time:75111ms step_avg:58.91ms
step:1276/2330 train_time:75173ms step_avg:58.91ms
step:1277/2330 train_time:75229ms step_avg:58.91ms
step:1278/2330 train_time:75293ms step_avg:58.91ms
step:1279/2330 train_time:75349ms step_avg:58.91ms
step:1280/2330 train_time:75411ms step_avg:58.91ms
step:1281/2330 train_time:75468ms step_avg:58.91ms
step:1282/2330 train_time:75530ms step_avg:58.92ms
step:1283/2330 train_time:75587ms step_avg:58.91ms
step:1284/2330 train_time:75649ms step_avg:58.92ms
step:1285/2330 train_time:75706ms step_avg:58.92ms
step:1286/2330 train_time:75768ms step_avg:58.92ms
step:1287/2330 train_time:75825ms step_avg:58.92ms
step:1288/2330 train_time:75888ms step_avg:58.92ms
step:1289/2330 train_time:75946ms step_avg:58.92ms
step:1290/2330 train_time:76008ms step_avg:58.92ms
step:1291/2330 train_time:76066ms step_avg:58.92ms
step:1292/2330 train_time:76127ms step_avg:58.92ms
step:1293/2330 train_time:76185ms step_avg:58.92ms
step:1294/2330 train_time:76246ms step_avg:58.92ms
step:1295/2330 train_time:76303ms step_avg:58.92ms
step:1296/2330 train_time:76364ms step_avg:58.92ms
step:1297/2330 train_time:76420ms step_avg:58.92ms
step:1298/2330 train_time:76482ms step_avg:58.92ms
step:1299/2330 train_time:76539ms step_avg:58.92ms
step:1300/2330 train_time:76600ms step_avg:58.92ms
step:1301/2330 train_time:76657ms step_avg:58.92ms
step:1302/2330 train_time:76718ms step_avg:58.92ms
step:1303/2330 train_time:76775ms step_avg:58.92ms
step:1304/2330 train_time:76837ms step_avg:58.92ms
step:1305/2330 train_time:76894ms step_avg:58.92ms
step:1306/2330 train_time:76957ms step_avg:58.93ms
step:1307/2330 train_time:77013ms step_avg:58.92ms
step:1308/2330 train_time:77075ms step_avg:58.93ms
step:1309/2330 train_time:77132ms step_avg:58.92ms
step:1310/2330 train_time:77194ms step_avg:58.93ms
step:1311/2330 train_time:77251ms step_avg:58.93ms
step:1312/2330 train_time:77313ms step_avg:58.93ms
step:1313/2330 train_time:77370ms step_avg:58.93ms
step:1314/2330 train_time:77432ms step_avg:58.93ms
step:1315/2330 train_time:77488ms step_avg:58.93ms
step:1316/2330 train_time:77550ms step_avg:58.93ms
step:1317/2330 train_time:77607ms step_avg:58.93ms
step:1318/2330 train_time:77670ms step_avg:58.93ms
step:1319/2330 train_time:77727ms step_avg:58.93ms
step:1320/2330 train_time:77789ms step_avg:58.93ms
step:1321/2330 train_time:77847ms step_avg:58.93ms
step:1322/2330 train_time:77908ms step_avg:58.93ms
step:1323/2330 train_time:77965ms step_avg:58.93ms
step:1324/2330 train_time:78027ms step_avg:58.93ms
step:1325/2330 train_time:78085ms step_avg:58.93ms
step:1326/2330 train_time:78147ms step_avg:58.93ms
step:1327/2330 train_time:78204ms step_avg:58.93ms
step:1328/2330 train_time:78266ms step_avg:58.94ms
step:1329/2330 train_time:78324ms step_avg:58.93ms
step:1330/2330 train_time:78385ms step_avg:58.94ms
step:1331/2330 train_time:78443ms step_avg:58.94ms
step:1332/2330 train_time:78504ms step_avg:58.94ms
step:1333/2330 train_time:78561ms step_avg:58.94ms
step:1334/2330 train_time:78623ms step_avg:58.94ms
step:1335/2330 train_time:78680ms step_avg:58.94ms
step:1336/2330 train_time:78742ms step_avg:58.94ms
step:1337/2330 train_time:78799ms step_avg:58.94ms
step:1338/2330 train_time:78861ms step_avg:58.94ms
step:1339/2330 train_time:78917ms step_avg:58.94ms
step:1340/2330 train_time:78978ms step_avg:58.94ms
step:1341/2330 train_time:79035ms step_avg:58.94ms
step:1342/2330 train_time:79096ms step_avg:58.94ms
step:1343/2330 train_time:79153ms step_avg:58.94ms
step:1344/2330 train_time:79215ms step_avg:58.94ms
step:1345/2330 train_time:79272ms step_avg:58.94ms
step:1346/2330 train_time:79334ms step_avg:58.94ms
step:1347/2330 train_time:79391ms step_avg:58.94ms
step:1348/2330 train_time:79454ms step_avg:58.94ms
step:1349/2330 train_time:79510ms step_avg:58.94ms
step:1350/2330 train_time:79574ms step_avg:58.94ms
step:1351/2330 train_time:79630ms step_avg:58.94ms
step:1352/2330 train_time:79694ms step_avg:58.95ms
step:1353/2330 train_time:79750ms step_avg:58.94ms
step:1354/2330 train_time:79813ms step_avg:58.95ms
step:1355/2330 train_time:79870ms step_avg:58.94ms
step:1356/2330 train_time:79932ms step_avg:58.95ms
step:1357/2330 train_time:79989ms step_avg:58.95ms
step:1358/2330 train_time:80052ms step_avg:58.95ms
step:1359/2330 train_time:80109ms step_avg:58.95ms
step:1360/2330 train_time:80171ms step_avg:58.95ms
step:1361/2330 train_time:80229ms step_avg:58.95ms
step:1362/2330 train_time:80291ms step_avg:58.95ms
step:1363/2330 train_time:80347ms step_avg:58.95ms
step:1364/2330 train_time:80410ms step_avg:58.95ms
step:1365/2330 train_time:80467ms step_avg:58.95ms
step:1366/2330 train_time:80529ms step_avg:58.95ms
step:1367/2330 train_time:80586ms step_avg:58.95ms
step:1368/2330 train_time:80649ms step_avg:58.95ms
step:1369/2330 train_time:80706ms step_avg:58.95ms
step:1370/2330 train_time:80769ms step_avg:58.96ms
step:1371/2330 train_time:80826ms step_avg:58.95ms
step:1372/2330 train_time:80888ms step_avg:58.96ms
step:1373/2330 train_time:80946ms step_avg:58.96ms
step:1374/2330 train_time:81007ms step_avg:58.96ms
step:1375/2330 train_time:81064ms step_avg:58.96ms
step:1376/2330 train_time:81127ms step_avg:58.96ms
step:1377/2330 train_time:81184ms step_avg:58.96ms
step:1378/2330 train_time:81246ms step_avg:58.96ms
step:1379/2330 train_time:81303ms step_avg:58.96ms
step:1380/2330 train_time:81364ms step_avg:58.96ms
step:1381/2330 train_time:81421ms step_avg:58.96ms
step:1382/2330 train_time:81483ms step_avg:58.96ms
step:1383/2330 train_time:81540ms step_avg:58.96ms
step:1384/2330 train_time:81602ms step_avg:58.96ms
step:1385/2330 train_time:81658ms step_avg:58.96ms
step:1386/2330 train_time:81720ms step_avg:58.96ms
step:1387/2330 train_time:81776ms step_avg:58.96ms
step:1388/2330 train_time:81839ms step_avg:58.96ms
step:1389/2330 train_time:81895ms step_avg:58.96ms
step:1390/2330 train_time:81958ms step_avg:58.96ms
step:1391/2330 train_time:82014ms step_avg:58.96ms
step:1392/2330 train_time:82076ms step_avg:58.96ms
step:1393/2330 train_time:82133ms step_avg:58.96ms
step:1394/2330 train_time:82196ms step_avg:58.96ms
step:1395/2330 train_time:82252ms step_avg:58.96ms
step:1396/2330 train_time:82314ms step_avg:58.96ms
step:1397/2330 train_time:82371ms step_avg:58.96ms
step:1398/2330 train_time:82434ms step_avg:58.97ms
step:1399/2330 train_time:82490ms step_avg:58.96ms
step:1400/2330 train_time:82554ms step_avg:58.97ms
step:1401/2330 train_time:82610ms step_avg:58.97ms
step:1402/2330 train_time:82672ms step_avg:58.97ms
step:1403/2330 train_time:82729ms step_avg:58.97ms
step:1404/2330 train_time:82793ms step_avg:58.97ms
step:1405/2330 train_time:82850ms step_avg:58.97ms
step:1406/2330 train_time:82912ms step_avg:58.97ms
step:1407/2330 train_time:82969ms step_avg:58.97ms
step:1408/2330 train_time:83032ms step_avg:58.97ms
step:1409/2330 train_time:83089ms step_avg:58.97ms
step:1410/2330 train_time:83151ms step_avg:58.97ms
step:1411/2330 train_time:83208ms step_avg:58.97ms
step:1412/2330 train_time:83270ms step_avg:58.97ms
step:1413/2330 train_time:83327ms step_avg:58.97ms
step:1414/2330 train_time:83390ms step_avg:58.97ms
step:1415/2330 train_time:83447ms step_avg:58.97ms
step:1416/2330 train_time:83509ms step_avg:58.98ms
step:1417/2330 train_time:83565ms step_avg:58.97ms
step:1418/2330 train_time:83628ms step_avg:58.98ms
step:1419/2330 train_time:83686ms step_avg:58.98ms
step:1420/2330 train_time:83748ms step_avg:58.98ms
step:1421/2330 train_time:83805ms step_avg:58.98ms
step:1422/2330 train_time:83868ms step_avg:58.98ms
step:1423/2330 train_time:83925ms step_avg:58.98ms
step:1424/2330 train_time:83987ms step_avg:58.98ms
step:1425/2330 train_time:84045ms step_avg:58.98ms
step:1426/2330 train_time:84107ms step_avg:58.98ms
step:1427/2330 train_time:84164ms step_avg:58.98ms
step:1428/2330 train_time:84226ms step_avg:58.98ms
step:1429/2330 train_time:84283ms step_avg:58.98ms
step:1430/2330 train_time:84345ms step_avg:58.98ms
step:1431/2330 train_time:84402ms step_avg:58.98ms
step:1432/2330 train_time:84464ms step_avg:58.98ms
step:1433/2330 train_time:84521ms step_avg:58.98ms
step:1434/2330 train_time:84583ms step_avg:58.98ms
step:1435/2330 train_time:84641ms step_avg:58.98ms
step:1436/2330 train_time:84702ms step_avg:58.98ms
step:1437/2330 train_time:84759ms step_avg:58.98ms
step:1438/2330 train_time:84820ms step_avg:58.98ms
step:1439/2330 train_time:84876ms step_avg:58.98ms
step:1440/2330 train_time:84938ms step_avg:58.98ms
step:1441/2330 train_time:84995ms step_avg:58.98ms
step:1442/2330 train_time:85057ms step_avg:58.99ms
step:1443/2330 train_time:85113ms step_avg:58.98ms
step:1444/2330 train_time:85175ms step_avg:58.99ms
step:1445/2330 train_time:85231ms step_avg:58.98ms
step:1446/2330 train_time:85293ms step_avg:58.99ms
step:1447/2330 train_time:85350ms step_avg:58.98ms
step:1448/2330 train_time:85412ms step_avg:58.99ms
step:1449/2330 train_time:85469ms step_avg:58.98ms
step:1450/2330 train_time:85532ms step_avg:58.99ms
step:1451/2330 train_time:85589ms step_avg:58.99ms
step:1452/2330 train_time:85651ms step_avg:58.99ms
step:1453/2330 train_time:85708ms step_avg:58.99ms
step:1454/2330 train_time:85770ms step_avg:58.99ms
step:1455/2330 train_time:85828ms step_avg:58.99ms
step:1456/2330 train_time:85891ms step_avg:58.99ms
step:1457/2330 train_time:85948ms step_avg:58.99ms
step:1458/2330 train_time:86010ms step_avg:58.99ms
step:1459/2330 train_time:86068ms step_avg:58.99ms
step:1460/2330 train_time:86130ms step_avg:58.99ms
step:1461/2330 train_time:86187ms step_avg:58.99ms
step:1462/2330 train_time:86248ms step_avg:58.99ms
step:1463/2330 train_time:86305ms step_avg:58.99ms
step:1464/2330 train_time:86367ms step_avg:58.99ms
step:1465/2330 train_time:86423ms step_avg:58.99ms
step:1466/2330 train_time:86486ms step_avg:58.99ms
step:1467/2330 train_time:86543ms step_avg:58.99ms
step:1468/2330 train_time:86606ms step_avg:59.00ms
step:1469/2330 train_time:86663ms step_avg:58.99ms
step:1470/2330 train_time:86724ms step_avg:59.00ms
step:1471/2330 train_time:86782ms step_avg:59.00ms
step:1472/2330 train_time:86844ms step_avg:59.00ms
step:1473/2330 train_time:86901ms step_avg:59.00ms
step:1474/2330 train_time:86962ms step_avg:59.00ms
step:1475/2330 train_time:87020ms step_avg:59.00ms
step:1476/2330 train_time:87081ms step_avg:59.00ms
step:1477/2330 train_time:87138ms step_avg:59.00ms
step:1478/2330 train_time:87199ms step_avg:59.00ms
step:1479/2330 train_time:87256ms step_avg:59.00ms
step:1480/2330 train_time:87317ms step_avg:59.00ms
step:1481/2330 train_time:87374ms step_avg:59.00ms
step:1482/2330 train_time:87437ms step_avg:59.00ms
step:1483/2330 train_time:87493ms step_avg:59.00ms
step:1484/2330 train_time:87556ms step_avg:59.00ms
step:1485/2330 train_time:87612ms step_avg:59.00ms
step:1486/2330 train_time:87675ms step_avg:59.00ms
step:1487/2330 train_time:87732ms step_avg:59.00ms
step:1488/2330 train_time:87795ms step_avg:59.00ms
step:1489/2330 train_time:87851ms step_avg:59.00ms
step:1490/2330 train_time:87913ms step_avg:59.00ms
step:1491/2330 train_time:87970ms step_avg:59.00ms
step:1492/2330 train_time:88032ms step_avg:59.00ms
step:1493/2330 train_time:88089ms step_avg:59.00ms
step:1494/2330 train_time:88152ms step_avg:59.00ms
step:1495/2330 train_time:88209ms step_avg:59.00ms
step:1496/2330 train_time:88272ms step_avg:59.01ms
step:1497/2330 train_time:88329ms step_avg:59.00ms
step:1498/2330 train_time:88392ms step_avg:59.01ms
step:1499/2330 train_time:88449ms step_avg:59.01ms
step:1500/2330 train_time:88510ms step_avg:59.01ms
step:1500/2330 val_loss:3.9565 train_time:88589ms step_avg:59.06ms
step:1501/2330 train_time:88610ms step_avg:59.03ms
step:1502/2330 train_time:88631ms step_avg:59.01ms
step:1503/2330 train_time:88690ms step_avg:59.01ms
step:1504/2330 train_time:88759ms step_avg:59.02ms
step:1505/2330 train_time:88815ms step_avg:59.01ms
step:1506/2330 train_time:88879ms step_avg:59.02ms
step:1507/2330 train_time:88935ms step_avg:59.01ms
step:1508/2330 train_time:88997ms step_avg:59.02ms
step:1509/2330 train_time:89053ms step_avg:59.01ms
step:1510/2330 train_time:89115ms step_avg:59.02ms
step:1511/2330 train_time:89171ms step_avg:59.01ms
step:1512/2330 train_time:89234ms step_avg:59.02ms
step:1513/2330 train_time:89290ms step_avg:59.02ms
step:1514/2330 train_time:89351ms step_avg:59.02ms
step:1515/2330 train_time:89407ms step_avg:59.01ms
step:1516/2330 train_time:89468ms step_avg:59.02ms
step:1517/2330 train_time:89526ms step_avg:59.02ms
step:1518/2330 train_time:89589ms step_avg:59.02ms
step:1519/2330 train_time:89648ms step_avg:59.02ms
step:1520/2330 train_time:89712ms step_avg:59.02ms
step:1521/2330 train_time:89771ms step_avg:59.02ms
step:1522/2330 train_time:89833ms step_avg:59.02ms
step:1523/2330 train_time:89890ms step_avg:59.02ms
step:1524/2330 train_time:89952ms step_avg:59.02ms
step:1525/2330 train_time:90009ms step_avg:59.02ms
step:1526/2330 train_time:90070ms step_avg:59.02ms
step:1527/2330 train_time:90127ms step_avg:59.02ms
step:1528/2330 train_time:90188ms step_avg:59.02ms
step:1529/2330 train_time:90246ms step_avg:59.02ms
step:1530/2330 train_time:90306ms step_avg:59.02ms
step:1531/2330 train_time:90363ms step_avg:59.02ms
step:1532/2330 train_time:90424ms step_avg:59.02ms
step:1533/2330 train_time:90481ms step_avg:59.02ms
step:1534/2330 train_time:90542ms step_avg:59.02ms
step:1535/2330 train_time:90599ms step_avg:59.02ms
step:1536/2330 train_time:90661ms step_avg:59.02ms
step:1537/2330 train_time:90719ms step_avg:59.02ms
step:1538/2330 train_time:90782ms step_avg:59.03ms
step:1539/2330 train_time:90840ms step_avg:59.03ms
step:1540/2330 train_time:90902ms step_avg:59.03ms
step:1541/2330 train_time:90959ms step_avg:59.03ms
step:1542/2330 train_time:91022ms step_avg:59.03ms
step:1543/2330 train_time:91079ms step_avg:59.03ms
step:1544/2330 train_time:91142ms step_avg:59.03ms
step:1545/2330 train_time:91199ms step_avg:59.03ms
step:1546/2330 train_time:91262ms step_avg:59.03ms
step:1547/2330 train_time:91319ms step_avg:59.03ms
step:1548/2330 train_time:91381ms step_avg:59.03ms
step:1549/2330 train_time:91438ms step_avg:59.03ms
step:1550/2330 train_time:91500ms step_avg:59.03ms
step:1551/2330 train_time:91557ms step_avg:59.03ms
step:1552/2330 train_time:91620ms step_avg:59.03ms
step:1553/2330 train_time:91677ms step_avg:59.03ms
step:1554/2330 train_time:91740ms step_avg:59.03ms
step:1555/2330 train_time:91797ms step_avg:59.03ms
step:1556/2330 train_time:91860ms step_avg:59.04ms
step:1557/2330 train_time:91917ms step_avg:59.03ms
step:1558/2330 train_time:91980ms step_avg:59.04ms
step:1559/2330 train_time:92036ms step_avg:59.04ms
step:1560/2330 train_time:92100ms step_avg:59.04ms
step:1561/2330 train_time:92157ms step_avg:59.04ms
step:1562/2330 train_time:92221ms step_avg:59.04ms
step:1563/2330 train_time:92278ms step_avg:59.04ms
step:1564/2330 train_time:92339ms step_avg:59.04ms
step:1565/2330 train_time:92396ms step_avg:59.04ms
step:1566/2330 train_time:92459ms step_avg:59.04ms
step:1567/2330 train_time:92516ms step_avg:59.04ms
step:1568/2330 train_time:92578ms step_avg:59.04ms
step:1569/2330 train_time:92635ms step_avg:59.04ms
step:1570/2330 train_time:92699ms step_avg:59.04ms
step:1571/2330 train_time:92756ms step_avg:59.04ms
step:1572/2330 train_time:92819ms step_avg:59.05ms
step:1573/2330 train_time:92876ms step_avg:59.04ms
step:1574/2330 train_time:92939ms step_avg:59.05ms
step:1575/2330 train_time:92996ms step_avg:59.04ms
step:1576/2330 train_time:93059ms step_avg:59.05ms
step:1577/2330 train_time:93117ms step_avg:59.05ms
step:1578/2330 train_time:93180ms step_avg:59.05ms
step:1579/2330 train_time:93237ms step_avg:59.05ms
step:1580/2330 train_time:93298ms step_avg:59.05ms
step:1581/2330 train_time:93356ms step_avg:59.05ms
step:1582/2330 train_time:93418ms step_avg:59.05ms
step:1583/2330 train_time:93475ms step_avg:59.05ms
step:1584/2330 train_time:93538ms step_avg:59.05ms
step:1585/2330 train_time:93595ms step_avg:59.05ms
step:1586/2330 train_time:93658ms step_avg:59.05ms
step:1587/2330 train_time:93715ms step_avg:59.05ms
step:1588/2330 train_time:93780ms step_avg:59.06ms
step:1589/2330 train_time:93837ms step_avg:59.05ms
step:1590/2330 train_time:93900ms step_avg:59.06ms
step:1591/2330 train_time:93957ms step_avg:59.06ms
step:1592/2330 train_time:94021ms step_avg:59.06ms
step:1593/2330 train_time:94078ms step_avg:59.06ms
step:1594/2330 train_time:94141ms step_avg:59.06ms
step:1595/2330 train_time:94198ms step_avg:59.06ms
step:1596/2330 train_time:94260ms step_avg:59.06ms
step:1597/2330 train_time:94317ms step_avg:59.06ms
step:1598/2330 train_time:94379ms step_avg:59.06ms
step:1599/2330 train_time:94436ms step_avg:59.06ms
step:1600/2330 train_time:94500ms step_avg:59.06ms
step:1601/2330 train_time:94557ms step_avg:59.06ms
step:1602/2330 train_time:94619ms step_avg:59.06ms
step:1603/2330 train_time:94676ms step_avg:59.06ms
step:1604/2330 train_time:94739ms step_avg:59.06ms
step:1605/2330 train_time:94796ms step_avg:59.06ms
step:1606/2330 train_time:94858ms step_avg:59.06ms
step:1607/2330 train_time:94915ms step_avg:59.06ms
step:1608/2330 train_time:94979ms step_avg:59.07ms
step:1609/2330 train_time:95036ms step_avg:59.06ms
step:1610/2330 train_time:95099ms step_avg:59.07ms
step:1611/2330 train_time:95156ms step_avg:59.07ms
step:1612/2330 train_time:95219ms step_avg:59.07ms
step:1613/2330 train_time:95276ms step_avg:59.07ms
step:1614/2330 train_time:95338ms step_avg:59.07ms
step:1615/2330 train_time:95396ms step_avg:59.07ms
step:1616/2330 train_time:95459ms step_avg:59.07ms
step:1617/2330 train_time:95516ms step_avg:59.07ms
step:1618/2330 train_time:95579ms step_avg:59.07ms
step:1619/2330 train_time:95636ms step_avg:59.07ms
step:1620/2330 train_time:95699ms step_avg:59.07ms
step:1621/2330 train_time:95756ms step_avg:59.07ms
step:1622/2330 train_time:95819ms step_avg:59.07ms
step:1623/2330 train_time:95876ms step_avg:59.07ms
step:1624/2330 train_time:95939ms step_avg:59.08ms
step:1625/2330 train_time:95996ms step_avg:59.07ms
step:1626/2330 train_time:96059ms step_avg:59.08ms
step:1627/2330 train_time:96117ms step_avg:59.08ms
step:1628/2330 train_time:96180ms step_avg:59.08ms
step:1629/2330 train_time:96237ms step_avg:59.08ms
step:1630/2330 train_time:96299ms step_avg:59.08ms
step:1631/2330 train_time:96356ms step_avg:59.08ms
step:1632/2330 train_time:96420ms step_avg:59.08ms
step:1633/2330 train_time:96476ms step_avg:59.08ms
step:1634/2330 train_time:96539ms step_avg:59.08ms
step:1635/2330 train_time:96596ms step_avg:59.08ms
step:1636/2330 train_time:96659ms step_avg:59.08ms
step:1637/2330 train_time:96717ms step_avg:59.08ms
step:1638/2330 train_time:96779ms step_avg:59.08ms
step:1639/2330 train_time:96836ms step_avg:59.08ms
step:1640/2330 train_time:96899ms step_avg:59.08ms
step:1641/2330 train_time:96956ms step_avg:59.08ms
step:1642/2330 train_time:97020ms step_avg:59.09ms
step:1643/2330 train_time:97077ms step_avg:59.09ms
step:1644/2330 train_time:97140ms step_avg:59.09ms
step:1645/2330 train_time:97197ms step_avg:59.09ms
step:1646/2330 train_time:97260ms step_avg:59.09ms
step:1647/2330 train_time:97317ms step_avg:59.09ms
step:1648/2330 train_time:97380ms step_avg:59.09ms
step:1649/2330 train_time:97437ms step_avg:59.09ms
step:1650/2330 train_time:97499ms step_avg:59.09ms
step:1651/2330 train_time:97556ms step_avg:59.09ms
step:1652/2330 train_time:97619ms step_avg:59.09ms
step:1653/2330 train_time:97676ms step_avg:59.09ms
step:1654/2330 train_time:97739ms step_avg:59.09ms
step:1655/2330 train_time:97796ms step_avg:59.09ms
step:1656/2330 train_time:97859ms step_avg:59.09ms
step:1657/2330 train_time:97916ms step_avg:59.09ms
step:1658/2330 train_time:97980ms step_avg:59.10ms
step:1659/2330 train_time:98037ms step_avg:59.09ms
step:1660/2330 train_time:98099ms step_avg:59.10ms
step:1661/2330 train_time:98156ms step_avg:59.09ms
step:1662/2330 train_time:98220ms step_avg:59.10ms
step:1663/2330 train_time:98277ms step_avg:59.10ms
step:1664/2330 train_time:98340ms step_avg:59.10ms
step:1665/2330 train_time:98397ms step_avg:59.10ms
step:1666/2330 train_time:98459ms step_avg:59.10ms
step:1667/2330 train_time:98517ms step_avg:59.10ms
step:1668/2330 train_time:98578ms step_avg:59.10ms
step:1669/2330 train_time:98635ms step_avg:59.10ms
step:1670/2330 train_time:98698ms step_avg:59.10ms
step:1671/2330 train_time:98755ms step_avg:59.10ms
step:1672/2330 train_time:98819ms step_avg:59.10ms
step:1673/2330 train_time:98876ms step_avg:59.10ms
step:1674/2330 train_time:98938ms step_avg:59.10ms
step:1675/2330 train_time:98995ms step_avg:59.10ms
step:1676/2330 train_time:99058ms step_avg:59.10ms
step:1677/2330 train_time:99115ms step_avg:59.10ms
step:1678/2330 train_time:99178ms step_avg:59.10ms
step:1679/2330 train_time:99235ms step_avg:59.10ms
step:1680/2330 train_time:99298ms step_avg:59.11ms
step:1681/2330 train_time:99356ms step_avg:59.11ms
step:1682/2330 train_time:99419ms step_avg:59.11ms
step:1683/2330 train_time:99475ms step_avg:59.11ms
step:1684/2330 train_time:99538ms step_avg:59.11ms
step:1685/2330 train_time:99595ms step_avg:59.11ms
step:1686/2330 train_time:99657ms step_avg:59.11ms
step:1687/2330 train_time:99715ms step_avg:59.11ms
step:1688/2330 train_time:99778ms step_avg:59.11ms
step:1689/2330 train_time:99835ms step_avg:59.11ms
step:1690/2330 train_time:99898ms step_avg:59.11ms
step:1691/2330 train_time:99955ms step_avg:59.11ms
step:1692/2330 train_time:100018ms step_avg:59.11ms
step:1693/2330 train_time:100075ms step_avg:59.11ms
step:1694/2330 train_time:100139ms step_avg:59.11ms
step:1695/2330 train_time:100196ms step_avg:59.11ms
step:1696/2330 train_time:100259ms step_avg:59.11ms
step:1697/2330 train_time:100316ms step_avg:59.11ms
step:1698/2330 train_time:100378ms step_avg:59.12ms
step:1699/2330 train_time:100435ms step_avg:59.11ms
step:1700/2330 train_time:100498ms step_avg:59.12ms
step:1701/2330 train_time:100554ms step_avg:59.11ms
step:1702/2330 train_time:100618ms step_avg:59.12ms
step:1703/2330 train_time:100674ms step_avg:59.12ms
step:1704/2330 train_time:100737ms step_avg:59.12ms
step:1705/2330 train_time:100794ms step_avg:59.12ms
step:1706/2330 train_time:100858ms step_avg:59.12ms
step:1707/2330 train_time:100915ms step_avg:59.12ms
step:1708/2330 train_time:100978ms step_avg:59.12ms
step:1709/2330 train_time:101035ms step_avg:59.12ms
step:1710/2330 train_time:101098ms step_avg:59.12ms
step:1711/2330 train_time:101155ms step_avg:59.12ms
step:1712/2330 train_time:101219ms step_avg:59.12ms
step:1713/2330 train_time:101276ms step_avg:59.12ms
step:1714/2330 train_time:101339ms step_avg:59.12ms
step:1715/2330 train_time:101396ms step_avg:59.12ms
step:1716/2330 train_time:101459ms step_avg:59.13ms
step:1717/2330 train_time:101516ms step_avg:59.12ms
step:1718/2330 train_time:101579ms step_avg:59.13ms
step:1719/2330 train_time:101636ms step_avg:59.12ms
step:1720/2330 train_time:101698ms step_avg:59.13ms
step:1721/2330 train_time:101755ms step_avg:59.13ms
step:1722/2330 train_time:101819ms step_avg:59.13ms
step:1723/2330 train_time:101876ms step_avg:59.13ms
step:1724/2330 train_time:101939ms step_avg:59.13ms
step:1725/2330 train_time:101996ms step_avg:59.13ms
step:1726/2330 train_time:102059ms step_avg:59.13ms
step:1727/2330 train_time:102117ms step_avg:59.13ms
step:1728/2330 train_time:102179ms step_avg:59.13ms
step:1729/2330 train_time:102236ms step_avg:59.13ms
step:1730/2330 train_time:102299ms step_avg:59.13ms
step:1731/2330 train_time:102356ms step_avg:59.13ms
step:1732/2330 train_time:102419ms step_avg:59.13ms
step:1733/2330 train_time:102476ms step_avg:59.13ms
step:1734/2330 train_time:102539ms step_avg:59.13ms
step:1735/2330 train_time:102596ms step_avg:59.13ms
step:1736/2330 train_time:102659ms step_avg:59.14ms
step:1737/2330 train_time:102717ms step_avg:59.13ms
step:1738/2330 train_time:102779ms step_avg:59.14ms
step:1739/2330 train_time:102836ms step_avg:59.14ms
step:1740/2330 train_time:102899ms step_avg:59.14ms
step:1741/2330 train_time:102956ms step_avg:59.14ms
step:1742/2330 train_time:103019ms step_avg:59.14ms
step:1743/2330 train_time:103076ms step_avg:59.14ms
step:1744/2330 train_time:103140ms step_avg:59.14ms
step:1745/2330 train_time:103196ms step_avg:59.14ms
step:1746/2330 train_time:103259ms step_avg:59.14ms
step:1747/2330 train_time:103316ms step_avg:59.14ms
step:1748/2330 train_time:103379ms step_avg:59.14ms
step:1749/2330 train_time:103437ms step_avg:59.14ms
step:1750/2330 train_time:103499ms step_avg:59.14ms
step:1750/2330 val_loss:3.8728 train_time:103578ms step_avg:59.19ms
step:1751/2330 train_time:103597ms step_avg:59.16ms
step:1752/2330 train_time:103621ms step_avg:59.14ms
step:1753/2330 train_time:103681ms step_avg:59.14ms
step:1754/2330 train_time:103748ms step_avg:59.15ms
step:1755/2330 train_time:103805ms step_avg:59.15ms
step:1756/2330 train_time:103869ms step_avg:59.15ms
step:1757/2330 train_time:103926ms step_avg:59.15ms
step:1758/2330 train_time:103988ms step_avg:59.15ms
step:1759/2330 train_time:104045ms step_avg:59.15ms
step:1760/2330 train_time:104106ms step_avg:59.15ms
step:1761/2330 train_time:104162ms step_avg:59.15ms
step:1762/2330 train_time:104224ms step_avg:59.15ms
step:1763/2330 train_time:104280ms step_avg:59.15ms
step:1764/2330 train_time:104342ms step_avg:59.15ms
step:1765/2330 train_time:104399ms step_avg:59.15ms
step:1766/2330 train_time:104460ms step_avg:59.15ms
step:1767/2330 train_time:104517ms step_avg:59.15ms
step:1768/2330 train_time:104580ms step_avg:59.15ms
step:1769/2330 train_time:104637ms step_avg:59.15ms
step:1770/2330 train_time:104703ms step_avg:59.15ms
step:1771/2330 train_time:104761ms step_avg:59.15ms
step:1772/2330 train_time:104824ms step_avg:59.16ms
step:1773/2330 train_time:104881ms step_avg:59.15ms
step:1774/2330 train_time:104945ms step_avg:59.16ms
step:1775/2330 train_time:105002ms step_avg:59.16ms
step:1776/2330 train_time:105065ms step_avg:59.16ms
step:1777/2330 train_time:105122ms step_avg:59.16ms
step:1778/2330 train_time:105184ms step_avg:59.16ms
step:1779/2330 train_time:105241ms step_avg:59.16ms
step:1780/2330 train_time:105303ms step_avg:59.16ms
step:1781/2330 train_time:105360ms step_avg:59.16ms
step:1782/2330 train_time:105421ms step_avg:59.16ms
step:1783/2330 train_time:105478ms step_avg:59.16ms
step:1784/2330 train_time:105541ms step_avg:59.16ms
step:1785/2330 train_time:105597ms step_avg:59.16ms
step:1786/2330 train_time:105661ms step_avg:59.16ms
step:1787/2330 train_time:105719ms step_avg:59.16ms
step:1788/2330 train_time:105783ms step_avg:59.16ms
step:1789/2330 train_time:105840ms step_avg:59.16ms
step:1790/2330 train_time:105904ms step_avg:59.16ms
step:1791/2330 train_time:105961ms step_avg:59.16ms
step:1792/2330 train_time:106024ms step_avg:59.17ms
step:1793/2330 train_time:106081ms step_avg:59.16ms
step:1794/2330 train_time:106143ms step_avg:59.17ms
step:1795/2330 train_time:106200ms step_avg:59.16ms
step:1796/2330 train_time:106262ms step_avg:59.17ms
step:1797/2330 train_time:106319ms step_avg:59.16ms
step:1798/2330 train_time:106381ms step_avg:59.17ms
step:1799/2330 train_time:106438ms step_avg:59.16ms
step:1800/2330 train_time:106500ms step_avg:59.17ms
step:1801/2330 train_time:106557ms step_avg:59.17ms
step:1802/2330 train_time:106621ms step_avg:59.17ms
step:1803/2330 train_time:106678ms step_avg:59.17ms
step:1804/2330 train_time:106742ms step_avg:59.17ms
step:1805/2330 train_time:106799ms step_avg:59.17ms
step:1806/2330 train_time:106863ms step_avg:59.17ms
step:1807/2330 train_time:106920ms step_avg:59.17ms
step:1808/2330 train_time:106984ms step_avg:59.17ms
step:1809/2330 train_time:107040ms step_avg:59.17ms
step:1810/2330 train_time:107104ms step_avg:59.17ms
step:1811/2330 train_time:107161ms step_avg:59.17ms
step:1812/2330 train_time:107223ms step_avg:59.17ms
step:1813/2330 train_time:107280ms step_avg:59.17ms
step:1814/2330 train_time:107342ms step_avg:59.17ms
step:1815/2330 train_time:107399ms step_avg:59.17ms
step:1816/2330 train_time:107461ms step_avg:59.17ms
step:1817/2330 train_time:107518ms step_avg:59.17ms
step:1818/2330 train_time:107581ms step_avg:59.18ms
step:1819/2330 train_time:107638ms step_avg:59.17ms
step:1820/2330 train_time:107702ms step_avg:59.18ms
step:1821/2330 train_time:107759ms step_avg:59.18ms
step:1822/2330 train_time:107823ms step_avg:59.18ms
step:1823/2330 train_time:107881ms step_avg:59.18ms
step:1824/2330 train_time:107943ms step_avg:59.18ms
step:1825/2330 train_time:108000ms step_avg:59.18ms
step:1826/2330 train_time:108063ms step_avg:59.18ms
step:1827/2330 train_time:108120ms step_avg:59.18ms
step:1828/2330 train_time:108183ms step_avg:59.18ms
step:1829/2330 train_time:108239ms step_avg:59.18ms
step:1830/2330 train_time:108302ms step_avg:59.18ms
step:1831/2330 train_time:108359ms step_avg:59.18ms
step:1832/2330 train_time:108421ms step_avg:59.18ms
step:1833/2330 train_time:108478ms step_avg:59.18ms
step:1834/2330 train_time:108540ms step_avg:59.18ms
step:1835/2330 train_time:108597ms step_avg:59.18ms
step:1836/2330 train_time:108661ms step_avg:59.18ms
step:1837/2330 train_time:108718ms step_avg:59.18ms
step:1838/2330 train_time:108782ms step_avg:59.18ms
step:1839/2330 train_time:108838ms step_avg:59.18ms
step:1840/2330 train_time:108902ms step_avg:59.19ms
step:1841/2330 train_time:108959ms step_avg:59.18ms
step:1842/2330 train_time:109023ms step_avg:59.19ms
step:1843/2330 train_time:109080ms step_avg:59.19ms
step:1844/2330 train_time:109142ms step_avg:59.19ms
step:1845/2330 train_time:109200ms step_avg:59.19ms
step:1846/2330 train_time:109263ms step_avg:59.19ms
step:1847/2330 train_time:109320ms step_avg:59.19ms
step:1848/2330 train_time:109382ms step_avg:59.19ms
step:1849/2330 train_time:109439ms step_avg:59.19ms
step:1850/2330 train_time:109502ms step_avg:59.19ms
step:1851/2330 train_time:109559ms step_avg:59.19ms
step:1852/2330 train_time:109621ms step_avg:59.19ms
step:1853/2330 train_time:109678ms step_avg:59.19ms
step:1854/2330 train_time:109742ms step_avg:59.19ms
step:1855/2330 train_time:109799ms step_avg:59.19ms
step:1856/2330 train_time:109863ms step_avg:59.19ms
step:1857/2330 train_time:109920ms step_avg:59.19ms
step:1858/2330 train_time:109983ms step_avg:59.19ms
step:1859/2330 train_time:110040ms step_avg:59.19ms
step:1860/2330 train_time:110104ms step_avg:59.20ms
step:1861/2330 train_time:110161ms step_avg:59.19ms
step:1862/2330 train_time:110223ms step_avg:59.20ms
step:1863/2330 train_time:110280ms step_avg:59.19ms
step:1864/2330 train_time:110343ms step_avg:59.20ms
step:1865/2330 train_time:110400ms step_avg:59.20ms
step:1866/2330 train_time:110462ms step_avg:59.20ms
step:1867/2330 train_time:110519ms step_avg:59.20ms
step:1868/2330 train_time:110582ms step_avg:59.20ms
step:1869/2330 train_time:110639ms step_avg:59.20ms
step:1870/2330 train_time:110702ms step_avg:59.20ms
step:1871/2330 train_time:110759ms step_avg:59.20ms
step:1872/2330 train_time:110822ms step_avg:59.20ms
step:1873/2330 train_time:110878ms step_avg:59.20ms
step:1874/2330 train_time:110942ms step_avg:59.20ms
step:1875/2330 train_time:110999ms step_avg:59.20ms
step:1876/2330 train_time:111061ms step_avg:59.20ms
step:1877/2330 train_time:111119ms step_avg:59.20ms
step:1878/2330 train_time:111181ms step_avg:59.20ms
step:1879/2330 train_time:111239ms step_avg:59.20ms
step:1880/2330 train_time:111301ms step_avg:59.20ms
step:1881/2330 train_time:111358ms step_avg:59.20ms
step:1882/2330 train_time:111421ms step_avg:59.20ms
step:1883/2330 train_time:111478ms step_avg:59.20ms
step:1884/2330 train_time:111541ms step_avg:59.20ms
step:1885/2330 train_time:111598ms step_avg:59.20ms
step:1886/2330 train_time:111662ms step_avg:59.21ms
step:1887/2330 train_time:111719ms step_avg:59.20ms
step:1888/2330 train_time:111782ms step_avg:59.21ms
step:1889/2330 train_time:111839ms step_avg:59.21ms
step:1890/2330 train_time:111902ms step_avg:59.21ms
step:1891/2330 train_time:111960ms step_avg:59.21ms
step:1892/2330 train_time:112023ms step_avg:59.21ms
step:1893/2330 train_time:112080ms step_avg:59.21ms
step:1894/2330 train_time:112142ms step_avg:59.21ms
step:1895/2330 train_time:112199ms step_avg:59.21ms
step:1896/2330 train_time:112262ms step_avg:59.21ms
step:1897/2330 train_time:112320ms step_avg:59.21ms
step:1898/2330 train_time:112382ms step_avg:59.21ms
step:1899/2330 train_time:112439ms step_avg:59.21ms
step:1900/2330 train_time:112502ms step_avg:59.21ms
step:1901/2330 train_time:112559ms step_avg:59.21ms
step:1902/2330 train_time:112622ms step_avg:59.21ms
step:1903/2330 train_time:112679ms step_avg:59.21ms
step:1904/2330 train_time:112742ms step_avg:59.21ms
step:1905/2330 train_time:112799ms step_avg:59.21ms
step:1906/2330 train_time:112862ms step_avg:59.21ms
step:1907/2330 train_time:112920ms step_avg:59.21ms
step:1908/2330 train_time:112983ms step_avg:59.22ms
step:1909/2330 train_time:113039ms step_avg:59.21ms
step:1910/2330 train_time:113102ms step_avg:59.22ms
step:1911/2330 train_time:113159ms step_avg:59.21ms
step:1912/2330 train_time:113222ms step_avg:59.22ms
step:1913/2330 train_time:113279ms step_avg:59.22ms
step:1914/2330 train_time:113343ms step_avg:59.22ms
step:1915/2330 train_time:113400ms step_avg:59.22ms
step:1916/2330 train_time:113463ms step_avg:59.22ms
step:1917/2330 train_time:113520ms step_avg:59.22ms
step:1918/2330 train_time:113583ms step_avg:59.22ms
step:1919/2330 train_time:113640ms step_avg:59.22ms
step:1920/2330 train_time:113702ms step_avg:59.22ms
step:1921/2330 train_time:113759ms step_avg:59.22ms
step:1922/2330 train_time:113822ms step_avg:59.22ms
step:1923/2330 train_time:113879ms step_avg:59.22ms
step:1924/2330 train_time:113942ms step_avg:59.22ms
step:1925/2330 train_time:113999ms step_avg:59.22ms
step:1926/2330 train_time:114062ms step_avg:59.22ms
step:1927/2330 train_time:114119ms step_avg:59.22ms
step:1928/2330 train_time:114183ms step_avg:59.22ms
step:1929/2330 train_time:114240ms step_avg:59.22ms
step:1930/2330 train_time:114302ms step_avg:59.22ms
step:1931/2330 train_time:114359ms step_avg:59.22ms
step:1932/2330 train_time:114422ms step_avg:59.22ms
step:1933/2330 train_time:114479ms step_avg:59.22ms
step:1934/2330 train_time:114542ms step_avg:59.23ms
step:1935/2330 train_time:114599ms step_avg:59.22ms
step:1936/2330 train_time:114662ms step_avg:59.23ms
step:1937/2330 train_time:114719ms step_avg:59.23ms
step:1938/2330 train_time:114782ms step_avg:59.23ms
step:1939/2330 train_time:114838ms step_avg:59.23ms
step:1940/2330 train_time:114901ms step_avg:59.23ms
step:1941/2330 train_time:114959ms step_avg:59.23ms
step:1942/2330 train_time:115021ms step_avg:59.23ms
step:1943/2330 train_time:115078ms step_avg:59.23ms
step:1944/2330 train_time:115142ms step_avg:59.23ms
step:1945/2330 train_time:115198ms step_avg:59.23ms
step:1946/2330 train_time:115262ms step_avg:59.23ms
step:1947/2330 train_time:115319ms step_avg:59.23ms
step:1948/2330 train_time:115382ms step_avg:59.23ms
step:1949/2330 train_time:115439ms step_avg:59.23ms
step:1950/2330 train_time:115501ms step_avg:59.23ms
step:1951/2330 train_time:115559ms step_avg:59.23ms
step:1952/2330 train_time:115622ms step_avg:59.23ms
step:1953/2330 train_time:115679ms step_avg:59.23ms
step:1954/2330 train_time:115741ms step_avg:59.23ms
step:1955/2330 train_time:115799ms step_avg:59.23ms
step:1956/2330 train_time:115862ms step_avg:59.23ms
step:1957/2330 train_time:115920ms step_avg:59.23ms
step:1958/2330 train_time:115982ms step_avg:59.23ms
step:1959/2330 train_time:116039ms step_avg:59.23ms
step:1960/2330 train_time:116102ms step_avg:59.24ms
step:1961/2330 train_time:116158ms step_avg:59.23ms
step:1962/2330 train_time:116222ms step_avg:59.24ms
step:1963/2330 train_time:116279ms step_avg:59.24ms
step:1964/2330 train_time:116342ms step_avg:59.24ms
step:1965/2330 train_time:116399ms step_avg:59.24ms
step:1966/2330 train_time:116462ms step_avg:59.24ms
step:1967/2330 train_time:116519ms step_avg:59.24ms
step:1968/2330 train_time:116581ms step_avg:59.24ms
step:1969/2330 train_time:116638ms step_avg:59.24ms
step:1970/2330 train_time:116702ms step_avg:59.24ms
step:1971/2330 train_time:116759ms step_avg:59.24ms
step:1972/2330 train_time:116822ms step_avg:59.24ms
step:1973/2330 train_time:116879ms step_avg:59.24ms
step:1974/2330 train_time:116942ms step_avg:59.24ms
step:1975/2330 train_time:116999ms step_avg:59.24ms
step:1976/2330 train_time:117061ms step_avg:59.24ms
step:1977/2330 train_time:117119ms step_avg:59.24ms
step:1978/2330 train_time:117182ms step_avg:59.24ms
step:1979/2330 train_time:117239ms step_avg:59.24ms
step:1980/2330 train_time:117302ms step_avg:59.24ms
step:1981/2330 train_time:117359ms step_avg:59.24ms
step:1982/2330 train_time:117422ms step_avg:59.24ms
step:1983/2330 train_time:117479ms step_avg:59.24ms
step:1984/2330 train_time:117541ms step_avg:59.24ms
step:1985/2330 train_time:117598ms step_avg:59.24ms
step:1986/2330 train_time:117662ms step_avg:59.25ms
step:1987/2330 train_time:117720ms step_avg:59.24ms
step:1988/2330 train_time:117782ms step_avg:59.25ms
step:1989/2330 train_time:117839ms step_avg:59.25ms
step:1990/2330 train_time:117903ms step_avg:59.25ms
step:1991/2330 train_time:117960ms step_avg:59.25ms
step:1992/2330 train_time:118023ms step_avg:59.25ms
step:1993/2330 train_time:118080ms step_avg:59.25ms
step:1994/2330 train_time:118143ms step_avg:59.25ms
step:1995/2330 train_time:118200ms step_avg:59.25ms
step:1996/2330 train_time:118262ms step_avg:59.25ms
step:1997/2330 train_time:118320ms step_avg:59.25ms
step:1998/2330 train_time:118382ms step_avg:59.25ms
step:1999/2330 train_time:118439ms step_avg:59.25ms
step:2000/2330 train_time:118502ms step_avg:59.25ms
step:2000/2330 val_loss:3.8069 train_time:118582ms step_avg:59.29ms
step:2001/2330 train_time:118601ms step_avg:59.27ms
step:2002/2330 train_time:118625ms step_avg:59.25ms
step:2003/2330 train_time:118685ms step_avg:59.25ms
step:2004/2330 train_time:118753ms step_avg:59.26ms
step:2005/2330 train_time:118810ms step_avg:59.26ms
step:2006/2330 train_time:118873ms step_avg:59.26ms
step:2007/2330 train_time:118930ms step_avg:59.26ms
step:2008/2330 train_time:118992ms step_avg:59.26ms
step:2009/2330 train_time:119048ms step_avg:59.26ms
step:2010/2330 train_time:119111ms step_avg:59.26ms
step:2011/2330 train_time:119168ms step_avg:59.26ms
step:2012/2330 train_time:119229ms step_avg:59.26ms
step:2013/2330 train_time:119286ms step_avg:59.26ms
step:2014/2330 train_time:119348ms step_avg:59.26ms
step:2015/2330 train_time:119404ms step_avg:59.26ms
step:2016/2330 train_time:119466ms step_avg:59.26ms
step:2017/2330 train_time:119523ms step_avg:59.26ms
step:2018/2330 train_time:119585ms step_avg:59.26ms
step:2019/2330 train_time:119642ms step_avg:59.26ms
step:2020/2330 train_time:119708ms step_avg:59.26ms
step:2021/2330 train_time:119765ms step_avg:59.26ms
step:2022/2330 train_time:119830ms step_avg:59.26ms
step:2023/2330 train_time:119887ms step_avg:59.26ms
step:2024/2330 train_time:119949ms step_avg:59.26ms
step:2025/2330 train_time:120006ms step_avg:59.26ms
step:2026/2330 train_time:120070ms step_avg:59.26ms
step:2027/2330 train_time:120127ms step_avg:59.26ms
step:2028/2330 train_time:120189ms step_avg:59.26ms
step:2029/2330 train_time:120246ms step_avg:59.26ms
step:2030/2330 train_time:120307ms step_avg:59.26ms
step:2031/2330 train_time:120364ms step_avg:59.26ms
step:2032/2330 train_time:120426ms step_avg:59.26ms
step:2033/2330 train_time:120482ms step_avg:59.26ms
step:2034/2330 train_time:120544ms step_avg:59.26ms
step:2035/2330 train_time:120602ms step_avg:59.26ms
step:2036/2330 train_time:120665ms step_avg:59.27ms
step:2037/2330 train_time:120722ms step_avg:59.26ms
step:2038/2330 train_time:120787ms step_avg:59.27ms
step:2039/2330 train_time:120844ms step_avg:59.27ms
step:2040/2330 train_time:120907ms step_avg:59.27ms
step:2041/2330 train_time:120965ms step_avg:59.27ms
step:2042/2330 train_time:121028ms step_avg:59.27ms
step:2043/2330 train_time:121085ms step_avg:59.27ms
step:2044/2330 train_time:121148ms step_avg:59.27ms
step:2045/2330 train_time:121206ms step_avg:59.27ms
step:2046/2330 train_time:121268ms step_avg:59.27ms
step:2047/2330 train_time:121325ms step_avg:59.27ms
step:2048/2330 train_time:121386ms step_avg:59.27ms
step:2049/2330 train_time:121443ms step_avg:59.27ms
step:2050/2330 train_time:121505ms step_avg:59.27ms
step:2051/2330 train_time:121562ms step_avg:59.27ms
step:2052/2330 train_time:121626ms step_avg:59.27ms
step:2053/2330 train_time:121683ms step_avg:59.27ms
step:2054/2330 train_time:121747ms step_avg:59.27ms
step:2055/2330 train_time:121804ms step_avg:59.27ms
step:2056/2330 train_time:121867ms step_avg:59.27ms
step:2057/2330 train_time:121924ms step_avg:59.27ms
step:2058/2330 train_time:121988ms step_avg:59.28ms
step:2059/2330 train_time:122046ms step_avg:59.27ms
step:2060/2330 train_time:122108ms step_avg:59.28ms
step:2061/2330 train_time:122165ms step_avg:59.27ms
step:2062/2330 train_time:122228ms step_avg:59.28ms
step:2063/2330 train_time:122285ms step_avg:59.28ms
step:2064/2330 train_time:122346ms step_avg:59.28ms
step:2065/2330 train_time:122404ms step_avg:59.28ms
step:2066/2330 train_time:122465ms step_avg:59.28ms
step:2067/2330 train_time:122522ms step_avg:59.28ms
step:2068/2330 train_time:122585ms step_avg:59.28ms
step:2069/2330 train_time:122642ms step_avg:59.28ms
step:2070/2330 train_time:122705ms step_avg:59.28ms
step:2071/2330 train_time:122763ms step_avg:59.28ms
step:2072/2330 train_time:122826ms step_avg:59.28ms
step:2073/2330 train_time:122884ms step_avg:59.28ms
step:2074/2330 train_time:122947ms step_avg:59.28ms
step:2075/2330 train_time:123004ms step_avg:59.28ms
step:2076/2330 train_time:123067ms step_avg:59.28ms
step:2077/2330 train_time:123125ms step_avg:59.28ms
step:2078/2330 train_time:123188ms step_avg:59.28ms
step:2079/2330 train_time:123245ms step_avg:59.28ms
step:2080/2330 train_time:123306ms step_avg:59.28ms
step:2081/2330 train_time:123363ms step_avg:59.28ms
step:2082/2330 train_time:123426ms step_avg:59.28ms
step:2083/2330 train_time:123483ms step_avg:59.28ms
step:2084/2330 train_time:123546ms step_avg:59.28ms
step:2085/2330 train_time:123603ms step_avg:59.28ms
step:2086/2330 train_time:123666ms step_avg:59.28ms
step:2087/2330 train_time:123723ms step_avg:59.28ms
step:2088/2330 train_time:123785ms step_avg:59.28ms
step:2089/2330 train_time:123842ms step_avg:59.28ms
step:2090/2330 train_time:123907ms step_avg:59.29ms
step:2091/2330 train_time:123964ms step_avg:59.28ms
step:2092/2330 train_time:124027ms step_avg:59.29ms
step:2093/2330 train_time:124084ms step_avg:59.29ms
step:2094/2330 train_time:124148ms step_avg:59.29ms
step:2095/2330 train_time:124204ms step_avg:59.29ms
step:2096/2330 train_time:124267ms step_avg:59.29ms
step:2097/2330 train_time:124324ms step_avg:59.29ms
step:2098/2330 train_time:124386ms step_avg:59.29ms
step:2099/2330 train_time:124444ms step_avg:59.29ms
step:2100/2330 train_time:124506ms step_avg:59.29ms
step:2101/2330 train_time:124563ms step_avg:59.29ms
step:2102/2330 train_time:124626ms step_avg:59.29ms
step:2103/2330 train_time:124683ms step_avg:59.29ms
step:2104/2330 train_time:124746ms step_avg:59.29ms
step:2105/2330 train_time:124803ms step_avg:59.29ms
step:2106/2330 train_time:124867ms step_avg:59.29ms
step:2107/2330 train_time:124924ms step_avg:59.29ms
step:2108/2330 train_time:124988ms step_avg:59.29ms
step:2109/2330 train_time:125045ms step_avg:59.29ms
step:2110/2330 train_time:125108ms step_avg:59.29ms
step:2111/2330 train_time:125165ms step_avg:59.29ms
step:2112/2330 train_time:125228ms step_avg:59.29ms
step:2113/2330 train_time:125285ms step_avg:59.29ms
step:2114/2330 train_time:125348ms step_avg:59.29ms
step:2115/2330 train_time:125405ms step_avg:59.29ms
step:2116/2330 train_time:125467ms step_avg:59.29ms
step:2117/2330 train_time:125523ms step_avg:59.29ms
step:2118/2330 train_time:125586ms step_avg:59.29ms
step:2119/2330 train_time:125643ms step_avg:59.29ms
step:2120/2330 train_time:125706ms step_avg:59.30ms
step:2121/2330 train_time:125763ms step_avg:59.29ms
step:2122/2330 train_time:125826ms step_avg:59.30ms
step:2123/2330 train_time:125883ms step_avg:59.29ms
step:2124/2330 train_time:125946ms step_avg:59.30ms
step:2125/2330 train_time:126003ms step_avg:59.30ms
step:2126/2330 train_time:126066ms step_avg:59.30ms
step:2127/2330 train_time:126124ms step_avg:59.30ms
step:2128/2330 train_time:126187ms step_avg:59.30ms
step:2129/2330 train_time:126244ms step_avg:59.30ms
step:2130/2330 train_time:126306ms step_avg:59.30ms
step:2131/2330 train_time:126363ms step_avg:59.30ms
step:2132/2330 train_time:126426ms step_avg:59.30ms
step:2133/2330 train_time:126483ms step_avg:59.30ms
step:2134/2330 train_time:126547ms step_avg:59.30ms
step:2135/2330 train_time:126604ms step_avg:59.30ms
step:2136/2330 train_time:126666ms step_avg:59.30ms
step:2137/2330 train_time:126723ms step_avg:59.30ms
step:2138/2330 train_time:126785ms step_avg:59.30ms
step:2139/2330 train_time:126843ms step_avg:59.30ms
step:2140/2330 train_time:126906ms step_avg:59.30ms
step:2141/2330 train_time:126962ms step_avg:59.30ms
step:2142/2330 train_time:127026ms step_avg:59.30ms
step:2143/2330 train_time:127084ms step_avg:59.30ms
step:2144/2330 train_time:127146ms step_avg:59.30ms
step:2145/2330 train_time:127203ms step_avg:59.30ms
step:2146/2330 train_time:127267ms step_avg:59.30ms
step:2147/2330 train_time:127324ms step_avg:59.30ms
step:2148/2330 train_time:127387ms step_avg:59.30ms
step:2149/2330 train_time:127444ms step_avg:59.30ms
step:2150/2330 train_time:127507ms step_avg:59.31ms
step:2151/2330 train_time:127563ms step_avg:59.30ms
step:2152/2330 train_time:127626ms step_avg:59.31ms
step:2153/2330 train_time:127684ms step_avg:59.30ms
step:2154/2330 train_time:127746ms step_avg:59.31ms
step:2155/2330 train_time:127803ms step_avg:59.31ms
step:2156/2330 train_time:127866ms step_avg:59.31ms
step:2157/2330 train_time:127923ms step_avg:59.31ms
step:2158/2330 train_time:127986ms step_avg:59.31ms
step:2159/2330 train_time:128044ms step_avg:59.31ms
step:2160/2330 train_time:128107ms step_avg:59.31ms
step:2161/2330 train_time:128164ms step_avg:59.31ms
step:2162/2330 train_time:128227ms step_avg:59.31ms
step:2163/2330 train_time:128284ms step_avg:59.31ms
step:2164/2330 train_time:128347ms step_avg:59.31ms
step:2165/2330 train_time:128404ms step_avg:59.31ms
step:2166/2330 train_time:128467ms step_avg:59.31ms
step:2167/2330 train_time:128524ms step_avg:59.31ms
step:2168/2330 train_time:128586ms step_avg:59.31ms
step:2169/2330 train_time:128644ms step_avg:59.31ms
step:2170/2330 train_time:128707ms step_avg:59.31ms
step:2171/2330 train_time:128763ms step_avg:59.31ms
step:2172/2330 train_time:128827ms step_avg:59.31ms
step:2173/2330 train_time:128884ms step_avg:59.31ms
step:2174/2330 train_time:128947ms step_avg:59.31ms
step:2175/2330 train_time:129004ms step_avg:59.31ms
step:2176/2330 train_time:129067ms step_avg:59.31ms
step:2177/2330 train_time:129124ms step_avg:59.31ms
step:2178/2330 train_time:129187ms step_avg:59.31ms
step:2179/2330 train_time:129244ms step_avg:59.31ms
step:2180/2330 train_time:129308ms step_avg:59.32ms
step:2181/2330 train_time:129365ms step_avg:59.31ms
step:2182/2330 train_time:129428ms step_avg:59.32ms
step:2183/2330 train_time:129485ms step_avg:59.32ms
step:2184/2330 train_time:129547ms step_avg:59.32ms
step:2185/2330 train_time:129604ms step_avg:59.32ms
step:2186/2330 train_time:129666ms step_avg:59.32ms
step:2187/2330 train_time:129723ms step_avg:59.32ms
step:2188/2330 train_time:129787ms step_avg:59.32ms
step:2189/2330 train_time:129844ms step_avg:59.32ms
step:2190/2330 train_time:129907ms step_avg:59.32ms
step:2191/2330 train_time:129964ms step_avg:59.32ms
step:2192/2330 train_time:130027ms step_avg:59.32ms
step:2193/2330 train_time:130084ms step_avg:59.32ms
step:2194/2330 train_time:130147ms step_avg:59.32ms
step:2195/2330 train_time:130204ms step_avg:59.32ms
step:2196/2330 train_time:130266ms step_avg:59.32ms
step:2197/2330 train_time:130323ms step_avg:59.32ms
step:2198/2330 train_time:130387ms step_avg:59.32ms
step:2199/2330 train_time:130444ms step_avg:59.32ms
step:2200/2330 train_time:130507ms step_avg:59.32ms
step:2201/2330 train_time:130564ms step_avg:59.32ms
step:2202/2330 train_time:130627ms step_avg:59.32ms
step:2203/2330 train_time:130684ms step_avg:59.32ms
step:2204/2330 train_time:130747ms step_avg:59.32ms
step:2205/2330 train_time:130805ms step_avg:59.32ms
step:2206/2330 train_time:130867ms step_avg:59.32ms
step:2207/2330 train_time:130923ms step_avg:59.32ms
step:2208/2330 train_time:130987ms step_avg:59.32ms
step:2209/2330 train_time:131045ms step_avg:59.32ms
step:2210/2330 train_time:131107ms step_avg:59.32ms
step:2211/2330 train_time:131164ms step_avg:59.32ms
step:2212/2330 train_time:131226ms step_avg:59.32ms
step:2213/2330 train_time:131283ms step_avg:59.32ms
step:2214/2330 train_time:131347ms step_avg:59.33ms
step:2215/2330 train_time:131404ms step_avg:59.32ms
step:2216/2330 train_time:131467ms step_avg:59.33ms
step:2217/2330 train_time:131524ms step_avg:59.33ms
step:2218/2330 train_time:131586ms step_avg:59.33ms
step:2219/2330 train_time:131643ms step_avg:59.33ms
step:2220/2330 train_time:131707ms step_avg:59.33ms
step:2221/2330 train_time:131763ms step_avg:59.33ms
step:2222/2330 train_time:131826ms step_avg:59.33ms
step:2223/2330 train_time:131884ms step_avg:59.33ms
step:2224/2330 train_time:131947ms step_avg:59.33ms
step:2225/2330 train_time:132004ms step_avg:59.33ms
step:2226/2330 train_time:132067ms step_avg:59.33ms
step:2227/2330 train_time:132124ms step_avg:59.33ms
step:2228/2330 train_time:132187ms step_avg:59.33ms
step:2229/2330 train_time:132244ms step_avg:59.33ms
step:2230/2330 train_time:132306ms step_avg:59.33ms
step:2231/2330 train_time:132363ms step_avg:59.33ms
step:2232/2330 train_time:132426ms step_avg:59.33ms
step:2233/2330 train_time:132483ms step_avg:59.33ms
step:2234/2330 train_time:132546ms step_avg:59.33ms
step:2235/2330 train_time:132603ms step_avg:59.33ms
step:2236/2330 train_time:132667ms step_avg:59.33ms
step:2237/2330 train_time:132724ms step_avg:59.33ms
step:2238/2330 train_time:132787ms step_avg:59.33ms
step:2239/2330 train_time:132844ms step_avg:59.33ms
step:2240/2330 train_time:132907ms step_avg:59.33ms
step:2241/2330 train_time:132964ms step_avg:59.33ms
step:2242/2330 train_time:133027ms step_avg:59.33ms
step:2243/2330 train_time:133084ms step_avg:59.33ms
step:2244/2330 train_time:133147ms step_avg:59.33ms
step:2245/2330 train_time:133203ms step_avg:59.33ms
step:2246/2330 train_time:133267ms step_avg:59.34ms
step:2247/2330 train_time:133324ms step_avg:59.33ms
step:2248/2330 train_time:133388ms step_avg:59.34ms
step:2249/2330 train_time:133444ms step_avg:59.34ms
step:2250/2330 train_time:133507ms step_avg:59.34ms
step:2250/2330 val_loss:3.7591 train_time:133586ms step_avg:59.37ms
step:2251/2330 train_time:133605ms step_avg:59.35ms
step:2252/2330 train_time:133629ms step_avg:59.34ms
step:2253/2330 train_time:133690ms step_avg:59.34ms
step:2254/2330 train_time:133759ms step_avg:59.34ms
step:2255/2330 train_time:133816ms step_avg:59.34ms
step:2256/2330 train_time:133878ms step_avg:59.34ms
step:2257/2330 train_time:133935ms step_avg:59.34ms
step:2258/2330 train_time:133996ms step_avg:59.34ms
step:2259/2330 train_time:134053ms step_avg:59.34ms
step:2260/2330 train_time:134114ms step_avg:59.34ms
step:2261/2330 train_time:134172ms step_avg:59.34ms
step:2262/2330 train_time:134232ms step_avg:59.34ms
step:2263/2330 train_time:134289ms step_avg:59.34ms
step:2264/2330 train_time:134351ms step_avg:59.34ms
step:2265/2330 train_time:134408ms step_avg:59.34ms
step:2266/2330 train_time:134470ms step_avg:59.34ms
step:2267/2330 train_time:134527ms step_avg:59.34ms
step:2268/2330 train_time:134591ms step_avg:59.34ms
step:2269/2330 train_time:134648ms step_avg:59.34ms
step:2270/2330 train_time:134714ms step_avg:59.35ms
step:2271/2330 train_time:134772ms step_avg:59.34ms
step:2272/2330 train_time:134837ms step_avg:59.35ms
step:2273/2330 train_time:134894ms step_avg:59.35ms
step:2274/2330 train_time:134956ms step_avg:59.35ms
step:2275/2330 train_time:135013ms step_avg:59.35ms
step:2276/2330 train_time:135075ms step_avg:59.35ms
step:2277/2330 train_time:135132ms step_avg:59.35ms
step:2278/2330 train_time:135193ms step_avg:59.35ms
step:2279/2330 train_time:135250ms step_avg:59.35ms
step:2280/2330 train_time:135311ms step_avg:59.35ms
step:2281/2330 train_time:135368ms step_avg:59.35ms
step:2282/2330 train_time:135431ms step_avg:59.35ms
step:2283/2330 train_time:135488ms step_avg:59.35ms
step:2284/2330 train_time:135550ms step_avg:59.35ms
step:2285/2330 train_time:135608ms step_avg:59.35ms
step:2286/2330 train_time:135672ms step_avg:59.35ms
step:2287/2330 train_time:135730ms step_avg:59.35ms
step:2288/2330 train_time:135794ms step_avg:59.35ms
step:2289/2330 train_time:135852ms step_avg:59.35ms
step:2290/2330 train_time:135915ms step_avg:59.35ms
step:2291/2330 train_time:135973ms step_avg:59.35ms
step:2292/2330 train_time:136034ms step_avg:59.35ms
step:2293/2330 train_time:136091ms step_avg:59.35ms
step:2294/2330 train_time:136154ms step_avg:59.35ms
step:2295/2330 train_time:136212ms step_avg:59.35ms
step:2296/2330 train_time:136274ms step_avg:59.35ms
step:2297/2330 train_time:136330ms step_avg:59.35ms
step:2298/2330 train_time:136393ms step_avg:59.35ms
step:2299/2330 train_time:136449ms step_avg:59.35ms
step:2300/2330 train_time:136513ms step_avg:59.35ms
step:2301/2330 train_time:136570ms step_avg:59.35ms
step:2302/2330 train_time:136635ms step_avg:59.35ms
step:2303/2330 train_time:136692ms step_avg:59.35ms
step:2304/2330 train_time:136756ms step_avg:59.36ms
step:2305/2330 train_time:136813ms step_avg:59.36ms
step:2306/2330 train_time:136876ms step_avg:59.36ms
step:2307/2330 train_time:136933ms step_avg:59.36ms
step:2308/2330 train_time:136996ms step_avg:59.36ms
step:2309/2330 train_time:137053ms step_avg:59.36ms
step:2310/2330 train_time:137115ms step_avg:59.36ms
step:2311/2330 train_time:137172ms step_avg:59.36ms
step:2312/2330 train_time:137234ms step_avg:59.36ms
step:2313/2330 train_time:137291ms step_avg:59.36ms
step:2314/2330 train_time:137353ms step_avg:59.36ms
step:2315/2330 train_time:137410ms step_avg:59.36ms
step:2316/2330 train_time:137472ms step_avg:59.36ms
step:2317/2330 train_time:137529ms step_avg:59.36ms
step:2318/2330 train_time:137594ms step_avg:59.36ms
step:2319/2330 train_time:137651ms step_avg:59.36ms
step:2320/2330 train_time:137715ms step_avg:59.36ms
step:2321/2330 train_time:137772ms step_avg:59.36ms
step:2322/2330 train_time:137836ms step_avg:59.36ms
step:2323/2330 train_time:137893ms step_avg:59.36ms
step:2324/2330 train_time:137955ms step_avg:59.36ms
step:2325/2330 train_time:138012ms step_avg:59.36ms
step:2326/2330 train_time:138075ms step_avg:59.36ms
step:2327/2330 train_time:138133ms step_avg:59.36ms
step:2328/2330 train_time:138195ms step_avg:59.36ms
step:2329/2330 train_time:138252ms step_avg:59.36ms
step:2330/2330 train_time:138313ms step_avg:59.36ms
step:2330/2330 val_loss:3.7436 train_time:138393ms step_avg:59.40ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
