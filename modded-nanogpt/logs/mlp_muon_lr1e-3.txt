import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        # self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "mlp_muon_lr1e-3"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
# embed_params = [p for n, p in model.named_parameters() if "embed" in n]
embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=1e-3, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 03:59:59 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   33C    P0             112W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   31C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   31C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             116W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:88ms step_avg:87.97ms
step:2/2330 train_time:160ms step_avg:80.19ms
step:3/2330 train_time:173ms step_avg:57.69ms
step:4/2330 train_time:186ms step_avg:46.61ms
step:5/2330 train_time:198ms step_avg:39.64ms
step:6/2330 train_time:308ms step_avg:51.34ms
step:7/2330 train_time:342ms step_avg:48.80ms
step:8/2330 train_time:385ms step_avg:48.17ms
step:9/2330 train_time:420ms step_avg:46.64ms
step:10/2330 train_time:463ms step_avg:46.34ms
step:11/2330 train_time:498ms step_avg:45.25ms
step:12/2330 train_time:541ms step_avg:45.10ms
step:13/2330 train_time:576ms step_avg:44.27ms
step:14/2330 train_time:619ms step_avg:44.23ms
step:15/2330 train_time:654ms step_avg:43.57ms
step:16/2330 train_time:698ms step_avg:43.59ms
step:17/2330 train_time:733ms step_avg:43.10ms
step:18/2330 train_time:777ms step_avg:43.15ms
step:19/2330 train_time:811ms step_avg:42.70ms
step:20/2330 train_time:856ms step_avg:42.80ms
step:21/2330 train_time:891ms step_avg:42.42ms
step:22/2330 train_time:935ms step_avg:42.51ms
step:23/2330 train_time:970ms step_avg:42.18ms
step:24/2330 train_time:1014ms step_avg:42.26ms
step:25/2330 train_time:1049ms step_avg:41.96ms
step:26/2330 train_time:1094ms step_avg:42.06ms
step:27/2330 train_time:1128ms step_avg:41.79ms
step:28/2330 train_time:1174ms step_avg:41.94ms
step:29/2330 train_time:1215ms step_avg:41.88ms
step:30/2330 train_time:1265ms step_avg:42.15ms
step:31/2330 train_time:1305ms step_avg:42.11ms
step:32/2330 train_time:1351ms step_avg:42.21ms
step:33/2330 train_time:1387ms step_avg:42.03ms
step:34/2330 train_time:1432ms step_avg:42.11ms
step:35/2330 train_time:1467ms step_avg:41.91ms
step:36/2330 train_time:1512ms step_avg:42.01ms
step:37/2330 train_time:1548ms step_avg:41.84ms
step:38/2330 train_time:1593ms step_avg:41.93ms
step:39/2330 train_time:1628ms step_avg:41.75ms
step:40/2330 train_time:1672ms step_avg:41.80ms
step:41/2330 train_time:1707ms step_avg:41.64ms
step:42/2330 train_time:1751ms step_avg:41.70ms
step:43/2330 train_time:1787ms step_avg:41.56ms
step:44/2330 train_time:1831ms step_avg:41.62ms
step:45/2330 train_time:1866ms step_avg:41.47ms
step:46/2330 train_time:1910ms step_avg:41.53ms
step:47/2330 train_time:1945ms step_avg:41.39ms
step:48/2330 train_time:1990ms step_avg:41.46ms
step:49/2330 train_time:2025ms step_avg:41.33ms
step:50/2330 train_time:2070ms step_avg:41.40ms
step:51/2330 train_time:2106ms step_avg:41.30ms
step:52/2330 train_time:2152ms step_avg:41.39ms
step:53/2330 train_time:2189ms step_avg:41.29ms
step:54/2330 train_time:2234ms step_avg:41.37ms
step:55/2330 train_time:2270ms step_avg:41.28ms
step:56/2330 train_time:2317ms step_avg:41.37ms
step:57/2330 train_time:2354ms step_avg:41.30ms
step:58/2330 train_time:2400ms step_avg:41.37ms
step:59/2330 train_time:2435ms step_avg:41.28ms
step:60/2330 train_time:2481ms step_avg:41.35ms
step:61/2330 train_time:2517ms step_avg:41.26ms
step:62/2330 train_time:2562ms step_avg:41.33ms
step:63/2330 train_time:2598ms step_avg:41.24ms
step:64/2330 train_time:2643ms step_avg:41.30ms
step:65/2330 train_time:2679ms step_avg:41.21ms
step:66/2330 train_time:2724ms step_avg:41.28ms
step:67/2330 train_time:2760ms step_avg:41.19ms
step:68/2330 train_time:2804ms step_avg:41.24ms
step:69/2330 train_time:2839ms step_avg:41.14ms
step:70/2330 train_time:2885ms step_avg:41.21ms
step:71/2330 train_time:2921ms step_avg:41.14ms
step:72/2330 train_time:2966ms step_avg:41.20ms
step:73/2330 train_time:3002ms step_avg:41.12ms
step:74/2330 train_time:3046ms step_avg:41.17ms
step:75/2330 train_time:3082ms step_avg:41.09ms
step:76/2330 train_time:3129ms step_avg:41.17ms
step:77/2330 train_time:3164ms step_avg:41.09ms
step:78/2330 train_time:3210ms step_avg:41.15ms
step:79/2330 train_time:3246ms step_avg:41.09ms
step:80/2330 train_time:3292ms step_avg:41.15ms
step:81/2330 train_time:3328ms step_avg:41.09ms
step:82/2330 train_time:3373ms step_avg:41.14ms
step:83/2330 train_time:3409ms step_avg:41.08ms
step:84/2330 train_time:3455ms step_avg:41.13ms
step:85/2330 train_time:3490ms step_avg:41.06ms
step:86/2330 train_time:3535ms step_avg:41.10ms
step:87/2330 train_time:3571ms step_avg:41.04ms
step:88/2330 train_time:3615ms step_avg:41.08ms
step:89/2330 train_time:3650ms step_avg:41.02ms
step:90/2330 train_time:3695ms step_avg:41.06ms
step:91/2330 train_time:3732ms step_avg:41.01ms
step:92/2330 train_time:3776ms step_avg:41.05ms
step:93/2330 train_time:3811ms step_avg:40.98ms
step:94/2330 train_time:3857ms step_avg:41.03ms
step:95/2330 train_time:3893ms step_avg:40.98ms
step:96/2330 train_time:3938ms step_avg:41.02ms
step:97/2330 train_time:3973ms step_avg:40.96ms
step:98/2330 train_time:4018ms step_avg:41.00ms
step:99/2330 train_time:4054ms step_avg:40.95ms
step:100/2330 train_time:4099ms step_avg:40.99ms
step:101/2330 train_time:4134ms step_avg:40.93ms
step:102/2330 train_time:4180ms step_avg:40.98ms
step:103/2330 train_time:4217ms step_avg:40.94ms
step:104/2330 train_time:4263ms step_avg:41.00ms
step:105/2330 train_time:4301ms step_avg:40.96ms
step:106/2330 train_time:4347ms step_avg:41.01ms
step:107/2330 train_time:4384ms step_avg:40.97ms
step:108/2330 train_time:4430ms step_avg:41.02ms
step:109/2330 train_time:4466ms step_avg:40.97ms
step:110/2330 train_time:4511ms step_avg:41.01ms
step:111/2330 train_time:4546ms step_avg:40.95ms
step:112/2330 train_time:4591ms step_avg:40.99ms
step:113/2330 train_time:4626ms step_avg:40.94ms
step:114/2330 train_time:4671ms step_avg:40.98ms
step:115/2330 train_time:4707ms step_avg:40.93ms
step:116/2330 train_time:4751ms step_avg:40.96ms
step:117/2330 train_time:4786ms step_avg:40.91ms
step:118/2330 train_time:4831ms step_avg:40.94ms
step:119/2330 train_time:4868ms step_avg:40.91ms
step:120/2330 train_time:4913ms step_avg:40.94ms
step:121/2330 train_time:4949ms step_avg:40.90ms
step:122/2330 train_time:4995ms step_avg:40.94ms
step:123/2330 train_time:5030ms step_avg:40.89ms
step:124/2330 train_time:5075ms step_avg:40.92ms
step:125/2330 train_time:5111ms step_avg:40.89ms
step:126/2330 train_time:5155ms step_avg:40.92ms
step:127/2330 train_time:5191ms step_avg:40.87ms
step:128/2330 train_time:5237ms step_avg:40.91ms
step:129/2330 train_time:5273ms step_avg:40.88ms
step:130/2330 train_time:5319ms step_avg:40.92ms
step:131/2330 train_time:5356ms step_avg:40.89ms
step:132/2330 train_time:5401ms step_avg:40.92ms
step:133/2330 train_time:5437ms step_avg:40.88ms
step:134/2330 train_time:5483ms step_avg:40.92ms
step:135/2330 train_time:5520ms step_avg:40.89ms
step:136/2330 train_time:5566ms step_avg:40.93ms
step:137/2330 train_time:5602ms step_avg:40.89ms
step:138/2330 train_time:5647ms step_avg:40.92ms
step:139/2330 train_time:5683ms step_avg:40.89ms
step:140/2330 train_time:5728ms step_avg:40.92ms
step:141/2330 train_time:5763ms step_avg:40.87ms
step:142/2330 train_time:5808ms step_avg:40.90ms
step:143/2330 train_time:5844ms step_avg:40.87ms
step:144/2330 train_time:5890ms step_avg:40.90ms
step:145/2330 train_time:5925ms step_avg:40.86ms
step:146/2330 train_time:5970ms step_avg:40.89ms
step:147/2330 train_time:6006ms step_avg:40.86ms
step:148/2330 train_time:6052ms step_avg:40.89ms
step:149/2330 train_time:6087ms step_avg:40.85ms
step:150/2330 train_time:6132ms step_avg:40.88ms
step:151/2330 train_time:6167ms step_avg:40.84ms
step:152/2330 train_time:6213ms step_avg:40.88ms
step:153/2330 train_time:6250ms step_avg:40.85ms
step:154/2330 train_time:6296ms step_avg:40.88ms
step:155/2330 train_time:6331ms step_avg:40.84ms
step:156/2330 train_time:6376ms step_avg:40.87ms
step:157/2330 train_time:6412ms step_avg:40.84ms
step:158/2330 train_time:6457ms step_avg:40.87ms
step:159/2330 train_time:6493ms step_avg:40.84ms
step:160/2330 train_time:6539ms step_avg:40.87ms
step:161/2330 train_time:6574ms step_avg:40.84ms
step:162/2330 train_time:6619ms step_avg:40.86ms
step:163/2330 train_time:6656ms step_avg:40.83ms
step:164/2330 train_time:6700ms step_avg:40.86ms
step:165/2330 train_time:6736ms step_avg:40.82ms
step:166/2330 train_time:6781ms step_avg:40.85ms
step:167/2330 train_time:6817ms step_avg:40.82ms
step:168/2330 train_time:6863ms step_avg:40.85ms
step:169/2330 train_time:6900ms step_avg:40.83ms
step:170/2330 train_time:6945ms step_avg:40.85ms
step:171/2330 train_time:6981ms step_avg:40.83ms
step:172/2330 train_time:7027ms step_avg:40.85ms
step:173/2330 train_time:7063ms step_avg:40.83ms
step:174/2330 train_time:7108ms step_avg:40.85ms
step:175/2330 train_time:7143ms step_avg:40.82ms
step:176/2330 train_time:7189ms step_avg:40.85ms
step:177/2330 train_time:7226ms step_avg:40.83ms
step:178/2330 train_time:7271ms step_avg:40.85ms
step:179/2330 train_time:7306ms step_avg:40.82ms
step:180/2330 train_time:7351ms step_avg:40.84ms
step:181/2330 train_time:7387ms step_avg:40.81ms
step:182/2330 train_time:7433ms step_avg:40.84ms
step:183/2330 train_time:7468ms step_avg:40.81ms
step:184/2330 train_time:7514ms step_avg:40.84ms
step:185/2330 train_time:7549ms step_avg:40.80ms
step:186/2330 train_time:7595ms step_avg:40.83ms
step:187/2330 train_time:7631ms step_avg:40.81ms
step:188/2330 train_time:7675ms step_avg:40.83ms
step:189/2330 train_time:7711ms step_avg:40.80ms
step:190/2330 train_time:7756ms step_avg:40.82ms
step:191/2330 train_time:7791ms step_avg:40.79ms
step:192/2330 train_time:7837ms step_avg:40.82ms
step:193/2330 train_time:7872ms step_avg:40.79ms
step:194/2330 train_time:7918ms step_avg:40.81ms
step:195/2330 train_time:7954ms step_avg:40.79ms
step:196/2330 train_time:7999ms step_avg:40.81ms
step:197/2330 train_time:8035ms step_avg:40.78ms
step:198/2330 train_time:8080ms step_avg:40.81ms
step:199/2330 train_time:8116ms step_avg:40.78ms
step:200/2330 train_time:8162ms step_avg:40.81ms
step:201/2330 train_time:8198ms step_avg:40.79ms
step:202/2330 train_time:8244ms step_avg:40.81ms
step:203/2330 train_time:8281ms step_avg:40.79ms
step:204/2330 train_time:8327ms step_avg:40.82ms
step:205/2330 train_time:8362ms step_avg:40.79ms
step:206/2330 train_time:8407ms step_avg:40.81ms
step:207/2330 train_time:8444ms step_avg:40.79ms
step:208/2330 train_time:8490ms step_avg:40.82ms
step:209/2330 train_time:8526ms step_avg:40.80ms
step:210/2330 train_time:8572ms step_avg:40.82ms
step:211/2330 train_time:8607ms step_avg:40.79ms
step:212/2330 train_time:8651ms step_avg:40.81ms
step:213/2330 train_time:8687ms step_avg:40.78ms
step:214/2330 train_time:8732ms step_avg:40.80ms
step:215/2330 train_time:8767ms step_avg:40.78ms
step:216/2330 train_time:8813ms step_avg:40.80ms
step:217/2330 train_time:8848ms step_avg:40.77ms
step:218/2330 train_time:8894ms step_avg:40.80ms
step:219/2330 train_time:8930ms step_avg:40.78ms
step:220/2330 train_time:8974ms step_avg:40.79ms
step:221/2330 train_time:9010ms step_avg:40.77ms
step:222/2330 train_time:9055ms step_avg:40.79ms
step:223/2330 train_time:9091ms step_avg:40.77ms
step:224/2330 train_time:9137ms step_avg:40.79ms
step:225/2330 train_time:9173ms step_avg:40.77ms
step:226/2330 train_time:9218ms step_avg:40.79ms
step:227/2330 train_time:9253ms step_avg:40.76ms
step:228/2330 train_time:9298ms step_avg:40.78ms
step:229/2330 train_time:9334ms step_avg:40.76ms
step:230/2330 train_time:9379ms step_avg:40.78ms
step:231/2330 train_time:9416ms step_avg:40.76ms
step:232/2330 train_time:9461ms step_avg:40.78ms
step:233/2330 train_time:9498ms step_avg:40.76ms
step:234/2330 train_time:9543ms step_avg:40.78ms
step:235/2330 train_time:9581ms step_avg:40.77ms
step:236/2330 train_time:9626ms step_avg:40.79ms
step:237/2330 train_time:9662ms step_avg:40.77ms
step:238/2330 train_time:9707ms step_avg:40.79ms
step:239/2330 train_time:9743ms step_avg:40.76ms
step:240/2330 train_time:9788ms step_avg:40.78ms
step:241/2330 train_time:9823ms step_avg:40.76ms
step:242/2330 train_time:9869ms step_avg:40.78ms
step:243/2330 train_time:9905ms step_avg:40.76ms
step:244/2330 train_time:9951ms step_avg:40.78ms
step:245/2330 train_time:9987ms step_avg:40.76ms
step:246/2330 train_time:10032ms step_avg:40.78ms
step:247/2330 train_time:10067ms step_avg:40.76ms
step:248/2330 train_time:10112ms step_avg:40.77ms
step:249/2330 train_time:10148ms step_avg:40.75ms
step:250/2330 train_time:10193ms step_avg:40.77ms
step:250/2330 val_loss:5.6521 train_time:10282ms step_avg:41.13ms
step:251/2330 train_time:10295ms step_avg:41.02ms
step:252/2330 train_time:10307ms step_avg:40.90ms
step:253/2330 train_time:10318ms step_avg:40.78ms
step:254/2330 train_time:10355ms step_avg:40.77ms
step:255/2330 train_time:10389ms step_avg:40.74ms
step:256/2330 train_time:10433ms step_avg:40.76ms
step:257/2330 train_time:10468ms step_avg:40.73ms
step:258/2330 train_time:10512ms step_avg:40.75ms
step:259/2330 train_time:10547ms step_avg:40.72ms
step:260/2330 train_time:10594ms step_avg:40.75ms
step:261/2330 train_time:10637ms step_avg:40.75ms
step:262/2330 train_time:10687ms step_avg:40.79ms
step:263/2330 train_time:10724ms step_avg:40.78ms
step:264/2330 train_time:10770ms step_avg:40.79ms
step:265/2330 train_time:10805ms step_avg:40.78ms
step:266/2330 train_time:10850ms step_avg:40.79ms
step:267/2330 train_time:10885ms step_avg:40.77ms
step:268/2330 train_time:10930ms step_avg:40.78ms
step:269/2330 train_time:10965ms step_avg:40.76ms
step:270/2330 train_time:11010ms step_avg:40.78ms
step:271/2330 train_time:11046ms step_avg:40.76ms
step:272/2330 train_time:11091ms step_avg:40.77ms
step:273/2330 train_time:11127ms step_avg:40.76ms
step:274/2330 train_time:11173ms step_avg:40.78ms
step:275/2330 train_time:11210ms step_avg:40.76ms
step:276/2330 train_time:11256ms step_avg:40.78ms
step:277/2330 train_time:11291ms step_avg:40.76ms
step:278/2330 train_time:11335ms step_avg:40.77ms
step:279/2330 train_time:11370ms step_avg:40.75ms
step:280/2330 train_time:11414ms step_avg:40.77ms
step:281/2330 train_time:11449ms step_avg:40.74ms
step:282/2330 train_time:11494ms step_avg:40.76ms
step:283/2330 train_time:11528ms step_avg:40.74ms
step:284/2330 train_time:11576ms step_avg:40.76ms
step:285/2330 train_time:11613ms step_avg:40.75ms
step:286/2330 train_time:11659ms step_avg:40.77ms
step:287/2330 train_time:11696ms step_avg:40.75ms
step:288/2330 train_time:11742ms step_avg:40.77ms
step:289/2330 train_time:11779ms step_avg:40.76ms
step:290/2330 train_time:11823ms step_avg:40.77ms
step:291/2330 train_time:11860ms step_avg:40.76ms
step:292/2330 train_time:11906ms step_avg:40.77ms
step:293/2330 train_time:11941ms step_avg:40.76ms
step:294/2330 train_time:11986ms step_avg:40.77ms
step:295/2330 train_time:12021ms step_avg:40.75ms
step:296/2330 train_time:12065ms step_avg:40.76ms
step:297/2330 train_time:12101ms step_avg:40.74ms
step:298/2330 train_time:12147ms step_avg:40.76ms
step:299/2330 train_time:12183ms step_avg:40.75ms
step:300/2330 train_time:12228ms step_avg:40.76ms
step:301/2330 train_time:12264ms step_avg:40.74ms
step:302/2330 train_time:12309ms step_avg:40.76ms
step:303/2330 train_time:12344ms step_avg:40.74ms
step:304/2330 train_time:12390ms step_avg:40.76ms
step:305/2330 train_time:12425ms step_avg:40.74ms
step:306/2330 train_time:12470ms step_avg:40.75ms
step:307/2330 train_time:12507ms step_avg:40.74ms
step:308/2330 train_time:12552ms step_avg:40.75ms
step:309/2330 train_time:12588ms step_avg:40.74ms
step:310/2330 train_time:12633ms step_avg:40.75ms
step:311/2330 train_time:12668ms step_avg:40.73ms
step:312/2330 train_time:12715ms step_avg:40.75ms
step:313/2330 train_time:12751ms step_avg:40.74ms
step:314/2330 train_time:12796ms step_avg:40.75ms
step:315/2330 train_time:12832ms step_avg:40.74ms
step:316/2330 train_time:12878ms step_avg:40.75ms
step:317/2330 train_time:12914ms step_avg:40.74ms
step:318/2330 train_time:12959ms step_avg:40.75ms
step:319/2330 train_time:12995ms step_avg:40.74ms
step:320/2330 train_time:13041ms step_avg:40.75ms
step:321/2330 train_time:13076ms step_avg:40.73ms
step:322/2330 train_time:13121ms step_avg:40.75ms
step:323/2330 train_time:13157ms step_avg:40.73ms
step:324/2330 train_time:13202ms step_avg:40.75ms
step:325/2330 train_time:13239ms step_avg:40.73ms
step:326/2330 train_time:13285ms step_avg:40.75ms
step:327/2330 train_time:13320ms step_avg:40.74ms
step:328/2330 train_time:13366ms step_avg:40.75ms
step:329/2330 train_time:13401ms step_avg:40.73ms
step:330/2330 train_time:13448ms step_avg:40.75ms
step:331/2330 train_time:13483ms step_avg:40.73ms
step:332/2330 train_time:13529ms step_avg:40.75ms
step:333/2330 train_time:13565ms step_avg:40.73ms
step:334/2330 train_time:13611ms step_avg:40.75ms
step:335/2330 train_time:13646ms step_avg:40.74ms
step:336/2330 train_time:13692ms step_avg:40.75ms
step:337/2330 train_time:13727ms step_avg:40.73ms
step:338/2330 train_time:13774ms step_avg:40.75ms
step:339/2330 train_time:13810ms step_avg:40.74ms
step:340/2330 train_time:13855ms step_avg:40.75ms
step:341/2330 train_time:13891ms step_avg:40.74ms
step:342/2330 train_time:13936ms step_avg:40.75ms
step:343/2330 train_time:13972ms step_avg:40.73ms
step:344/2330 train_time:14017ms step_avg:40.75ms
step:345/2330 train_time:14052ms step_avg:40.73ms
step:346/2330 train_time:14097ms step_avg:40.74ms
step:347/2330 train_time:14134ms step_avg:40.73ms
step:348/2330 train_time:14179ms step_avg:40.74ms
step:349/2330 train_time:14215ms step_avg:40.73ms
step:350/2330 train_time:14260ms step_avg:40.74ms
step:351/2330 train_time:14296ms step_avg:40.73ms
step:352/2330 train_time:14341ms step_avg:40.74ms
step:353/2330 train_time:14376ms step_avg:40.73ms
step:354/2330 train_time:14421ms step_avg:40.74ms
step:355/2330 train_time:14458ms step_avg:40.73ms
step:356/2330 train_time:14503ms step_avg:40.74ms
step:357/2330 train_time:14540ms step_avg:40.73ms
step:358/2330 train_time:14586ms step_avg:40.74ms
step:359/2330 train_time:14622ms step_avg:40.73ms
step:360/2330 train_time:14668ms step_avg:40.74ms
step:361/2330 train_time:14704ms step_avg:40.73ms
step:362/2330 train_time:14749ms step_avg:40.74ms
step:363/2330 train_time:14785ms step_avg:40.73ms
step:364/2330 train_time:14831ms step_avg:40.74ms
step:365/2330 train_time:14866ms step_avg:40.73ms
step:366/2330 train_time:14911ms step_avg:40.74ms
step:367/2330 train_time:14947ms step_avg:40.73ms
step:368/2330 train_time:14993ms step_avg:40.74ms
step:369/2330 train_time:15029ms step_avg:40.73ms
step:370/2330 train_time:15075ms step_avg:40.74ms
step:371/2330 train_time:15111ms step_avg:40.73ms
step:372/2330 train_time:15157ms step_avg:40.74ms
step:373/2330 train_time:15193ms step_avg:40.73ms
step:374/2330 train_time:15238ms step_avg:40.74ms
step:375/2330 train_time:15273ms step_avg:40.73ms
step:376/2330 train_time:15319ms step_avg:40.74ms
step:377/2330 train_time:15354ms step_avg:40.73ms
step:378/2330 train_time:15400ms step_avg:40.74ms
step:379/2330 train_time:15436ms step_avg:40.73ms
step:380/2330 train_time:15480ms step_avg:40.74ms
step:381/2330 train_time:15516ms step_avg:40.72ms
step:382/2330 train_time:15562ms step_avg:40.74ms
step:383/2330 train_time:15598ms step_avg:40.73ms
step:384/2330 train_time:15644ms step_avg:40.74ms
step:385/2330 train_time:15680ms step_avg:40.73ms
step:386/2330 train_time:15726ms step_avg:40.74ms
step:387/2330 train_time:15763ms step_avg:40.73ms
step:388/2330 train_time:15809ms step_avg:40.74ms
step:389/2330 train_time:15844ms step_avg:40.73ms
step:390/2330 train_time:15890ms step_avg:40.74ms
step:391/2330 train_time:15925ms step_avg:40.73ms
step:392/2330 train_time:15970ms step_avg:40.74ms
step:393/2330 train_time:16006ms step_avg:40.73ms
step:394/2330 train_time:16051ms step_avg:40.74ms
step:395/2330 train_time:16087ms step_avg:40.73ms
step:396/2330 train_time:16133ms step_avg:40.74ms
step:397/2330 train_time:16169ms step_avg:40.73ms
step:398/2330 train_time:16214ms step_avg:40.74ms
step:399/2330 train_time:16250ms step_avg:40.73ms
step:400/2330 train_time:16296ms step_avg:40.74ms
step:401/2330 train_time:16331ms step_avg:40.73ms
step:402/2330 train_time:16377ms step_avg:40.74ms
step:403/2330 train_time:16413ms step_avg:40.73ms
step:404/2330 train_time:16457ms step_avg:40.74ms
step:405/2330 train_time:16494ms step_avg:40.72ms
step:406/2330 train_time:16539ms step_avg:40.74ms
step:407/2330 train_time:16575ms step_avg:40.72ms
step:408/2330 train_time:16620ms step_avg:40.74ms
step:409/2330 train_time:16656ms step_avg:40.72ms
step:410/2330 train_time:16702ms step_avg:40.74ms
step:411/2330 train_time:16738ms step_avg:40.73ms
step:412/2330 train_time:16784ms step_avg:40.74ms
step:413/2330 train_time:16820ms step_avg:40.73ms
step:414/2330 train_time:16867ms step_avg:40.74ms
step:415/2330 train_time:16903ms step_avg:40.73ms
step:416/2330 train_time:16947ms step_avg:40.74ms
step:417/2330 train_time:16984ms step_avg:40.73ms
step:418/2330 train_time:17029ms step_avg:40.74ms
step:419/2330 train_time:17065ms step_avg:40.73ms
step:420/2330 train_time:17111ms step_avg:40.74ms
step:421/2330 train_time:17146ms step_avg:40.73ms
step:422/2330 train_time:17192ms step_avg:40.74ms
step:423/2330 train_time:17227ms step_avg:40.73ms
step:424/2330 train_time:17273ms step_avg:40.74ms
step:425/2330 train_time:17309ms step_avg:40.73ms
step:426/2330 train_time:17354ms step_avg:40.74ms
step:427/2330 train_time:17389ms step_avg:40.72ms
step:428/2330 train_time:17435ms step_avg:40.74ms
step:429/2330 train_time:17471ms step_avg:40.72ms
step:430/2330 train_time:17517ms step_avg:40.74ms
step:431/2330 train_time:17552ms step_avg:40.72ms
step:432/2330 train_time:17597ms step_avg:40.73ms
step:433/2330 train_time:17634ms step_avg:40.72ms
step:434/2330 train_time:17680ms step_avg:40.74ms
step:435/2330 train_time:17716ms step_avg:40.73ms
step:436/2330 train_time:17762ms step_avg:40.74ms
step:437/2330 train_time:17798ms step_avg:40.73ms
step:438/2330 train_time:17844ms step_avg:40.74ms
step:439/2330 train_time:17880ms step_avg:40.73ms
step:440/2330 train_time:17925ms step_avg:40.74ms
step:441/2330 train_time:17961ms step_avg:40.73ms
step:442/2330 train_time:18006ms step_avg:40.74ms
step:443/2330 train_time:18042ms step_avg:40.73ms
step:444/2330 train_time:18088ms step_avg:40.74ms
step:445/2330 train_time:18124ms step_avg:40.73ms
step:446/2330 train_time:18170ms step_avg:40.74ms
step:447/2330 train_time:18206ms step_avg:40.73ms
step:448/2330 train_time:18251ms step_avg:40.74ms
step:449/2330 train_time:18286ms step_avg:40.73ms
step:450/2330 train_time:18331ms step_avg:40.74ms
step:451/2330 train_time:18368ms step_avg:40.73ms
step:452/2330 train_time:18414ms step_avg:40.74ms
step:453/2330 train_time:18449ms step_avg:40.73ms
step:454/2330 train_time:18494ms step_avg:40.74ms
step:455/2330 train_time:18531ms step_avg:40.73ms
step:456/2330 train_time:18576ms step_avg:40.74ms
step:457/2330 train_time:18613ms step_avg:40.73ms
step:458/2330 train_time:18658ms step_avg:40.74ms
step:459/2330 train_time:18695ms step_avg:40.73ms
step:460/2330 train_time:18741ms step_avg:40.74ms
step:461/2330 train_time:18776ms step_avg:40.73ms
step:462/2330 train_time:18822ms step_avg:40.74ms
step:463/2330 train_time:18858ms step_avg:40.73ms
step:464/2330 train_time:18903ms step_avg:40.74ms
step:465/2330 train_time:18939ms step_avg:40.73ms
step:466/2330 train_time:18985ms step_avg:40.74ms
step:467/2330 train_time:19021ms step_avg:40.73ms
step:468/2330 train_time:19066ms step_avg:40.74ms
step:469/2330 train_time:19103ms step_avg:40.73ms
step:470/2330 train_time:19149ms step_avg:40.74ms
step:471/2330 train_time:19185ms step_avg:40.73ms
step:472/2330 train_time:19230ms step_avg:40.74ms
step:473/2330 train_time:19266ms step_avg:40.73ms
step:474/2330 train_time:19311ms step_avg:40.74ms
step:475/2330 train_time:19347ms step_avg:40.73ms
step:476/2330 train_time:19392ms step_avg:40.74ms
step:477/2330 train_time:19428ms step_avg:40.73ms
step:478/2330 train_time:19473ms step_avg:40.74ms
step:479/2330 train_time:19509ms step_avg:40.73ms
step:480/2330 train_time:19554ms step_avg:40.74ms
step:481/2330 train_time:19591ms step_avg:40.73ms
step:482/2330 train_time:19637ms step_avg:40.74ms
step:483/2330 train_time:19672ms step_avg:40.73ms
step:484/2330 train_time:19718ms step_avg:40.74ms
step:485/2330 train_time:19754ms step_avg:40.73ms
step:486/2330 train_time:19799ms step_avg:40.74ms
step:487/2330 train_time:19834ms step_avg:40.73ms
step:488/2330 train_time:19879ms step_avg:40.74ms
step:489/2330 train_time:19916ms step_avg:40.73ms
step:490/2330 train_time:19962ms step_avg:40.74ms
step:491/2330 train_time:19997ms step_avg:40.73ms
step:492/2330 train_time:20043ms step_avg:40.74ms
step:493/2330 train_time:20080ms step_avg:40.73ms
step:494/2330 train_time:20125ms step_avg:40.74ms
step:495/2330 train_time:20161ms step_avg:40.73ms
step:496/2330 train_time:20207ms step_avg:40.74ms
step:497/2330 train_time:20243ms step_avg:40.73ms
step:498/2330 train_time:20289ms step_avg:40.74ms
step:499/2330 train_time:20325ms step_avg:40.73ms
step:500/2330 train_time:20371ms step_avg:40.74ms
step:500/2330 val_loss:5.3656 train_time:20458ms step_avg:40.92ms
step:501/2330 train_time:20472ms step_avg:40.86ms
step:502/2330 train_time:20484ms step_avg:40.80ms
step:503/2330 train_time:20494ms step_avg:40.74ms
step:504/2330 train_time:20532ms step_avg:40.74ms
step:505/2330 train_time:20566ms step_avg:40.73ms
step:506/2330 train_time:20611ms step_avg:40.73ms
step:507/2330 train_time:20646ms step_avg:40.72ms
step:508/2330 train_time:20690ms step_avg:40.73ms
step:509/2330 train_time:20726ms step_avg:40.72ms
step:510/2330 train_time:20772ms step_avg:40.73ms
step:511/2330 train_time:20813ms step_avg:40.73ms
step:512/2330 train_time:20863ms step_avg:40.75ms
step:513/2330 train_time:20901ms step_avg:40.74ms
step:514/2330 train_time:20947ms step_avg:40.75ms
step:515/2330 train_time:20983ms step_avg:40.74ms
step:516/2330 train_time:21028ms step_avg:40.75ms
step:517/2330 train_time:21064ms step_avg:40.74ms
step:518/2330 train_time:21109ms step_avg:40.75ms
step:519/2330 train_time:21144ms step_avg:40.74ms
step:520/2330 train_time:21189ms step_avg:40.75ms
step:521/2330 train_time:21223ms step_avg:40.74ms
step:522/2330 train_time:21268ms step_avg:40.74ms
step:523/2330 train_time:21304ms step_avg:40.73ms
step:524/2330 train_time:21348ms step_avg:40.74ms
step:525/2330 train_time:21383ms step_avg:40.73ms
step:526/2330 train_time:21429ms step_avg:40.74ms
step:527/2330 train_time:21465ms step_avg:40.73ms
step:528/2330 train_time:21510ms step_avg:40.74ms
step:529/2330 train_time:21545ms step_avg:40.73ms
step:530/2330 train_time:21589ms step_avg:40.73ms
step:531/2330 train_time:21625ms step_avg:40.72ms
step:532/2330 train_time:21670ms step_avg:40.73ms
step:533/2330 train_time:21706ms step_avg:40.72ms
step:534/2330 train_time:21753ms step_avg:40.74ms
step:535/2330 train_time:21790ms step_avg:40.73ms
step:536/2330 train_time:21837ms step_avg:40.74ms
step:537/2330 train_time:21874ms step_avg:40.73ms
step:538/2330 train_time:21920ms step_avg:40.74ms
step:539/2330 train_time:21957ms step_avg:40.74ms
step:540/2330 train_time:22003ms step_avg:40.75ms
step:541/2330 train_time:22040ms step_avg:40.74ms
step:542/2330 train_time:22085ms step_avg:40.75ms
step:543/2330 train_time:22120ms step_avg:40.74ms
step:544/2330 train_time:22165ms step_avg:40.74ms
step:545/2330 train_time:22200ms step_avg:40.73ms
step:546/2330 train_time:22245ms step_avg:40.74ms
step:547/2330 train_time:22280ms step_avg:40.73ms
step:548/2330 train_time:22326ms step_avg:40.74ms
step:549/2330 train_time:22361ms step_avg:40.73ms
step:550/2330 train_time:22406ms step_avg:40.74ms
step:551/2330 train_time:22442ms step_avg:40.73ms
step:552/2330 train_time:22488ms step_avg:40.74ms
step:553/2330 train_time:22524ms step_avg:40.73ms
step:554/2330 train_time:22569ms step_avg:40.74ms
step:555/2330 train_time:22604ms step_avg:40.73ms
step:556/2330 train_time:22650ms step_avg:40.74ms
step:557/2330 train_time:22686ms step_avg:40.73ms
step:558/2330 train_time:22732ms step_avg:40.74ms
step:559/2330 train_time:22768ms step_avg:40.73ms
step:560/2330 train_time:22814ms step_avg:40.74ms
step:561/2330 train_time:22850ms step_avg:40.73ms
step:562/2330 train_time:22896ms step_avg:40.74ms
step:563/2330 train_time:22932ms step_avg:40.73ms
step:564/2330 train_time:22978ms step_avg:40.74ms
step:565/2330 train_time:23014ms step_avg:40.73ms
step:566/2330 train_time:23060ms step_avg:40.74ms
step:567/2330 train_time:23096ms step_avg:40.73ms
step:568/2330 train_time:23141ms step_avg:40.74ms
step:569/2330 train_time:23177ms step_avg:40.73ms
step:570/2330 train_time:23222ms step_avg:40.74ms
step:571/2330 train_time:23259ms step_avg:40.73ms
step:572/2330 train_time:23304ms step_avg:40.74ms
step:573/2330 train_time:23340ms step_avg:40.73ms
step:574/2330 train_time:23385ms step_avg:40.74ms
step:575/2330 train_time:23421ms step_avg:40.73ms
step:576/2330 train_time:23467ms step_avg:40.74ms
step:577/2330 train_time:23502ms step_avg:40.73ms
step:578/2330 train_time:23547ms step_avg:40.74ms
step:579/2330 train_time:23582ms step_avg:40.73ms
step:580/2330 train_time:23629ms step_avg:40.74ms
step:581/2330 train_time:23665ms step_avg:40.73ms
step:582/2330 train_time:23710ms step_avg:40.74ms
step:583/2330 train_time:23746ms step_avg:40.73ms
step:584/2330 train_time:23793ms step_avg:40.74ms
step:585/2330 train_time:23829ms step_avg:40.73ms
step:586/2330 train_time:23874ms step_avg:40.74ms
step:587/2330 train_time:23910ms step_avg:40.73ms
step:588/2330 train_time:23956ms step_avg:40.74ms
step:589/2330 train_time:23992ms step_avg:40.73ms
step:590/2330 train_time:24037ms step_avg:40.74ms
step:591/2330 train_time:24073ms step_avg:40.73ms
step:592/2330 train_time:24118ms step_avg:40.74ms
step:593/2330 train_time:24154ms step_avg:40.73ms
step:594/2330 train_time:24199ms step_avg:40.74ms
step:595/2330 train_time:24235ms step_avg:40.73ms
step:596/2330 train_time:24280ms step_avg:40.74ms
step:597/2330 train_time:24317ms step_avg:40.73ms
step:598/2330 train_time:24363ms step_avg:40.74ms
step:599/2330 train_time:24399ms step_avg:40.73ms
step:600/2330 train_time:24444ms step_avg:40.74ms
step:601/2330 train_time:24480ms step_avg:40.73ms
step:602/2330 train_time:24526ms step_avg:40.74ms
step:603/2330 train_time:24563ms step_avg:40.73ms
step:604/2330 train_time:24608ms step_avg:40.74ms
step:605/2330 train_time:24644ms step_avg:40.73ms
step:606/2330 train_time:24689ms step_avg:40.74ms
step:607/2330 train_time:24726ms step_avg:40.73ms
step:608/2330 train_time:24772ms step_avg:40.74ms
step:609/2330 train_time:24808ms step_avg:40.74ms
step:610/2330 train_time:24854ms step_avg:40.74ms
step:611/2330 train_time:24890ms step_avg:40.74ms
step:612/2330 train_time:24936ms step_avg:40.74ms
step:613/2330 train_time:24972ms step_avg:40.74ms
step:614/2330 train_time:25017ms step_avg:40.74ms
step:615/2330 train_time:25052ms step_avg:40.74ms
step:616/2330 train_time:25098ms step_avg:40.74ms
step:617/2330 train_time:25133ms step_avg:40.73ms
step:618/2330 train_time:25179ms step_avg:40.74ms
step:619/2330 train_time:25215ms step_avg:40.73ms
step:620/2330 train_time:25260ms step_avg:40.74ms
step:621/2330 train_time:25296ms step_avg:40.73ms
step:622/2330 train_time:25341ms step_avg:40.74ms
step:623/2330 train_time:25377ms step_avg:40.73ms
step:624/2330 train_time:25423ms step_avg:40.74ms
step:625/2330 train_time:25459ms step_avg:40.74ms
step:626/2330 train_time:25504ms step_avg:40.74ms
step:627/2330 train_time:25540ms step_avg:40.73ms
step:628/2330 train_time:25585ms step_avg:40.74ms
step:629/2330 train_time:25622ms step_avg:40.73ms
step:630/2330 train_time:25668ms step_avg:40.74ms
step:631/2330 train_time:25703ms step_avg:40.73ms
step:632/2330 train_time:25749ms step_avg:40.74ms
step:633/2330 train_time:25785ms step_avg:40.74ms
step:634/2330 train_time:25831ms step_avg:40.74ms
step:635/2330 train_time:25866ms step_avg:40.73ms
step:636/2330 train_time:25911ms step_avg:40.74ms
step:637/2330 train_time:25947ms step_avg:40.73ms
step:638/2330 train_time:25993ms step_avg:40.74ms
step:639/2330 train_time:26028ms step_avg:40.73ms
step:640/2330 train_time:26074ms step_avg:40.74ms
step:641/2330 train_time:26109ms step_avg:40.73ms
step:642/2330 train_time:26154ms step_avg:40.74ms
step:643/2330 train_time:26191ms step_avg:40.73ms
step:644/2330 train_time:26237ms step_avg:40.74ms
step:645/2330 train_time:26273ms step_avg:40.73ms
step:646/2330 train_time:26318ms step_avg:40.74ms
step:647/2330 train_time:26354ms step_avg:40.73ms
step:648/2330 train_time:26399ms step_avg:40.74ms
step:649/2330 train_time:26436ms step_avg:40.73ms
step:650/2330 train_time:26481ms step_avg:40.74ms
step:651/2330 train_time:26517ms step_avg:40.73ms
step:652/2330 train_time:26564ms step_avg:40.74ms
step:653/2330 train_time:26599ms step_avg:40.73ms
step:654/2330 train_time:26646ms step_avg:40.74ms
step:655/2330 train_time:26682ms step_avg:40.74ms
step:656/2330 train_time:26729ms step_avg:40.75ms
step:657/2330 train_time:26764ms step_avg:40.74ms
step:658/2330 train_time:26810ms step_avg:40.74ms
step:659/2330 train_time:26846ms step_avg:40.74ms
step:660/2330 train_time:26891ms step_avg:40.74ms
step:661/2330 train_time:26927ms step_avg:40.74ms
step:662/2330 train_time:26973ms step_avg:40.74ms
step:663/2330 train_time:27008ms step_avg:40.74ms
step:664/2330 train_time:27053ms step_avg:40.74ms
step:665/2330 train_time:27089ms step_avg:40.73ms
step:666/2330 train_time:27135ms step_avg:40.74ms
step:667/2330 train_time:27171ms step_avg:40.74ms
step:668/2330 train_time:27216ms step_avg:40.74ms
step:669/2330 train_time:27251ms step_avg:40.73ms
step:670/2330 train_time:27296ms step_avg:40.74ms
step:671/2330 train_time:27332ms step_avg:40.73ms
step:672/2330 train_time:27378ms step_avg:40.74ms
step:673/2330 train_time:27414ms step_avg:40.73ms
step:674/2330 train_time:27460ms step_avg:40.74ms
step:675/2330 train_time:27496ms step_avg:40.73ms
step:676/2330 train_time:27541ms step_avg:40.74ms
step:677/2330 train_time:27577ms step_avg:40.73ms
step:678/2330 train_time:27623ms step_avg:40.74ms
step:679/2330 train_time:27659ms step_avg:40.74ms
step:680/2330 train_time:27705ms step_avg:40.74ms
step:681/2330 train_time:27741ms step_avg:40.74ms
step:682/2330 train_time:27786ms step_avg:40.74ms
step:683/2330 train_time:27822ms step_avg:40.73ms
step:684/2330 train_time:27868ms step_avg:40.74ms
step:685/2330 train_time:27905ms step_avg:40.74ms
step:686/2330 train_time:27951ms step_avg:40.75ms
step:687/2330 train_time:27987ms step_avg:40.74ms
step:688/2330 train_time:28032ms step_avg:40.74ms
step:689/2330 train_time:28067ms step_avg:40.74ms
step:690/2330 train_time:28113ms step_avg:40.74ms
step:691/2330 train_time:28148ms step_avg:40.74ms
step:692/2330 train_time:28194ms step_avg:40.74ms
step:693/2330 train_time:28229ms step_avg:40.74ms
step:694/2330 train_time:28275ms step_avg:40.74ms
step:695/2330 train_time:28311ms step_avg:40.74ms
step:696/2330 train_time:28357ms step_avg:40.74ms
step:697/2330 train_time:28393ms step_avg:40.74ms
step:698/2330 train_time:28439ms step_avg:40.74ms
step:699/2330 train_time:28475ms step_avg:40.74ms
step:700/2330 train_time:28520ms step_avg:40.74ms
step:701/2330 train_time:28556ms step_avg:40.74ms
step:702/2330 train_time:28602ms step_avg:40.74ms
step:703/2330 train_time:28638ms step_avg:40.74ms
step:704/2330 train_time:28684ms step_avg:40.74ms
step:705/2330 train_time:28721ms step_avg:40.74ms
step:706/2330 train_time:28766ms step_avg:40.75ms
step:707/2330 train_time:28801ms step_avg:40.74ms
step:708/2330 train_time:28847ms step_avg:40.74ms
step:709/2330 train_time:28883ms step_avg:40.74ms
step:710/2330 train_time:28929ms step_avg:40.74ms
step:711/2330 train_time:28965ms step_avg:40.74ms
step:712/2330 train_time:29010ms step_avg:40.74ms
step:713/2330 train_time:29046ms step_avg:40.74ms
step:714/2330 train_time:29091ms step_avg:40.74ms
step:715/2330 train_time:29126ms step_avg:40.74ms
step:716/2330 train_time:29172ms step_avg:40.74ms
step:717/2330 train_time:29209ms step_avg:40.74ms
step:718/2330 train_time:29254ms step_avg:40.74ms
step:719/2330 train_time:29289ms step_avg:40.74ms
step:720/2330 train_time:29336ms step_avg:40.74ms
step:721/2330 train_time:29373ms step_avg:40.74ms
step:722/2330 train_time:29417ms step_avg:40.74ms
step:723/2330 train_time:29452ms step_avg:40.74ms
step:724/2330 train_time:29497ms step_avg:40.74ms
step:725/2330 train_time:29533ms step_avg:40.74ms
step:726/2330 train_time:29579ms step_avg:40.74ms
step:727/2330 train_time:29615ms step_avg:40.74ms
step:728/2330 train_time:29662ms step_avg:40.74ms
step:729/2330 train_time:29698ms step_avg:40.74ms
step:730/2330 train_time:29743ms step_avg:40.74ms
step:731/2330 train_time:29778ms step_avg:40.74ms
step:732/2330 train_time:29824ms step_avg:40.74ms
step:733/2330 train_time:29861ms step_avg:40.74ms
step:734/2330 train_time:29906ms step_avg:40.74ms
step:735/2330 train_time:29942ms step_avg:40.74ms
step:736/2330 train_time:29988ms step_avg:40.74ms
step:737/2330 train_time:30025ms step_avg:40.74ms
step:738/2330 train_time:30070ms step_avg:40.75ms
step:739/2330 train_time:30106ms step_avg:40.74ms
step:740/2330 train_time:30151ms step_avg:40.74ms
step:741/2330 train_time:30187ms step_avg:40.74ms
step:742/2330 train_time:30232ms step_avg:40.74ms
step:743/2330 train_time:30268ms step_avg:40.74ms
step:744/2330 train_time:30314ms step_avg:40.74ms
step:745/2330 train_time:30350ms step_avg:40.74ms
step:746/2330 train_time:30395ms step_avg:40.74ms
step:747/2330 train_time:30431ms step_avg:40.74ms
step:748/2330 train_time:30476ms step_avg:40.74ms
step:749/2330 train_time:30512ms step_avg:40.74ms
step:750/2330 train_time:30557ms step_avg:40.74ms
step:750/2330 val_loss:5.2813 train_time:30647ms step_avg:40.86ms
step:751/2330 train_time:30659ms step_avg:40.82ms
step:752/2330 train_time:30671ms step_avg:40.79ms
step:753/2330 train_time:30681ms step_avg:40.75ms
step:754/2330 train_time:30720ms step_avg:40.74ms
step:755/2330 train_time:30755ms step_avg:40.73ms
step:756/2330 train_time:30799ms step_avg:40.74ms
step:757/2330 train_time:30834ms step_avg:40.73ms
step:758/2330 train_time:30879ms step_avg:40.74ms
step:759/2330 train_time:30914ms step_avg:40.73ms
step:760/2330 train_time:30959ms step_avg:40.74ms
step:761/2330 train_time:31001ms step_avg:40.74ms
step:762/2330 train_time:31048ms step_avg:40.75ms
step:763/2330 train_time:31086ms step_avg:40.74ms
step:764/2330 train_time:31132ms step_avg:40.75ms
step:765/2330 train_time:31171ms step_avg:40.75ms
step:766/2330 train_time:31216ms step_avg:40.75ms
step:767/2330 train_time:31252ms step_avg:40.75ms
step:768/2330 train_time:31298ms step_avg:40.75ms
step:769/2330 train_time:31334ms step_avg:40.75ms
step:770/2330 train_time:31379ms step_avg:40.75ms
step:771/2330 train_time:31415ms step_avg:40.75ms
step:772/2330 train_time:31460ms step_avg:40.75ms
step:773/2330 train_time:31495ms step_avg:40.74ms
step:774/2330 train_time:31541ms step_avg:40.75ms
step:775/2330 train_time:31578ms step_avg:40.75ms
step:776/2330 train_time:31624ms step_avg:40.75ms
step:777/2330 train_time:31659ms step_avg:40.75ms
step:778/2330 train_time:31703ms step_avg:40.75ms
step:779/2330 train_time:31739ms step_avg:40.74ms
step:780/2330 train_time:31783ms step_avg:40.75ms
step:781/2330 train_time:31818ms step_avg:40.74ms
step:782/2330 train_time:31863ms step_avg:40.75ms
step:783/2330 train_time:31899ms step_avg:40.74ms
step:784/2330 train_time:31945ms step_avg:40.75ms
step:785/2330 train_time:31982ms step_avg:40.74ms
step:786/2330 train_time:32029ms step_avg:40.75ms
step:787/2330 train_time:32066ms step_avg:40.74ms
step:788/2330 train_time:32112ms step_avg:40.75ms
step:789/2330 train_time:32148ms step_avg:40.75ms
step:790/2330 train_time:32193ms step_avg:40.75ms
step:791/2330 train_time:32229ms step_avg:40.74ms
step:792/2330 train_time:32274ms step_avg:40.75ms
step:793/2330 train_time:32310ms step_avg:40.74ms
step:794/2330 train_time:32355ms step_avg:40.75ms
step:795/2330 train_time:32390ms step_avg:40.74ms
step:796/2330 train_time:32435ms step_avg:40.75ms
step:797/2330 train_time:32471ms step_avg:40.74ms
step:798/2330 train_time:32516ms step_avg:40.75ms
step:799/2330 train_time:32552ms step_avg:40.74ms
step:800/2330 train_time:32598ms step_avg:40.75ms
step:801/2330 train_time:32635ms step_avg:40.74ms
step:802/2330 train_time:32681ms step_avg:40.75ms
step:803/2330 train_time:32716ms step_avg:40.74ms
step:804/2330 train_time:32761ms step_avg:40.75ms
step:805/2330 train_time:32797ms step_avg:40.74ms
step:806/2330 train_time:32842ms step_avg:40.75ms
step:807/2330 train_time:32878ms step_avg:40.74ms
step:808/2330 train_time:32923ms step_avg:40.75ms
step:809/2330 train_time:32960ms step_avg:40.74ms
step:810/2330 train_time:33006ms step_avg:40.75ms
step:811/2330 train_time:33041ms step_avg:40.74ms
step:812/2330 train_time:33087ms step_avg:40.75ms
step:813/2330 train_time:33123ms step_avg:40.74ms
step:814/2330 train_time:33170ms step_avg:40.75ms
step:815/2330 train_time:33205ms step_avg:40.74ms
step:816/2330 train_time:33251ms step_avg:40.75ms
step:817/2330 train_time:33287ms step_avg:40.74ms
step:818/2330 train_time:33332ms step_avg:40.75ms
step:819/2330 train_time:33367ms step_avg:40.74ms
step:820/2330 train_time:33412ms step_avg:40.75ms
step:821/2330 train_time:33448ms step_avg:40.74ms
step:822/2330 train_time:33493ms step_avg:40.75ms
step:823/2330 train_time:33529ms step_avg:40.74ms
step:824/2330 train_time:33574ms step_avg:40.74ms
step:825/2330 train_time:33610ms step_avg:40.74ms
step:826/2330 train_time:33656ms step_avg:40.75ms
step:827/2330 train_time:33691ms step_avg:40.74ms
step:828/2330 train_time:33737ms step_avg:40.74ms
step:829/2330 train_time:33773ms step_avg:40.74ms
step:830/2330 train_time:33818ms step_avg:40.75ms
step:831/2330 train_time:33855ms step_avg:40.74ms
step:832/2330 train_time:33900ms step_avg:40.74ms
step:833/2330 train_time:33936ms step_avg:40.74ms
step:834/2330 train_time:33983ms step_avg:40.75ms
step:835/2330 train_time:34020ms step_avg:40.74ms
step:836/2330 train_time:34065ms step_avg:40.75ms
step:837/2330 train_time:34101ms step_avg:40.74ms
step:838/2330 train_time:34147ms step_avg:40.75ms
step:839/2330 train_time:34182ms step_avg:40.74ms
step:840/2330 train_time:34228ms step_avg:40.75ms
step:841/2330 train_time:34264ms step_avg:40.74ms
step:842/2330 train_time:34310ms step_avg:40.75ms
step:843/2330 train_time:34346ms step_avg:40.74ms
step:844/2330 train_time:34391ms step_avg:40.75ms
step:845/2330 train_time:34427ms step_avg:40.74ms
step:846/2330 train_time:34472ms step_avg:40.75ms
step:847/2330 train_time:34508ms step_avg:40.74ms
step:848/2330 train_time:34553ms step_avg:40.75ms
step:849/2330 train_time:34589ms step_avg:40.74ms
step:850/2330 train_time:34634ms step_avg:40.75ms
step:851/2330 train_time:34670ms step_avg:40.74ms
step:852/2330 train_time:34716ms step_avg:40.75ms
step:853/2330 train_time:34752ms step_avg:40.74ms
step:854/2330 train_time:34798ms step_avg:40.75ms
step:855/2330 train_time:34834ms step_avg:40.74ms
step:856/2330 train_time:34880ms step_avg:40.75ms
step:857/2330 train_time:34916ms step_avg:40.74ms
step:858/2330 train_time:34961ms step_avg:40.75ms
step:859/2330 train_time:34997ms step_avg:40.74ms
step:860/2330 train_time:35044ms step_avg:40.75ms
step:861/2330 train_time:35081ms step_avg:40.74ms
step:862/2330 train_time:35126ms step_avg:40.75ms
step:863/2330 train_time:35162ms step_avg:40.74ms
step:864/2330 train_time:35208ms step_avg:40.75ms
step:865/2330 train_time:35243ms step_avg:40.74ms
step:866/2330 train_time:35288ms step_avg:40.75ms
step:867/2330 train_time:35323ms step_avg:40.74ms
step:868/2330 train_time:35369ms step_avg:40.75ms
step:869/2330 train_time:35405ms step_avg:40.74ms
step:870/2330 train_time:35450ms step_avg:40.75ms
step:871/2330 train_time:35485ms step_avg:40.74ms
step:872/2330 train_time:35531ms step_avg:40.75ms
step:873/2330 train_time:35566ms step_avg:40.74ms
step:874/2330 train_time:35611ms step_avg:40.75ms
step:875/2330 train_time:35647ms step_avg:40.74ms
step:876/2330 train_time:35693ms step_avg:40.75ms
step:877/2330 train_time:35729ms step_avg:40.74ms
step:878/2330 train_time:35774ms step_avg:40.75ms
step:879/2330 train_time:35810ms step_avg:40.74ms
step:880/2330 train_time:35855ms step_avg:40.74ms
step:881/2330 train_time:35891ms step_avg:40.74ms
step:882/2330 train_time:35937ms step_avg:40.74ms
step:883/2330 train_time:35974ms step_avg:40.74ms
step:884/2330 train_time:36020ms step_avg:40.75ms
step:885/2330 train_time:36056ms step_avg:40.74ms
step:886/2330 train_time:36102ms step_avg:40.75ms
step:887/2330 train_time:36139ms step_avg:40.74ms
step:888/2330 train_time:36185ms step_avg:40.75ms
step:889/2330 train_time:36221ms step_avg:40.74ms
step:890/2330 train_time:36266ms step_avg:40.75ms
step:891/2330 train_time:36302ms step_avg:40.74ms
step:892/2330 train_time:36347ms step_avg:40.75ms
step:893/2330 train_time:36383ms step_avg:40.74ms
step:894/2330 train_time:36429ms step_avg:40.75ms
step:895/2330 train_time:36464ms step_avg:40.74ms
step:896/2330 train_time:36511ms step_avg:40.75ms
step:897/2330 train_time:36546ms step_avg:40.74ms
step:898/2330 train_time:36592ms step_avg:40.75ms
step:899/2330 train_time:36628ms step_avg:40.74ms
step:900/2330 train_time:36674ms step_avg:40.75ms
step:901/2330 train_time:36709ms step_avg:40.74ms
step:902/2330 train_time:36754ms step_avg:40.75ms
step:903/2330 train_time:36790ms step_avg:40.74ms
step:904/2330 train_time:36835ms step_avg:40.75ms
step:905/2330 train_time:36872ms step_avg:40.74ms
step:906/2330 train_time:36918ms step_avg:40.75ms
step:907/2330 train_time:36953ms step_avg:40.74ms
step:908/2330 train_time:36999ms step_avg:40.75ms
step:909/2330 train_time:37035ms step_avg:40.74ms
step:910/2330 train_time:37081ms step_avg:40.75ms
step:911/2330 train_time:37118ms step_avg:40.74ms
step:912/2330 train_time:37163ms step_avg:40.75ms
step:913/2330 train_time:37200ms step_avg:40.74ms
step:914/2330 train_time:37244ms step_avg:40.75ms
step:915/2330 train_time:37281ms step_avg:40.74ms
step:916/2330 train_time:37326ms step_avg:40.75ms
step:917/2330 train_time:37363ms step_avg:40.74ms
step:918/2330 train_time:37408ms step_avg:40.75ms
step:919/2330 train_time:37443ms step_avg:40.74ms
step:920/2330 train_time:37489ms step_avg:40.75ms
step:921/2330 train_time:37524ms step_avg:40.74ms
step:922/2330 train_time:37570ms step_avg:40.75ms
step:923/2330 train_time:37607ms step_avg:40.74ms
step:924/2330 train_time:37652ms step_avg:40.75ms
step:925/2330 train_time:37688ms step_avg:40.74ms
step:926/2330 train_time:37733ms step_avg:40.75ms
step:927/2330 train_time:37769ms step_avg:40.74ms
step:928/2330 train_time:37814ms step_avg:40.75ms
step:929/2330 train_time:37850ms step_avg:40.74ms
step:930/2330 train_time:37896ms step_avg:40.75ms
step:931/2330 train_time:37932ms step_avg:40.74ms
step:932/2330 train_time:37979ms step_avg:40.75ms
step:933/2330 train_time:38015ms step_avg:40.75ms
step:934/2330 train_time:38061ms step_avg:40.75ms
step:935/2330 train_time:38096ms step_avg:40.74ms
step:936/2330 train_time:38140ms step_avg:40.75ms
step:937/2330 train_time:38177ms step_avg:40.74ms
step:938/2330 train_time:38222ms step_avg:40.75ms
step:939/2330 train_time:38259ms step_avg:40.74ms
step:940/2330 train_time:38304ms step_avg:40.75ms
step:941/2330 train_time:38340ms step_avg:40.74ms
step:942/2330 train_time:38385ms step_avg:40.75ms
step:943/2330 train_time:38421ms step_avg:40.74ms
step:944/2330 train_time:38466ms step_avg:40.75ms
step:945/2330 train_time:38502ms step_avg:40.74ms
step:946/2330 train_time:38548ms step_avg:40.75ms
step:947/2330 train_time:38584ms step_avg:40.74ms
step:948/2330 train_time:38630ms step_avg:40.75ms
step:949/2330 train_time:38665ms step_avg:40.74ms
step:950/2330 train_time:38711ms step_avg:40.75ms
step:951/2330 train_time:38746ms step_avg:40.74ms
step:952/2330 train_time:38792ms step_avg:40.75ms
step:953/2330 train_time:38828ms step_avg:40.74ms
step:954/2330 train_time:38874ms step_avg:40.75ms
step:955/2330 train_time:38910ms step_avg:40.74ms
step:956/2330 train_time:38955ms step_avg:40.75ms
step:957/2330 train_time:38991ms step_avg:40.74ms
step:958/2330 train_time:39036ms step_avg:40.75ms
step:959/2330 train_time:39073ms step_avg:40.74ms
step:960/2330 train_time:39119ms step_avg:40.75ms
step:961/2330 train_time:39156ms step_avg:40.74ms
step:962/2330 train_time:39201ms step_avg:40.75ms
step:963/2330 train_time:39237ms step_avg:40.74ms
step:964/2330 train_time:39282ms step_avg:40.75ms
step:965/2330 train_time:39318ms step_avg:40.74ms
step:966/2330 train_time:39364ms step_avg:40.75ms
step:967/2330 train_time:39400ms step_avg:40.74ms
step:968/2330 train_time:39445ms step_avg:40.75ms
step:969/2330 train_time:39481ms step_avg:40.74ms
step:970/2330 train_time:39527ms step_avg:40.75ms
step:971/2330 train_time:39562ms step_avg:40.74ms
step:972/2330 train_time:39607ms step_avg:40.75ms
step:973/2330 train_time:39643ms step_avg:40.74ms
step:974/2330 train_time:39688ms step_avg:40.75ms
step:975/2330 train_time:39724ms step_avg:40.74ms
step:976/2330 train_time:39770ms step_avg:40.75ms
step:977/2330 train_time:39807ms step_avg:40.74ms
step:978/2330 train_time:39853ms step_avg:40.75ms
step:979/2330 train_time:39888ms step_avg:40.74ms
step:980/2330 train_time:39933ms step_avg:40.75ms
step:981/2330 train_time:39968ms step_avg:40.74ms
step:982/2330 train_time:40014ms step_avg:40.75ms
step:983/2330 train_time:40050ms step_avg:40.74ms
step:984/2330 train_time:40095ms step_avg:40.75ms
step:985/2330 train_time:40131ms step_avg:40.74ms
step:986/2330 train_time:40177ms step_avg:40.75ms
step:987/2330 train_time:40214ms step_avg:40.74ms
step:988/2330 train_time:40260ms step_avg:40.75ms
step:989/2330 train_time:40296ms step_avg:40.74ms
step:990/2330 train_time:40342ms step_avg:40.75ms
step:991/2330 train_time:40378ms step_avg:40.74ms
step:992/2330 train_time:40424ms step_avg:40.75ms
step:993/2330 train_time:40460ms step_avg:40.75ms
step:994/2330 train_time:40506ms step_avg:40.75ms
step:995/2330 train_time:40541ms step_avg:40.74ms
step:996/2330 train_time:40587ms step_avg:40.75ms
step:997/2330 train_time:40623ms step_avg:40.75ms
step:998/2330 train_time:40670ms step_avg:40.75ms
step:999/2330 train_time:40705ms step_avg:40.75ms
step:1000/2330 train_time:40751ms step_avg:40.75ms
step:1000/2330 val_loss:5.2361 train_time:40841ms step_avg:40.84ms
step:1001/2330 train_time:40854ms step_avg:40.81ms
step:1002/2330 train_time:40866ms step_avg:40.78ms
step:1003/2330 train_time:40876ms step_avg:40.75ms
step:1004/2330 train_time:40915ms step_avg:40.75ms
step:1005/2330 train_time:40949ms step_avg:40.75ms
step:1006/2330 train_time:40994ms step_avg:40.75ms
step:1007/2330 train_time:41029ms step_avg:40.74ms
step:1008/2330 train_time:41074ms step_avg:40.75ms
step:1009/2330 train_time:41109ms step_avg:40.74ms
step:1010/2330 train_time:41156ms step_avg:40.75ms
step:1011/2330 train_time:41199ms step_avg:40.75ms
step:1012/2330 train_time:41248ms step_avg:40.76ms
step:1013/2330 train_time:41284ms step_avg:40.75ms
step:1014/2330 train_time:41330ms step_avg:40.76ms
step:1015/2330 train_time:41365ms step_avg:40.75ms
step:1016/2330 train_time:41409ms step_avg:40.76ms
step:1017/2330 train_time:41445ms step_avg:40.75ms
step:1018/2330 train_time:41490ms step_avg:40.76ms
step:1019/2330 train_time:41525ms step_avg:40.75ms
step:1020/2330 train_time:41570ms step_avg:40.76ms
step:1021/2330 train_time:41605ms step_avg:40.75ms
step:1022/2330 train_time:41649ms step_avg:40.75ms
step:1023/2330 train_time:41684ms step_avg:40.75ms
step:1024/2330 train_time:41731ms step_avg:40.75ms
step:1025/2330 train_time:41768ms step_avg:40.75ms
step:1026/2330 train_time:41815ms step_avg:40.76ms
step:1027/2330 train_time:41850ms step_avg:40.75ms
step:1028/2330 train_time:41895ms step_avg:40.75ms
step:1029/2330 train_time:41930ms step_avg:40.75ms
step:1030/2330 train_time:41975ms step_avg:40.75ms
step:1031/2330 train_time:42009ms step_avg:40.75ms
step:1032/2330 train_time:42054ms step_avg:40.75ms
step:1033/2330 train_time:42091ms step_avg:40.75ms
step:1034/2330 train_time:42139ms step_avg:40.75ms
step:1035/2330 train_time:42176ms step_avg:40.75ms
step:1036/2330 train_time:42223ms step_avg:40.76ms
step:1037/2330 train_time:42261ms step_avg:40.75ms
step:1038/2330 train_time:42307ms step_avg:40.76ms
step:1039/2330 train_time:42343ms step_avg:40.75ms
step:1040/2330 train_time:42387ms step_avg:40.76ms
step:1041/2330 train_time:42422ms step_avg:40.75ms
step:1042/2330 train_time:42467ms step_avg:40.76ms
step:1043/2330 train_time:42503ms step_avg:40.75ms
step:1044/2330 train_time:42548ms step_avg:40.75ms
step:1045/2330 train_time:42583ms step_avg:40.75ms
step:1046/2330 train_time:42628ms step_avg:40.75ms
step:1047/2330 train_time:42663ms step_avg:40.75ms
step:1048/2330 train_time:42708ms step_avg:40.75ms
step:1049/2330 train_time:42745ms step_avg:40.75ms
step:1050/2330 train_time:42791ms step_avg:40.75ms
step:1051/2330 train_time:42827ms step_avg:40.75ms
step:1052/2330 train_time:42871ms step_avg:40.75ms
step:1053/2330 train_time:42906ms step_avg:40.75ms
step:1054/2330 train_time:42952ms step_avg:40.75ms
step:1055/2330 train_time:42987ms step_avg:40.75ms
step:1056/2330 train_time:43033ms step_avg:40.75ms
step:1057/2330 train_time:43070ms step_avg:40.75ms
step:1058/2330 train_time:43116ms step_avg:40.75ms
step:1059/2330 train_time:43152ms step_avg:40.75ms
step:1060/2330 train_time:43199ms step_avg:40.75ms
step:1061/2330 train_time:43235ms step_avg:40.75ms
step:1062/2330 train_time:43280ms step_avg:40.75ms
step:1063/2330 train_time:43316ms step_avg:40.75ms
step:1064/2330 train_time:43362ms step_avg:40.75ms
step:1065/2330 train_time:43399ms step_avg:40.75ms
step:1066/2330 train_time:43445ms step_avg:40.76ms
step:1067/2330 train_time:43481ms step_avg:40.75ms
step:1068/2330 train_time:43526ms step_avg:40.75ms
step:1069/2330 train_time:43562ms step_avg:40.75ms
step:1070/2330 train_time:43607ms step_avg:40.75ms
step:1071/2330 train_time:43642ms step_avg:40.75ms
step:1072/2330 train_time:43687ms step_avg:40.75ms
step:1073/2330 train_time:43725ms step_avg:40.75ms
step:1074/2330 train_time:43770ms step_avg:40.75ms
step:1075/2330 train_time:43805ms step_avg:40.75ms
step:1076/2330 train_time:43851ms step_avg:40.75ms
step:1077/2330 train_time:43887ms step_avg:40.75ms
step:1078/2330 train_time:43932ms step_avg:40.75ms
step:1079/2330 train_time:43968ms step_avg:40.75ms
step:1080/2330 train_time:44013ms step_avg:40.75ms
step:1081/2330 train_time:44049ms step_avg:40.75ms
step:1082/2330 train_time:44095ms step_avg:40.75ms
step:1083/2330 train_time:44131ms step_avg:40.75ms
step:1084/2330 train_time:44178ms step_avg:40.75ms
step:1085/2330 train_time:44213ms step_avg:40.75ms
step:1086/2330 train_time:44259ms step_avg:40.75ms
step:1087/2330 train_time:44294ms step_avg:40.75ms
step:1088/2330 train_time:44340ms step_avg:40.75ms
step:1089/2330 train_time:44377ms step_avg:40.75ms
step:1090/2330 train_time:44423ms step_avg:40.75ms
step:1091/2330 train_time:44459ms step_avg:40.75ms
step:1092/2330 train_time:44504ms step_avg:40.75ms
step:1093/2330 train_time:44540ms step_avg:40.75ms
step:1094/2330 train_time:44586ms step_avg:40.75ms
step:1095/2330 train_time:44622ms step_avg:40.75ms
step:1096/2330 train_time:44667ms step_avg:40.75ms
step:1097/2330 train_time:44703ms step_avg:40.75ms
step:1098/2330 train_time:44748ms step_avg:40.75ms
step:1099/2330 train_time:44783ms step_avg:40.75ms
step:1100/2330 train_time:44828ms step_avg:40.75ms
step:1101/2330 train_time:44864ms step_avg:40.75ms
step:1102/2330 train_time:44910ms step_avg:40.75ms
step:1103/2330 train_time:44945ms step_avg:40.75ms
step:1104/2330 train_time:44992ms step_avg:40.75ms
step:1105/2330 train_time:45028ms step_avg:40.75ms
step:1106/2330 train_time:45074ms step_avg:40.75ms
step:1107/2330 train_time:45109ms step_avg:40.75ms
step:1108/2330 train_time:45156ms step_avg:40.75ms
step:1109/2330 train_time:45192ms step_avg:40.75ms
step:1110/2330 train_time:45237ms step_avg:40.75ms
step:1111/2330 train_time:45273ms step_avg:40.75ms
step:1112/2330 train_time:45319ms step_avg:40.75ms
step:1113/2330 train_time:45355ms step_avg:40.75ms
step:1114/2330 train_time:45400ms step_avg:40.75ms
step:1115/2330 train_time:45436ms step_avg:40.75ms
step:1116/2330 train_time:45481ms step_avg:40.75ms
step:1117/2330 train_time:45517ms step_avg:40.75ms
step:1118/2330 train_time:45563ms step_avg:40.75ms
step:1119/2330 train_time:45599ms step_avg:40.75ms
step:1120/2330 train_time:45644ms step_avg:40.75ms
step:1121/2330 train_time:45681ms step_avg:40.75ms
step:1122/2330 train_time:45727ms step_avg:40.76ms
step:1123/2330 train_time:45763ms step_avg:40.75ms
step:1124/2330 train_time:45808ms step_avg:40.75ms
step:1125/2330 train_time:45844ms step_avg:40.75ms
step:1126/2330 train_time:45889ms step_avg:40.75ms
step:1127/2330 train_time:45925ms step_avg:40.75ms
step:1128/2330 train_time:45971ms step_avg:40.75ms
step:1129/2330 train_time:46007ms step_avg:40.75ms
step:1130/2330 train_time:46052ms step_avg:40.75ms
step:1131/2330 train_time:46088ms step_avg:40.75ms
step:1132/2330 train_time:46134ms step_avg:40.75ms
step:1133/2330 train_time:46170ms step_avg:40.75ms
step:1134/2330 train_time:46216ms step_avg:40.75ms
step:1135/2330 train_time:46252ms step_avg:40.75ms
step:1136/2330 train_time:46298ms step_avg:40.76ms
step:1137/2330 train_time:46335ms step_avg:40.75ms
step:1138/2330 train_time:46379ms step_avg:40.75ms
step:1139/2330 train_time:46414ms step_avg:40.75ms
step:1140/2330 train_time:46460ms step_avg:40.75ms
step:1141/2330 train_time:46496ms step_avg:40.75ms
step:1142/2330 train_time:46541ms step_avg:40.75ms
step:1143/2330 train_time:46577ms step_avg:40.75ms
step:1144/2330 train_time:46623ms step_avg:40.75ms
step:1145/2330 train_time:46659ms step_avg:40.75ms
step:1146/2330 train_time:46705ms step_avg:40.75ms
step:1147/2330 train_time:46741ms step_avg:40.75ms
step:1148/2330 train_time:46786ms step_avg:40.75ms
step:1149/2330 train_time:46822ms step_avg:40.75ms
step:1150/2330 train_time:46867ms step_avg:40.75ms
step:1151/2330 train_time:46904ms step_avg:40.75ms
step:1152/2330 train_time:46949ms step_avg:40.75ms
step:1153/2330 train_time:46985ms step_avg:40.75ms
step:1154/2330 train_time:47032ms step_avg:40.76ms
step:1155/2330 train_time:47068ms step_avg:40.75ms
step:1156/2330 train_time:47113ms step_avg:40.76ms
step:1157/2330 train_time:47150ms step_avg:40.75ms
step:1158/2330 train_time:47196ms step_avg:40.76ms
step:1159/2330 train_time:47232ms step_avg:40.75ms
step:1160/2330 train_time:47278ms step_avg:40.76ms
step:1161/2330 train_time:47313ms step_avg:40.75ms
step:1162/2330 train_time:47359ms step_avg:40.76ms
step:1163/2330 train_time:47395ms step_avg:40.75ms
step:1164/2330 train_time:47440ms step_avg:40.76ms
step:1165/2330 train_time:47475ms step_avg:40.75ms
step:1166/2330 train_time:47520ms step_avg:40.76ms
step:1167/2330 train_time:47557ms step_avg:40.75ms
step:1168/2330 train_time:47603ms step_avg:40.76ms
step:1169/2330 train_time:47639ms step_avg:40.75ms
step:1170/2330 train_time:47684ms step_avg:40.76ms
step:1171/2330 train_time:47720ms step_avg:40.75ms
step:1172/2330 train_time:47766ms step_avg:40.76ms
step:1173/2330 train_time:47802ms step_avg:40.75ms
step:1174/2330 train_time:47847ms step_avg:40.76ms
step:1175/2330 train_time:47883ms step_avg:40.75ms
step:1176/2330 train_time:47929ms step_avg:40.76ms
step:1177/2330 train_time:47964ms step_avg:40.75ms
step:1178/2330 train_time:48010ms step_avg:40.76ms
step:1179/2330 train_time:48046ms step_avg:40.75ms
step:1180/2330 train_time:48091ms step_avg:40.76ms
step:1181/2330 train_time:48128ms step_avg:40.75ms
step:1182/2330 train_time:48174ms step_avg:40.76ms
step:1183/2330 train_time:48210ms step_avg:40.75ms
step:1184/2330 train_time:48255ms step_avg:40.76ms
step:1185/2330 train_time:48291ms step_avg:40.75ms
step:1186/2330 train_time:48338ms step_avg:40.76ms
step:1187/2330 train_time:48374ms step_avg:40.75ms
step:1188/2330 train_time:48419ms step_avg:40.76ms
step:1189/2330 train_time:48454ms step_avg:40.75ms
step:1190/2330 train_time:48500ms step_avg:40.76ms
step:1191/2330 train_time:48536ms step_avg:40.75ms
step:1192/2330 train_time:48580ms step_avg:40.76ms
step:1193/2330 train_time:48616ms step_avg:40.75ms
step:1194/2330 train_time:48662ms step_avg:40.76ms
step:1195/2330 train_time:48698ms step_avg:40.75ms
step:1196/2330 train_time:48744ms step_avg:40.76ms
step:1197/2330 train_time:48780ms step_avg:40.75ms
step:1198/2330 train_time:48826ms step_avg:40.76ms
step:1199/2330 train_time:48862ms step_avg:40.75ms
step:1200/2330 train_time:48908ms step_avg:40.76ms
step:1201/2330 train_time:48943ms step_avg:40.75ms
step:1202/2330 train_time:48987ms step_avg:40.75ms
step:1203/2330 train_time:49025ms step_avg:40.75ms
step:1204/2330 train_time:49072ms step_avg:40.76ms
step:1205/2330 train_time:49107ms step_avg:40.75ms
step:1206/2330 train_time:49153ms step_avg:40.76ms
step:1207/2330 train_time:49189ms step_avg:40.75ms
step:1208/2330 train_time:49234ms step_avg:40.76ms
step:1209/2330 train_time:49270ms step_avg:40.75ms
step:1210/2330 train_time:49315ms step_avg:40.76ms
step:1211/2330 train_time:49351ms step_avg:40.75ms
step:1212/2330 train_time:49397ms step_avg:40.76ms
step:1213/2330 train_time:49433ms step_avg:40.75ms
step:1214/2330 train_time:49478ms step_avg:40.76ms
step:1215/2330 train_time:49514ms step_avg:40.75ms
step:1216/2330 train_time:49560ms step_avg:40.76ms
step:1217/2330 train_time:49595ms step_avg:40.75ms
step:1218/2330 train_time:49641ms step_avg:40.76ms
step:1219/2330 train_time:49676ms step_avg:40.75ms
step:1220/2330 train_time:49722ms step_avg:40.76ms
step:1221/2330 train_time:49758ms step_avg:40.75ms
step:1222/2330 train_time:49805ms step_avg:40.76ms
step:1223/2330 train_time:49841ms step_avg:40.75ms
step:1224/2330 train_time:49886ms step_avg:40.76ms
step:1225/2330 train_time:49922ms step_avg:40.75ms
step:1226/2330 train_time:49967ms step_avg:40.76ms
step:1227/2330 train_time:50003ms step_avg:40.75ms
step:1228/2330 train_time:50049ms step_avg:40.76ms
step:1229/2330 train_time:50085ms step_avg:40.75ms
step:1230/2330 train_time:50130ms step_avg:40.76ms
step:1231/2330 train_time:50167ms step_avg:40.75ms
step:1232/2330 train_time:50212ms step_avg:40.76ms
step:1233/2330 train_time:50248ms step_avg:40.75ms
step:1234/2330 train_time:50293ms step_avg:40.76ms
step:1235/2330 train_time:50329ms step_avg:40.75ms
step:1236/2330 train_time:50375ms step_avg:40.76ms
step:1237/2330 train_time:50411ms step_avg:40.75ms
step:1238/2330 train_time:50457ms step_avg:40.76ms
step:1239/2330 train_time:50493ms step_avg:40.75ms
step:1240/2330 train_time:50538ms step_avg:40.76ms
step:1241/2330 train_time:50574ms step_avg:40.75ms
step:1242/2330 train_time:50620ms step_avg:40.76ms
step:1243/2330 train_time:50655ms step_avg:40.75ms
step:1244/2330 train_time:50701ms step_avg:40.76ms
step:1245/2330 train_time:50737ms step_avg:40.75ms
step:1246/2330 train_time:50782ms step_avg:40.76ms
step:1247/2330 train_time:50818ms step_avg:40.75ms
step:1248/2330 train_time:50864ms step_avg:40.76ms
step:1249/2330 train_time:50899ms step_avg:40.75ms
step:1250/2330 train_time:50945ms step_avg:40.76ms
step:1250/2330 val_loss:5.2009 train_time:51035ms step_avg:40.83ms
step:1251/2330 train_time:51049ms step_avg:40.81ms
step:1252/2330 train_time:51061ms step_avg:40.78ms
step:1253/2330 train_time:51072ms step_avg:40.76ms
step:1254/2330 train_time:51109ms step_avg:40.76ms
step:1255/2330 train_time:51144ms step_avg:40.75ms
step:1256/2330 train_time:51188ms step_avg:40.75ms
step:1257/2330 train_time:51223ms step_avg:40.75ms
step:1258/2330 train_time:51268ms step_avg:40.75ms
step:1259/2330 train_time:51302ms step_avg:40.75ms
step:1260/2330 train_time:51348ms step_avg:40.75ms
step:1261/2330 train_time:51388ms step_avg:40.75ms
step:1262/2330 train_time:51434ms step_avg:40.76ms
step:1263/2330 train_time:51471ms step_avg:40.75ms
step:1264/2330 train_time:51516ms step_avg:40.76ms
step:1265/2330 train_time:51551ms step_avg:40.75ms
step:1266/2330 train_time:51596ms step_avg:40.75ms
step:1267/2330 train_time:51631ms step_avg:40.75ms
step:1268/2330 train_time:51676ms step_avg:40.75ms
step:1269/2330 train_time:51711ms step_avg:40.75ms
step:1270/2330 train_time:51755ms step_avg:40.75ms
step:1271/2330 train_time:51790ms step_avg:40.75ms
step:1272/2330 train_time:51835ms step_avg:40.75ms
step:1273/2330 train_time:51871ms step_avg:40.75ms
step:1274/2330 train_time:51916ms step_avg:40.75ms
step:1275/2330 train_time:51954ms step_avg:40.75ms
step:1276/2330 train_time:52001ms step_avg:40.75ms
step:1277/2330 train_time:52037ms step_avg:40.75ms
step:1278/2330 train_time:52083ms step_avg:40.75ms
step:1279/2330 train_time:52119ms step_avg:40.75ms
step:1280/2330 train_time:52164ms step_avg:40.75ms
step:1281/2330 train_time:52199ms step_avg:40.75ms
step:1282/2330 train_time:52245ms step_avg:40.75ms
step:1283/2330 train_time:52281ms step_avg:40.75ms
step:1284/2330 train_time:52328ms step_avg:40.75ms
step:1285/2330 train_time:52363ms step_avg:40.75ms
step:1286/2330 train_time:52408ms step_avg:40.75ms
step:1287/2330 train_time:52444ms step_avg:40.75ms
step:1288/2330 train_time:52492ms step_avg:40.75ms
step:1289/2330 train_time:52528ms step_avg:40.75ms
step:1290/2330 train_time:52574ms step_avg:40.75ms
step:1291/2330 train_time:52609ms step_avg:40.75ms
step:1292/2330 train_time:52653ms step_avg:40.75ms
step:1293/2330 train_time:52689ms step_avg:40.75ms
step:1294/2330 train_time:52734ms step_avg:40.75ms
step:1295/2330 train_time:52770ms step_avg:40.75ms
step:1296/2330 train_time:52814ms step_avg:40.75ms
step:1297/2330 train_time:52850ms step_avg:40.75ms
step:1298/2330 train_time:52896ms step_avg:40.75ms
step:1299/2330 train_time:52931ms step_avg:40.75ms
step:1300/2330 train_time:52978ms step_avg:40.75ms
step:1301/2330 train_time:53014ms step_avg:40.75ms
step:1302/2330 train_time:53060ms step_avg:40.75ms
step:1303/2330 train_time:53096ms step_avg:40.75ms
step:1304/2330 train_time:53141ms step_avg:40.75ms
step:1305/2330 train_time:53176ms step_avg:40.75ms
step:1306/2330 train_time:53222ms step_avg:40.75ms
step:1307/2330 train_time:53258ms step_avg:40.75ms
step:1308/2330 train_time:53304ms step_avg:40.75ms
step:1309/2330 train_time:53341ms step_avg:40.75ms
step:1310/2330 train_time:53388ms step_avg:40.75ms
step:1311/2330 train_time:53424ms step_avg:40.75ms
step:1312/2330 train_time:53470ms step_avg:40.75ms
step:1313/2330 train_time:53506ms step_avg:40.75ms
step:1314/2330 train_time:53551ms step_avg:40.75ms
step:1315/2330 train_time:53586ms step_avg:40.75ms
step:1316/2330 train_time:53631ms step_avg:40.75ms
step:1317/2330 train_time:53668ms step_avg:40.75ms
step:1318/2330 train_time:53712ms step_avg:40.75ms
step:1319/2330 train_time:53748ms step_avg:40.75ms
step:1320/2330 train_time:53793ms step_avg:40.75ms
step:1321/2330 train_time:53829ms step_avg:40.75ms
step:1322/2330 train_time:53875ms step_avg:40.75ms
step:1323/2330 train_time:53911ms step_avg:40.75ms
step:1324/2330 train_time:53957ms step_avg:40.75ms
step:1325/2330 train_time:53992ms step_avg:40.75ms
step:1326/2330 train_time:54038ms step_avg:40.75ms
step:1327/2330 train_time:54074ms step_avg:40.75ms
step:1328/2330 train_time:54119ms step_avg:40.75ms
step:1329/2330 train_time:54155ms step_avg:40.75ms
step:1330/2330 train_time:54200ms step_avg:40.75ms
step:1331/2330 train_time:54236ms step_avg:40.75ms
step:1332/2330 train_time:54281ms step_avg:40.75ms
step:1333/2330 train_time:54318ms step_avg:40.75ms
step:1334/2330 train_time:54363ms step_avg:40.75ms
step:1335/2330 train_time:54400ms step_avg:40.75ms
step:1336/2330 train_time:54447ms step_avg:40.75ms
step:1337/2330 train_time:54484ms step_avg:40.75ms
step:1338/2330 train_time:54529ms step_avg:40.75ms
step:1339/2330 train_time:54565ms step_avg:40.75ms
step:1340/2330 train_time:54611ms step_avg:40.75ms
step:1341/2330 train_time:54647ms step_avg:40.75ms
step:1342/2330 train_time:54691ms step_avg:40.75ms
step:1343/2330 train_time:54727ms step_avg:40.75ms
step:1344/2330 train_time:54773ms step_avg:40.75ms
step:1345/2330 train_time:54809ms step_avg:40.75ms
step:1346/2330 train_time:54854ms step_avg:40.75ms
step:1347/2330 train_time:54891ms step_avg:40.75ms
step:1348/2330 train_time:54937ms step_avg:40.75ms
step:1349/2330 train_time:54973ms step_avg:40.75ms
step:1350/2330 train_time:55018ms step_avg:40.75ms
step:1351/2330 train_time:55053ms step_avg:40.75ms
step:1352/2330 train_time:55099ms step_avg:40.75ms
step:1353/2330 train_time:55136ms step_avg:40.75ms
step:1354/2330 train_time:55181ms step_avg:40.75ms
step:1355/2330 train_time:55218ms step_avg:40.75ms
step:1356/2330 train_time:55264ms step_avg:40.76ms
step:1357/2330 train_time:55299ms step_avg:40.75ms
step:1358/2330 train_time:55344ms step_avg:40.75ms
step:1359/2330 train_time:55380ms step_avg:40.75ms
step:1360/2330 train_time:55427ms step_avg:40.76ms
step:1361/2330 train_time:55463ms step_avg:40.75ms
step:1362/2330 train_time:55508ms step_avg:40.75ms
step:1363/2330 train_time:55543ms step_avg:40.75ms
step:1364/2330 train_time:55588ms step_avg:40.75ms
step:1365/2330 train_time:55625ms step_avg:40.75ms
step:1366/2330 train_time:55670ms step_avg:40.75ms
step:1367/2330 train_time:55706ms step_avg:40.75ms
step:1368/2330 train_time:55751ms step_avg:40.75ms
step:1369/2330 train_time:55787ms step_avg:40.75ms
step:1370/2330 train_time:55833ms step_avg:40.75ms
step:1371/2330 train_time:55869ms step_avg:40.75ms
step:1372/2330 train_time:55915ms step_avg:40.75ms
step:1373/2330 train_time:55950ms step_avg:40.75ms
step:1374/2330 train_time:55996ms step_avg:40.75ms
step:1375/2330 train_time:56032ms step_avg:40.75ms
step:1376/2330 train_time:56078ms step_avg:40.75ms
step:1377/2330 train_time:56113ms step_avg:40.75ms
step:1378/2330 train_time:56158ms step_avg:40.75ms
step:1379/2330 train_time:56194ms step_avg:40.75ms
step:1380/2330 train_time:56240ms step_avg:40.75ms
step:1381/2330 train_time:56276ms step_avg:40.75ms
step:1382/2330 train_time:56323ms step_avg:40.75ms
step:1383/2330 train_time:56360ms step_avg:40.75ms
step:1384/2330 train_time:56406ms step_avg:40.76ms
step:1385/2330 train_time:56442ms step_avg:40.75ms
step:1386/2330 train_time:56487ms step_avg:40.76ms
step:1387/2330 train_time:56523ms step_avg:40.75ms
step:1388/2330 train_time:56568ms step_avg:40.75ms
step:1389/2330 train_time:56604ms step_avg:40.75ms
step:1390/2330 train_time:56649ms step_avg:40.75ms
step:1391/2330 train_time:56685ms step_avg:40.75ms
step:1392/2330 train_time:56730ms step_avg:40.75ms
step:1393/2330 train_time:56766ms step_avg:40.75ms
step:1394/2330 train_time:56812ms step_avg:40.75ms
step:1395/2330 train_time:56849ms step_avg:40.75ms
step:1396/2330 train_time:56895ms step_avg:40.76ms
step:1397/2330 train_time:56931ms step_avg:40.75ms
step:1398/2330 train_time:56976ms step_avg:40.76ms
step:1399/2330 train_time:57012ms step_avg:40.75ms
step:1400/2330 train_time:57058ms step_avg:40.76ms
step:1401/2330 train_time:57093ms step_avg:40.75ms
step:1402/2330 train_time:57139ms step_avg:40.76ms
step:1403/2330 train_time:57174ms step_avg:40.75ms
step:1404/2330 train_time:57219ms step_avg:40.75ms
step:1405/2330 train_time:57256ms step_avg:40.75ms
step:1406/2330 train_time:57301ms step_avg:40.75ms
step:1407/2330 train_time:57338ms step_avg:40.75ms
step:1408/2330 train_time:57384ms step_avg:40.76ms
step:1409/2330 train_time:57421ms step_avg:40.75ms
step:1410/2330 train_time:57467ms step_avg:40.76ms
step:1411/2330 train_time:57502ms step_avg:40.75ms
step:1412/2330 train_time:57548ms step_avg:40.76ms
step:1413/2330 train_time:57583ms step_avg:40.75ms
step:1414/2330 train_time:57628ms step_avg:40.76ms
step:1415/2330 train_time:57664ms step_avg:40.75ms
step:1416/2330 train_time:57709ms step_avg:40.75ms
step:1417/2330 train_time:57745ms step_avg:40.75ms
step:1418/2330 train_time:57791ms step_avg:40.76ms
step:1419/2330 train_time:57827ms step_avg:40.75ms
step:1420/2330 train_time:57873ms step_avg:40.76ms
step:1421/2330 train_time:57910ms step_avg:40.75ms
step:1422/2330 train_time:57955ms step_avg:40.76ms
step:1423/2330 train_time:57991ms step_avg:40.75ms
step:1424/2330 train_time:58037ms step_avg:40.76ms
step:1425/2330 train_time:58073ms step_avg:40.75ms
step:1426/2330 train_time:58118ms step_avg:40.76ms
step:1427/2330 train_time:58154ms step_avg:40.75ms
step:1428/2330 train_time:58200ms step_avg:40.76ms
step:1429/2330 train_time:58236ms step_avg:40.75ms
step:1430/2330 train_time:58280ms step_avg:40.76ms
step:1431/2330 train_time:58316ms step_avg:40.75ms
step:1432/2330 train_time:58361ms step_avg:40.76ms
step:1433/2330 train_time:58398ms step_avg:40.75ms
step:1434/2330 train_time:58444ms step_avg:40.76ms
step:1435/2330 train_time:58481ms step_avg:40.75ms
step:1436/2330 train_time:58527ms step_avg:40.76ms
step:1437/2330 train_time:58563ms step_avg:40.75ms
step:1438/2330 train_time:58607ms step_avg:40.76ms
step:1439/2330 train_time:58643ms step_avg:40.75ms
step:1440/2330 train_time:58688ms step_avg:40.76ms
step:1441/2330 train_time:58724ms step_avg:40.75ms
step:1442/2330 train_time:58770ms step_avg:40.76ms
step:1443/2330 train_time:58805ms step_avg:40.75ms
step:1444/2330 train_time:58851ms step_avg:40.76ms
step:1445/2330 train_time:58887ms step_avg:40.75ms
step:1446/2330 train_time:58933ms step_avg:40.76ms
step:1447/2330 train_time:58968ms step_avg:40.75ms
step:1448/2330 train_time:59014ms step_avg:40.76ms
step:1449/2330 train_time:59051ms step_avg:40.75ms
step:1450/2330 train_time:59097ms step_avg:40.76ms
step:1451/2330 train_time:59133ms step_avg:40.75ms
step:1452/2330 train_time:59179ms step_avg:40.76ms
step:1453/2330 train_time:59214ms step_avg:40.75ms
step:1454/2330 train_time:59260ms step_avg:40.76ms
step:1455/2330 train_time:59296ms step_avg:40.75ms
step:1456/2330 train_time:59341ms step_avg:40.76ms
step:1457/2330 train_time:59377ms step_avg:40.75ms
step:1458/2330 train_time:59423ms step_avg:40.76ms
step:1459/2330 train_time:59460ms step_avg:40.75ms
step:1460/2330 train_time:59506ms step_avg:40.76ms
step:1461/2330 train_time:59543ms step_avg:40.75ms
step:1462/2330 train_time:59589ms step_avg:40.76ms
step:1463/2330 train_time:59624ms step_avg:40.75ms
step:1464/2330 train_time:59669ms step_avg:40.76ms
step:1465/2330 train_time:59705ms step_avg:40.75ms
step:1466/2330 train_time:59750ms step_avg:40.76ms
step:1467/2330 train_time:59786ms step_avg:40.75ms
step:1468/2330 train_time:59831ms step_avg:40.76ms
step:1469/2330 train_time:59868ms step_avg:40.75ms
step:1470/2330 train_time:59913ms step_avg:40.76ms
step:1471/2330 train_time:59949ms step_avg:40.75ms
step:1472/2330 train_time:59994ms step_avg:40.76ms
step:1473/2330 train_time:60030ms step_avg:40.75ms
step:1474/2330 train_time:60076ms step_avg:40.76ms
step:1475/2330 train_time:60111ms step_avg:40.75ms
step:1476/2330 train_time:60157ms step_avg:40.76ms
step:1477/2330 train_time:60194ms step_avg:40.75ms
step:1478/2330 train_time:60239ms step_avg:40.76ms
step:1479/2330 train_time:60275ms step_avg:40.75ms
step:1480/2330 train_time:60320ms step_avg:40.76ms
step:1481/2330 train_time:60356ms step_avg:40.75ms
step:1482/2330 train_time:60402ms step_avg:40.76ms
step:1483/2330 train_time:60438ms step_avg:40.75ms
step:1484/2330 train_time:60484ms step_avg:40.76ms
step:1485/2330 train_time:60521ms step_avg:40.75ms
step:1486/2330 train_time:60566ms step_avg:40.76ms
step:1487/2330 train_time:60602ms step_avg:40.75ms
step:1488/2330 train_time:60647ms step_avg:40.76ms
step:1489/2330 train_time:60684ms step_avg:40.75ms
step:1490/2330 train_time:60729ms step_avg:40.76ms
step:1491/2330 train_time:60766ms step_avg:40.76ms
step:1492/2330 train_time:60811ms step_avg:40.76ms
step:1493/2330 train_time:60847ms step_avg:40.75ms
step:1494/2330 train_time:60893ms step_avg:40.76ms
step:1495/2330 train_time:60928ms step_avg:40.75ms
step:1496/2330 train_time:60974ms step_avg:40.76ms
step:1497/2330 train_time:61010ms step_avg:40.75ms
step:1498/2330 train_time:61055ms step_avg:40.76ms
step:1499/2330 train_time:61091ms step_avg:40.75ms
step:1500/2330 train_time:61137ms step_avg:40.76ms
step:1500/2330 val_loss:5.1601 train_time:61226ms step_avg:40.82ms
step:1501/2330 train_time:61240ms step_avg:40.80ms
step:1502/2330 train_time:61253ms step_avg:40.78ms
step:1503/2330 train_time:61264ms step_avg:40.76ms
step:1504/2330 train_time:61300ms step_avg:40.76ms
step:1505/2330 train_time:61335ms step_avg:40.75ms
step:1506/2330 train_time:61379ms step_avg:40.76ms
step:1507/2330 train_time:61414ms step_avg:40.75ms
step:1508/2330 train_time:61459ms step_avg:40.76ms
step:1509/2330 train_time:61494ms step_avg:40.75ms
step:1510/2330 train_time:61541ms step_avg:40.76ms
step:1511/2330 train_time:61582ms step_avg:40.76ms
step:1512/2330 train_time:61630ms step_avg:40.76ms
step:1513/2330 train_time:61666ms step_avg:40.76ms
step:1514/2330 train_time:61712ms step_avg:40.76ms
step:1515/2330 train_time:61748ms step_avg:40.76ms
step:1516/2330 train_time:61793ms step_avg:40.76ms
step:1517/2330 train_time:61830ms step_avg:40.76ms
step:1518/2330 train_time:61875ms step_avg:40.76ms
step:1519/2330 train_time:61909ms step_avg:40.76ms
step:1520/2330 train_time:62122ms step_avg:40.87ms
step:1521/2330 train_time:62134ms step_avg:40.85ms
step:1522/2330 train_time:62146ms step_avg:40.83ms
step:1523/2330 train_time:62169ms step_avg:40.82ms
step:1524/2330 train_time:62212ms step_avg:40.82ms
step:1525/2330 train_time:62247ms step_avg:40.82ms
step:1526/2330 train_time:62321ms step_avg:40.84ms
step:1527/2330 train_time:62440ms step_avg:40.89ms
step:1528/2330 train_time:62483ms step_avg:40.89ms
step:1529/2330 train_time:62519ms step_avg:40.89ms
step:1530/2330 train_time:62563ms step_avg:40.89ms
step:1531/2330 train_time:62597ms step_avg:40.89ms
step:1532/2330 train_time:62641ms step_avg:40.89ms
step:1533/2330 train_time:62676ms step_avg:40.88ms
step:1534/2330 train_time:62720ms step_avg:40.89ms
step:1535/2330 train_time:62755ms step_avg:40.88ms
step:1536/2330 train_time:62800ms step_avg:40.89ms
step:1537/2330 train_time:62835ms step_avg:40.88ms
step:1538/2330 train_time:62880ms step_avg:40.88ms
step:1539/2330 train_time:62915ms step_avg:40.88ms
step:1540/2330 train_time:62959ms step_avg:40.88ms
step:1541/2330 train_time:62994ms step_avg:40.88ms
step:1542/2330 train_time:63039ms step_avg:40.88ms
step:1543/2330 train_time:63074ms step_avg:40.88ms
step:1544/2330 train_time:63118ms step_avg:40.88ms
step:1545/2330 train_time:63153ms step_avg:40.88ms
step:1546/2330 train_time:63198ms step_avg:40.88ms
step:1547/2330 train_time:63233ms step_avg:40.87ms
step:1548/2330 train_time:63278ms step_avg:40.88ms
step:1549/2330 train_time:63318ms step_avg:40.88ms
step:1550/2330 train_time:63371ms step_avg:40.88ms
step:1551/2330 train_time:63410ms step_avg:40.88ms
step:1552/2330 train_time:63457ms step_avg:40.89ms
step:1553/2330 train_time:63494ms step_avg:40.88ms
step:1554/2330 train_time:63540ms step_avg:40.89ms
step:1555/2330 train_time:63576ms step_avg:40.88ms
step:1556/2330 train_time:63621ms step_avg:40.89ms
step:1557/2330 train_time:63656ms step_avg:40.88ms
step:1558/2330 train_time:63701ms step_avg:40.89ms
step:1559/2330 train_time:63736ms step_avg:40.88ms
step:1560/2330 train_time:63781ms step_avg:40.89ms
step:1561/2330 train_time:63816ms step_avg:40.88ms
step:1562/2330 train_time:63861ms step_avg:40.88ms
step:1563/2330 train_time:63895ms step_avg:40.88ms
step:1564/2330 train_time:63941ms step_avg:40.88ms
step:1565/2330 train_time:63977ms step_avg:40.88ms
step:1566/2330 train_time:64022ms step_avg:40.88ms
step:1567/2330 train_time:64056ms step_avg:40.88ms
step:1568/2330 train_time:64101ms step_avg:40.88ms
step:1569/2330 train_time:64136ms step_avg:40.88ms
step:1570/2330 train_time:64180ms step_avg:40.88ms
step:1571/2330 train_time:64216ms step_avg:40.88ms
step:1572/2330 train_time:64264ms step_avg:40.88ms
step:1573/2330 train_time:64303ms step_avg:40.88ms
step:1574/2330 train_time:64352ms step_avg:40.88ms
step:1575/2330 train_time:64389ms step_avg:40.88ms
step:1576/2330 train_time:64436ms step_avg:40.89ms
step:1577/2330 train_time:64472ms step_avg:40.88ms
step:1578/2330 train_time:64518ms step_avg:40.89ms
step:1579/2330 train_time:64555ms step_avg:40.88ms
step:1580/2330 train_time:64601ms step_avg:40.89ms
step:1581/2330 train_time:64636ms step_avg:40.88ms
step:1582/2330 train_time:64681ms step_avg:40.89ms
step:1583/2330 train_time:64717ms step_avg:40.88ms
step:1584/2330 train_time:64761ms step_avg:40.88ms
step:1585/2330 train_time:64797ms step_avg:40.88ms
step:1586/2330 train_time:64842ms step_avg:40.88ms
step:1587/2330 train_time:64877ms step_avg:40.88ms
step:1588/2330 train_time:64922ms step_avg:40.88ms
step:1589/2330 train_time:64958ms step_avg:40.88ms
step:1590/2330 train_time:65003ms step_avg:40.88ms
step:1591/2330 train_time:65038ms step_avg:40.88ms
step:1592/2330 train_time:65083ms step_avg:40.88ms
step:1593/2330 train_time:65118ms step_avg:40.88ms
step:1594/2330 train_time:65163ms step_avg:40.88ms
step:1595/2330 train_time:65198ms step_avg:40.88ms
step:1596/2330 train_time:65245ms step_avg:40.88ms
step:1597/2330 train_time:65283ms step_avg:40.88ms
step:1598/2330 train_time:65330ms step_avg:40.88ms
step:1599/2330 train_time:65365ms step_avg:40.88ms
step:1600/2330 train_time:65412ms step_avg:40.88ms
step:1601/2330 train_time:65450ms step_avg:40.88ms
step:1602/2330 train_time:65496ms step_avg:40.88ms
step:1603/2330 train_time:65532ms step_avg:40.88ms
step:1604/2330 train_time:65578ms step_avg:40.88ms
step:1605/2330 train_time:65613ms step_avg:40.88ms
step:1606/2330 train_time:65659ms step_avg:40.88ms
step:1607/2330 train_time:65694ms step_avg:40.88ms
step:1608/2330 train_time:65739ms step_avg:40.88ms
step:1609/2330 train_time:65774ms step_avg:40.88ms
step:1610/2330 train_time:65819ms step_avg:40.88ms
step:1611/2330 train_time:65856ms step_avg:40.88ms
step:1612/2330 train_time:65900ms step_avg:40.88ms
step:1613/2330 train_time:65935ms step_avg:40.88ms
step:1614/2330 train_time:65980ms step_avg:40.88ms
step:1615/2330 train_time:66016ms step_avg:40.88ms
step:1616/2330 train_time:66061ms step_avg:40.88ms
step:1617/2330 train_time:66096ms step_avg:40.88ms
step:1618/2330 train_time:66142ms step_avg:40.88ms
step:1619/2330 train_time:66178ms step_avg:40.88ms
step:1620/2330 train_time:66224ms step_avg:40.88ms
step:1621/2330 train_time:66262ms step_avg:40.88ms
step:1622/2330 train_time:66308ms step_avg:40.88ms
step:1623/2330 train_time:66345ms step_avg:40.88ms
step:1624/2330 train_time:66391ms step_avg:40.88ms
step:1625/2330 train_time:66427ms step_avg:40.88ms
step:1626/2330 train_time:66473ms step_avg:40.88ms
step:1627/2330 train_time:66509ms step_avg:40.88ms
step:1628/2330 train_time:66555ms step_avg:40.88ms
step:1629/2330 train_time:66591ms step_avg:40.88ms
step:1630/2330 train_time:66636ms step_avg:40.88ms
step:1631/2330 train_time:66671ms step_avg:40.88ms
step:1632/2330 train_time:66716ms step_avg:40.88ms
step:1633/2330 train_time:66753ms step_avg:40.88ms
step:1634/2330 train_time:66799ms step_avg:40.88ms
step:1635/2330 train_time:66834ms step_avg:40.88ms
step:1636/2330 train_time:66880ms step_avg:40.88ms
step:1637/2330 train_time:66915ms step_avg:40.88ms
step:1638/2330 train_time:66960ms step_avg:40.88ms
step:1639/2330 train_time:66995ms step_avg:40.88ms
step:1640/2330 train_time:67039ms step_avg:40.88ms
step:1641/2330 train_time:67075ms step_avg:40.87ms
step:1642/2330 train_time:67120ms step_avg:40.88ms
step:1643/2330 train_time:67155ms step_avg:40.87ms
step:1644/2330 train_time:67201ms step_avg:40.88ms
step:1645/2330 train_time:67238ms step_avg:40.87ms
step:1646/2330 train_time:67284ms step_avg:40.88ms
step:1647/2330 train_time:67321ms step_avg:40.87ms
step:1648/2330 train_time:67367ms step_avg:40.88ms
step:1649/2330 train_time:67403ms step_avg:40.87ms
step:1650/2330 train_time:67449ms step_avg:40.88ms
step:1651/2330 train_time:67485ms step_avg:40.88ms
step:1652/2330 train_time:67532ms step_avg:40.88ms
step:1653/2330 train_time:67567ms step_avg:40.88ms
step:1654/2330 train_time:67612ms step_avg:40.88ms
step:1655/2330 train_time:67648ms step_avg:40.87ms
step:1656/2330 train_time:67694ms step_avg:40.88ms
step:1657/2330 train_time:67729ms step_avg:40.87ms
step:1658/2330 train_time:67775ms step_avg:40.88ms
step:1659/2330 train_time:67810ms step_avg:40.87ms
step:1660/2330 train_time:67856ms step_avg:40.88ms
step:1661/2330 train_time:67892ms step_avg:40.87ms
step:1662/2330 train_time:67937ms step_avg:40.88ms
step:1663/2330 train_time:67972ms step_avg:40.87ms
step:1664/2330 train_time:68017ms step_avg:40.88ms
step:1665/2330 train_time:68053ms step_avg:40.87ms
step:1666/2330 train_time:68099ms step_avg:40.88ms
step:1667/2330 train_time:68135ms step_avg:40.87ms
step:1668/2330 train_time:68181ms step_avg:40.88ms
step:1669/2330 train_time:68216ms step_avg:40.87ms
step:1670/2330 train_time:68262ms step_avg:40.88ms
step:1671/2330 train_time:68297ms step_avg:40.87ms
step:1672/2330 train_time:68344ms step_avg:40.88ms
step:1673/2330 train_time:68381ms step_avg:40.87ms
step:1674/2330 train_time:68428ms step_avg:40.88ms
step:1675/2330 train_time:68463ms step_avg:40.87ms
step:1676/2330 train_time:68509ms step_avg:40.88ms
step:1677/2330 train_time:68544ms step_avg:40.87ms
step:1678/2330 train_time:68591ms step_avg:40.88ms
step:1679/2330 train_time:68627ms step_avg:40.87ms
step:1680/2330 train_time:68673ms step_avg:40.88ms
step:1681/2330 train_time:68708ms step_avg:40.87ms
step:1682/2330 train_time:68754ms step_avg:40.88ms
step:1683/2330 train_time:68790ms step_avg:40.87ms
step:1684/2330 train_time:68836ms step_avg:40.88ms
step:1685/2330 train_time:68872ms step_avg:40.87ms
step:1686/2330 train_time:68916ms step_avg:40.88ms
step:1687/2330 train_time:68952ms step_avg:40.87ms
step:1688/2330 train_time:68997ms step_avg:40.88ms
step:1689/2330 train_time:69033ms step_avg:40.87ms
step:1690/2330 train_time:69078ms step_avg:40.87ms
step:1691/2330 train_time:69113ms step_avg:40.87ms
step:1692/2330 train_time:69159ms step_avg:40.87ms
step:1693/2330 train_time:69195ms step_avg:40.87ms
step:1694/2330 train_time:69240ms step_avg:40.87ms
step:1695/2330 train_time:69277ms step_avg:40.87ms
step:1696/2330 train_time:69322ms step_avg:40.87ms
step:1697/2330 train_time:69357ms step_avg:40.87ms
step:1698/2330 train_time:69403ms step_avg:40.87ms
step:1699/2330 train_time:69440ms step_avg:40.87ms
step:1700/2330 train_time:69486ms step_avg:40.87ms
step:1701/2330 train_time:69523ms step_avg:40.87ms
step:1702/2330 train_time:69568ms step_avg:40.87ms
step:1703/2330 train_time:69604ms step_avg:40.87ms
step:1704/2330 train_time:69650ms step_avg:40.87ms
step:1705/2330 train_time:69686ms step_avg:40.87ms
step:1706/2330 train_time:69731ms step_avg:40.87ms
step:1707/2330 train_time:69767ms step_avg:40.87ms
step:1708/2330 train_time:69813ms step_avg:40.87ms
step:1709/2330 train_time:69848ms step_avg:40.87ms
step:1710/2330 train_time:69893ms step_avg:40.87ms
step:1711/2330 train_time:69930ms step_avg:40.87ms
step:1712/2330 train_time:69976ms step_avg:40.87ms
step:1713/2330 train_time:70011ms step_avg:40.87ms
step:1714/2330 train_time:70057ms step_avg:40.87ms
step:1715/2330 train_time:70093ms step_avg:40.87ms
step:1716/2330 train_time:70138ms step_avg:40.87ms
step:1717/2330 train_time:70173ms step_avg:40.87ms
step:1718/2330 train_time:70219ms step_avg:40.87ms
step:1719/2330 train_time:70255ms step_avg:40.87ms
step:1720/2330 train_time:70301ms step_avg:40.87ms
step:1721/2330 train_time:70337ms step_avg:40.87ms
step:1722/2330 train_time:70382ms step_avg:40.87ms
step:1723/2330 train_time:70418ms step_avg:40.87ms
step:1724/2330 train_time:70463ms step_avg:40.87ms
step:1725/2330 train_time:70500ms step_avg:40.87ms
step:1726/2330 train_time:70546ms step_avg:40.87ms
step:1727/2330 train_time:70582ms step_avg:40.87ms
step:1728/2330 train_time:70628ms step_avg:40.87ms
step:1729/2330 train_time:70664ms step_avg:40.87ms
step:1730/2330 train_time:70709ms step_avg:40.87ms
step:1731/2330 train_time:70745ms step_avg:40.87ms
step:1732/2330 train_time:70791ms step_avg:40.87ms
step:1733/2330 train_time:70827ms step_avg:40.87ms
step:1734/2330 train_time:70873ms step_avg:40.87ms
step:1735/2330 train_time:70909ms step_avg:40.87ms
step:1736/2330 train_time:70954ms step_avg:40.87ms
step:1737/2330 train_time:70990ms step_avg:40.87ms
step:1738/2330 train_time:71036ms step_avg:40.87ms
step:1739/2330 train_time:71071ms step_avg:40.87ms
step:1740/2330 train_time:71118ms step_avg:40.87ms
step:1741/2330 train_time:71153ms step_avg:40.87ms
step:1742/2330 train_time:71199ms step_avg:40.87ms
step:1743/2330 train_time:71235ms step_avg:40.87ms
step:1744/2330 train_time:71280ms step_avg:40.87ms
step:1745/2330 train_time:71316ms step_avg:40.87ms
step:1746/2330 train_time:71361ms step_avg:40.87ms
step:1747/2330 train_time:71397ms step_avg:40.87ms
step:1748/2330 train_time:71443ms step_avg:40.87ms
step:1749/2330 train_time:71479ms step_avg:40.87ms
step:1750/2330 train_time:71525ms step_avg:40.87ms
step:1750/2330 val_loss:5.1213 train_time:71614ms step_avg:40.92ms
step:1751/2330 train_time:71628ms step_avg:40.91ms
step:1752/2330 train_time:71640ms step_avg:40.89ms
step:1753/2330 train_time:71650ms step_avg:40.87ms
step:1754/2330 train_time:71688ms step_avg:40.87ms
step:1755/2330 train_time:71722ms step_avg:40.87ms
step:1756/2330 train_time:71767ms step_avg:40.87ms
step:1757/2330 train_time:71802ms step_avg:40.87ms
step:1758/2330 train_time:71846ms step_avg:40.87ms
step:1759/2330 train_time:71881ms step_avg:40.86ms
step:1760/2330 train_time:71925ms step_avg:40.87ms
step:1761/2330 train_time:71962ms step_avg:40.86ms
step:1762/2330 train_time:72011ms step_avg:40.87ms
step:1763/2330 train_time:72050ms step_avg:40.87ms
step:1764/2330 train_time:72097ms step_avg:40.87ms
step:1765/2330 train_time:72132ms step_avg:40.87ms
step:1766/2330 train_time:72177ms step_avg:40.87ms
step:1767/2330 train_time:72213ms step_avg:40.87ms
step:1768/2330 train_time:72257ms step_avg:40.87ms
step:1769/2330 train_time:72292ms step_avg:40.87ms
step:1770/2330 train_time:72336ms step_avg:40.87ms
step:1771/2330 train_time:72371ms step_avg:40.86ms
step:1772/2330 train_time:72415ms step_avg:40.87ms
step:1773/2330 train_time:72450ms step_avg:40.86ms
step:1774/2330 train_time:72497ms step_avg:40.87ms
step:1775/2330 train_time:72537ms step_avg:40.87ms
step:1776/2330 train_time:72587ms step_avg:40.87ms
step:1777/2330 train_time:72624ms step_avg:40.87ms
step:1778/2330 train_time:72669ms step_avg:40.87ms
step:1779/2330 train_time:72706ms step_avg:40.87ms
step:1780/2330 train_time:72751ms step_avg:40.87ms
step:1781/2330 train_time:72787ms step_avg:40.87ms
step:1782/2330 train_time:72833ms step_avg:40.87ms
step:1783/2330 train_time:72872ms step_avg:40.87ms
step:1784/2330 train_time:72917ms step_avg:40.87ms
step:1785/2330 train_time:72952ms step_avg:40.87ms
step:1786/2330 train_time:72998ms step_avg:40.87ms
step:1787/2330 train_time:73033ms step_avg:40.87ms
step:1788/2330 train_time:73077ms step_avg:40.87ms
step:1789/2330 train_time:73113ms step_avg:40.87ms
step:1790/2330 train_time:73157ms step_avg:40.87ms
step:1791/2330 train_time:73193ms step_avg:40.87ms
step:1792/2330 train_time:73237ms step_avg:40.87ms
step:1793/2330 train_time:73273ms step_avg:40.87ms
step:1794/2330 train_time:73317ms step_avg:40.87ms
step:1795/2330 train_time:73351ms step_avg:40.86ms
step:1796/2330 train_time:73396ms step_avg:40.87ms
step:1797/2330 train_time:73432ms step_avg:40.86ms
step:1798/2330 train_time:73479ms step_avg:40.87ms
step:1799/2330 train_time:73517ms step_avg:40.87ms
step:1800/2330 train_time:73564ms step_avg:40.87ms
step:1801/2330 train_time:73601ms step_avg:40.87ms
step:1802/2330 train_time:73647ms step_avg:40.87ms
step:1803/2330 train_time:73682ms step_avg:40.87ms
step:1804/2330 train_time:73728ms step_avg:40.87ms
step:1805/2330 train_time:73763ms step_avg:40.87ms
step:1806/2330 train_time:73809ms step_avg:40.87ms
step:1807/2330 train_time:73844ms step_avg:40.87ms
step:1808/2330 train_time:73890ms step_avg:40.87ms
step:1809/2330 train_time:73926ms step_avg:40.87ms
step:1810/2330 train_time:73972ms step_avg:40.87ms
step:1811/2330 train_time:74008ms step_avg:40.87ms
step:1812/2330 train_time:74054ms step_avg:40.87ms
step:1813/2330 train_time:74089ms step_avg:40.87ms
step:1814/2330 train_time:74134ms step_avg:40.87ms
step:1815/2330 train_time:74170ms step_avg:40.87ms
step:1816/2330 train_time:74215ms step_avg:40.87ms
step:1817/2330 train_time:74251ms step_avg:40.86ms
step:1818/2330 train_time:74296ms step_avg:40.87ms
step:1819/2330 train_time:74331ms step_avg:40.86ms
step:1820/2330 train_time:74376ms step_avg:40.87ms
step:1821/2330 train_time:74412ms step_avg:40.86ms
step:1822/2330 train_time:74457ms step_avg:40.87ms
step:1823/2330 train_time:74494ms step_avg:40.86ms
step:1824/2330 train_time:74541ms step_avg:40.87ms
step:1825/2330 train_time:74577ms step_avg:40.86ms
step:1826/2330 train_time:74623ms step_avg:40.87ms
step:1827/2330 train_time:74659ms step_avg:40.86ms
step:1828/2330 train_time:74705ms step_avg:40.87ms
step:1829/2330 train_time:74740ms step_avg:40.86ms
step:1830/2330 train_time:74786ms step_avg:40.87ms
step:1831/2330 train_time:74821ms step_avg:40.86ms
step:1832/2330 train_time:74866ms step_avg:40.87ms
step:1833/2330 train_time:74902ms step_avg:40.86ms
step:1834/2330 train_time:74947ms step_avg:40.87ms
step:1835/2330 train_time:74983ms step_avg:40.86ms
step:1836/2330 train_time:75029ms step_avg:40.87ms
step:1837/2330 train_time:75065ms step_avg:40.86ms
step:1838/2330 train_time:75110ms step_avg:40.87ms
step:1839/2330 train_time:75146ms step_avg:40.86ms
step:1840/2330 train_time:75191ms step_avg:40.86ms
step:1841/2330 train_time:75226ms step_avg:40.86ms
step:1842/2330 train_time:75272ms step_avg:40.86ms
step:1843/2330 train_time:75307ms step_avg:40.86ms
step:1844/2330 train_time:75352ms step_avg:40.86ms
step:1845/2330 train_time:75389ms step_avg:40.86ms
step:1846/2330 train_time:75434ms step_avg:40.86ms
step:1847/2330 train_time:75472ms step_avg:40.86ms
step:1848/2330 train_time:75518ms step_avg:40.86ms
step:1849/2330 train_time:75555ms step_avg:40.86ms
step:1850/2330 train_time:75601ms step_avg:40.87ms
step:1851/2330 train_time:75636ms step_avg:40.86ms
step:1852/2330 train_time:75682ms step_avg:40.87ms
step:1853/2330 train_time:75718ms step_avg:40.86ms
step:1854/2330 train_time:75763ms step_avg:40.86ms
step:1855/2330 train_time:75800ms step_avg:40.86ms
step:1856/2330 train_time:75845ms step_avg:40.86ms
step:1857/2330 train_time:75880ms step_avg:40.86ms
step:1858/2330 train_time:75925ms step_avg:40.86ms
step:1859/2330 train_time:75961ms step_avg:40.86ms
step:1860/2330 train_time:76007ms step_avg:40.86ms
step:1861/2330 train_time:76044ms step_avg:40.86ms
step:1862/2330 train_time:76089ms step_avg:40.86ms
step:1863/2330 train_time:76125ms step_avg:40.86ms
step:1864/2330 train_time:76171ms step_avg:40.86ms
step:1865/2330 train_time:76206ms step_avg:40.86ms
step:1866/2330 train_time:76252ms step_avg:40.86ms
step:1867/2330 train_time:76288ms step_avg:40.86ms
step:1868/2330 train_time:76332ms step_avg:40.86ms
step:1869/2330 train_time:76369ms step_avg:40.86ms
step:1870/2330 train_time:76415ms step_avg:40.86ms
step:1871/2330 train_time:76452ms step_avg:40.86ms
step:1872/2330 train_time:76497ms step_avg:40.86ms
step:1873/2330 train_time:76532ms step_avg:40.86ms
step:1874/2330 train_time:76579ms step_avg:40.86ms
step:1875/2330 train_time:76615ms step_avg:40.86ms
step:1876/2330 train_time:76661ms step_avg:40.86ms
step:1877/2330 train_time:76697ms step_avg:40.86ms
step:1878/2330 train_time:76743ms step_avg:40.86ms
step:1879/2330 train_time:76778ms step_avg:40.86ms
step:1880/2330 train_time:76824ms step_avg:40.86ms
step:1881/2330 train_time:76860ms step_avg:40.86ms
step:1882/2330 train_time:76905ms step_avg:40.86ms
step:1883/2330 train_time:76941ms step_avg:40.86ms
step:1884/2330 train_time:76987ms step_avg:40.86ms
step:1885/2330 train_time:77022ms step_avg:40.86ms
step:1886/2330 train_time:77067ms step_avg:40.86ms
step:1887/2330 train_time:77102ms step_avg:40.86ms
step:1888/2330 train_time:77148ms step_avg:40.86ms
step:1889/2330 train_time:77184ms step_avg:40.86ms
step:1890/2330 train_time:77229ms step_avg:40.86ms
step:1891/2330 train_time:77266ms step_avg:40.86ms
step:1892/2330 train_time:77311ms step_avg:40.86ms
step:1893/2330 train_time:77347ms step_avg:40.86ms
step:1894/2330 train_time:77393ms step_avg:40.86ms
step:1895/2330 train_time:77428ms step_avg:40.86ms
step:1896/2330 train_time:77474ms step_avg:40.86ms
step:1897/2330 train_time:77512ms step_avg:40.86ms
step:1898/2330 train_time:77556ms step_avg:40.86ms
step:1899/2330 train_time:77593ms step_avg:40.86ms
step:1900/2330 train_time:77638ms step_avg:40.86ms
step:1901/2330 train_time:77674ms step_avg:40.86ms
step:1902/2330 train_time:77719ms step_avg:40.86ms
step:1903/2330 train_time:77755ms step_avg:40.86ms
step:1904/2330 train_time:77801ms step_avg:40.86ms
step:1905/2330 train_time:77837ms step_avg:40.86ms
step:1906/2330 train_time:77882ms step_avg:40.86ms
step:1907/2330 train_time:77920ms step_avg:40.86ms
step:1908/2330 train_time:77966ms step_avg:40.86ms
step:1909/2330 train_time:78001ms step_avg:40.86ms
step:1910/2330 train_time:78046ms step_avg:40.86ms
step:1911/2330 train_time:78081ms step_avg:40.86ms
step:1912/2330 train_time:78127ms step_avg:40.86ms
step:1913/2330 train_time:78162ms step_avg:40.86ms
step:1914/2330 train_time:78208ms step_avg:40.86ms
step:1915/2330 train_time:78244ms step_avg:40.86ms
step:1916/2330 train_time:78289ms step_avg:40.86ms
step:1917/2330 train_time:78325ms step_avg:40.86ms
step:1918/2330 train_time:78370ms step_avg:40.86ms
step:1919/2330 train_time:78406ms step_avg:40.86ms
step:1920/2330 train_time:78451ms step_avg:40.86ms
step:1921/2330 train_time:78488ms step_avg:40.86ms
step:1922/2330 train_time:78533ms step_avg:40.86ms
step:1923/2330 train_time:78570ms step_avg:40.86ms
step:1924/2330 train_time:78616ms step_avg:40.86ms
step:1925/2330 train_time:78651ms step_avg:40.86ms
step:1926/2330 train_time:78697ms step_avg:40.86ms
step:1927/2330 train_time:78733ms step_avg:40.86ms
step:1928/2330 train_time:78779ms step_avg:40.86ms
step:1929/2330 train_time:78815ms step_avg:40.86ms
step:1930/2330 train_time:78861ms step_avg:40.86ms
step:1931/2330 train_time:78897ms step_avg:40.86ms
step:1932/2330 train_time:78942ms step_avg:40.86ms
step:1933/2330 train_time:78979ms step_avg:40.86ms
step:1934/2330 train_time:79024ms step_avg:40.86ms
step:1935/2330 train_time:79059ms step_avg:40.86ms
step:1936/2330 train_time:79104ms step_avg:40.86ms
step:1937/2330 train_time:79140ms step_avg:40.86ms
step:1938/2330 train_time:79185ms step_avg:40.86ms
step:1939/2330 train_time:79221ms step_avg:40.86ms
step:1940/2330 train_time:79267ms step_avg:40.86ms
step:1941/2330 train_time:79302ms step_avg:40.86ms
step:1942/2330 train_time:79348ms step_avg:40.86ms
step:1943/2330 train_time:79384ms step_avg:40.86ms
step:1944/2330 train_time:79429ms step_avg:40.86ms
step:1945/2330 train_time:79465ms step_avg:40.86ms
step:1946/2330 train_time:79511ms step_avg:40.86ms
step:1947/2330 train_time:79546ms step_avg:40.86ms
step:1948/2330 train_time:79592ms step_avg:40.86ms
step:1949/2330 train_time:79628ms step_avg:40.86ms
step:1950/2330 train_time:79674ms step_avg:40.86ms
step:1951/2330 train_time:79711ms step_avg:40.86ms
step:1952/2330 train_time:79756ms step_avg:40.86ms
step:1953/2330 train_time:79792ms step_avg:40.86ms
step:1954/2330 train_time:79838ms step_avg:40.86ms
step:1955/2330 train_time:79875ms step_avg:40.86ms
step:1956/2330 train_time:79921ms step_avg:40.86ms
step:1957/2330 train_time:79957ms step_avg:40.86ms
step:1958/2330 train_time:80002ms step_avg:40.86ms
step:1959/2330 train_time:80037ms step_avg:40.86ms
step:1960/2330 train_time:80084ms step_avg:40.86ms
step:1961/2330 train_time:80120ms step_avg:40.86ms
step:1962/2330 train_time:80166ms step_avg:40.86ms
step:1963/2330 train_time:80201ms step_avg:40.86ms
step:1964/2330 train_time:80247ms step_avg:40.86ms
step:1965/2330 train_time:80282ms step_avg:40.86ms
step:1966/2330 train_time:80327ms step_avg:40.86ms
step:1967/2330 train_time:80363ms step_avg:40.86ms
step:1968/2330 train_time:80408ms step_avg:40.86ms
step:1969/2330 train_time:80444ms step_avg:40.86ms
step:1970/2330 train_time:80490ms step_avg:40.86ms
step:1971/2330 train_time:80526ms step_avg:40.86ms
step:1972/2330 train_time:80571ms step_avg:40.86ms
step:1973/2330 train_time:80607ms step_avg:40.86ms
step:1974/2330 train_time:80653ms step_avg:40.86ms
step:1975/2330 train_time:80688ms step_avg:40.85ms
step:1976/2330 train_time:80734ms step_avg:40.86ms
step:1977/2330 train_time:80770ms step_avg:40.86ms
step:1978/2330 train_time:80817ms step_avg:40.86ms
step:1979/2330 train_time:80852ms step_avg:40.86ms
step:1980/2330 train_time:80898ms step_avg:40.86ms
step:1981/2330 train_time:80934ms step_avg:40.85ms
step:1982/2330 train_time:80980ms step_avg:40.86ms
step:1983/2330 train_time:81015ms step_avg:40.85ms
step:1984/2330 train_time:81061ms step_avg:40.86ms
step:1985/2330 train_time:81096ms step_avg:40.85ms
step:1986/2330 train_time:81143ms step_avg:40.86ms
step:1987/2330 train_time:81178ms step_avg:40.85ms
step:1988/2330 train_time:81224ms step_avg:40.86ms
step:1989/2330 train_time:81259ms step_avg:40.85ms
step:1990/2330 train_time:81306ms step_avg:40.86ms
step:1991/2330 train_time:81342ms step_avg:40.85ms
step:1992/2330 train_time:81387ms step_avg:40.86ms
step:1993/2330 train_time:81423ms step_avg:40.85ms
step:1994/2330 train_time:81468ms step_avg:40.86ms
step:1995/2330 train_time:81505ms step_avg:40.85ms
step:1996/2330 train_time:81550ms step_avg:40.86ms
step:1997/2330 train_time:81586ms step_avg:40.85ms
step:1998/2330 train_time:81631ms step_avg:40.86ms
step:1999/2330 train_time:81667ms step_avg:40.85ms
step:2000/2330 train_time:81713ms step_avg:40.86ms
step:2000/2330 val_loss:5.0916 train_time:81803ms step_avg:40.90ms
step:2001/2330 train_time:81816ms step_avg:40.89ms
step:2002/2330 train_time:81829ms step_avg:40.87ms
step:2003/2330 train_time:81840ms step_avg:40.86ms
step:2004/2330 train_time:81877ms step_avg:40.86ms
step:2005/2330 train_time:81912ms step_avg:40.85ms
step:2006/2330 train_time:81956ms step_avg:40.86ms
step:2007/2330 train_time:81991ms step_avg:40.85ms
step:2008/2330 train_time:82036ms step_avg:40.85ms
step:2009/2330 train_time:82070ms step_avg:40.85ms
step:2010/2330 train_time:82119ms step_avg:40.86ms
step:2011/2330 train_time:82158ms step_avg:40.85ms
step:2012/2330 train_time:82205ms step_avg:40.86ms
step:2013/2330 train_time:82241ms step_avg:40.86ms
step:2014/2330 train_time:82287ms step_avg:40.86ms
step:2015/2330 train_time:82322ms step_avg:40.85ms
step:2016/2330 train_time:82367ms step_avg:40.86ms
step:2017/2330 train_time:82403ms step_avg:40.85ms
step:2018/2330 train_time:82448ms step_avg:40.86ms
step:2019/2330 train_time:82483ms step_avg:40.85ms
step:2020/2330 train_time:82528ms step_avg:40.86ms
step:2021/2330 train_time:82563ms step_avg:40.85ms
step:2022/2330 train_time:82608ms step_avg:40.85ms
step:2023/2330 train_time:82643ms step_avg:40.85ms
step:2024/2330 train_time:82688ms step_avg:40.85ms
step:2025/2330 train_time:82725ms step_avg:40.85ms
step:2026/2330 train_time:82772ms step_avg:40.85ms
step:2027/2330 train_time:82808ms step_avg:40.85ms
step:2028/2330 train_time:82854ms step_avg:40.85ms
step:2029/2330 train_time:82889ms step_avg:40.85ms
step:2030/2330 train_time:82934ms step_avg:40.85ms
step:2031/2330 train_time:82970ms step_avg:40.85ms
step:2032/2330 train_time:83015ms step_avg:40.85ms
step:2033/2330 train_time:83052ms step_avg:40.85ms
step:2034/2330 train_time:83099ms step_avg:40.85ms
step:2035/2330 train_time:83137ms step_avg:40.85ms
step:2036/2330 train_time:83184ms step_avg:40.86ms
step:2037/2330 train_time:83220ms step_avg:40.85ms
step:2038/2330 train_time:83266ms step_avg:40.86ms
step:2039/2330 train_time:83301ms step_avg:40.85ms
step:2040/2330 train_time:83346ms step_avg:40.86ms
step:2041/2330 train_time:83381ms step_avg:40.85ms
step:2042/2330 train_time:83426ms step_avg:40.86ms
step:2043/2330 train_time:83462ms step_avg:40.85ms
step:2044/2330 train_time:83508ms step_avg:40.86ms
step:2045/2330 train_time:83543ms step_avg:40.85ms
step:2046/2330 train_time:83588ms step_avg:40.85ms
step:2047/2330 train_time:83622ms step_avg:40.85ms
step:2048/2330 train_time:83668ms step_avg:40.85ms
step:2049/2330 train_time:83703ms step_avg:40.85ms
step:2050/2330 train_time:83749ms step_avg:40.85ms
step:2051/2330 train_time:83785ms step_avg:40.85ms
step:2052/2330 train_time:83831ms step_avg:40.85ms
step:2053/2330 train_time:83866ms step_avg:40.85ms
step:2054/2330 train_time:83911ms step_avg:40.85ms
step:2055/2330 train_time:83947ms step_avg:40.85ms
step:2056/2330 train_time:83993ms step_avg:40.85ms
step:2057/2330 train_time:84030ms step_avg:40.85ms
step:2058/2330 train_time:84076ms step_avg:40.85ms
step:2059/2330 train_time:84113ms step_avg:40.85ms
step:2060/2330 train_time:84159ms step_avg:40.85ms
step:2061/2330 train_time:84195ms step_avg:40.85ms
step:2062/2330 train_time:84242ms step_avg:40.85ms
step:2063/2330 train_time:84278ms step_avg:40.85ms
step:2064/2330 train_time:84323ms step_avg:40.85ms
step:2065/2330 train_time:84359ms step_avg:40.85ms
step:2066/2330 train_time:84405ms step_avg:40.85ms
step:2067/2330 train_time:84441ms step_avg:40.85ms
step:2068/2330 train_time:84486ms step_avg:40.85ms
step:2069/2330 train_time:84521ms step_avg:40.85ms
step:2070/2330 train_time:84566ms step_avg:40.85ms
step:2071/2330 train_time:84601ms step_avg:40.85ms
step:2072/2330 train_time:84647ms step_avg:40.85ms
step:2073/2330 train_time:84684ms step_avg:40.85ms
step:2074/2330 train_time:84729ms step_avg:40.85ms
step:2075/2330 train_time:84765ms step_avg:40.85ms
step:2076/2330 train_time:84810ms step_avg:40.85ms
step:2077/2330 train_time:84845ms step_avg:40.85ms
step:2078/2330 train_time:84890ms step_avg:40.85ms
step:2079/2330 train_time:84927ms step_avg:40.85ms
step:2080/2330 train_time:84973ms step_avg:40.85ms
step:2081/2330 train_time:85009ms step_avg:40.85ms
step:2082/2330 train_time:85055ms step_avg:40.85ms
step:2083/2330 train_time:85091ms step_avg:40.85ms
step:2084/2330 train_time:85137ms step_avg:40.85ms
step:2085/2330 train_time:85174ms step_avg:40.85ms
step:2086/2330 train_time:85221ms step_avg:40.85ms
step:2087/2330 train_time:85257ms step_avg:40.85ms
step:2088/2330 train_time:85302ms step_avg:40.85ms
step:2089/2330 train_time:85338ms step_avg:40.85ms
step:2090/2330 train_time:85383ms step_avg:40.85ms
step:2091/2330 train_time:85418ms step_avg:40.85ms
step:2092/2330 train_time:85463ms step_avg:40.85ms
step:2093/2330 train_time:85498ms step_avg:40.85ms
step:2094/2330 train_time:85545ms step_avg:40.85ms
step:2095/2330 train_time:85580ms step_avg:40.85ms
step:2096/2330 train_time:85626ms step_avg:40.85ms
step:2097/2330 train_time:85661ms step_avg:40.85ms
step:2098/2330 train_time:85707ms step_avg:40.85ms
step:2099/2330 train_time:85743ms step_avg:40.85ms
step:2100/2330 train_time:85788ms step_avg:40.85ms
step:2101/2330 train_time:85823ms step_avg:40.85ms
step:2102/2330 train_time:85868ms step_avg:40.85ms
step:2103/2330 train_time:85904ms step_avg:40.85ms
step:2104/2330 train_time:85949ms step_avg:40.85ms
step:2105/2330 train_time:85985ms step_avg:40.85ms
step:2106/2330 train_time:86030ms step_avg:40.85ms
step:2107/2330 train_time:86066ms step_avg:40.85ms
step:2108/2330 train_time:86112ms step_avg:40.85ms
step:2109/2330 train_time:86148ms step_avg:40.85ms
step:2110/2330 train_time:86194ms step_avg:40.85ms
step:2111/2330 train_time:86232ms step_avg:40.85ms
step:2112/2330 train_time:86279ms step_avg:40.85ms
step:2113/2330 train_time:86315ms step_avg:40.85ms
step:2114/2330 train_time:86360ms step_avg:40.85ms
step:2115/2330 train_time:86396ms step_avg:40.85ms
step:2116/2330 train_time:86441ms step_avg:40.85ms
step:2117/2330 train_time:86477ms step_avg:40.85ms
step:2118/2330 train_time:86522ms step_avg:40.85ms
step:2119/2330 train_time:86558ms step_avg:40.85ms
step:2120/2330 train_time:86603ms step_avg:40.85ms
step:2121/2330 train_time:86639ms step_avg:40.85ms
step:2122/2330 train_time:86684ms step_avg:40.85ms
step:2123/2330 train_time:86720ms step_avg:40.85ms
step:2124/2330 train_time:86765ms step_avg:40.85ms
step:2125/2330 train_time:86801ms step_avg:40.85ms
step:2126/2330 train_time:86846ms step_avg:40.85ms
step:2127/2330 train_time:86883ms step_avg:40.85ms
step:2128/2330 train_time:86928ms step_avg:40.85ms
step:2129/2330 train_time:86965ms step_avg:40.85ms
step:2130/2330 train_time:87010ms step_avg:40.85ms
step:2131/2330 train_time:87046ms step_avg:40.85ms
step:2132/2330 train_time:87091ms step_avg:40.85ms
step:2133/2330 train_time:87127ms step_avg:40.85ms
step:2134/2330 train_time:87174ms step_avg:40.85ms
step:2135/2330 train_time:87210ms step_avg:40.85ms
step:2136/2330 train_time:87256ms step_avg:40.85ms
step:2137/2330 train_time:87292ms step_avg:40.85ms
step:2138/2330 train_time:87338ms step_avg:40.85ms
step:2139/2330 train_time:87373ms step_avg:40.85ms
step:2140/2330 train_time:87420ms step_avg:40.85ms
step:2141/2330 train_time:87455ms step_avg:40.85ms
step:2142/2330 train_time:87501ms step_avg:40.85ms
step:2143/2330 train_time:87536ms step_avg:40.85ms
step:2144/2330 train_time:87582ms step_avg:40.85ms
step:2145/2330 train_time:87617ms step_avg:40.85ms
step:2146/2330 train_time:87662ms step_avg:40.85ms
step:2147/2330 train_time:87698ms step_avg:40.85ms
step:2148/2330 train_time:87744ms step_avg:40.85ms
step:2149/2330 train_time:87779ms step_avg:40.85ms
step:2150/2330 train_time:87824ms step_avg:40.85ms
step:2151/2330 train_time:87860ms step_avg:40.85ms
step:2152/2330 train_time:87907ms step_avg:40.85ms
step:2153/2330 train_time:87943ms step_avg:40.85ms
step:2154/2330 train_time:87988ms step_avg:40.85ms
step:2155/2330 train_time:88024ms step_avg:40.85ms
step:2156/2330 train_time:88070ms step_avg:40.85ms
step:2157/2330 train_time:88107ms step_avg:40.85ms
step:2158/2330 train_time:88152ms step_avg:40.85ms
step:2159/2330 train_time:88189ms step_avg:40.85ms
step:2160/2330 train_time:88234ms step_avg:40.85ms
step:2161/2330 train_time:88270ms step_avg:40.85ms
step:2162/2330 train_time:88316ms step_avg:40.85ms
step:2163/2330 train_time:88353ms step_avg:40.85ms
step:2164/2330 train_time:88398ms step_avg:40.85ms
step:2165/2330 train_time:88433ms step_avg:40.85ms
step:2166/2330 train_time:88479ms step_avg:40.85ms
step:2167/2330 train_time:88516ms step_avg:40.85ms
step:2168/2330 train_time:88561ms step_avg:40.85ms
step:2169/2330 train_time:88597ms step_avg:40.85ms
step:2170/2330 train_time:88643ms step_avg:40.85ms
step:2171/2330 train_time:88678ms step_avg:40.85ms
step:2172/2330 train_time:88723ms step_avg:40.85ms
step:2173/2330 train_time:88758ms step_avg:40.85ms
step:2174/2330 train_time:88804ms step_avg:40.85ms
step:2175/2330 train_time:88840ms step_avg:40.85ms
step:2176/2330 train_time:88886ms step_avg:40.85ms
step:2177/2330 train_time:88921ms step_avg:40.85ms
step:2178/2330 train_time:88966ms step_avg:40.85ms
step:2179/2330 train_time:89003ms step_avg:40.85ms
step:2180/2330 train_time:89049ms step_avg:40.85ms
step:2181/2330 train_time:89085ms step_avg:40.85ms
step:2182/2330 train_time:89130ms step_avg:40.85ms
step:2183/2330 train_time:89166ms step_avg:40.85ms
step:2184/2330 train_time:89212ms step_avg:40.85ms
step:2185/2330 train_time:89248ms step_avg:40.85ms
step:2186/2330 train_time:89293ms step_avg:40.85ms
step:2187/2330 train_time:89329ms step_avg:40.85ms
step:2188/2330 train_time:89375ms step_avg:40.85ms
step:2189/2330 train_time:89412ms step_avg:40.85ms
step:2190/2330 train_time:89456ms step_avg:40.85ms
step:2191/2330 train_time:89493ms step_avg:40.85ms
step:2192/2330 train_time:89538ms step_avg:40.85ms
step:2193/2330 train_time:89575ms step_avg:40.85ms
step:2194/2330 train_time:89620ms step_avg:40.85ms
step:2195/2330 train_time:89656ms step_avg:40.85ms
step:2196/2330 train_time:89701ms step_avg:40.85ms
step:2197/2330 train_time:89737ms step_avg:40.85ms
step:2198/2330 train_time:89783ms step_avg:40.85ms
step:2199/2330 train_time:89819ms step_avg:40.85ms
step:2200/2330 train_time:89865ms step_avg:40.85ms
step:2201/2330 train_time:89900ms step_avg:40.85ms
step:2202/2330 train_time:89946ms step_avg:40.85ms
step:2203/2330 train_time:89982ms step_avg:40.85ms
step:2204/2330 train_time:90028ms step_avg:40.85ms
step:2205/2330 train_time:90064ms step_avg:40.85ms
step:2206/2330 train_time:90109ms step_avg:40.85ms
step:2207/2330 train_time:90145ms step_avg:40.84ms
step:2208/2330 train_time:90190ms step_avg:40.85ms
step:2209/2330 train_time:90225ms step_avg:40.84ms
step:2210/2330 train_time:90270ms step_avg:40.85ms
step:2211/2330 train_time:90306ms step_avg:40.84ms
step:2212/2330 train_time:90352ms step_avg:40.85ms
step:2213/2330 train_time:90389ms step_avg:40.84ms
step:2214/2330 train_time:90435ms step_avg:40.85ms
step:2215/2330 train_time:90472ms step_avg:40.84ms
step:2216/2330 train_time:90517ms step_avg:40.85ms
step:2217/2330 train_time:90553ms step_avg:40.84ms
step:2218/2330 train_time:90598ms step_avg:40.85ms
step:2219/2330 train_time:90633ms step_avg:40.84ms
step:2220/2330 train_time:90679ms step_avg:40.85ms
step:2221/2330 train_time:90715ms step_avg:40.84ms
step:2222/2330 train_time:90761ms step_avg:40.85ms
step:2223/2330 train_time:90796ms step_avg:40.84ms
step:2224/2330 train_time:90843ms step_avg:40.85ms
step:2225/2330 train_time:90878ms step_avg:40.84ms
step:2226/2330 train_time:90923ms step_avg:40.85ms
step:2227/2330 train_time:90959ms step_avg:40.84ms
step:2228/2330 train_time:91006ms step_avg:40.85ms
step:2229/2330 train_time:91042ms step_avg:40.84ms
step:2230/2330 train_time:91087ms step_avg:40.85ms
step:2231/2330 train_time:91123ms step_avg:40.84ms
step:2232/2330 train_time:91168ms step_avg:40.85ms
step:2233/2330 train_time:91204ms step_avg:40.84ms
step:2234/2330 train_time:91249ms step_avg:40.85ms
step:2235/2330 train_time:91286ms step_avg:40.84ms
step:2236/2330 train_time:91331ms step_avg:40.85ms
step:2237/2330 train_time:91367ms step_avg:40.84ms
step:2238/2330 train_time:91413ms step_avg:40.85ms
step:2239/2330 train_time:91450ms step_avg:40.84ms
step:2240/2330 train_time:91496ms step_avg:40.85ms
step:2241/2330 train_time:91532ms step_avg:40.84ms
step:2242/2330 train_time:91576ms step_avg:40.85ms
step:2243/2330 train_time:91613ms step_avg:40.84ms
step:2244/2330 train_time:91657ms step_avg:40.85ms
step:2245/2330 train_time:91694ms step_avg:40.84ms
step:2246/2330 train_time:91740ms step_avg:40.85ms
step:2247/2330 train_time:91776ms step_avg:40.84ms
step:2248/2330 train_time:91822ms step_avg:40.85ms
step:2249/2330 train_time:91858ms step_avg:40.84ms
step:2250/2330 train_time:91903ms step_avg:40.85ms
step:2250/2330 val_loss:5.0655 train_time:91991ms step_avg:40.88ms
step:2251/2330 train_time:92005ms step_avg:40.87ms
step:2252/2330 train_time:92017ms step_avg:40.86ms
step:2253/2330 train_time:92027ms step_avg:40.85ms
step:2254/2330 train_time:92066ms step_avg:40.85ms
step:2255/2330 train_time:92100ms step_avg:40.84ms
step:2256/2330 train_time:92145ms step_avg:40.84ms
step:2257/2330 train_time:92180ms step_avg:40.84ms
step:2258/2330 train_time:92225ms step_avg:40.84ms
step:2259/2330 train_time:92260ms step_avg:40.84ms
step:2260/2330 train_time:92308ms step_avg:40.84ms
step:2261/2330 train_time:92347ms step_avg:40.84ms
step:2262/2330 train_time:92395ms step_avg:40.85ms
step:2263/2330 train_time:92432ms step_avg:40.85ms
step:2264/2330 train_time:92479ms step_avg:40.85ms
step:2265/2330 train_time:92514ms step_avg:40.84ms
step:2266/2330 train_time:92559ms step_avg:40.85ms
step:2267/2330 train_time:92595ms step_avg:40.84ms
step:2268/2330 train_time:92640ms step_avg:40.85ms
step:2269/2330 train_time:92674ms step_avg:40.84ms
step:2270/2330 train_time:92719ms step_avg:40.85ms
step:2271/2330 train_time:92754ms step_avg:40.84ms
step:2272/2330 train_time:92799ms step_avg:40.84ms
step:2273/2330 train_time:92834ms step_avg:40.84ms
step:2274/2330 train_time:92879ms step_avg:40.84ms
step:2275/2330 train_time:92915ms step_avg:40.84ms
step:2276/2330 train_time:92962ms step_avg:40.84ms
step:2277/2330 train_time:92997ms step_avg:40.84ms
step:2278/2330 train_time:93042ms step_avg:40.84ms
step:2279/2330 train_time:93077ms step_avg:40.84ms
step:2280/2330 train_time:93121ms step_avg:40.84ms
step:2281/2330 train_time:93156ms step_avg:40.84ms
step:2282/2330 train_time:93201ms step_avg:40.84ms
step:2283/2330 train_time:93239ms step_avg:40.84ms
step:2284/2330 train_time:93285ms step_avg:40.84ms
step:2285/2330 train_time:93322ms step_avg:40.84ms
step:2286/2330 train_time:93368ms step_avg:40.84ms
step:2287/2330 train_time:93405ms step_avg:40.84ms
step:2288/2330 train_time:93451ms step_avg:40.84ms
step:2289/2330 train_time:93487ms step_avg:40.84ms
step:2290/2330 train_time:93533ms step_avg:40.84ms
step:2291/2330 train_time:93570ms step_avg:40.84ms
step:2292/2330 train_time:93616ms step_avg:40.84ms
step:2293/2330 train_time:93651ms step_avg:40.84ms
step:2294/2330 train_time:93697ms step_avg:40.84ms
step:2295/2330 train_time:93733ms step_avg:40.84ms
step:2296/2330 train_time:93777ms step_avg:40.84ms
step:2297/2330 train_time:93813ms step_avg:40.84ms
step:2298/2330 train_time:93858ms step_avg:40.84ms
step:2299/2330 train_time:93893ms step_avg:40.84ms
step:2300/2330 train_time:93940ms step_avg:40.84ms
step:2301/2330 train_time:93975ms step_avg:40.84ms
step:2302/2330 train_time:94020ms step_avg:40.84ms
step:2303/2330 train_time:94055ms step_avg:40.84ms
step:2304/2330 train_time:94100ms step_avg:40.84ms
step:2305/2330 train_time:94135ms step_avg:40.84ms
step:2306/2330 train_time:94181ms step_avg:40.84ms
step:2307/2330 train_time:94218ms step_avg:40.84ms
step:2308/2330 train_time:94264ms step_avg:40.84ms
step:2309/2330 train_time:94301ms step_avg:40.84ms
step:2310/2330 train_time:94347ms step_avg:40.84ms
step:2311/2330 train_time:94383ms step_avg:40.84ms
step:2312/2330 train_time:94428ms step_avg:40.84ms
step:2313/2330 train_time:94464ms step_avg:40.84ms
step:2314/2330 train_time:94509ms step_avg:40.84ms
step:2315/2330 train_time:94546ms step_avg:40.84ms
step:2316/2330 train_time:94591ms step_avg:40.84ms
step:2317/2330 train_time:94627ms step_avg:40.84ms
step:2318/2330 train_time:94672ms step_avg:40.84ms
step:2319/2330 train_time:94708ms step_avg:40.84ms
step:2320/2330 train_time:94753ms step_avg:40.84ms
step:2321/2330 train_time:94789ms step_avg:40.84ms
step:2322/2330 train_time:94835ms step_avg:40.84ms
step:2323/2330 train_time:94871ms step_avg:40.84ms
step:2324/2330 train_time:94917ms step_avg:40.84ms
step:2325/2330 train_time:94952ms step_avg:40.84ms
step:2326/2330 train_time:94998ms step_avg:40.84ms
step:2327/2330 train_time:95034ms step_avg:40.84ms
step:2328/2330 train_time:95079ms step_avg:40.84ms
step:2329/2330 train_time:95114ms step_avg:40.84ms
step:2330/2330 train_time:95159ms step_avg:40.84ms
step:2330/2330 val_loss:5.0589 train_time:95250ms step_avg:40.88ms
peak memory allocated: 29328 MiB reserved: 38888 MiB
