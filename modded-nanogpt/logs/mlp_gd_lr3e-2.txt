import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = divide_by_norm(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = divide_by_norm(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        # self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "mlp_gd_lr3e-2"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
# embed_params = [p for n, p in model.named_parameters() if "embed" in n]
embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=3e-2, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 05:26:26 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   33C    P0             112W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   31C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   28C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   31C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   32C    P0             115W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:82ms step_avg:82.34ms
step:2/2330 train_time:142ms step_avg:71.18ms
step:3/2330 train_time:154ms step_avg:51.26ms
step:4/2330 train_time:167ms step_avg:41.63ms
step:5/2330 train_time:177ms step_avg:35.41ms
step:6/2330 train_time:203ms step_avg:33.75ms
step:7/2330 train_time:224ms step_avg:31.95ms
step:8/2330 train_time:278ms step_avg:34.77ms
step:9/2330 train_time:300ms step_avg:33.36ms
step:10/2330 train_time:355ms step_avg:35.51ms
step:11/2330 train_time:377ms step_avg:34.28ms
step:12/2330 train_time:433ms step_avg:36.05ms
step:13/2330 train_time:455ms step_avg:34.99ms
step:14/2330 train_time:511ms step_avg:36.51ms
step:15/2330 train_time:533ms step_avg:35.55ms
step:16/2330 train_time:589ms step_avg:36.80ms
step:17/2330 train_time:611ms step_avg:35.96ms
step:18/2330 train_time:667ms step_avg:37.06ms
step:19/2330 train_time:690ms step_avg:36.29ms
step:20/2330 train_time:746ms step_avg:37.30ms
step:21/2330 train_time:768ms step_avg:36.56ms
step:22/2330 train_time:824ms step_avg:37.44ms
step:23/2330 train_time:845ms step_avg:36.75ms
step:24/2330 train_time:901ms step_avg:37.55ms
step:25/2330 train_time:923ms step_avg:36.93ms
step:26/2330 train_time:983ms step_avg:37.82ms
step:27/2330 train_time:1007ms step_avg:37.31ms
step:28/2330 train_time:1070ms step_avg:38.21ms
step:29/2330 train_time:1094ms step_avg:37.71ms
step:30/2330 train_time:1153ms step_avg:38.43ms
step:31/2330 train_time:1177ms step_avg:37.95ms
step:32/2330 train_time:1233ms step_avg:38.54ms
step:33/2330 train_time:1257ms step_avg:38.08ms
step:34/2330 train_time:1313ms step_avg:38.62ms
step:35/2330 train_time:1335ms step_avg:38.15ms
step:36/2330 train_time:1391ms step_avg:38.64ms
step:37/2330 train_time:1414ms step_avg:38.22ms
step:38/2330 train_time:1470ms step_avg:38.69ms
step:39/2330 train_time:1492ms step_avg:38.27ms
step:40/2330 train_time:1548ms step_avg:38.71ms
step:41/2330 train_time:1570ms step_avg:38.29ms
step:42/2330 train_time:1626ms step_avg:38.70ms
step:43/2330 train_time:1648ms step_avg:38.32ms
step:44/2330 train_time:1703ms step_avg:38.72ms
step:45/2330 train_time:1725ms step_avg:38.33ms
step:46/2330 train_time:1781ms step_avg:38.71ms
step:47/2330 train_time:1803ms step_avg:38.36ms
step:48/2330 train_time:1859ms step_avg:38.73ms
step:49/2330 train_time:1881ms step_avg:38.39ms
step:50/2330 train_time:1937ms step_avg:38.75ms
step:51/2330 train_time:1961ms step_avg:38.45ms
step:52/2330 train_time:2019ms step_avg:38.82ms
step:53/2330 train_time:2042ms step_avg:38.53ms
step:54/2330 train_time:2101ms step_avg:38.91ms
step:55/2330 train_time:2125ms step_avg:38.63ms
step:56/2330 train_time:2183ms step_avg:38.98ms
step:57/2330 train_time:2205ms step_avg:38.69ms
step:58/2330 train_time:2263ms step_avg:39.01ms
step:59/2330 train_time:2284ms step_avg:38.72ms
step:60/2330 train_time:2341ms step_avg:39.02ms
step:61/2330 train_time:2363ms step_avg:38.74ms
step:62/2330 train_time:2420ms step_avg:39.04ms
step:63/2330 train_time:2442ms step_avg:38.77ms
step:64/2330 train_time:2498ms step_avg:39.03ms
step:65/2330 train_time:2521ms step_avg:38.79ms
step:66/2330 train_time:2576ms step_avg:39.03ms
step:67/2330 train_time:2599ms step_avg:38.79ms
step:68/2330 train_time:2655ms step_avg:39.04ms
step:69/2330 train_time:2677ms step_avg:38.80ms
step:70/2330 train_time:2734ms step_avg:39.05ms
step:71/2330 train_time:2756ms step_avg:38.82ms
step:72/2330 train_time:2813ms step_avg:39.06ms
step:73/2330 train_time:2836ms step_avg:38.85ms
step:74/2330 train_time:2892ms step_avg:39.08ms
step:75/2330 train_time:2916ms step_avg:38.88ms
step:76/2330 train_time:2974ms step_avg:39.13ms
step:77/2330 train_time:2998ms step_avg:38.94ms
step:78/2330 train_time:3056ms step_avg:39.17ms
step:79/2330 train_time:3079ms step_avg:38.98ms
step:80/2330 train_time:3136ms step_avg:39.20ms
step:81/2330 train_time:3160ms step_avg:39.01ms
step:82/2330 train_time:3217ms step_avg:39.23ms
step:83/2330 train_time:3240ms step_avg:39.04ms
step:84/2330 train_time:3296ms step_avg:39.24ms
step:85/2330 train_time:3319ms step_avg:39.05ms
step:86/2330 train_time:3376ms step_avg:39.26ms
step:87/2330 train_time:3400ms step_avg:39.08ms
step:88/2330 train_time:3456ms step_avg:39.28ms
step:89/2330 train_time:3479ms step_avg:39.09ms
step:90/2330 train_time:3534ms step_avg:39.27ms
step:91/2330 train_time:3558ms step_avg:39.10ms
step:92/2330 train_time:3614ms step_avg:39.28ms
step:93/2330 train_time:3637ms step_avg:39.10ms
step:94/2330 train_time:3693ms step_avg:39.28ms
step:95/2330 train_time:3716ms step_avg:39.11ms
step:96/2330 train_time:3772ms step_avg:39.29ms
step:97/2330 train_time:3795ms step_avg:39.12ms
step:98/2330 train_time:3852ms step_avg:39.30ms
step:99/2330 train_time:3875ms step_avg:39.14ms
step:100/2330 train_time:3932ms step_avg:39.32ms
step:101/2330 train_time:3955ms step_avg:39.16ms
step:102/2330 train_time:4013ms step_avg:39.35ms
step:103/2330 train_time:4036ms step_avg:39.19ms
step:104/2330 train_time:4094ms step_avg:39.37ms
step:105/2330 train_time:4118ms step_avg:39.22ms
step:106/2330 train_time:4176ms step_avg:39.39ms
step:107/2330 train_time:4199ms step_avg:39.24ms
step:108/2330 train_time:4255ms step_avg:39.40ms
step:109/2330 train_time:4279ms step_avg:39.26ms
step:110/2330 train_time:4336ms step_avg:39.42ms
step:111/2330 train_time:4360ms step_avg:39.28ms
step:112/2330 train_time:4416ms step_avg:39.43ms
step:113/2330 train_time:4439ms step_avg:39.29ms
step:114/2330 train_time:4496ms step_avg:39.44ms
step:115/2330 train_time:4519ms step_avg:39.29ms
step:116/2330 train_time:4575ms step_avg:39.44ms
step:117/2330 train_time:4598ms step_avg:39.30ms
step:118/2330 train_time:4654ms step_avg:39.44ms
step:119/2330 train_time:4677ms step_avg:39.30ms
step:120/2330 train_time:4733ms step_avg:39.44ms
step:121/2330 train_time:4757ms step_avg:39.31ms
step:122/2330 train_time:4813ms step_avg:39.45ms
step:123/2330 train_time:4837ms step_avg:39.32ms
step:124/2330 train_time:4894ms step_avg:39.47ms
step:125/2330 train_time:4918ms step_avg:39.35ms
step:126/2330 train_time:4975ms step_avg:39.48ms
step:127/2330 train_time:4998ms step_avg:39.36ms
step:128/2330 train_time:5055ms step_avg:39.49ms
step:129/2330 train_time:5079ms step_avg:39.37ms
step:130/2330 train_time:5135ms step_avg:39.50ms
step:131/2330 train_time:5160ms step_avg:39.39ms
step:132/2330 train_time:5216ms step_avg:39.52ms
step:133/2330 train_time:5239ms step_avg:39.39ms
step:134/2330 train_time:5296ms step_avg:39.53ms
step:135/2330 train_time:5320ms step_avg:39.41ms
step:136/2330 train_time:5376ms step_avg:39.53ms
step:137/2330 train_time:5399ms step_avg:39.41ms
step:138/2330 train_time:5456ms step_avg:39.53ms
step:139/2330 train_time:5479ms step_avg:39.42ms
step:140/2330 train_time:5535ms step_avg:39.54ms
step:141/2330 train_time:5558ms step_avg:39.42ms
step:142/2330 train_time:5615ms step_avg:39.54ms
step:143/2330 train_time:5638ms step_avg:39.43ms
step:144/2330 train_time:5694ms step_avg:39.54ms
step:145/2330 train_time:5717ms step_avg:39.43ms
step:146/2330 train_time:5774ms step_avg:39.55ms
step:147/2330 train_time:5797ms step_avg:39.44ms
step:148/2330 train_time:5854ms step_avg:39.55ms
step:149/2330 train_time:5878ms step_avg:39.45ms
step:150/2330 train_time:5935ms step_avg:39.56ms
step:151/2330 train_time:5958ms step_avg:39.46ms
step:152/2330 train_time:6015ms step_avg:39.57ms
step:153/2330 train_time:6038ms step_avg:39.47ms
step:154/2330 train_time:6095ms step_avg:39.58ms
step:155/2330 train_time:6119ms step_avg:39.48ms
step:156/2330 train_time:6175ms step_avg:39.59ms
step:157/2330 train_time:6199ms step_avg:39.48ms
step:158/2330 train_time:6255ms step_avg:39.59ms
step:159/2330 train_time:6279ms step_avg:39.49ms
step:160/2330 train_time:6336ms step_avg:39.60ms
step:161/2330 train_time:6359ms step_avg:39.50ms
step:162/2330 train_time:6415ms step_avg:39.60ms
step:163/2330 train_time:6439ms step_avg:39.50ms
step:164/2330 train_time:6495ms step_avg:39.61ms
step:165/2330 train_time:6518ms step_avg:39.51ms
step:166/2330 train_time:6575ms step_avg:39.61ms
step:167/2330 train_time:6598ms step_avg:39.51ms
step:168/2330 train_time:6654ms step_avg:39.61ms
step:169/2330 train_time:6678ms step_avg:39.51ms
step:170/2330 train_time:6734ms step_avg:39.61ms
step:171/2330 train_time:6757ms step_avg:39.52ms
step:172/2330 train_time:6814ms step_avg:39.62ms
step:173/2330 train_time:6838ms step_avg:39.52ms
step:174/2330 train_time:6894ms step_avg:39.62ms
step:175/2330 train_time:6918ms step_avg:39.53ms
step:176/2330 train_time:6975ms step_avg:39.63ms
step:177/2330 train_time:6998ms step_avg:39.54ms
step:178/2330 train_time:7054ms step_avg:39.63ms
step:179/2330 train_time:7077ms step_avg:39.54ms
step:180/2330 train_time:7134ms step_avg:39.64ms
step:181/2330 train_time:7158ms step_avg:39.55ms
step:182/2330 train_time:7215ms step_avg:39.64ms
step:183/2330 train_time:7238ms step_avg:39.55ms
step:184/2330 train_time:7295ms step_avg:39.65ms
step:185/2330 train_time:7319ms step_avg:39.56ms
step:186/2330 train_time:7375ms step_avg:39.65ms
step:187/2330 train_time:7399ms step_avg:39.57ms
step:188/2330 train_time:7455ms step_avg:39.66ms
step:189/2330 train_time:7479ms step_avg:39.57ms
step:190/2330 train_time:7535ms step_avg:39.66ms
step:191/2330 train_time:7558ms step_avg:39.57ms
step:192/2330 train_time:7615ms step_avg:39.66ms
step:193/2330 train_time:7638ms step_avg:39.57ms
step:194/2330 train_time:7694ms step_avg:39.66ms
step:195/2330 train_time:7718ms step_avg:39.58ms
step:196/2330 train_time:7774ms step_avg:39.67ms
step:197/2330 train_time:7797ms step_avg:39.58ms
step:198/2330 train_time:7854ms step_avg:39.67ms
step:199/2330 train_time:7877ms step_avg:39.58ms
step:200/2330 train_time:7934ms step_avg:39.67ms
step:201/2330 train_time:7958ms step_avg:39.59ms
step:202/2330 train_time:8015ms step_avg:39.68ms
step:203/2330 train_time:8038ms step_avg:39.60ms
step:204/2330 train_time:8095ms step_avg:39.68ms
step:205/2330 train_time:8118ms step_avg:39.60ms
step:206/2330 train_time:8174ms step_avg:39.68ms
step:207/2330 train_time:8198ms step_avg:39.60ms
step:208/2330 train_time:8255ms step_avg:39.69ms
step:209/2330 train_time:8278ms step_avg:39.61ms
step:210/2330 train_time:8335ms step_avg:39.69ms
step:211/2330 train_time:8358ms step_avg:39.61ms
step:212/2330 train_time:8415ms step_avg:39.69ms
step:213/2330 train_time:8438ms step_avg:39.62ms
step:214/2330 train_time:8495ms step_avg:39.69ms
step:215/2330 train_time:8518ms step_avg:39.62ms
step:216/2330 train_time:8575ms step_avg:39.70ms
step:217/2330 train_time:8598ms step_avg:39.62ms
step:218/2330 train_time:8654ms step_avg:39.70ms
step:219/2330 train_time:8677ms step_avg:39.62ms
step:220/2330 train_time:8734ms step_avg:39.70ms
step:221/2330 train_time:8757ms step_avg:39.62ms
step:222/2330 train_time:8814ms step_avg:39.70ms
step:223/2330 train_time:8837ms step_avg:39.63ms
step:224/2330 train_time:8894ms step_avg:39.70ms
step:225/2330 train_time:8917ms step_avg:39.63ms
step:226/2330 train_time:8975ms step_avg:39.71ms
step:227/2330 train_time:8999ms step_avg:39.64ms
step:228/2330 train_time:9056ms step_avg:39.72ms
step:229/2330 train_time:9079ms step_avg:39.65ms
step:230/2330 train_time:9136ms step_avg:39.72ms
step:231/2330 train_time:9159ms step_avg:39.65ms
step:232/2330 train_time:9216ms step_avg:39.72ms
step:233/2330 train_time:9240ms step_avg:39.66ms
step:234/2330 train_time:9296ms step_avg:39.73ms
step:235/2330 train_time:9320ms step_avg:39.66ms
step:236/2330 train_time:9377ms step_avg:39.73ms
step:237/2330 train_time:9400ms step_avg:39.66ms
step:238/2330 train_time:9456ms step_avg:39.73ms
step:239/2330 train_time:9479ms step_avg:39.66ms
step:240/2330 train_time:9536ms step_avg:39.73ms
step:241/2330 train_time:9559ms step_avg:39.67ms
step:242/2330 train_time:9615ms step_avg:39.73ms
step:243/2330 train_time:9639ms step_avg:39.66ms
step:244/2330 train_time:9695ms step_avg:39.73ms
step:245/2330 train_time:9718ms step_avg:39.67ms
step:246/2330 train_time:9775ms step_avg:39.73ms
step:247/2330 train_time:9799ms step_avg:39.67ms
step:248/2330 train_time:9855ms step_avg:39.74ms
step:249/2330 train_time:9878ms step_avg:39.67ms
step:250/2330 train_time:9935ms step_avg:39.74ms
step:250/2330 val_loss:5.5765 train_time:10033ms step_avg:40.13ms
step:251/2330 train_time:10045ms step_avg:40.02ms
step:252/2330 train_time:10056ms step_avg:39.91ms
step:253/2330 train_time:10066ms step_avg:39.79ms
step:254/2330 train_time:10096ms step_avg:39.75ms
step:255/2330 train_time:10118ms step_avg:39.68ms
step:256/2330 train_time:10173ms step_avg:39.74ms
step:257/2330 train_time:10196ms step_avg:39.67ms
step:258/2330 train_time:10251ms step_avg:39.73ms
step:259/2330 train_time:10274ms step_avg:39.67ms
step:260/2330 train_time:10330ms step_avg:39.73ms
step:261/2330 train_time:10358ms step_avg:39.69ms
step:262/2330 train_time:10418ms step_avg:39.76ms
step:263/2330 train_time:10444ms step_avg:39.71ms
step:264/2330 train_time:10502ms step_avg:39.78ms
step:265/2330 train_time:10525ms step_avg:39.72ms
step:266/2330 train_time:10581ms step_avg:39.78ms
step:267/2330 train_time:10604ms step_avg:39.72ms
step:268/2330 train_time:10660ms step_avg:39.78ms
step:269/2330 train_time:10682ms step_avg:39.71ms
step:270/2330 train_time:10739ms step_avg:39.77ms
step:271/2330 train_time:10761ms step_avg:39.71ms
step:272/2330 train_time:10817ms step_avg:39.77ms
step:273/2330 train_time:10840ms step_avg:39.71ms
step:274/2330 train_time:10896ms step_avg:39.77ms
step:275/2330 train_time:10919ms step_avg:39.70ms
step:276/2330 train_time:10977ms step_avg:39.77ms
step:277/2330 train_time:11001ms step_avg:39.71ms
step:278/2330 train_time:11057ms step_avg:39.77ms
step:279/2330 train_time:11079ms step_avg:39.71ms
step:280/2330 train_time:11136ms step_avg:39.77ms
step:281/2330 train_time:11158ms step_avg:39.71ms
step:282/2330 train_time:11214ms step_avg:39.77ms
step:283/2330 train_time:11237ms step_avg:39.71ms
step:284/2330 train_time:11294ms step_avg:39.77ms
step:285/2330 train_time:11319ms step_avg:39.71ms
step:286/2330 train_time:11376ms step_avg:39.78ms
step:287/2330 train_time:11401ms step_avg:39.72ms
step:288/2330 train_time:11458ms step_avg:39.79ms
step:289/2330 train_time:11483ms step_avg:39.73ms
step:290/2330 train_time:11540ms step_avg:39.79ms
step:291/2330 train_time:11563ms step_avg:39.73ms
step:292/2330 train_time:11620ms step_avg:39.79ms
step:293/2330 train_time:11642ms step_avg:39.73ms
step:294/2330 train_time:11699ms step_avg:39.79ms
step:295/2330 train_time:11722ms step_avg:39.73ms
step:296/2330 train_time:11778ms step_avg:39.79ms
step:297/2330 train_time:11801ms step_avg:39.73ms
step:298/2330 train_time:11857ms step_avg:39.79ms
step:299/2330 train_time:11880ms step_avg:39.73ms
step:300/2330 train_time:11936ms step_avg:39.79ms
step:301/2330 train_time:11960ms step_avg:39.74ms
step:302/2330 train_time:12016ms step_avg:39.79ms
step:303/2330 train_time:12039ms step_avg:39.73ms
step:304/2330 train_time:12095ms step_avg:39.79ms
step:305/2330 train_time:12119ms step_avg:39.73ms
step:306/2330 train_time:12175ms step_avg:39.79ms
step:307/2330 train_time:12198ms step_avg:39.73ms
step:308/2330 train_time:12255ms step_avg:39.79ms
step:309/2330 train_time:12278ms step_avg:39.74ms
step:310/2330 train_time:12335ms step_avg:39.79ms
step:311/2330 train_time:12359ms step_avg:39.74ms
step:312/2330 train_time:12416ms step_avg:39.80ms
step:313/2330 train_time:12441ms step_avg:39.75ms
step:314/2330 train_time:12497ms step_avg:39.80ms
step:315/2330 train_time:12521ms step_avg:39.75ms
step:316/2330 train_time:12577ms step_avg:39.80ms
step:317/2330 train_time:12600ms step_avg:39.75ms
step:318/2330 train_time:12656ms step_avg:39.80ms
step:319/2330 train_time:12679ms step_avg:39.75ms
step:320/2330 train_time:12735ms step_avg:39.80ms
step:321/2330 train_time:12759ms step_avg:39.75ms
step:322/2330 train_time:12815ms step_avg:39.80ms
step:323/2330 train_time:12838ms step_avg:39.74ms
step:324/2330 train_time:12893ms step_avg:39.79ms
step:325/2330 train_time:12916ms step_avg:39.74ms
step:326/2330 train_time:12973ms step_avg:39.79ms
step:327/2330 train_time:12996ms step_avg:39.74ms
step:328/2330 train_time:13052ms step_avg:39.79ms
step:329/2330 train_time:13074ms step_avg:39.74ms
step:330/2330 train_time:13131ms step_avg:39.79ms
step:331/2330 train_time:13154ms step_avg:39.74ms
step:332/2330 train_time:13210ms step_avg:39.79ms
step:333/2330 train_time:13233ms step_avg:39.74ms
step:334/2330 train_time:13290ms step_avg:39.79ms
step:335/2330 train_time:13314ms step_avg:39.74ms
step:336/2330 train_time:13371ms step_avg:39.80ms
step:337/2330 train_time:13394ms step_avg:39.74ms
step:338/2330 train_time:13450ms step_avg:39.79ms
step:339/2330 train_time:13475ms step_avg:39.75ms
step:340/2330 train_time:13532ms step_avg:39.80ms
step:341/2330 train_time:13555ms step_avg:39.75ms
step:342/2330 train_time:13612ms step_avg:39.80ms
step:343/2330 train_time:13636ms step_avg:39.75ms
step:344/2330 train_time:13692ms step_avg:39.80ms
step:345/2330 train_time:13715ms step_avg:39.75ms
step:346/2330 train_time:13772ms step_avg:39.80ms
step:347/2330 train_time:13795ms step_avg:39.76ms
step:348/2330 train_time:13852ms step_avg:39.80ms
step:349/2330 train_time:13875ms step_avg:39.76ms
step:350/2330 train_time:13931ms step_avg:39.80ms
step:351/2330 train_time:13954ms step_avg:39.75ms
step:352/2330 train_time:14010ms step_avg:39.80ms
step:353/2330 train_time:14032ms step_avg:39.75ms
step:354/2330 train_time:14089ms step_avg:39.80ms
step:355/2330 train_time:14111ms step_avg:39.75ms
step:356/2330 train_time:14169ms step_avg:39.80ms
step:357/2330 train_time:14191ms step_avg:39.75ms
step:358/2330 train_time:14248ms step_avg:39.80ms
step:359/2330 train_time:14270ms step_avg:39.75ms
step:360/2330 train_time:14328ms step_avg:39.80ms
step:361/2330 train_time:14351ms step_avg:39.75ms
step:362/2330 train_time:14409ms step_avg:39.80ms
step:363/2330 train_time:14432ms step_avg:39.76ms
step:364/2330 train_time:14489ms step_avg:39.80ms
step:365/2330 train_time:14513ms step_avg:39.76ms
step:366/2330 train_time:14570ms step_avg:39.81ms
step:367/2330 train_time:14594ms step_avg:39.77ms
step:368/2330 train_time:14650ms step_avg:39.81ms
step:369/2330 train_time:14674ms step_avg:39.77ms
step:370/2330 train_time:14731ms step_avg:39.81ms
step:371/2330 train_time:14755ms step_avg:39.77ms
step:372/2330 train_time:14811ms step_avg:39.82ms
step:373/2330 train_time:14835ms step_avg:39.77ms
step:374/2330 train_time:14892ms step_avg:39.82ms
step:375/2330 train_time:14915ms step_avg:39.77ms
step:376/2330 train_time:14972ms step_avg:39.82ms
step:377/2330 train_time:14994ms step_avg:39.77ms
step:378/2330 train_time:15051ms step_avg:39.82ms
step:379/2330 train_time:15074ms step_avg:39.77ms
step:380/2330 train_time:15130ms step_avg:39.82ms
step:381/2330 train_time:15154ms step_avg:39.77ms
step:382/2330 train_time:15211ms step_avg:39.82ms
step:383/2330 train_time:15234ms step_avg:39.78ms
step:384/2330 train_time:15291ms step_avg:39.82ms
step:385/2330 train_time:15314ms step_avg:39.78ms
step:386/2330 train_time:15372ms step_avg:39.82ms
step:387/2330 train_time:15396ms step_avg:39.78ms
step:388/2330 train_time:15453ms step_avg:39.83ms
step:389/2330 train_time:15477ms step_avg:39.79ms
step:390/2330 train_time:15533ms step_avg:39.83ms
step:391/2330 train_time:15557ms step_avg:39.79ms
step:392/2330 train_time:15613ms step_avg:39.83ms
step:393/2330 train_time:15637ms step_avg:39.79ms
step:394/2330 train_time:15693ms step_avg:39.83ms
step:395/2330 train_time:15717ms step_avg:39.79ms
step:396/2330 train_time:15773ms step_avg:39.83ms
step:397/2330 train_time:15796ms step_avg:39.79ms
step:398/2330 train_time:15852ms step_avg:39.83ms
step:399/2330 train_time:15876ms step_avg:39.79ms
step:400/2330 train_time:15932ms step_avg:39.83ms
step:401/2330 train_time:15956ms step_avg:39.79ms
step:402/2330 train_time:16012ms step_avg:39.83ms
step:403/2330 train_time:16035ms step_avg:39.79ms
step:404/2330 train_time:16091ms step_avg:39.83ms
step:405/2330 train_time:16114ms step_avg:39.79ms
step:406/2330 train_time:16171ms step_avg:39.83ms
step:407/2330 train_time:16194ms step_avg:39.79ms
step:408/2330 train_time:16251ms step_avg:39.83ms
step:409/2330 train_time:16274ms step_avg:39.79ms
step:410/2330 train_time:16332ms step_avg:39.83ms
step:411/2330 train_time:16355ms step_avg:39.79ms
step:412/2330 train_time:16412ms step_avg:39.83ms
step:413/2330 train_time:16435ms step_avg:39.79ms
step:414/2330 train_time:16492ms step_avg:39.84ms
step:415/2330 train_time:16515ms step_avg:39.80ms
step:416/2330 train_time:16572ms step_avg:39.84ms
step:417/2330 train_time:16595ms step_avg:39.80ms
step:418/2330 train_time:16652ms step_avg:39.84ms
step:419/2330 train_time:16676ms step_avg:39.80ms
step:420/2330 train_time:16733ms step_avg:39.84ms
step:421/2330 train_time:16756ms step_avg:39.80ms
step:422/2330 train_time:16812ms step_avg:39.84ms
step:423/2330 train_time:16835ms step_avg:39.80ms
step:424/2330 train_time:16892ms step_avg:39.84ms
step:425/2330 train_time:16915ms step_avg:39.80ms
step:426/2330 train_time:16972ms step_avg:39.84ms
step:427/2330 train_time:16995ms step_avg:39.80ms
step:428/2330 train_time:17051ms step_avg:39.84ms
step:429/2330 train_time:17074ms step_avg:39.80ms
step:430/2330 train_time:17131ms step_avg:39.84ms
step:431/2330 train_time:17154ms step_avg:39.80ms
step:432/2330 train_time:17210ms step_avg:39.84ms
step:433/2330 train_time:17233ms step_avg:39.80ms
step:434/2330 train_time:17290ms step_avg:39.84ms
step:435/2330 train_time:17314ms step_avg:39.80ms
step:436/2330 train_time:17371ms step_avg:39.84ms
step:437/2330 train_time:17395ms step_avg:39.80ms
step:438/2330 train_time:17452ms step_avg:39.84ms
step:439/2330 train_time:17475ms step_avg:39.81ms
step:440/2330 train_time:17532ms step_avg:39.84ms
step:441/2330 train_time:17555ms step_avg:39.81ms
step:442/2330 train_time:17613ms step_avg:39.85ms
step:443/2330 train_time:17636ms step_avg:39.81ms
step:444/2330 train_time:17693ms step_avg:39.85ms
step:445/2330 train_time:17716ms step_avg:39.81ms
step:446/2330 train_time:17772ms step_avg:39.85ms
step:447/2330 train_time:17796ms step_avg:39.81ms
step:448/2330 train_time:17852ms step_avg:39.85ms
step:449/2330 train_time:17875ms step_avg:39.81ms
step:450/2330 train_time:17932ms step_avg:39.85ms
step:451/2330 train_time:17955ms step_avg:39.81ms
step:452/2330 train_time:18012ms step_avg:39.85ms
step:453/2330 train_time:18035ms step_avg:39.81ms
step:454/2330 train_time:18092ms step_avg:39.85ms
step:455/2330 train_time:18114ms step_avg:39.81ms
step:456/2330 train_time:18171ms step_avg:39.85ms
step:457/2330 train_time:18194ms step_avg:39.81ms
step:458/2330 train_time:18251ms step_avg:39.85ms
step:459/2330 train_time:18273ms step_avg:39.81ms
step:460/2330 train_time:18330ms step_avg:39.85ms
step:461/2330 train_time:18353ms step_avg:39.81ms
step:462/2330 train_time:18410ms step_avg:39.85ms
step:463/2330 train_time:18433ms step_avg:39.81ms
step:464/2330 train_time:18489ms step_avg:39.85ms
step:465/2330 train_time:18512ms step_avg:39.81ms
step:466/2330 train_time:18569ms step_avg:39.85ms
step:467/2330 train_time:18592ms step_avg:39.81ms
step:468/2330 train_time:18648ms step_avg:39.85ms
step:469/2330 train_time:18671ms step_avg:39.81ms
step:470/2330 train_time:18729ms step_avg:39.85ms
step:471/2330 train_time:18751ms step_avg:39.81ms
step:472/2330 train_time:18808ms step_avg:39.85ms
step:473/2330 train_time:18830ms step_avg:39.81ms
step:474/2330 train_time:18887ms step_avg:39.85ms
step:475/2330 train_time:18910ms step_avg:39.81ms
step:476/2330 train_time:18966ms step_avg:39.85ms
step:477/2330 train_time:18990ms step_avg:39.81ms
step:478/2330 train_time:19047ms step_avg:39.85ms
step:479/2330 train_time:19069ms step_avg:39.81ms
step:480/2330 train_time:19126ms step_avg:39.84ms
step:481/2330 train_time:19148ms step_avg:39.81ms
step:482/2330 train_time:19205ms step_avg:39.84ms
step:483/2330 train_time:19227ms step_avg:39.81ms
step:484/2330 train_time:19284ms step_avg:39.84ms
step:485/2330 train_time:19306ms step_avg:39.81ms
step:486/2330 train_time:19363ms step_avg:39.84ms
step:487/2330 train_time:19385ms step_avg:39.80ms
step:488/2330 train_time:19442ms step_avg:39.84ms
step:489/2330 train_time:19465ms step_avg:39.80ms
step:490/2330 train_time:19522ms step_avg:39.84ms
step:491/2330 train_time:19544ms step_avg:39.80ms
step:492/2330 train_time:19601ms step_avg:39.84ms
step:493/2330 train_time:19623ms step_avg:39.80ms
step:494/2330 train_time:19680ms step_avg:39.84ms
step:495/2330 train_time:19702ms step_avg:39.80ms
step:496/2330 train_time:19759ms step_avg:39.84ms
step:497/2330 train_time:19781ms step_avg:39.80ms
step:498/2330 train_time:19839ms step_avg:39.84ms
step:499/2330 train_time:19861ms step_avg:39.80ms
step:500/2330 train_time:19917ms step_avg:39.83ms
step:500/2330 val_loss:5.3741 train_time:20014ms step_avg:40.03ms
step:501/2330 train_time:20026ms step_avg:39.97ms
step:502/2330 train_time:20038ms step_avg:39.92ms
step:503/2330 train_time:20048ms step_avg:39.86ms
step:504/2330 train_time:20078ms step_avg:39.84ms
step:505/2330 train_time:20099ms step_avg:39.80ms
step:506/2330 train_time:20155ms step_avg:39.83ms
step:507/2330 train_time:20176ms step_avg:39.80ms
step:508/2330 train_time:20232ms step_avg:39.83ms
step:509/2330 train_time:20254ms step_avg:39.79ms
step:510/2330 train_time:20312ms step_avg:39.83ms
step:511/2330 train_time:20337ms step_avg:39.80ms
step:512/2330 train_time:20397ms step_avg:39.84ms
step:513/2330 train_time:20422ms step_avg:39.81ms
step:514/2330 train_time:20479ms step_avg:39.84ms
step:515/2330 train_time:20501ms step_avg:39.81ms
step:516/2330 train_time:20557ms step_avg:39.84ms
step:517/2330 train_time:20580ms step_avg:39.81ms
step:518/2330 train_time:20637ms step_avg:39.84ms
step:519/2330 train_time:20659ms step_avg:39.81ms
step:520/2330 train_time:20716ms step_avg:39.84ms
step:521/2330 train_time:20737ms step_avg:39.80ms
step:522/2330 train_time:20794ms step_avg:39.83ms
step:523/2330 train_time:20816ms step_avg:39.80ms
step:524/2330 train_time:20872ms step_avg:39.83ms
step:525/2330 train_time:20894ms step_avg:39.80ms
step:526/2330 train_time:20952ms step_avg:39.83ms
step:527/2330 train_time:20976ms step_avg:39.80ms
step:528/2330 train_time:21033ms step_avg:39.84ms
step:529/2330 train_time:21055ms step_avg:39.80ms
step:530/2330 train_time:21111ms step_avg:39.83ms
step:531/2330 train_time:21133ms step_avg:39.80ms
step:532/2330 train_time:21189ms step_avg:39.83ms
step:533/2330 train_time:21211ms step_avg:39.80ms
step:534/2330 train_time:21268ms step_avg:39.83ms
step:535/2330 train_time:21292ms step_avg:39.80ms
step:536/2330 train_time:21350ms step_avg:39.83ms
step:537/2330 train_time:21373ms step_avg:39.80ms
step:538/2330 train_time:21432ms step_avg:39.84ms
step:539/2330 train_time:21454ms step_avg:39.80ms
step:540/2330 train_time:21511ms step_avg:39.84ms
step:541/2330 train_time:21534ms step_avg:39.80ms
step:542/2330 train_time:21591ms step_avg:39.84ms
step:543/2330 train_time:21613ms step_avg:39.80ms
step:544/2330 train_time:21669ms step_avg:39.83ms
step:545/2330 train_time:21692ms step_avg:39.80ms
step:546/2330 train_time:21748ms step_avg:39.83ms
step:547/2330 train_time:21771ms step_avg:39.80ms
step:548/2330 train_time:21826ms step_avg:39.83ms
step:549/2330 train_time:21849ms step_avg:39.80ms
step:550/2330 train_time:21906ms step_avg:39.83ms
step:551/2330 train_time:21929ms step_avg:39.80ms
step:552/2330 train_time:21986ms step_avg:39.83ms
step:553/2330 train_time:22008ms step_avg:39.80ms
step:554/2330 train_time:22064ms step_avg:39.83ms
step:555/2330 train_time:22087ms step_avg:39.80ms
step:556/2330 train_time:22144ms step_avg:39.83ms
step:557/2330 train_time:22167ms step_avg:39.80ms
step:558/2330 train_time:22224ms step_avg:39.83ms
step:559/2330 train_time:22247ms step_avg:39.80ms
step:560/2330 train_time:22305ms step_avg:39.83ms
step:561/2330 train_time:22329ms step_avg:39.80ms
step:562/2330 train_time:22385ms step_avg:39.83ms
step:563/2330 train_time:22409ms step_avg:39.80ms
step:564/2330 train_time:22466ms step_avg:39.83ms
step:565/2330 train_time:22489ms step_avg:39.80ms
step:566/2330 train_time:22546ms step_avg:39.83ms
step:567/2330 train_time:22569ms step_avg:39.80ms
step:568/2330 train_time:22625ms step_avg:39.83ms
step:569/2330 train_time:22648ms step_avg:39.80ms
step:570/2330 train_time:22705ms step_avg:39.83ms
step:571/2330 train_time:22728ms step_avg:39.80ms
step:572/2330 train_time:22785ms step_avg:39.83ms
step:573/2330 train_time:22807ms step_avg:39.80ms
step:574/2330 train_time:22863ms step_avg:39.83ms
step:575/2330 train_time:22886ms step_avg:39.80ms
step:576/2330 train_time:22943ms step_avg:39.83ms
step:577/2330 train_time:22966ms step_avg:39.80ms
step:578/2330 train_time:23023ms step_avg:39.83ms
step:579/2330 train_time:23045ms step_avg:39.80ms
step:580/2330 train_time:23101ms step_avg:39.83ms
step:581/2330 train_time:23125ms step_avg:39.80ms
step:582/2330 train_time:23181ms step_avg:39.83ms
step:583/2330 train_time:23205ms step_avg:39.80ms
step:584/2330 train_time:23262ms step_avg:39.83ms
step:585/2330 train_time:23285ms step_avg:39.80ms
step:586/2330 train_time:23344ms step_avg:39.84ms
step:587/2330 train_time:23367ms step_avg:39.81ms
step:588/2330 train_time:23424ms step_avg:39.84ms
step:589/2330 train_time:23447ms step_avg:39.81ms
step:590/2330 train_time:23503ms step_avg:39.84ms
step:591/2330 train_time:23526ms step_avg:39.81ms
step:592/2330 train_time:23605ms step_avg:39.87ms
step:593/2330 train_time:23615ms step_avg:39.82ms
step:594/2330 train_time:23664ms step_avg:39.84ms
step:595/2330 train_time:23687ms step_avg:39.81ms
step:596/2330 train_time:23744ms step_avg:39.84ms
step:597/2330 train_time:23766ms step_avg:39.81ms
step:598/2330 train_time:23823ms step_avg:39.84ms
step:599/2330 train_time:23846ms step_avg:39.81ms
step:600/2330 train_time:23902ms step_avg:39.84ms
step:601/2330 train_time:23925ms step_avg:39.81ms
step:602/2330 train_time:23982ms step_avg:39.84ms
step:603/2330 train_time:24004ms step_avg:39.81ms
step:604/2330 train_time:24060ms step_avg:39.84ms
step:605/2330 train_time:24084ms step_avg:39.81ms
step:606/2330 train_time:24140ms step_avg:39.84ms
step:607/2330 train_time:24163ms step_avg:39.81ms
step:608/2330 train_time:24220ms step_avg:39.83ms
step:609/2330 train_time:24242ms step_avg:39.81ms
step:610/2330 train_time:24301ms step_avg:39.84ms
step:611/2330 train_time:24324ms step_avg:39.81ms
step:612/2330 train_time:24381ms step_avg:39.84ms
step:613/2330 train_time:24404ms step_avg:39.81ms
step:614/2330 train_time:24461ms step_avg:39.84ms
step:615/2330 train_time:24484ms step_avg:39.81ms
step:616/2330 train_time:24542ms step_avg:39.84ms
step:617/2330 train_time:24564ms step_avg:39.81ms
step:618/2330 train_time:24622ms step_avg:39.84ms
step:619/2330 train_time:24645ms step_avg:39.81ms
step:620/2330 train_time:24702ms step_avg:39.84ms
step:621/2330 train_time:24725ms step_avg:39.81ms
step:622/2330 train_time:24782ms step_avg:39.84ms
step:623/2330 train_time:24804ms step_avg:39.81ms
step:624/2330 train_time:24861ms step_avg:39.84ms
step:625/2330 train_time:24883ms step_avg:39.81ms
step:626/2330 train_time:24940ms step_avg:39.84ms
step:627/2330 train_time:24962ms step_avg:39.81ms
step:628/2330 train_time:25018ms step_avg:39.84ms
step:629/2330 train_time:25041ms step_avg:39.81ms
step:630/2330 train_time:25098ms step_avg:39.84ms
step:631/2330 train_time:25121ms step_avg:39.81ms
step:632/2330 train_time:25178ms step_avg:39.84ms
step:633/2330 train_time:25200ms step_avg:39.81ms
step:634/2330 train_time:25257ms step_avg:39.84ms
step:635/2330 train_time:25280ms step_avg:39.81ms
step:636/2330 train_time:25338ms step_avg:39.84ms
step:637/2330 train_time:25360ms step_avg:39.81ms
step:638/2330 train_time:25417ms step_avg:39.84ms
step:639/2330 train_time:25439ms step_avg:39.81ms
step:640/2330 train_time:25497ms step_avg:39.84ms
step:641/2330 train_time:25519ms step_avg:39.81ms
step:642/2330 train_time:25576ms step_avg:39.84ms
step:643/2330 train_time:25599ms step_avg:39.81ms
step:644/2330 train_time:25656ms step_avg:39.84ms
step:645/2330 train_time:25678ms step_avg:39.81ms
step:646/2330 train_time:25735ms step_avg:39.84ms
step:647/2330 train_time:25758ms step_avg:39.81ms
step:648/2330 train_time:25815ms step_avg:39.84ms
step:649/2330 train_time:25837ms step_avg:39.81ms
step:650/2330 train_time:25893ms step_avg:39.84ms
step:651/2330 train_time:25915ms step_avg:39.81ms
step:652/2330 train_time:25971ms step_avg:39.83ms
step:653/2330 train_time:25993ms step_avg:39.81ms
step:654/2330 train_time:26050ms step_avg:39.83ms
step:655/2330 train_time:26072ms step_avg:39.80ms
step:656/2330 train_time:26128ms step_avg:39.83ms
step:657/2330 train_time:26151ms step_avg:39.80ms
step:658/2330 train_time:26208ms step_avg:39.83ms
step:659/2330 train_time:26231ms step_avg:39.80ms
step:660/2330 train_time:26287ms step_avg:39.83ms
step:661/2330 train_time:26310ms step_avg:39.80ms
step:662/2330 train_time:26367ms step_avg:39.83ms
step:663/2330 train_time:26390ms step_avg:39.80ms
step:664/2330 train_time:26447ms step_avg:39.83ms
step:665/2330 train_time:26469ms step_avg:39.80ms
step:666/2330 train_time:26525ms step_avg:39.83ms
step:667/2330 train_time:26549ms step_avg:39.80ms
step:668/2330 train_time:26606ms step_avg:39.83ms
step:669/2330 train_time:26629ms step_avg:39.80ms
step:670/2330 train_time:26685ms step_avg:39.83ms
step:671/2330 train_time:26709ms step_avg:39.80ms
step:672/2330 train_time:26765ms step_avg:39.83ms
step:673/2330 train_time:26788ms step_avg:39.80ms
step:674/2330 train_time:26845ms step_avg:39.83ms
step:675/2330 train_time:26868ms step_avg:39.80ms
step:676/2330 train_time:26924ms step_avg:39.83ms
step:677/2330 train_time:26947ms step_avg:39.80ms
step:678/2330 train_time:27004ms step_avg:39.83ms
step:679/2330 train_time:27026ms step_avg:39.80ms
step:680/2330 train_time:27083ms step_avg:39.83ms
step:681/2330 train_time:27106ms step_avg:39.80ms
step:682/2330 train_time:27163ms step_avg:39.83ms
step:683/2330 train_time:27186ms step_avg:39.80ms
step:684/2330 train_time:27243ms step_avg:39.83ms
step:685/2330 train_time:27266ms step_avg:39.80ms
step:686/2330 train_time:27323ms step_avg:39.83ms
step:687/2330 train_time:27347ms step_avg:39.81ms
step:688/2330 train_time:27404ms step_avg:39.83ms
step:689/2330 train_time:27427ms step_avg:39.81ms
step:690/2330 train_time:27484ms step_avg:39.83ms
step:691/2330 train_time:27508ms step_avg:39.81ms
step:692/2330 train_time:27564ms step_avg:39.83ms
step:693/2330 train_time:27588ms step_avg:39.81ms
step:694/2330 train_time:27645ms step_avg:39.83ms
step:695/2330 train_time:27667ms step_avg:39.81ms
step:696/2330 train_time:27724ms step_avg:39.83ms
step:697/2330 train_time:27747ms step_avg:39.81ms
step:698/2330 train_time:27805ms step_avg:39.83ms
step:699/2330 train_time:27828ms step_avg:39.81ms
step:700/2330 train_time:27884ms step_avg:39.83ms
step:701/2330 train_time:27907ms step_avg:39.81ms
step:702/2330 train_time:27963ms step_avg:39.83ms
step:703/2330 train_time:27987ms step_avg:39.81ms
step:704/2330 train_time:28044ms step_avg:39.84ms
step:705/2330 train_time:28067ms step_avg:39.81ms
step:706/2330 train_time:28123ms step_avg:39.83ms
step:707/2330 train_time:28147ms step_avg:39.81ms
step:708/2330 train_time:28204ms step_avg:39.84ms
step:709/2330 train_time:28227ms step_avg:39.81ms
step:710/2330 train_time:28284ms step_avg:39.84ms
step:711/2330 train_time:28307ms step_avg:39.81ms
step:712/2330 train_time:28364ms step_avg:39.84ms
step:713/2330 train_time:28388ms step_avg:39.81ms
step:714/2330 train_time:28445ms step_avg:39.84ms
step:715/2330 train_time:28468ms step_avg:39.82ms
step:716/2330 train_time:28525ms step_avg:39.84ms
step:717/2330 train_time:28549ms step_avg:39.82ms
step:718/2330 train_time:28605ms step_avg:39.84ms
step:719/2330 train_time:28628ms step_avg:39.82ms
step:720/2330 train_time:28685ms step_avg:39.84ms
step:721/2330 train_time:28708ms step_avg:39.82ms
step:722/2330 train_time:28765ms step_avg:39.84ms
step:723/2330 train_time:28788ms step_avg:39.82ms
step:724/2330 train_time:28844ms step_avg:39.84ms
step:725/2330 train_time:28868ms step_avg:39.82ms
step:726/2330 train_time:28924ms step_avg:39.84ms
step:727/2330 train_time:28947ms step_avg:39.82ms
step:728/2330 train_time:29004ms step_avg:39.84ms
step:729/2330 train_time:29028ms step_avg:39.82ms
step:730/2330 train_time:29084ms step_avg:39.84ms
step:731/2330 train_time:29108ms step_avg:39.82ms
step:732/2330 train_time:29164ms step_avg:39.84ms
step:733/2330 train_time:29187ms step_avg:39.82ms
step:734/2330 train_time:29244ms step_avg:39.84ms
step:735/2330 train_time:29266ms step_avg:39.82ms
step:736/2330 train_time:29323ms step_avg:39.84ms
step:737/2330 train_time:29346ms step_avg:39.82ms
step:738/2330 train_time:29403ms step_avg:39.84ms
step:739/2330 train_time:29427ms step_avg:39.82ms
step:740/2330 train_time:29484ms step_avg:39.84ms
step:741/2330 train_time:29507ms step_avg:39.82ms
step:742/2330 train_time:29565ms step_avg:39.84ms
step:743/2330 train_time:29588ms step_avg:39.82ms
step:744/2330 train_time:29645ms step_avg:39.85ms
step:745/2330 train_time:29668ms step_avg:39.82ms
step:746/2330 train_time:29725ms step_avg:39.85ms
step:747/2330 train_time:29748ms step_avg:39.82ms
step:748/2330 train_time:29804ms step_avg:39.85ms
step:749/2330 train_time:29828ms step_avg:39.82ms
step:750/2330 train_time:29884ms step_avg:39.85ms
step:750/2330 val_loss:5.2871 train_time:29982ms step_avg:39.98ms
step:751/2330 train_time:29994ms step_avg:39.94ms
step:752/2330 train_time:30005ms step_avg:39.90ms
step:753/2330 train_time:30015ms step_avg:39.86ms
step:754/2330 train_time:30044ms step_avg:39.85ms
step:755/2330 train_time:30066ms step_avg:39.82ms
step:756/2330 train_time:30122ms step_avg:39.84ms
step:757/2330 train_time:30144ms step_avg:39.82ms
step:758/2330 train_time:30200ms step_avg:39.84ms
step:759/2330 train_time:30222ms step_avg:39.82ms
step:760/2330 train_time:30281ms step_avg:39.84ms
step:761/2330 train_time:30305ms step_avg:39.82ms
step:762/2330 train_time:30365ms step_avg:39.85ms
step:763/2330 train_time:30388ms step_avg:39.83ms
step:764/2330 train_time:30445ms step_avg:39.85ms
step:765/2330 train_time:30469ms step_avg:39.83ms
step:766/2330 train_time:30526ms step_avg:39.85ms
step:767/2330 train_time:30548ms step_avg:39.83ms
step:768/2330 train_time:30604ms step_avg:39.85ms
step:769/2330 train_time:30626ms step_avg:39.83ms
step:770/2330 train_time:30682ms step_avg:39.85ms
step:771/2330 train_time:30705ms step_avg:39.82ms
step:772/2330 train_time:30761ms step_avg:39.85ms
step:773/2330 train_time:30783ms step_avg:39.82ms
step:774/2330 train_time:30839ms step_avg:39.84ms
step:775/2330 train_time:30861ms step_avg:39.82ms
step:776/2330 train_time:30919ms step_avg:39.84ms
step:777/2330 train_time:30942ms step_avg:39.82ms
step:778/2330 train_time:30999ms step_avg:39.84ms
step:779/2330 train_time:31021ms step_avg:39.82ms
step:780/2330 train_time:31077ms step_avg:39.84ms
step:781/2330 train_time:31099ms step_avg:39.82ms
step:782/2330 train_time:31155ms step_avg:39.84ms
step:783/2330 train_time:31177ms step_avg:39.82ms
step:784/2330 train_time:31235ms step_avg:39.84ms
step:785/2330 train_time:31257ms step_avg:39.82ms
step:786/2330 train_time:31316ms step_avg:39.84ms
step:787/2330 train_time:31339ms step_avg:39.82ms
step:788/2330 train_time:31396ms step_avg:39.84ms
step:789/2330 train_time:31419ms step_avg:39.82ms
step:790/2330 train_time:31476ms step_avg:39.84ms
step:791/2330 train_time:31498ms step_avg:39.82ms
step:792/2330 train_time:31555ms step_avg:39.84ms
step:793/2330 train_time:31577ms step_avg:39.82ms
step:794/2330 train_time:31634ms step_avg:39.84ms
step:795/2330 train_time:31657ms step_avg:39.82ms
step:796/2330 train_time:31712ms step_avg:39.84ms
step:797/2330 train_time:31735ms step_avg:39.82ms
step:798/2330 train_time:31791ms step_avg:39.84ms
step:799/2330 train_time:31813ms step_avg:39.82ms
step:800/2330 train_time:31870ms step_avg:39.84ms
step:801/2330 train_time:31892ms step_avg:39.82ms
step:802/2330 train_time:31948ms step_avg:39.84ms
step:803/2330 train_time:31971ms step_avg:39.81ms
step:804/2330 train_time:32028ms step_avg:39.84ms
step:805/2330 train_time:32050ms step_avg:39.81ms
step:806/2330 train_time:32107ms step_avg:39.84ms
step:807/2330 train_time:32130ms step_avg:39.81ms
step:808/2330 train_time:32187ms step_avg:39.84ms
step:809/2330 train_time:32210ms step_avg:39.81ms
step:810/2330 train_time:32267ms step_avg:39.84ms
step:811/2330 train_time:32291ms step_avg:39.82ms
step:812/2330 train_time:32348ms step_avg:39.84ms
step:813/2330 train_time:32371ms step_avg:39.82ms
step:814/2330 train_time:32428ms step_avg:39.84ms
step:815/2330 train_time:32451ms step_avg:39.82ms
step:816/2330 train_time:32509ms step_avg:39.84ms
step:817/2330 train_time:32531ms step_avg:39.82ms
step:818/2330 train_time:32588ms step_avg:39.84ms
step:819/2330 train_time:32611ms step_avg:39.82ms
step:820/2330 train_time:32667ms step_avg:39.84ms
step:821/2330 train_time:32690ms step_avg:39.82ms
step:822/2330 train_time:32746ms step_avg:39.84ms
step:823/2330 train_time:32769ms step_avg:39.82ms
step:824/2330 train_time:32825ms step_avg:39.84ms
step:825/2330 train_time:32848ms step_avg:39.82ms
step:826/2330 train_time:32904ms step_avg:39.84ms
step:827/2330 train_time:32927ms step_avg:39.82ms
step:828/2330 train_time:32983ms step_avg:39.83ms
step:829/2330 train_time:33006ms step_avg:39.81ms
step:830/2330 train_time:33063ms step_avg:39.83ms
step:831/2330 train_time:33085ms step_avg:39.81ms
step:832/2330 train_time:33142ms step_avg:39.83ms
step:833/2330 train_time:33164ms step_avg:39.81ms
step:834/2330 train_time:33221ms step_avg:39.83ms
step:835/2330 train_time:33244ms step_avg:39.81ms
step:836/2330 train_time:33301ms step_avg:39.83ms
step:837/2330 train_time:33323ms step_avg:39.81ms
step:838/2330 train_time:33381ms step_avg:39.83ms
step:839/2330 train_time:33403ms step_avg:39.81ms
step:840/2330 train_time:33461ms step_avg:39.83ms
step:841/2330 train_time:33483ms step_avg:39.81ms
step:842/2330 train_time:33541ms step_avg:39.84ms
step:843/2330 train_time:33564ms step_avg:39.81ms
step:844/2330 train_time:33621ms step_avg:39.83ms
step:845/2330 train_time:33643ms step_avg:39.81ms
step:846/2330 train_time:33699ms step_avg:39.83ms
step:847/2330 train_time:33721ms step_avg:39.81ms
step:848/2330 train_time:33777ms step_avg:39.83ms
step:849/2330 train_time:33799ms step_avg:39.81ms
step:850/2330 train_time:33857ms step_avg:39.83ms
step:851/2330 train_time:33878ms step_avg:39.81ms
step:852/2330 train_time:33936ms step_avg:39.83ms
step:853/2330 train_time:33958ms step_avg:39.81ms
step:854/2330 train_time:34014ms step_avg:39.83ms
step:855/2330 train_time:34036ms step_avg:39.81ms
step:856/2330 train_time:34092ms step_avg:39.83ms
step:857/2330 train_time:34115ms step_avg:39.81ms
step:858/2330 train_time:34172ms step_avg:39.83ms
step:859/2330 train_time:34194ms step_avg:39.81ms
step:860/2330 train_time:34251ms step_avg:39.83ms
step:861/2330 train_time:34274ms step_avg:39.81ms
step:862/2330 train_time:34330ms step_avg:39.83ms
step:863/2330 train_time:34353ms step_avg:39.81ms
step:864/2330 train_time:34410ms step_avg:39.83ms
step:865/2330 train_time:34433ms step_avg:39.81ms
step:866/2330 train_time:34489ms step_avg:39.83ms
step:867/2330 train_time:34513ms step_avg:39.81ms
step:868/2330 train_time:34569ms step_avg:39.83ms
step:869/2330 train_time:34592ms step_avg:39.81ms
step:870/2330 train_time:34649ms step_avg:39.83ms
step:871/2330 train_time:34672ms step_avg:39.81ms
step:872/2330 train_time:34728ms step_avg:39.83ms
step:873/2330 train_time:34751ms step_avg:39.81ms
step:874/2330 train_time:34807ms step_avg:39.82ms
step:875/2330 train_time:34830ms step_avg:39.81ms
step:876/2330 train_time:34887ms step_avg:39.83ms
step:877/2330 train_time:34910ms step_avg:39.81ms
step:878/2330 train_time:34966ms step_avg:39.83ms
step:879/2330 train_time:34989ms step_avg:39.81ms
step:880/2330 train_time:35046ms step_avg:39.82ms
step:881/2330 train_time:35068ms step_avg:39.80ms
step:882/2330 train_time:35124ms step_avg:39.82ms
step:883/2330 train_time:35149ms step_avg:39.81ms
step:884/2330 train_time:35206ms step_avg:39.83ms
step:885/2330 train_time:35229ms step_avg:39.81ms
step:886/2330 train_time:35285ms step_avg:39.82ms
step:887/2330 train_time:35308ms step_avg:39.81ms
step:888/2330 train_time:35365ms step_avg:39.83ms
step:889/2330 train_time:35388ms step_avg:39.81ms
step:890/2330 train_time:35445ms step_avg:39.83ms
step:891/2330 train_time:35468ms step_avg:39.81ms
step:892/2330 train_time:35525ms step_avg:39.83ms
step:893/2330 train_time:35549ms step_avg:39.81ms
step:894/2330 train_time:35605ms step_avg:39.83ms
step:895/2330 train_time:35628ms step_avg:39.81ms
step:896/2330 train_time:35684ms step_avg:39.83ms
step:897/2330 train_time:35707ms step_avg:39.81ms
step:898/2330 train_time:35765ms step_avg:39.83ms
step:899/2330 train_time:35787ms step_avg:39.81ms
step:900/2330 train_time:35844ms step_avg:39.83ms
step:901/2330 train_time:35866ms step_avg:39.81ms
step:902/2330 train_time:35923ms step_avg:39.83ms
step:903/2330 train_time:35946ms step_avg:39.81ms
step:904/2330 train_time:36003ms step_avg:39.83ms
step:905/2330 train_time:36025ms step_avg:39.81ms
step:906/2330 train_time:36082ms step_avg:39.83ms
step:907/2330 train_time:36104ms step_avg:39.81ms
step:908/2330 train_time:36160ms step_avg:39.82ms
step:909/2330 train_time:36182ms step_avg:39.80ms
step:910/2330 train_time:36239ms step_avg:39.82ms
step:911/2330 train_time:36261ms step_avg:39.80ms
step:912/2330 train_time:36318ms step_avg:39.82ms
step:913/2330 train_time:36340ms step_avg:39.80ms
step:914/2330 train_time:36396ms step_avg:39.82ms
step:915/2330 train_time:36419ms step_avg:39.80ms
step:916/2330 train_time:36476ms step_avg:39.82ms
step:917/2330 train_time:36498ms step_avg:39.80ms
step:918/2330 train_time:36555ms step_avg:39.82ms
step:919/2330 train_time:36577ms step_avg:39.80ms
step:920/2330 train_time:36634ms step_avg:39.82ms
step:921/2330 train_time:36657ms step_avg:39.80ms
step:922/2330 train_time:36713ms step_avg:39.82ms
step:923/2330 train_time:36737ms step_avg:39.80ms
step:924/2330 train_time:36794ms step_avg:39.82ms
step:925/2330 train_time:36817ms step_avg:39.80ms
step:926/2330 train_time:36873ms step_avg:39.82ms
step:927/2330 train_time:36896ms step_avg:39.80ms
step:928/2330 train_time:36952ms step_avg:39.82ms
step:929/2330 train_time:36975ms step_avg:39.80ms
step:930/2330 train_time:37031ms step_avg:39.82ms
step:931/2330 train_time:37054ms step_avg:39.80ms
step:932/2330 train_time:37110ms step_avg:39.82ms
step:933/2330 train_time:37133ms step_avg:39.80ms
step:934/2330 train_time:37189ms step_avg:39.82ms
step:935/2330 train_time:37212ms step_avg:39.80ms
step:936/2330 train_time:37268ms step_avg:39.82ms
step:937/2330 train_time:37291ms step_avg:39.80ms
step:938/2330 train_time:37348ms step_avg:39.82ms
step:939/2330 train_time:37371ms step_avg:39.80ms
step:940/2330 train_time:37428ms step_avg:39.82ms
step:941/2330 train_time:37451ms step_avg:39.80ms
step:942/2330 train_time:37508ms step_avg:39.82ms
step:943/2330 train_time:37531ms step_avg:39.80ms
step:944/2330 train_time:37587ms step_avg:39.82ms
step:945/2330 train_time:37610ms step_avg:39.80ms
step:946/2330 train_time:37667ms step_avg:39.82ms
step:947/2330 train_time:37689ms step_avg:39.80ms
step:948/2330 train_time:37746ms step_avg:39.82ms
step:949/2330 train_time:37769ms step_avg:39.80ms
step:950/2330 train_time:37826ms step_avg:39.82ms
step:951/2330 train_time:37849ms step_avg:39.80ms
step:952/2330 train_time:37906ms step_avg:39.82ms
step:953/2330 train_time:37929ms step_avg:39.80ms
step:954/2330 train_time:37985ms step_avg:39.82ms
step:955/2330 train_time:38008ms step_avg:39.80ms
step:956/2330 train_time:38065ms step_avg:39.82ms
step:957/2330 train_time:38087ms step_avg:39.80ms
step:958/2330 train_time:38145ms step_avg:39.82ms
step:959/2330 train_time:38167ms step_avg:39.80ms
step:960/2330 train_time:38223ms step_avg:39.82ms
step:961/2330 train_time:38246ms step_avg:39.80ms
step:962/2330 train_time:38303ms step_avg:39.82ms
step:963/2330 train_time:38325ms step_avg:39.80ms
step:964/2330 train_time:38382ms step_avg:39.82ms
step:965/2330 train_time:38404ms step_avg:39.80ms
step:966/2330 train_time:38460ms step_avg:39.81ms
step:967/2330 train_time:38482ms step_avg:39.80ms
step:968/2330 train_time:38539ms step_avg:39.81ms
step:969/2330 train_time:38561ms step_avg:39.80ms
step:970/2330 train_time:38619ms step_avg:39.81ms
step:971/2330 train_time:38642ms step_avg:39.80ms
step:972/2330 train_time:38699ms step_avg:39.81ms
step:973/2330 train_time:38722ms step_avg:39.80ms
step:974/2330 train_time:38779ms step_avg:39.81ms
step:975/2330 train_time:38801ms step_avg:39.80ms
step:976/2330 train_time:38858ms step_avg:39.81ms
step:977/2330 train_time:38880ms step_avg:39.79ms
step:978/2330 train_time:38937ms step_avg:39.81ms
step:979/2330 train_time:38959ms step_avg:39.79ms
step:980/2330 train_time:39016ms step_avg:39.81ms
step:981/2330 train_time:39038ms step_avg:39.79ms
step:982/2330 train_time:39095ms step_avg:39.81ms
step:983/2330 train_time:39118ms step_avg:39.79ms
step:984/2330 train_time:39174ms step_avg:39.81ms
step:985/2330 train_time:39197ms step_avg:39.79ms
step:986/2330 train_time:39253ms step_avg:39.81ms
step:987/2330 train_time:39276ms step_avg:39.79ms
step:988/2330 train_time:39332ms step_avg:39.81ms
step:989/2330 train_time:39355ms step_avg:39.79ms
step:990/2330 train_time:39411ms step_avg:39.81ms
step:991/2330 train_time:39434ms step_avg:39.79ms
step:992/2330 train_time:39490ms step_avg:39.81ms
step:993/2330 train_time:39514ms step_avg:39.79ms
step:994/2330 train_time:39570ms step_avg:39.81ms
step:995/2330 train_time:39593ms step_avg:39.79ms
step:996/2330 train_time:39650ms step_avg:39.81ms
step:997/2330 train_time:39673ms step_avg:39.79ms
step:998/2330 train_time:39730ms step_avg:39.81ms
step:999/2330 train_time:39753ms step_avg:39.79ms
step:1000/2330 train_time:39809ms step_avg:39.81ms
step:1000/2330 val_loss:5.2519 train_time:39906ms step_avg:39.91ms
step:1001/2330 train_time:39918ms step_avg:39.88ms
step:1002/2330 train_time:39929ms step_avg:39.85ms
step:1003/2330 train_time:39939ms step_avg:39.82ms
step:1004/2330 train_time:39968ms step_avg:39.81ms
step:1005/2330 train_time:39990ms step_avg:39.79ms
step:1006/2330 train_time:40045ms step_avg:39.81ms
step:1007/2330 train_time:40068ms step_avg:39.79ms
step:1008/2330 train_time:40123ms step_avg:39.80ms
step:1009/2330 train_time:40145ms step_avg:39.79ms
step:1010/2330 train_time:40202ms step_avg:39.80ms
step:1011/2330 train_time:40227ms step_avg:39.79ms
step:1012/2330 train_time:40288ms step_avg:39.81ms
step:1013/2330 train_time:40314ms step_avg:39.80ms
step:1014/2330 train_time:40370ms step_avg:39.81ms
step:1015/2330 train_time:40393ms step_avg:39.80ms
step:1016/2330 train_time:40449ms step_avg:39.81ms
step:1017/2330 train_time:40473ms step_avg:39.80ms
step:1018/2330 train_time:40529ms step_avg:39.81ms
step:1019/2330 train_time:40551ms step_avg:39.80ms
step:1020/2330 train_time:40607ms step_avg:39.81ms
step:1021/2330 train_time:40630ms step_avg:39.79ms
step:1022/2330 train_time:40685ms step_avg:39.81ms
step:1023/2330 train_time:40709ms step_avg:39.79ms
step:1024/2330 train_time:40764ms step_avg:39.81ms
step:1025/2330 train_time:40787ms step_avg:39.79ms
step:1026/2330 train_time:40845ms step_avg:39.81ms
step:1027/2330 train_time:40869ms step_avg:39.79ms
step:1028/2330 train_time:40926ms step_avg:39.81ms
step:1029/2330 train_time:40949ms step_avg:39.79ms
step:1030/2330 train_time:41004ms step_avg:39.81ms
step:1031/2330 train_time:41027ms step_avg:39.79ms
step:1032/2330 train_time:41082ms step_avg:39.81ms
step:1033/2330 train_time:41105ms step_avg:39.79ms
step:1034/2330 train_time:41161ms step_avg:39.81ms
step:1035/2330 train_time:41185ms step_avg:39.79ms
step:1036/2330 train_time:41244ms step_avg:39.81ms
step:1037/2330 train_time:41268ms step_avg:39.80ms
step:1038/2330 train_time:41325ms step_avg:39.81ms
step:1039/2330 train_time:41349ms step_avg:39.80ms
step:1040/2330 train_time:41405ms step_avg:39.81ms
step:1041/2330 train_time:41429ms step_avg:39.80ms
step:1042/2330 train_time:41486ms step_avg:39.81ms
step:1043/2330 train_time:41510ms step_avg:39.80ms
step:1044/2330 train_time:41566ms step_avg:39.81ms
step:1045/2330 train_time:41589ms step_avg:39.80ms
step:1046/2330 train_time:41646ms step_avg:39.81ms
step:1047/2330 train_time:41668ms step_avg:39.80ms
step:1048/2330 train_time:41724ms step_avg:39.81ms
step:1049/2330 train_time:41747ms step_avg:39.80ms
step:1050/2330 train_time:41804ms step_avg:39.81ms
step:1051/2330 train_time:41827ms step_avg:39.80ms
step:1052/2330 train_time:41883ms step_avg:39.81ms
step:1053/2330 train_time:41906ms step_avg:39.80ms
step:1054/2330 train_time:41961ms step_avg:39.81ms
step:1055/2330 train_time:41984ms step_avg:39.79ms
step:1056/2330 train_time:42040ms step_avg:39.81ms
step:1057/2330 train_time:42062ms step_avg:39.79ms
step:1058/2330 train_time:42119ms step_avg:39.81ms
step:1059/2330 train_time:42141ms step_avg:39.79ms
step:1060/2330 train_time:42199ms step_avg:39.81ms
step:1061/2330 train_time:42222ms step_avg:39.79ms
step:1062/2330 train_time:42279ms step_avg:39.81ms
step:1063/2330 train_time:42301ms step_avg:39.79ms
step:1064/2330 train_time:42358ms step_avg:39.81ms
step:1065/2330 train_time:42381ms step_avg:39.79ms
step:1066/2330 train_time:42438ms step_avg:39.81ms
step:1067/2330 train_time:42461ms step_avg:39.79ms
step:1068/2330 train_time:42518ms step_avg:39.81ms
step:1069/2330 train_time:42540ms step_avg:39.79ms
step:1070/2330 train_time:42598ms step_avg:39.81ms
step:1071/2330 train_time:42620ms step_avg:39.79ms
step:1072/2330 train_time:42677ms step_avg:39.81ms
step:1073/2330 train_time:42700ms step_avg:39.79ms
step:1074/2330 train_time:42756ms step_avg:39.81ms
step:1075/2330 train_time:42778ms step_avg:39.79ms
step:1076/2330 train_time:42835ms step_avg:39.81ms
step:1077/2330 train_time:42857ms step_avg:39.79ms
step:1078/2330 train_time:42914ms step_avg:39.81ms
step:1079/2330 train_time:42936ms step_avg:39.79ms
step:1080/2330 train_time:42992ms step_avg:39.81ms
step:1081/2330 train_time:43014ms step_avg:39.79ms
step:1082/2330 train_time:43070ms step_avg:39.81ms
step:1083/2330 train_time:43093ms step_avg:39.79ms
step:1084/2330 train_time:43149ms step_avg:39.81ms
step:1085/2330 train_time:43173ms step_avg:39.79ms
step:1086/2330 train_time:43229ms step_avg:39.81ms
step:1087/2330 train_time:43252ms step_avg:39.79ms
step:1088/2330 train_time:43309ms step_avg:39.81ms
step:1089/2330 train_time:43332ms step_avg:39.79ms
step:1090/2330 train_time:43388ms step_avg:39.81ms
step:1091/2330 train_time:43411ms step_avg:39.79ms
step:1092/2330 train_time:43467ms step_avg:39.81ms
step:1093/2330 train_time:43490ms step_avg:39.79ms
step:1094/2330 train_time:43546ms step_avg:39.80ms
step:1095/2330 train_time:43569ms step_avg:39.79ms
step:1096/2330 train_time:43626ms step_avg:39.80ms
step:1097/2330 train_time:43649ms step_avg:39.79ms
step:1098/2330 train_time:43706ms step_avg:39.81ms
step:1099/2330 train_time:43730ms step_avg:39.79ms
step:1100/2330 train_time:43786ms step_avg:39.81ms
step:1101/2330 train_time:43810ms step_avg:39.79ms
step:1102/2330 train_time:43866ms step_avg:39.81ms
step:1103/2330 train_time:43889ms step_avg:39.79ms
step:1104/2330 train_time:43946ms step_avg:39.81ms
step:1105/2330 train_time:43968ms step_avg:39.79ms
step:1106/2330 train_time:44025ms step_avg:39.81ms
step:1107/2330 train_time:44048ms step_avg:39.79ms
step:1108/2330 train_time:44105ms step_avg:39.81ms
step:1109/2330 train_time:44128ms step_avg:39.79ms
step:1110/2330 train_time:44185ms step_avg:39.81ms
step:1111/2330 train_time:44209ms step_avg:39.79ms
step:1112/2330 train_time:44265ms step_avg:39.81ms
step:1113/2330 train_time:44289ms step_avg:39.79ms
step:1114/2330 train_time:44345ms step_avg:39.81ms
step:1115/2330 train_time:44368ms step_avg:39.79ms
step:1116/2330 train_time:44425ms step_avg:39.81ms
step:1117/2330 train_time:44449ms step_avg:39.79ms
step:1118/2330 train_time:44506ms step_avg:39.81ms
step:1119/2330 train_time:44529ms step_avg:39.79ms
step:1120/2330 train_time:44585ms step_avg:39.81ms
step:1121/2330 train_time:44608ms step_avg:39.79ms
step:1122/2330 train_time:44665ms step_avg:39.81ms
step:1123/2330 train_time:44688ms step_avg:39.79ms
step:1124/2330 train_time:44745ms step_avg:39.81ms
step:1125/2330 train_time:44768ms step_avg:39.79ms
step:1126/2330 train_time:44824ms step_avg:39.81ms
step:1127/2330 train_time:44847ms step_avg:39.79ms
step:1128/2330 train_time:44904ms step_avg:39.81ms
step:1129/2330 train_time:44927ms step_avg:39.79ms
step:1130/2330 train_time:44983ms step_avg:39.81ms
step:1131/2330 train_time:45006ms step_avg:39.79ms
step:1132/2330 train_time:45062ms step_avg:39.81ms
step:1133/2330 train_time:45085ms step_avg:39.79ms
step:1134/2330 train_time:45142ms step_avg:39.81ms
step:1135/2330 train_time:45164ms step_avg:39.79ms
step:1136/2330 train_time:45221ms step_avg:39.81ms
step:1137/2330 train_time:45243ms step_avg:39.79ms
step:1138/2330 train_time:45300ms step_avg:39.81ms
step:1139/2330 train_time:45323ms step_avg:39.79ms
step:1140/2330 train_time:45380ms step_avg:39.81ms
step:1141/2330 train_time:45403ms step_avg:39.79ms
step:1142/2330 train_time:45460ms step_avg:39.81ms
step:1143/2330 train_time:45482ms step_avg:39.79ms
step:1144/2330 train_time:45539ms step_avg:39.81ms
step:1145/2330 train_time:45561ms step_avg:39.79ms
step:1146/2330 train_time:45618ms step_avg:39.81ms
step:1147/2330 train_time:45640ms step_avg:39.79ms
step:1148/2330 train_time:45697ms step_avg:39.81ms
step:1149/2330 train_time:45719ms step_avg:39.79ms
step:1150/2330 train_time:45776ms step_avg:39.81ms
step:1151/2330 train_time:45798ms step_avg:39.79ms
step:1152/2330 train_time:45854ms step_avg:39.80ms
step:1153/2330 train_time:45877ms step_avg:39.79ms
step:1154/2330 train_time:45933ms step_avg:39.80ms
step:1155/2330 train_time:45955ms step_avg:39.79ms
step:1156/2330 train_time:46013ms step_avg:39.80ms
step:1157/2330 train_time:46034ms step_avg:39.79ms
step:1158/2330 train_time:46091ms step_avg:39.80ms
step:1159/2330 train_time:46113ms step_avg:39.79ms
step:1160/2330 train_time:46170ms step_avg:39.80ms
step:1161/2330 train_time:46193ms step_avg:39.79ms
step:1162/2330 train_time:46249ms step_avg:39.80ms
step:1163/2330 train_time:46272ms step_avg:39.79ms
step:1164/2330 train_time:46328ms step_avg:39.80ms
step:1165/2330 train_time:46351ms step_avg:39.79ms
step:1166/2330 train_time:46407ms step_avg:39.80ms
step:1167/2330 train_time:46430ms step_avg:39.79ms
step:1168/2330 train_time:46486ms step_avg:39.80ms
step:1169/2330 train_time:46509ms step_avg:39.79ms
step:1170/2330 train_time:46566ms step_avg:39.80ms
step:1171/2330 train_time:46589ms step_avg:39.79ms
step:1172/2330 train_time:46646ms step_avg:39.80ms
step:1173/2330 train_time:46669ms step_avg:39.79ms
step:1174/2330 train_time:46725ms step_avg:39.80ms
step:1175/2330 train_time:46748ms step_avg:39.79ms
step:1176/2330 train_time:46804ms step_avg:39.80ms
step:1177/2330 train_time:46827ms step_avg:39.79ms
step:1178/2330 train_time:46884ms step_avg:39.80ms
step:1179/2330 train_time:46907ms step_avg:39.79ms
step:1180/2330 train_time:46964ms step_avg:39.80ms
step:1181/2330 train_time:46987ms step_avg:39.79ms
step:1182/2330 train_time:47043ms step_avg:39.80ms
step:1183/2330 train_time:47067ms step_avg:39.79ms
step:1184/2330 train_time:47123ms step_avg:39.80ms
step:1185/2330 train_time:47146ms step_avg:39.79ms
step:1186/2330 train_time:47203ms step_avg:39.80ms
step:1187/2330 train_time:47226ms step_avg:39.79ms
step:1188/2330 train_time:47283ms step_avg:39.80ms
step:1189/2330 train_time:47306ms step_avg:39.79ms
step:1190/2330 train_time:47363ms step_avg:39.80ms
step:1191/2330 train_time:47386ms step_avg:39.79ms
step:1192/2330 train_time:47443ms step_avg:39.80ms
step:1193/2330 train_time:47466ms step_avg:39.79ms
step:1194/2330 train_time:47523ms step_avg:39.80ms
step:1195/2330 train_time:47546ms step_avg:39.79ms
step:1196/2330 train_time:47603ms step_avg:39.80ms
step:1197/2330 train_time:47626ms step_avg:39.79ms
step:1198/2330 train_time:47683ms step_avg:39.80ms
step:1199/2330 train_time:47706ms step_avg:39.79ms
step:1200/2330 train_time:47762ms step_avg:39.80ms
step:1201/2330 train_time:47784ms step_avg:39.79ms
step:1202/2330 train_time:47841ms step_avg:39.80ms
step:1203/2330 train_time:47863ms step_avg:39.79ms
step:1204/2330 train_time:47920ms step_avg:39.80ms
step:1205/2330 train_time:47943ms step_avg:39.79ms
step:1206/2330 train_time:47999ms step_avg:39.80ms
step:1207/2330 train_time:48023ms step_avg:39.79ms
step:1208/2330 train_time:48080ms step_avg:39.80ms
step:1209/2330 train_time:48102ms step_avg:39.79ms
step:1210/2330 train_time:48159ms step_avg:39.80ms
step:1211/2330 train_time:48182ms step_avg:39.79ms
step:1212/2330 train_time:48239ms step_avg:39.80ms
step:1213/2330 train_time:48261ms step_avg:39.79ms
step:1214/2330 train_time:48317ms step_avg:39.80ms
step:1215/2330 train_time:48339ms step_avg:39.79ms
step:1216/2330 train_time:48397ms step_avg:39.80ms
step:1217/2330 train_time:48419ms step_avg:39.79ms
step:1218/2330 train_time:48476ms step_avg:39.80ms
step:1219/2330 train_time:48498ms step_avg:39.79ms
step:1220/2330 train_time:48555ms step_avg:39.80ms
step:1221/2330 train_time:48577ms step_avg:39.78ms
step:1222/2330 train_time:48634ms step_avg:39.80ms
step:1223/2330 train_time:48656ms step_avg:39.78ms
step:1224/2330 train_time:48713ms step_avg:39.80ms
step:1225/2330 train_time:48735ms step_avg:39.78ms
step:1226/2330 train_time:48792ms step_avg:39.80ms
step:1227/2330 train_time:48813ms step_avg:39.78ms
step:1228/2330 train_time:48870ms step_avg:39.80ms
step:1229/2330 train_time:48892ms step_avg:39.78ms
step:1230/2330 train_time:48949ms step_avg:39.80ms
step:1231/2330 train_time:48972ms step_avg:39.78ms
step:1232/2330 train_time:49028ms step_avg:39.80ms
step:1233/2330 train_time:49051ms step_avg:39.78ms
step:1234/2330 train_time:49106ms step_avg:39.79ms
step:1235/2330 train_time:49129ms step_avg:39.78ms
step:1236/2330 train_time:49186ms step_avg:39.79ms
step:1237/2330 train_time:49209ms step_avg:39.78ms
step:1238/2330 train_time:49265ms step_avg:39.79ms
step:1239/2330 train_time:49288ms step_avg:39.78ms
step:1240/2330 train_time:49345ms step_avg:39.79ms
step:1241/2330 train_time:49368ms step_avg:39.78ms
step:1242/2330 train_time:49424ms step_avg:39.79ms
step:1243/2330 train_time:49448ms step_avg:39.78ms
step:1244/2330 train_time:49505ms step_avg:39.80ms
step:1245/2330 train_time:49528ms step_avg:39.78ms
step:1246/2330 train_time:49585ms step_avg:39.80ms
step:1247/2330 train_time:49608ms step_avg:39.78ms
step:1248/2330 train_time:49665ms step_avg:39.80ms
step:1249/2330 train_time:49688ms step_avg:39.78ms
step:1250/2330 train_time:49744ms step_avg:39.80ms
step:1250/2330 val_loss:5.2089 train_time:49842ms step_avg:39.87ms
step:1251/2330 train_time:49854ms step_avg:39.85ms
step:1252/2330 train_time:49865ms step_avg:39.83ms
step:1253/2330 train_time:49874ms step_avg:39.80ms
step:1254/2330 train_time:49905ms step_avg:39.80ms
step:1255/2330 train_time:49927ms step_avg:39.78ms
step:1256/2330 train_time:49983ms step_avg:39.80ms
step:1257/2330 train_time:50006ms step_avg:39.78ms
step:1258/2330 train_time:50062ms step_avg:39.79ms
step:1259/2330 train_time:50084ms step_avg:39.78ms
step:1260/2330 train_time:50142ms step_avg:39.80ms
step:1261/2330 train_time:50167ms step_avg:39.78ms
step:1262/2330 train_time:50227ms step_avg:39.80ms
step:1263/2330 train_time:50251ms step_avg:39.79ms
step:1264/2330 train_time:50308ms step_avg:39.80ms
step:1265/2330 train_time:50331ms step_avg:39.79ms
step:1266/2330 train_time:50387ms step_avg:39.80ms
step:1267/2330 train_time:50410ms step_avg:39.79ms
step:1268/2330 train_time:50466ms step_avg:39.80ms
step:1269/2330 train_time:50488ms step_avg:39.79ms
step:1270/2330 train_time:50545ms step_avg:39.80ms
step:1271/2330 train_time:50567ms step_avg:39.79ms
step:1272/2330 train_time:50623ms step_avg:39.80ms
step:1273/2330 train_time:50647ms step_avg:39.79ms
step:1274/2330 train_time:50703ms step_avg:39.80ms
step:1275/2330 train_time:50725ms step_avg:39.78ms
step:1276/2330 train_time:50783ms step_avg:39.80ms
step:1277/2330 train_time:50806ms step_avg:39.79ms
step:1278/2330 train_time:50863ms step_avg:39.80ms
step:1279/2330 train_time:50886ms step_avg:39.79ms
step:1280/2330 train_time:50942ms step_avg:39.80ms
step:1281/2330 train_time:50964ms step_avg:39.78ms
step:1282/2330 train_time:51020ms step_avg:39.80ms
step:1283/2330 train_time:51043ms step_avg:39.78ms
step:1284/2330 train_time:51101ms step_avg:39.80ms
step:1285/2330 train_time:51124ms step_avg:39.79ms
step:1286/2330 train_time:51182ms step_avg:39.80ms
step:1287/2330 train_time:51205ms step_avg:39.79ms
step:1288/2330 train_time:51263ms step_avg:39.80ms
step:1289/2330 train_time:51286ms step_avg:39.79ms
step:1290/2330 train_time:51344ms step_avg:39.80ms
step:1291/2330 train_time:51366ms step_avg:39.79ms
step:1292/2330 train_time:51422ms step_avg:39.80ms
step:1293/2330 train_time:51444ms step_avg:39.79ms
step:1294/2330 train_time:51501ms step_avg:39.80ms
step:1295/2330 train_time:51523ms step_avg:39.79ms
step:1296/2330 train_time:51579ms step_avg:39.80ms
step:1297/2330 train_time:51601ms step_avg:39.78ms
step:1298/2330 train_time:51657ms step_avg:39.80ms
step:1299/2330 train_time:51679ms step_avg:39.78ms
step:1300/2330 train_time:51735ms step_avg:39.80ms
step:1301/2330 train_time:51757ms step_avg:39.78ms
step:1302/2330 train_time:51814ms step_avg:39.80ms
step:1303/2330 train_time:51836ms step_avg:39.78ms
step:1304/2330 train_time:51893ms step_avg:39.80ms
step:1305/2330 train_time:51915ms step_avg:39.78ms
step:1306/2330 train_time:51971ms step_avg:39.79ms
step:1307/2330 train_time:51993ms step_avg:39.78ms
step:1308/2330 train_time:52049ms step_avg:39.79ms
step:1309/2330 train_time:52072ms step_avg:39.78ms
step:1310/2330 train_time:52128ms step_avg:39.79ms
step:1311/2330 train_time:52152ms step_avg:39.78ms
step:1312/2330 train_time:52209ms step_avg:39.79ms
step:1313/2330 train_time:52233ms step_avg:39.78ms
step:1314/2330 train_time:52289ms step_avg:39.79ms
step:1315/2330 train_time:52312ms step_avg:39.78ms
step:1316/2330 train_time:52368ms step_avg:39.79ms
step:1317/2330 train_time:52391ms step_avg:39.78ms
step:1318/2330 train_time:52447ms step_avg:39.79ms
step:1319/2330 train_time:52470ms step_avg:39.78ms
step:1320/2330 train_time:52527ms step_avg:39.79ms
step:1321/2330 train_time:52550ms step_avg:39.78ms
step:1322/2330 train_time:52606ms step_avg:39.79ms
step:1323/2330 train_time:52628ms step_avg:39.78ms
step:1324/2330 train_time:52685ms step_avg:39.79ms
step:1325/2330 train_time:52708ms step_avg:39.78ms
step:1326/2330 train_time:52765ms step_avg:39.79ms
step:1327/2330 train_time:52787ms step_avg:39.78ms
step:1328/2330 train_time:52844ms step_avg:39.79ms
step:1329/2330 train_time:52866ms step_avg:39.78ms
step:1330/2330 train_time:52923ms step_avg:39.79ms
step:1331/2330 train_time:52947ms step_avg:39.78ms
step:1332/2330 train_time:53004ms step_avg:39.79ms
step:1333/2330 train_time:53027ms step_avg:39.78ms
step:1334/2330 train_time:53084ms step_avg:39.79ms
step:1335/2330 train_time:53107ms step_avg:39.78ms
step:1336/2330 train_time:53164ms step_avg:39.79ms
step:1337/2330 train_time:53188ms step_avg:39.78ms
step:1338/2330 train_time:53245ms step_avg:39.79ms
step:1339/2330 train_time:53268ms step_avg:39.78ms
step:1340/2330 train_time:53324ms step_avg:39.79ms
step:1341/2330 train_time:53347ms step_avg:39.78ms
step:1342/2330 train_time:53403ms step_avg:39.79ms
step:1343/2330 train_time:53425ms step_avg:39.78ms
step:1344/2330 train_time:53482ms step_avg:39.79ms
step:1345/2330 train_time:53505ms step_avg:39.78ms
step:1346/2330 train_time:53561ms step_avg:39.79ms
step:1347/2330 train_time:53584ms step_avg:39.78ms
step:1348/2330 train_time:53641ms step_avg:39.79ms
step:1349/2330 train_time:53663ms step_avg:39.78ms
step:1350/2330 train_time:53719ms step_avg:39.79ms
step:1351/2330 train_time:53742ms step_avg:39.78ms
step:1352/2330 train_time:53798ms step_avg:39.79ms
step:1353/2330 train_time:53820ms step_avg:39.78ms
step:1354/2330 train_time:53876ms step_avg:39.79ms
step:1355/2330 train_time:53899ms step_avg:39.78ms
step:1356/2330 train_time:53955ms step_avg:39.79ms
step:1357/2330 train_time:53977ms step_avg:39.78ms
step:1358/2330 train_time:54034ms step_avg:39.79ms
step:1359/2330 train_time:54056ms step_avg:39.78ms
step:1360/2330 train_time:54113ms step_avg:39.79ms
step:1361/2330 train_time:54136ms step_avg:39.78ms
step:1362/2330 train_time:54192ms step_avg:39.79ms
step:1363/2330 train_time:54215ms step_avg:39.78ms
step:1364/2330 train_time:54272ms step_avg:39.79ms
step:1365/2330 train_time:54294ms step_avg:39.78ms
step:1366/2330 train_time:54351ms step_avg:39.79ms
step:1367/2330 train_time:54374ms step_avg:39.78ms
step:1368/2330 train_time:54431ms step_avg:39.79ms
step:1369/2330 train_time:54453ms step_avg:39.78ms
step:1370/2330 train_time:54510ms step_avg:39.79ms
step:1371/2330 train_time:54533ms step_avg:39.78ms
step:1372/2330 train_time:54589ms step_avg:39.79ms
step:1373/2330 train_time:54612ms step_avg:39.78ms
step:1374/2330 train_time:54668ms step_avg:39.79ms
step:1375/2330 train_time:54691ms step_avg:39.78ms
step:1376/2330 train_time:54747ms step_avg:39.79ms
step:1377/2330 train_time:54770ms step_avg:39.77ms
step:1378/2330 train_time:54827ms step_avg:39.79ms
step:1379/2330 train_time:54849ms step_avg:39.77ms
step:1380/2330 train_time:54906ms step_avg:39.79ms
step:1381/2330 train_time:54929ms step_avg:39.77ms
step:1382/2330 train_time:54985ms step_avg:39.79ms
step:1383/2330 train_time:55009ms step_avg:39.77ms
step:1384/2330 train_time:55065ms step_avg:39.79ms
step:1385/2330 train_time:55088ms step_avg:39.77ms
step:1386/2330 train_time:55145ms step_avg:39.79ms
step:1387/2330 train_time:55168ms step_avg:39.77ms
step:1388/2330 train_time:55225ms step_avg:39.79ms
step:1389/2330 train_time:55248ms step_avg:39.78ms
step:1390/2330 train_time:55304ms step_avg:39.79ms
step:1391/2330 train_time:55327ms step_avg:39.77ms
step:1392/2330 train_time:55383ms step_avg:39.79ms
step:1393/2330 train_time:55407ms step_avg:39.78ms
step:1394/2330 train_time:55464ms step_avg:39.79ms
step:1395/2330 train_time:55487ms step_avg:39.78ms
step:1396/2330 train_time:55543ms step_avg:39.79ms
step:1397/2330 train_time:55566ms step_avg:39.77ms
step:1398/2330 train_time:55622ms step_avg:39.79ms
step:1399/2330 train_time:55645ms step_avg:39.77ms
step:1400/2330 train_time:55702ms step_avg:39.79ms
step:1401/2330 train_time:55725ms step_avg:39.77ms
step:1402/2330 train_time:55781ms step_avg:39.79ms
step:1403/2330 train_time:55805ms step_avg:39.78ms
step:1404/2330 train_time:55862ms step_avg:39.79ms
step:1405/2330 train_time:55885ms step_avg:39.78ms
step:1406/2330 train_time:55942ms step_avg:39.79ms
step:1407/2330 train_time:55963ms step_avg:39.78ms
step:1408/2330 train_time:56020ms step_avg:39.79ms
step:1409/2330 train_time:56042ms step_avg:39.77ms
step:1410/2330 train_time:56098ms step_avg:39.79ms
step:1411/2330 train_time:56120ms step_avg:39.77ms
step:1412/2330 train_time:56177ms step_avg:39.79ms
step:1413/2330 train_time:56199ms step_avg:39.77ms
step:1414/2330 train_time:56256ms step_avg:39.79ms
step:1415/2330 train_time:56279ms step_avg:39.77ms
step:1416/2330 train_time:56336ms step_avg:39.79ms
step:1417/2330 train_time:56358ms step_avg:39.77ms
step:1418/2330 train_time:56415ms step_avg:39.79ms
step:1419/2330 train_time:56437ms step_avg:39.77ms
step:1420/2330 train_time:56495ms step_avg:39.78ms
step:1421/2330 train_time:56517ms step_avg:39.77ms
step:1422/2330 train_time:56573ms step_avg:39.78ms
step:1423/2330 train_time:56595ms step_avg:39.77ms
step:1424/2330 train_time:56652ms step_avg:39.78ms
step:1425/2330 train_time:56675ms step_avg:39.77ms
step:1426/2330 train_time:56731ms step_avg:39.78ms
step:1427/2330 train_time:56754ms step_avg:39.77ms
step:1428/2330 train_time:56811ms step_avg:39.78ms
step:1429/2330 train_time:56834ms step_avg:39.77ms
step:1430/2330 train_time:56890ms step_avg:39.78ms
step:1431/2330 train_time:56913ms step_avg:39.77ms
step:1432/2330 train_time:56969ms step_avg:39.78ms
step:1433/2330 train_time:56992ms step_avg:39.77ms
step:1434/2330 train_time:57048ms step_avg:39.78ms
step:1435/2330 train_time:57071ms step_avg:39.77ms
step:1436/2330 train_time:57127ms step_avg:39.78ms
step:1437/2330 train_time:57150ms step_avg:39.77ms
step:1438/2330 train_time:57206ms step_avg:39.78ms
step:1439/2330 train_time:57229ms step_avg:39.77ms
step:1440/2330 train_time:57285ms step_avg:39.78ms
step:1441/2330 train_time:57308ms step_avg:39.77ms
step:1442/2330 train_time:57365ms step_avg:39.78ms
step:1443/2330 train_time:57389ms step_avg:39.77ms
step:1444/2330 train_time:57446ms step_avg:39.78ms
step:1445/2330 train_time:57469ms step_avg:39.77ms
step:1446/2330 train_time:57525ms step_avg:39.78ms
step:1447/2330 train_time:57548ms step_avg:39.77ms
step:1448/2330 train_time:57605ms step_avg:39.78ms
step:1449/2330 train_time:57627ms step_avg:39.77ms
step:1450/2330 train_time:57684ms step_avg:39.78ms
step:1451/2330 train_time:57707ms step_avg:39.77ms
step:1452/2330 train_time:57763ms step_avg:39.78ms
step:1453/2330 train_time:57785ms step_avg:39.77ms
step:1454/2330 train_time:57841ms step_avg:39.78ms
step:1455/2330 train_time:57864ms step_avg:39.77ms
step:1456/2330 train_time:57920ms step_avg:39.78ms
step:1457/2330 train_time:57943ms step_avg:39.77ms
step:1458/2330 train_time:58000ms step_avg:39.78ms
step:1459/2330 train_time:58022ms step_avg:39.77ms
step:1460/2330 train_time:58079ms step_avg:39.78ms
step:1461/2330 train_time:58101ms step_avg:39.77ms
step:1462/2330 train_time:58158ms step_avg:39.78ms
step:1463/2330 train_time:58180ms step_avg:39.77ms
step:1464/2330 train_time:58237ms step_avg:39.78ms
step:1465/2330 train_time:58259ms step_avg:39.77ms
step:1466/2330 train_time:58316ms step_avg:39.78ms
step:1467/2330 train_time:58338ms step_avg:39.77ms
step:1468/2330 train_time:58395ms step_avg:39.78ms
step:1469/2330 train_time:58417ms step_avg:39.77ms
step:1470/2330 train_time:58474ms step_avg:39.78ms
step:1471/2330 train_time:58497ms step_avg:39.77ms
step:1472/2330 train_time:58553ms step_avg:39.78ms
step:1473/2330 train_time:58576ms step_avg:39.77ms
step:1474/2330 train_time:58633ms step_avg:39.78ms
step:1475/2330 train_time:58655ms step_avg:39.77ms
step:1476/2330 train_time:58712ms step_avg:39.78ms
step:1477/2330 train_time:58734ms step_avg:39.77ms
step:1478/2330 train_time:58791ms step_avg:39.78ms
step:1479/2330 train_time:58813ms step_avg:39.77ms
step:1480/2330 train_time:58869ms step_avg:39.78ms
step:1481/2330 train_time:58893ms step_avg:39.77ms
step:1482/2330 train_time:58948ms step_avg:39.78ms
step:1483/2330 train_time:58972ms step_avg:39.77ms
step:1484/2330 train_time:59028ms step_avg:39.78ms
step:1485/2330 train_time:59051ms step_avg:39.76ms
step:1486/2330 train_time:59108ms step_avg:39.78ms
step:1487/2330 train_time:59130ms step_avg:39.76ms
step:1488/2330 train_time:59187ms step_avg:39.78ms
step:1489/2330 train_time:59209ms step_avg:39.76ms
step:1490/2330 train_time:59266ms step_avg:39.78ms
step:1491/2330 train_time:59289ms step_avg:39.76ms
step:1492/2330 train_time:59346ms step_avg:39.78ms
step:1493/2330 train_time:59369ms step_avg:39.76ms
step:1494/2330 train_time:59426ms step_avg:39.78ms
step:1495/2330 train_time:59449ms step_avg:39.77ms
step:1496/2330 train_time:59506ms step_avg:39.78ms
step:1497/2330 train_time:59529ms step_avg:39.77ms
step:1498/2330 train_time:59585ms step_avg:39.78ms
step:1499/2330 train_time:59608ms step_avg:39.77ms
step:1500/2330 train_time:59665ms step_avg:39.78ms
step:1500/2330 val_loss:5.1837 train_time:59762ms step_avg:39.84ms
step:1501/2330 train_time:59774ms step_avg:39.82ms
step:1502/2330 train_time:59785ms step_avg:39.80ms
step:1503/2330 train_time:59794ms step_avg:39.78ms
step:1504/2330 train_time:59826ms step_avg:39.78ms
step:1505/2330 train_time:59847ms step_avg:39.77ms
step:1506/2330 train_time:59903ms step_avg:39.78ms
step:1507/2330 train_time:59924ms step_avg:39.76ms
step:1508/2330 train_time:59980ms step_avg:39.77ms
step:1509/2330 train_time:60002ms step_avg:39.76ms
step:1510/2330 train_time:60058ms step_avg:39.77ms
step:1511/2330 train_time:60082ms step_avg:39.76ms
step:1512/2330 train_time:60142ms step_avg:39.78ms
step:1513/2330 train_time:60165ms step_avg:39.77ms
step:1514/2330 train_time:60222ms step_avg:39.78ms
step:1515/2330 train_time:60244ms step_avg:39.76ms
step:1516/2330 train_time:60300ms step_avg:39.78ms
step:1517/2330 train_time:60322ms step_avg:39.76ms
step:1518/2330 train_time:60379ms step_avg:39.78ms
step:1519/2330 train_time:60401ms step_avg:39.76ms
step:1520/2330 train_time:60458ms step_avg:39.77ms
step:1521/2330 train_time:60480ms step_avg:39.76ms
step:1522/2330 train_time:60536ms step_avg:39.77ms
step:1523/2330 train_time:60557ms step_avg:39.76ms
step:1524/2330 train_time:60614ms step_avg:39.77ms
step:1525/2330 train_time:60636ms step_avg:39.76ms
step:1526/2330 train_time:60694ms step_avg:39.77ms
step:1527/2330 train_time:60717ms step_avg:39.76ms
step:1528/2330 train_time:60776ms step_avg:39.77ms
step:1529/2330 train_time:60799ms step_avg:39.76ms
step:1530/2330 train_time:60857ms step_avg:39.78ms
step:1531/2330 train_time:60877ms step_avg:39.76ms
step:1532/2330 train_time:60934ms step_avg:39.77ms
step:1533/2330 train_time:60957ms step_avg:39.76ms
step:1534/2330 train_time:61013ms step_avg:39.77ms
step:1535/2330 train_time:61036ms step_avg:39.76ms
step:1536/2330 train_time:61092ms step_avg:39.77ms
step:1537/2330 train_time:61116ms step_avg:39.76ms
step:1538/2330 train_time:61172ms step_avg:39.77ms
step:1539/2330 train_time:61195ms step_avg:39.76ms
step:1540/2330 train_time:61251ms step_avg:39.77ms
step:1541/2330 train_time:61274ms step_avg:39.76ms
step:1542/2330 train_time:61330ms step_avg:39.77ms
step:1543/2330 train_time:61354ms step_avg:39.76ms
step:1544/2330 train_time:61410ms step_avg:39.77ms
step:1545/2330 train_time:61433ms step_avg:39.76ms
step:1546/2330 train_time:61489ms step_avg:39.77ms
step:1547/2330 train_time:61512ms step_avg:39.76ms
step:1548/2330 train_time:61567ms step_avg:39.77ms
step:1549/2330 train_time:61590ms step_avg:39.76ms
step:1550/2330 train_time:61647ms step_avg:39.77ms
step:1551/2330 train_time:61670ms step_avg:39.76ms
step:1552/2330 train_time:61727ms step_avg:39.77ms
step:1553/2330 train_time:61750ms step_avg:39.76ms
step:1554/2330 train_time:61807ms step_avg:39.77ms
step:1555/2330 train_time:61830ms step_avg:39.76ms
step:1556/2330 train_time:61886ms step_avg:39.77ms
step:1557/2330 train_time:61909ms step_avg:39.76ms
step:1558/2330 train_time:61966ms step_avg:39.77ms
step:1559/2330 train_time:61989ms step_avg:39.76ms
step:1560/2330 train_time:62046ms step_avg:39.77ms
step:1561/2330 train_time:62069ms step_avg:39.76ms
step:1562/2330 train_time:62126ms step_avg:39.77ms
step:1563/2330 train_time:62149ms step_avg:39.76ms
step:1564/2330 train_time:62206ms step_avg:39.77ms
step:1565/2330 train_time:62228ms step_avg:39.76ms
step:1566/2330 train_time:62285ms step_avg:39.77ms
step:1567/2330 train_time:62307ms step_avg:39.76ms
step:1568/2330 train_time:62364ms step_avg:39.77ms
step:1569/2330 train_time:62386ms step_avg:39.76ms
step:1570/2330 train_time:62443ms step_avg:39.77ms
step:1571/2330 train_time:62465ms step_avg:39.76ms
step:1572/2330 train_time:62521ms step_avg:39.77ms
step:1573/2330 train_time:62544ms step_avg:39.76ms
step:1574/2330 train_time:62600ms step_avg:39.77ms
step:1575/2330 train_time:62622ms step_avg:39.76ms
step:1576/2330 train_time:62679ms step_avg:39.77ms
step:1577/2330 train_time:62702ms step_avg:39.76ms
step:1578/2330 train_time:62759ms step_avg:39.77ms
step:1579/2330 train_time:62780ms step_avg:39.76ms
step:1580/2330 train_time:62837ms step_avg:39.77ms
step:1581/2330 train_time:62860ms step_avg:39.76ms
step:1582/2330 train_time:62917ms step_avg:39.77ms
step:1583/2330 train_time:62939ms step_avg:39.76ms
step:1584/2330 train_time:62996ms step_avg:39.77ms
step:1585/2330 train_time:63018ms step_avg:39.76ms
step:1586/2330 train_time:63075ms step_avg:39.77ms
step:1587/2330 train_time:63097ms step_avg:39.76ms
step:1588/2330 train_time:63155ms step_avg:39.77ms
step:1589/2330 train_time:63177ms step_avg:39.76ms
step:1590/2330 train_time:63234ms step_avg:39.77ms
step:1591/2330 train_time:63257ms step_avg:39.76ms
step:1592/2330 train_time:63314ms step_avg:39.77ms
step:1593/2330 train_time:63336ms step_avg:39.76ms
step:1594/2330 train_time:63393ms step_avg:39.77ms
step:1595/2330 train_time:63416ms step_avg:39.76ms
step:1596/2330 train_time:63472ms step_avg:39.77ms
step:1597/2330 train_time:63495ms step_avg:39.76ms
step:1598/2330 train_time:63550ms step_avg:39.77ms
step:1599/2330 train_time:63574ms step_avg:39.76ms
step:1600/2330 train_time:63630ms step_avg:39.77ms
step:1601/2330 train_time:63653ms step_avg:39.76ms
step:1602/2330 train_time:63709ms step_avg:39.77ms
step:1603/2330 train_time:63731ms step_avg:39.76ms
step:1604/2330 train_time:63789ms step_avg:39.77ms
step:1605/2330 train_time:63812ms step_avg:39.76ms
step:1606/2330 train_time:63869ms step_avg:39.77ms
step:1607/2330 train_time:63891ms step_avg:39.76ms
step:1608/2330 train_time:63948ms step_avg:39.77ms
step:1609/2330 train_time:63970ms step_avg:39.76ms
step:1610/2330 train_time:64027ms step_avg:39.77ms
step:1611/2330 train_time:64049ms step_avg:39.76ms
step:1612/2330 train_time:64106ms step_avg:39.77ms
step:1613/2330 train_time:64128ms step_avg:39.76ms
step:1614/2330 train_time:64185ms step_avg:39.77ms
step:1615/2330 train_time:64207ms step_avg:39.76ms
step:1616/2330 train_time:64265ms step_avg:39.77ms
step:1617/2330 train_time:64287ms step_avg:39.76ms
step:1618/2330 train_time:64343ms step_avg:39.77ms
step:1619/2330 train_time:64366ms step_avg:39.76ms
step:1620/2330 train_time:64422ms step_avg:39.77ms
step:1621/2330 train_time:64445ms step_avg:39.76ms
step:1622/2330 train_time:64502ms step_avg:39.77ms
step:1623/2330 train_time:64524ms step_avg:39.76ms
step:1624/2330 train_time:64581ms step_avg:39.77ms
step:1625/2330 train_time:64603ms step_avg:39.76ms
step:1626/2330 train_time:64659ms step_avg:39.77ms
step:1627/2330 train_time:64682ms step_avg:39.76ms
step:1628/2330 train_time:64740ms step_avg:39.77ms
step:1629/2330 train_time:64762ms step_avg:39.76ms
step:1630/2330 train_time:64818ms step_avg:39.77ms
step:1631/2330 train_time:64841ms step_avg:39.76ms
step:1632/2330 train_time:64897ms step_avg:39.77ms
step:1633/2330 train_time:64920ms step_avg:39.76ms
step:1634/2330 train_time:64976ms step_avg:39.77ms
step:1635/2330 train_time:64999ms step_avg:39.75ms
step:1636/2330 train_time:65055ms step_avg:39.76ms
step:1637/2330 train_time:65078ms step_avg:39.75ms
step:1638/2330 train_time:65134ms step_avg:39.76ms
step:1639/2330 train_time:65157ms step_avg:39.75ms
step:1640/2330 train_time:65215ms step_avg:39.76ms
step:1641/2330 train_time:65237ms step_avg:39.75ms
step:1642/2330 train_time:65293ms step_avg:39.76ms
step:1643/2330 train_time:65316ms step_avg:39.75ms
step:1644/2330 train_time:65372ms step_avg:39.76ms
step:1645/2330 train_time:65395ms step_avg:39.75ms
step:1646/2330 train_time:65451ms step_avg:39.76ms
step:1647/2330 train_time:65474ms step_avg:39.75ms
step:1648/2330 train_time:65530ms step_avg:39.76ms
step:1649/2330 train_time:65553ms step_avg:39.75ms
step:1650/2330 train_time:65609ms step_avg:39.76ms
step:1651/2330 train_time:65633ms step_avg:39.75ms
step:1652/2330 train_time:65689ms step_avg:39.76ms
step:1653/2330 train_time:65712ms step_avg:39.75ms
step:1654/2330 train_time:65769ms step_avg:39.76ms
step:1655/2330 train_time:65792ms step_avg:39.75ms
step:1656/2330 train_time:65848ms step_avg:39.76ms
step:1657/2330 train_time:65870ms step_avg:39.75ms
step:1658/2330 train_time:65926ms step_avg:39.76ms
step:1659/2330 train_time:65949ms step_avg:39.75ms
step:1660/2330 train_time:66005ms step_avg:39.76ms
step:1661/2330 train_time:66027ms step_avg:39.75ms
step:1662/2330 train_time:66084ms step_avg:39.76ms
step:1663/2330 train_time:66107ms step_avg:39.75ms
step:1664/2330 train_time:66163ms step_avg:39.76ms
step:1665/2330 train_time:66186ms step_avg:39.75ms
step:1666/2330 train_time:66242ms step_avg:39.76ms
step:1667/2330 train_time:66264ms step_avg:39.75ms
step:1668/2330 train_time:66322ms step_avg:39.76ms
step:1669/2330 train_time:66344ms step_avg:39.75ms
step:1670/2330 train_time:66401ms step_avg:39.76ms
step:1671/2330 train_time:66424ms step_avg:39.75ms
step:1672/2330 train_time:66481ms step_avg:39.76ms
step:1673/2330 train_time:66503ms step_avg:39.75ms
step:1674/2330 train_time:66560ms step_avg:39.76ms
step:1675/2330 train_time:66582ms step_avg:39.75ms
step:1676/2330 train_time:66639ms step_avg:39.76ms
step:1677/2330 train_time:66661ms step_avg:39.75ms
step:1678/2330 train_time:66718ms step_avg:39.76ms
step:1679/2330 train_time:66740ms step_avg:39.75ms
step:1680/2330 train_time:66797ms step_avg:39.76ms
step:1681/2330 train_time:66819ms step_avg:39.75ms
step:1682/2330 train_time:66875ms step_avg:39.76ms
step:1683/2330 train_time:66898ms step_avg:39.75ms
step:1684/2330 train_time:66954ms step_avg:39.76ms
step:1685/2330 train_time:66977ms step_avg:39.75ms
step:1686/2330 train_time:67033ms step_avg:39.76ms
step:1687/2330 train_time:67056ms step_avg:39.75ms
step:1688/2330 train_time:67113ms step_avg:39.76ms
step:1689/2330 train_time:67135ms step_avg:39.75ms
step:1690/2330 train_time:67191ms step_avg:39.76ms
step:1691/2330 train_time:67215ms step_avg:39.75ms
step:1692/2330 train_time:67271ms step_avg:39.76ms
step:1693/2330 train_time:67294ms step_avg:39.75ms
step:1694/2330 train_time:67350ms step_avg:39.76ms
step:1695/2330 train_time:67373ms step_avg:39.75ms
step:1696/2330 train_time:67430ms step_avg:39.76ms
step:1697/2330 train_time:67453ms step_avg:39.75ms
step:1698/2330 train_time:67509ms step_avg:39.76ms
step:1699/2330 train_time:67532ms step_avg:39.75ms
step:1700/2330 train_time:67589ms step_avg:39.76ms
step:1701/2330 train_time:67612ms step_avg:39.75ms
step:1702/2330 train_time:67669ms step_avg:39.76ms
step:1703/2330 train_time:67692ms step_avg:39.75ms
step:1704/2330 train_time:67748ms step_avg:39.76ms
step:1705/2330 train_time:67771ms step_avg:39.75ms
step:1706/2330 train_time:67827ms step_avg:39.76ms
step:1707/2330 train_time:67850ms step_avg:39.75ms
step:1708/2330 train_time:67906ms step_avg:39.76ms
step:1709/2330 train_time:67929ms step_avg:39.75ms
step:1710/2330 train_time:67985ms step_avg:39.76ms
step:1711/2330 train_time:68009ms step_avg:39.75ms
step:1712/2330 train_time:68065ms step_avg:39.76ms
step:1713/2330 train_time:68088ms step_avg:39.75ms
step:1714/2330 train_time:68144ms step_avg:39.76ms
step:1715/2330 train_time:68166ms step_avg:39.75ms
step:1716/2330 train_time:68224ms step_avg:39.76ms
step:1717/2330 train_time:68246ms step_avg:39.75ms
step:1718/2330 train_time:68303ms step_avg:39.76ms
step:1719/2330 train_time:68325ms step_avg:39.75ms
step:1720/2330 train_time:68382ms step_avg:39.76ms
step:1721/2330 train_time:68404ms step_avg:39.75ms
step:1722/2330 train_time:68461ms step_avg:39.76ms
step:1723/2330 train_time:68483ms step_avg:39.75ms
step:1724/2330 train_time:68540ms step_avg:39.76ms
step:1725/2330 train_time:68562ms step_avg:39.75ms
step:1726/2330 train_time:68619ms step_avg:39.76ms
step:1727/2330 train_time:68642ms step_avg:39.75ms
step:1728/2330 train_time:68698ms step_avg:39.76ms
step:1729/2330 train_time:68720ms step_avg:39.75ms
step:1730/2330 train_time:68777ms step_avg:39.76ms
step:1731/2330 train_time:68800ms step_avg:39.75ms
step:1732/2330 train_time:68856ms step_avg:39.76ms
step:1733/2330 train_time:68878ms step_avg:39.75ms
step:1734/2330 train_time:68935ms step_avg:39.75ms
step:1735/2330 train_time:68957ms step_avg:39.74ms
step:1736/2330 train_time:69013ms step_avg:39.75ms
step:1737/2330 train_time:69036ms step_avg:39.74ms
step:1738/2330 train_time:69092ms step_avg:39.75ms
step:1739/2330 train_time:69115ms step_avg:39.74ms
step:1740/2330 train_time:69172ms step_avg:39.75ms
step:1741/2330 train_time:69195ms step_avg:39.74ms
step:1742/2330 train_time:69251ms step_avg:39.75ms
step:1743/2330 train_time:69274ms step_avg:39.74ms
step:1744/2330 train_time:69330ms step_avg:39.75ms
step:1745/2330 train_time:69354ms step_avg:39.74ms
step:1746/2330 train_time:69410ms step_avg:39.75ms
step:1747/2330 train_time:69433ms step_avg:39.74ms
step:1748/2330 train_time:69489ms step_avg:39.75ms
step:1749/2330 train_time:69512ms step_avg:39.74ms
step:1750/2330 train_time:69569ms step_avg:39.75ms
step:1750/2330 val_loss:5.1321 train_time:69666ms step_avg:39.81ms
step:1751/2330 train_time:69678ms step_avg:39.79ms
step:1752/2330 train_time:69689ms step_avg:39.78ms
step:1753/2330 train_time:69699ms step_avg:39.76ms
step:1754/2330 train_time:69729ms step_avg:39.75ms
step:1755/2330 train_time:69750ms step_avg:39.74ms
step:1756/2330 train_time:69805ms step_avg:39.75ms
step:1757/2330 train_time:69827ms step_avg:39.74ms
step:1758/2330 train_time:69882ms step_avg:39.75ms
step:1759/2330 train_time:69904ms step_avg:39.74ms
step:1760/2330 train_time:69960ms step_avg:39.75ms
step:1761/2330 train_time:69987ms step_avg:39.74ms
step:1762/2330 train_time:70045ms step_avg:39.75ms
step:1763/2330 train_time:70069ms step_avg:39.74ms
step:1764/2330 train_time:70126ms step_avg:39.75ms
step:1765/2330 train_time:70149ms step_avg:39.74ms
step:1766/2330 train_time:70206ms step_avg:39.75ms
step:1767/2330 train_time:70228ms step_avg:39.74ms
step:1768/2330 train_time:70283ms step_avg:39.75ms
step:1769/2330 train_time:70306ms step_avg:39.74ms
step:1770/2330 train_time:70362ms step_avg:39.75ms
step:1771/2330 train_time:70384ms step_avg:39.74ms
step:1772/2330 train_time:70440ms step_avg:39.75ms
step:1773/2330 train_time:70463ms step_avg:39.74ms
step:1774/2330 train_time:70519ms step_avg:39.75ms
step:1775/2330 train_time:70541ms step_avg:39.74ms
step:1776/2330 train_time:70599ms step_avg:39.75ms
step:1777/2330 train_time:70624ms step_avg:39.74ms
step:1778/2330 train_time:70682ms step_avg:39.75ms
step:1779/2330 train_time:70707ms step_avg:39.75ms
step:1780/2330 train_time:70763ms step_avg:39.75ms
step:1781/2330 train_time:70786ms step_avg:39.74ms
step:1782/2330 train_time:70841ms step_avg:39.75ms
step:1783/2330 train_time:70863ms step_avg:39.74ms
step:1784/2330 train_time:70919ms step_avg:39.75ms
step:1785/2330 train_time:70943ms step_avg:39.74ms
step:1786/2330 train_time:71000ms step_avg:39.75ms
step:1787/2330 train_time:71025ms step_avg:39.75ms
step:1788/2330 train_time:71082ms step_avg:39.75ms
step:1789/2330 train_time:71105ms step_avg:39.75ms
step:1790/2330 train_time:71161ms step_avg:39.75ms
step:1791/2330 train_time:71185ms step_avg:39.75ms
step:1792/2330 train_time:71241ms step_avg:39.75ms
step:1793/2330 train_time:71263ms step_avg:39.75ms
step:1794/2330 train_time:71320ms step_avg:39.75ms
step:1795/2330 train_time:71343ms step_avg:39.75ms
step:1796/2330 train_time:71399ms step_avg:39.75ms
step:1797/2330 train_time:71421ms step_avg:39.74ms
step:1798/2330 train_time:71477ms step_avg:39.75ms
step:1799/2330 train_time:71500ms step_avg:39.74ms
step:1800/2330 train_time:71557ms step_avg:39.75ms
step:1801/2330 train_time:71580ms step_avg:39.74ms
step:1802/2330 train_time:71636ms step_avg:39.75ms
step:1803/2330 train_time:71659ms step_avg:39.74ms
step:1804/2330 train_time:71716ms step_avg:39.75ms
step:1805/2330 train_time:71739ms step_avg:39.74ms
step:1806/2330 train_time:71795ms step_avg:39.75ms
step:1807/2330 train_time:71817ms step_avg:39.74ms
step:1808/2330 train_time:71874ms step_avg:39.75ms
step:1809/2330 train_time:71896ms step_avg:39.74ms
step:1810/2330 train_time:71954ms step_avg:39.75ms
step:1811/2330 train_time:71977ms step_avg:39.74ms
step:1812/2330 train_time:72034ms step_avg:39.75ms
step:1813/2330 train_time:72056ms step_avg:39.74ms
step:1814/2330 train_time:72113ms step_avg:39.75ms
step:1815/2330 train_time:72136ms step_avg:39.74ms
step:1816/2330 train_time:72192ms step_avg:39.75ms
step:1817/2330 train_time:72215ms step_avg:39.74ms
step:1818/2330 train_time:72271ms step_avg:39.75ms
step:1819/2330 train_time:72293ms step_avg:39.74ms
step:1820/2330 train_time:72350ms step_avg:39.75ms
step:1821/2330 train_time:72371ms step_avg:39.74ms
step:1822/2330 train_time:72427ms step_avg:39.75ms
step:1823/2330 train_time:72450ms step_avg:39.74ms
step:1824/2330 train_time:72506ms step_avg:39.75ms
step:1825/2330 train_time:72529ms step_avg:39.74ms
step:1826/2330 train_time:72586ms step_avg:39.75ms
step:1827/2330 train_time:72608ms step_avg:39.74ms
step:1828/2330 train_time:72666ms step_avg:39.75ms
step:1829/2330 train_time:72689ms step_avg:39.74ms
step:1830/2330 train_time:72746ms step_avg:39.75ms
step:1831/2330 train_time:72768ms step_avg:39.74ms
step:1832/2330 train_time:72825ms step_avg:39.75ms
step:1833/2330 train_time:72848ms step_avg:39.74ms
step:1834/2330 train_time:72904ms step_avg:39.75ms
step:1835/2330 train_time:72928ms step_avg:39.74ms
step:1836/2330 train_time:72984ms step_avg:39.75ms
step:1837/2330 train_time:73007ms step_avg:39.74ms
step:1838/2330 train_time:73064ms step_avg:39.75ms
step:1839/2330 train_time:73087ms step_avg:39.74ms
step:1840/2330 train_time:73143ms step_avg:39.75ms
step:1841/2330 train_time:73166ms step_avg:39.74ms
step:1842/2330 train_time:73221ms step_avg:39.75ms
step:1843/2330 train_time:73244ms step_avg:39.74ms
step:1844/2330 train_time:73301ms step_avg:39.75ms
step:1845/2330 train_time:73324ms step_avg:39.74ms
step:1846/2330 train_time:73380ms step_avg:39.75ms
step:1847/2330 train_time:73403ms step_avg:39.74ms
step:1848/2330 train_time:73459ms step_avg:39.75ms
step:1849/2330 train_time:73482ms step_avg:39.74ms
step:1850/2330 train_time:73539ms step_avg:39.75ms
step:1851/2330 train_time:73562ms step_avg:39.74ms
step:1852/2330 train_time:73619ms step_avg:39.75ms
step:1853/2330 train_time:73642ms step_avg:39.74ms
step:1854/2330 train_time:73698ms step_avg:39.75ms
step:1855/2330 train_time:73722ms step_avg:39.74ms
step:1856/2330 train_time:73778ms step_avg:39.75ms
step:1857/2330 train_time:73802ms step_avg:39.74ms
step:1858/2330 train_time:73859ms step_avg:39.75ms
step:1859/2330 train_time:73882ms step_avg:39.74ms
step:1860/2330 train_time:73938ms step_avg:39.75ms
step:1861/2330 train_time:73962ms step_avg:39.74ms
step:1862/2330 train_time:74019ms step_avg:39.75ms
step:1863/2330 train_time:74042ms step_avg:39.74ms
step:1864/2330 train_time:74099ms step_avg:39.75ms
step:1865/2330 train_time:74122ms step_avg:39.74ms
step:1866/2330 train_time:74180ms step_avg:39.75ms
step:1867/2330 train_time:74203ms step_avg:39.74ms
step:1868/2330 train_time:74259ms step_avg:39.75ms
step:1869/2330 train_time:74282ms step_avg:39.74ms
step:1870/2330 train_time:74338ms step_avg:39.75ms
step:1871/2330 train_time:74361ms step_avg:39.74ms
step:1872/2330 train_time:74417ms step_avg:39.75ms
step:1873/2330 train_time:74441ms step_avg:39.74ms
step:1874/2330 train_time:74498ms step_avg:39.75ms
step:1875/2330 train_time:74521ms step_avg:39.74ms
step:1876/2330 train_time:74577ms step_avg:39.75ms
step:1877/2330 train_time:74601ms step_avg:39.74ms
step:1878/2330 train_time:74657ms step_avg:39.75ms
step:1879/2330 train_time:74681ms step_avg:39.74ms
step:1880/2330 train_time:74737ms step_avg:39.75ms
step:1881/2330 train_time:74759ms step_avg:39.74ms
step:1882/2330 train_time:74816ms step_avg:39.75ms
step:1883/2330 train_time:74839ms step_avg:39.74ms
step:1884/2330 train_time:74896ms step_avg:39.75ms
step:1885/2330 train_time:74919ms step_avg:39.74ms
step:1886/2330 train_time:74976ms step_avg:39.75ms
step:1887/2330 train_time:74999ms step_avg:39.75ms
step:1888/2330 train_time:75056ms step_avg:39.75ms
step:1889/2330 train_time:75078ms step_avg:39.74ms
step:1890/2330 train_time:75135ms step_avg:39.75ms
step:1891/2330 train_time:75158ms step_avg:39.75ms
step:1892/2330 train_time:75214ms step_avg:39.75ms
step:1893/2330 train_time:75237ms step_avg:39.75ms
step:1894/2330 train_time:75294ms step_avg:39.75ms
step:1895/2330 train_time:75317ms step_avg:39.75ms
step:1896/2330 train_time:75374ms step_avg:39.75ms
step:1897/2330 train_time:75396ms step_avg:39.74ms
step:1898/2330 train_time:75452ms step_avg:39.75ms
step:1899/2330 train_time:75475ms step_avg:39.74ms
step:1900/2330 train_time:75531ms step_avg:39.75ms
step:1901/2330 train_time:75554ms step_avg:39.74ms
step:1902/2330 train_time:75611ms step_avg:39.75ms
step:1903/2330 train_time:75633ms step_avg:39.74ms
step:1904/2330 train_time:75689ms step_avg:39.75ms
step:1905/2330 train_time:75711ms step_avg:39.74ms
step:1906/2330 train_time:75768ms step_avg:39.75ms
step:1907/2330 train_time:75790ms step_avg:39.74ms
step:1908/2330 train_time:75847ms step_avg:39.75ms
step:1909/2330 train_time:75869ms step_avg:39.74ms
step:1910/2330 train_time:75926ms step_avg:39.75ms
step:1911/2330 train_time:75948ms step_avg:39.74ms
step:1912/2330 train_time:76004ms step_avg:39.75ms
step:1913/2330 train_time:76027ms step_avg:39.74ms
step:1914/2330 train_time:76083ms step_avg:39.75ms
step:1915/2330 train_time:76106ms step_avg:39.74ms
step:1916/2330 train_time:76162ms step_avg:39.75ms
step:1917/2330 train_time:76185ms step_avg:39.74ms
step:1918/2330 train_time:76241ms step_avg:39.75ms
step:1919/2330 train_time:76264ms step_avg:39.74ms
step:1920/2330 train_time:76320ms step_avg:39.75ms
step:1921/2330 train_time:76343ms step_avg:39.74ms
step:1922/2330 train_time:76400ms step_avg:39.75ms
step:1923/2330 train_time:76423ms step_avg:39.74ms
step:1924/2330 train_time:76480ms step_avg:39.75ms
step:1925/2330 train_time:76503ms step_avg:39.74ms
step:1926/2330 train_time:76560ms step_avg:39.75ms
step:1927/2330 train_time:76583ms step_avg:39.74ms
step:1928/2330 train_time:76639ms step_avg:39.75ms
step:1929/2330 train_time:76662ms step_avg:39.74ms
step:1930/2330 train_time:76719ms step_avg:39.75ms
step:1931/2330 train_time:76742ms step_avg:39.74ms
step:1932/2330 train_time:76799ms step_avg:39.75ms
step:1933/2330 train_time:76823ms step_avg:39.74ms
step:1934/2330 train_time:76879ms step_avg:39.75ms
step:1935/2330 train_time:76902ms step_avg:39.74ms
step:1936/2330 train_time:76959ms step_avg:39.75ms
step:1937/2330 train_time:76982ms step_avg:39.74ms
step:1938/2330 train_time:77038ms step_avg:39.75ms
step:1939/2330 train_time:77061ms step_avg:39.74ms
step:1940/2330 train_time:77117ms step_avg:39.75ms
step:1941/2330 train_time:77140ms step_avg:39.74ms
step:1942/2330 train_time:77197ms step_avg:39.75ms
step:1943/2330 train_time:77220ms step_avg:39.74ms
step:1944/2330 train_time:77276ms step_avg:39.75ms
step:1945/2330 train_time:77299ms step_avg:39.74ms
step:1946/2330 train_time:77357ms step_avg:39.75ms
step:1947/2330 train_time:77380ms step_avg:39.74ms
step:1948/2330 train_time:77437ms step_avg:39.75ms
step:1949/2330 train_time:77460ms step_avg:39.74ms
step:1950/2330 train_time:77516ms step_avg:39.75ms
step:1951/2330 train_time:77541ms step_avg:39.74ms
step:1952/2330 train_time:77598ms step_avg:39.75ms
step:1953/2330 train_time:77621ms step_avg:39.74ms
step:1954/2330 train_time:77677ms step_avg:39.75ms
step:1955/2330 train_time:77700ms step_avg:39.74ms
step:1956/2330 train_time:77756ms step_avg:39.75ms
step:1957/2330 train_time:77779ms step_avg:39.74ms
step:1958/2330 train_time:77836ms step_avg:39.75ms
step:1959/2330 train_time:77859ms step_avg:39.74ms
step:1960/2330 train_time:77915ms step_avg:39.75ms
step:1961/2330 train_time:77938ms step_avg:39.74ms
step:1962/2330 train_time:77995ms step_avg:39.75ms
step:1963/2330 train_time:78017ms step_avg:39.74ms
step:1964/2330 train_time:78074ms step_avg:39.75ms
step:1965/2330 train_time:78097ms step_avg:39.74ms
step:1966/2330 train_time:78154ms step_avg:39.75ms
step:1967/2330 train_time:78175ms step_avg:39.74ms
step:1968/2330 train_time:78232ms step_avg:39.75ms
step:1969/2330 train_time:78254ms step_avg:39.74ms
step:1970/2330 train_time:78311ms step_avg:39.75ms
step:1971/2330 train_time:78333ms step_avg:39.74ms
step:1972/2330 train_time:78390ms step_avg:39.75ms
step:1973/2330 train_time:78412ms step_avg:39.74ms
step:1974/2330 train_time:78469ms step_avg:39.75ms
step:1975/2330 train_time:78491ms step_avg:39.74ms
step:1976/2330 train_time:78548ms step_avg:39.75ms
step:1977/2330 train_time:78570ms step_avg:39.74ms
step:1978/2330 train_time:78626ms step_avg:39.75ms
step:1979/2330 train_time:78649ms step_avg:39.74ms
step:1980/2330 train_time:78705ms step_avg:39.75ms
step:1981/2330 train_time:78728ms step_avg:39.74ms
step:1982/2330 train_time:78785ms step_avg:39.75ms
step:1983/2330 train_time:78808ms step_avg:39.74ms
step:1984/2330 train_time:78864ms step_avg:39.75ms
step:1985/2330 train_time:78887ms step_avg:39.74ms
step:1986/2330 train_time:78944ms step_avg:39.75ms
step:1987/2330 train_time:78966ms step_avg:39.74ms
step:1988/2330 train_time:79022ms step_avg:39.75ms
step:1989/2330 train_time:79045ms step_avg:39.74ms
step:1990/2330 train_time:79101ms step_avg:39.75ms
step:1991/2330 train_time:79124ms step_avg:39.74ms
step:1992/2330 train_time:79181ms step_avg:39.75ms
step:1993/2330 train_time:79204ms step_avg:39.74ms
step:1994/2330 train_time:79260ms step_avg:39.75ms
step:1995/2330 train_time:79284ms step_avg:39.74ms
step:1996/2330 train_time:79340ms step_avg:39.75ms
step:1997/2330 train_time:79363ms step_avg:39.74ms
step:1998/2330 train_time:79420ms step_avg:39.75ms
step:1999/2330 train_time:79443ms step_avg:39.74ms
step:2000/2330 train_time:79499ms step_avg:39.75ms
step:2000/2330 val_loss:5.1018 train_time:79597ms step_avg:39.80ms
step:2001/2330 train_time:79609ms step_avg:39.78ms
step:2002/2330 train_time:79621ms step_avg:39.77ms
step:2003/2330 train_time:79631ms step_avg:39.76ms
step:2004/2330 train_time:79661ms step_avg:39.75ms
step:2005/2330 train_time:79682ms step_avg:39.74ms
step:2006/2330 train_time:79738ms step_avg:39.75ms
step:2007/2330 train_time:79760ms step_avg:39.74ms
step:2008/2330 train_time:79817ms step_avg:39.75ms
step:2009/2330 train_time:79838ms step_avg:39.74ms
step:2010/2330 train_time:79896ms step_avg:39.75ms
step:2011/2330 train_time:79924ms step_avg:39.74ms
step:2012/2330 train_time:79983ms step_avg:39.75ms
step:2013/2330 train_time:80007ms step_avg:39.75ms
step:2014/2330 train_time:80065ms step_avg:39.75ms
step:2015/2330 train_time:80088ms step_avg:39.75ms
step:2016/2330 train_time:80144ms step_avg:39.75ms
step:2017/2330 train_time:80167ms step_avg:39.75ms
step:2018/2330 train_time:80223ms step_avg:39.75ms
step:2019/2330 train_time:80245ms step_avg:39.74ms
step:2020/2330 train_time:80301ms step_avg:39.75ms
step:2021/2330 train_time:80323ms step_avg:39.74ms
step:2022/2330 train_time:80379ms step_avg:39.75ms
step:2023/2330 train_time:80402ms step_avg:39.74ms
step:2024/2330 train_time:80457ms step_avg:39.75ms
step:2025/2330 train_time:80479ms step_avg:39.74ms
step:2026/2330 train_time:80537ms step_avg:39.75ms
step:2027/2330 train_time:80560ms step_avg:39.74ms
step:2028/2330 train_time:80616ms step_avg:39.75ms
step:2029/2330 train_time:80638ms step_avg:39.74ms
step:2030/2330 train_time:80695ms step_avg:39.75ms
step:2031/2330 train_time:80717ms step_avg:39.74ms
step:2032/2330 train_time:80773ms step_avg:39.75ms
step:2033/2330 train_time:80795ms step_avg:39.74ms
step:2034/2330 train_time:80852ms step_avg:39.75ms
step:2035/2330 train_time:80874ms step_avg:39.74ms
step:2036/2330 train_time:80933ms step_avg:39.75ms
step:2037/2330 train_time:80955ms step_avg:39.74ms
step:2038/2330 train_time:81013ms step_avg:39.75ms
step:2039/2330 train_time:81035ms step_avg:39.74ms
step:2040/2330 train_time:81093ms step_avg:39.75ms
step:2041/2330 train_time:81116ms step_avg:39.74ms
step:2042/2330 train_time:81172ms step_avg:39.75ms
step:2043/2330 train_time:81194ms step_avg:39.74ms
step:2044/2330 train_time:81250ms step_avg:39.75ms
step:2045/2330 train_time:81272ms step_avg:39.74ms
step:2046/2330 train_time:81328ms step_avg:39.75ms
step:2047/2330 train_time:81350ms step_avg:39.74ms
step:2048/2330 train_time:81406ms step_avg:39.75ms
step:2049/2330 train_time:81429ms step_avg:39.74ms
step:2050/2330 train_time:81485ms step_avg:39.75ms
step:2051/2330 train_time:81507ms step_avg:39.74ms
step:2052/2330 train_time:81563ms step_avg:39.75ms
step:2053/2330 train_time:81587ms step_avg:39.74ms
step:2054/2330 train_time:81643ms step_avg:39.75ms
step:2055/2330 train_time:81666ms step_avg:39.74ms
step:2056/2330 train_time:81722ms step_avg:39.75ms
step:2057/2330 train_time:81745ms step_avg:39.74ms
step:2058/2330 train_time:81801ms step_avg:39.75ms
step:2059/2330 train_time:81825ms step_avg:39.74ms
step:2060/2330 train_time:81882ms step_avg:39.75ms
step:2061/2330 train_time:81906ms step_avg:39.74ms
step:2062/2330 train_time:81964ms step_avg:39.75ms
step:2063/2330 train_time:81987ms step_avg:39.74ms
step:2064/2330 train_time:82044ms step_avg:39.75ms
step:2065/2330 train_time:82068ms step_avg:39.74ms
step:2066/2330 train_time:82124ms step_avg:39.75ms
step:2067/2330 train_time:82147ms step_avg:39.74ms
step:2068/2330 train_time:82203ms step_avg:39.75ms
step:2069/2330 train_time:82226ms step_avg:39.74ms
step:2070/2330 train_time:82282ms step_avg:39.75ms
step:2071/2330 train_time:82305ms step_avg:39.74ms
step:2072/2330 train_time:82361ms step_avg:39.75ms
step:2073/2330 train_time:82383ms step_avg:39.74ms
step:2074/2330 train_time:82439ms step_avg:39.75ms
step:2075/2330 train_time:82462ms step_avg:39.74ms
step:2076/2330 train_time:82518ms step_avg:39.75ms
step:2077/2330 train_time:82540ms step_avg:39.74ms
step:2078/2330 train_time:82597ms step_avg:39.75ms
step:2079/2330 train_time:82619ms step_avg:39.74ms
step:2080/2330 train_time:82676ms step_avg:39.75ms
step:2081/2330 train_time:82699ms step_avg:39.74ms
step:2082/2330 train_time:82756ms step_avg:39.75ms
step:2083/2330 train_time:82778ms step_avg:39.74ms
step:2084/2330 train_time:82835ms step_avg:39.75ms
step:2085/2330 train_time:82857ms step_avg:39.74ms
step:2086/2330 train_time:82915ms step_avg:39.75ms
step:2087/2330 train_time:82937ms step_avg:39.74ms
step:2088/2330 train_time:82994ms step_avg:39.75ms
step:2089/2330 train_time:83016ms step_avg:39.74ms
step:2090/2330 train_time:83073ms step_avg:39.75ms
step:2091/2330 train_time:83096ms step_avg:39.74ms
step:2092/2330 train_time:83153ms step_avg:39.75ms
step:2093/2330 train_time:83175ms step_avg:39.74ms
step:2094/2330 train_time:83232ms step_avg:39.75ms
step:2095/2330 train_time:83254ms step_avg:39.74ms
step:2096/2330 train_time:83310ms step_avg:39.75ms
step:2097/2330 train_time:83332ms step_avg:39.74ms
step:2098/2330 train_time:83388ms step_avg:39.75ms
step:2099/2330 train_time:83410ms step_avg:39.74ms
step:2100/2330 train_time:83466ms step_avg:39.75ms
step:2101/2330 train_time:83489ms step_avg:39.74ms
step:2102/2330 train_time:83545ms step_avg:39.75ms
step:2103/2330 train_time:83568ms step_avg:39.74ms
step:2104/2330 train_time:83624ms step_avg:39.75ms
step:2105/2330 train_time:83647ms step_avg:39.74ms
step:2106/2330 train_time:83703ms step_avg:39.74ms
step:2107/2330 train_time:83726ms step_avg:39.74ms
step:2108/2330 train_time:83782ms step_avg:39.74ms
step:2109/2330 train_time:83805ms step_avg:39.74ms
step:2110/2330 train_time:83862ms step_avg:39.75ms
step:2111/2330 train_time:83886ms step_avg:39.74ms
step:2112/2330 train_time:83942ms step_avg:39.75ms
step:2113/2330 train_time:83965ms step_avg:39.74ms
step:2114/2330 train_time:84022ms step_avg:39.75ms
step:2115/2330 train_time:84046ms step_avg:39.74ms
step:2116/2330 train_time:84103ms step_avg:39.75ms
step:2117/2330 train_time:84126ms step_avg:39.74ms
step:2118/2330 train_time:84182ms step_avg:39.75ms
step:2119/2330 train_time:84206ms step_avg:39.74ms
step:2120/2330 train_time:84262ms step_avg:39.75ms
step:2121/2330 train_time:84285ms step_avg:39.74ms
step:2122/2330 train_time:84342ms step_avg:39.75ms
step:2123/2330 train_time:84365ms step_avg:39.74ms
step:2124/2330 train_time:84421ms step_avg:39.75ms
step:2125/2330 train_time:84444ms step_avg:39.74ms
step:2126/2330 train_time:84500ms step_avg:39.75ms
step:2127/2330 train_time:84523ms step_avg:39.74ms
step:2128/2330 train_time:84578ms step_avg:39.75ms
step:2129/2330 train_time:84601ms step_avg:39.74ms
step:2130/2330 train_time:84658ms step_avg:39.75ms
step:2131/2330 train_time:84679ms step_avg:39.74ms
step:2132/2330 train_time:84735ms step_avg:39.74ms
step:2133/2330 train_time:84758ms step_avg:39.74ms
step:2134/2330 train_time:84815ms step_avg:39.74ms
step:2135/2330 train_time:84837ms step_avg:39.74ms
step:2136/2330 train_time:84894ms step_avg:39.74ms
step:2137/2330 train_time:84916ms step_avg:39.74ms
step:2138/2330 train_time:84973ms step_avg:39.74ms
step:2139/2330 train_time:84995ms step_avg:39.74ms
step:2140/2330 train_time:85052ms step_avg:39.74ms
step:2141/2330 train_time:85074ms step_avg:39.74ms
step:2142/2330 train_time:85131ms step_avg:39.74ms
step:2143/2330 train_time:85153ms step_avg:39.74ms
step:2144/2330 train_time:85211ms step_avg:39.74ms
step:2145/2330 train_time:85233ms step_avg:39.74ms
step:2146/2330 train_time:85290ms step_avg:39.74ms
step:2147/2330 train_time:85312ms step_avg:39.74ms
step:2148/2330 train_time:85369ms step_avg:39.74ms
step:2149/2330 train_time:85391ms step_avg:39.74ms
step:2150/2330 train_time:85448ms step_avg:39.74ms
step:2151/2330 train_time:85470ms step_avg:39.74ms
step:2152/2330 train_time:85527ms step_avg:39.74ms
step:2153/2330 train_time:85550ms step_avg:39.74ms
step:2154/2330 train_time:85606ms step_avg:39.74ms
step:2155/2330 train_time:85629ms step_avg:39.73ms
step:2156/2330 train_time:85685ms step_avg:39.74ms
step:2157/2330 train_time:85708ms step_avg:39.73ms
step:2158/2330 train_time:85764ms step_avg:39.74ms
step:2159/2330 train_time:85787ms step_avg:39.73ms
step:2160/2330 train_time:85843ms step_avg:39.74ms
step:2161/2330 train_time:85866ms step_avg:39.73ms
step:2162/2330 train_time:85922ms step_avg:39.74ms
step:2163/2330 train_time:85945ms step_avg:39.73ms
step:2164/2330 train_time:86002ms step_avg:39.74ms
step:2165/2330 train_time:86025ms step_avg:39.73ms
step:2166/2330 train_time:86082ms step_avg:39.74ms
step:2167/2330 train_time:86105ms step_avg:39.73ms
step:2168/2330 train_time:86163ms step_avg:39.74ms
step:2169/2330 train_time:86186ms step_avg:39.74ms
step:2170/2330 train_time:86243ms step_avg:39.74ms
step:2171/2330 train_time:86266ms step_avg:39.74ms
step:2172/2330 train_time:86322ms step_avg:39.74ms
step:2173/2330 train_time:86345ms step_avg:39.74ms
step:2174/2330 train_time:86401ms step_avg:39.74ms
step:2175/2330 train_time:86425ms step_avg:39.74ms
step:2176/2330 train_time:86481ms step_avg:39.74ms
step:2177/2330 train_time:86504ms step_avg:39.74ms
step:2178/2330 train_time:86560ms step_avg:39.74ms
step:2179/2330 train_time:86584ms step_avg:39.74ms
step:2180/2330 train_time:86640ms step_avg:39.74ms
step:2181/2330 train_time:86663ms step_avg:39.74ms
step:2182/2330 train_time:86719ms step_avg:39.74ms
step:2183/2330 train_time:86743ms step_avg:39.74ms
step:2184/2330 train_time:86800ms step_avg:39.74ms
step:2185/2330 train_time:86823ms step_avg:39.74ms
step:2186/2330 train_time:86880ms step_avg:39.74ms
step:2187/2330 train_time:86903ms step_avg:39.74ms
step:2188/2330 train_time:86960ms step_avg:39.74ms
step:2189/2330 train_time:86983ms step_avg:39.74ms
step:2190/2330 train_time:87040ms step_avg:39.74ms
step:2191/2330 train_time:87063ms step_avg:39.74ms
step:2192/2330 train_time:87120ms step_avg:39.74ms
step:2193/2330 train_time:87144ms step_avg:39.74ms
step:2194/2330 train_time:87201ms step_avg:39.75ms
step:2195/2330 train_time:87224ms step_avg:39.74ms
step:2196/2330 train_time:87280ms step_avg:39.75ms
step:2197/2330 train_time:87303ms step_avg:39.74ms
step:2198/2330 train_time:87360ms step_avg:39.75ms
step:2199/2330 train_time:87383ms step_avg:39.74ms
step:2200/2330 train_time:87439ms step_avg:39.75ms
step:2201/2330 train_time:87462ms step_avg:39.74ms
step:2202/2330 train_time:87518ms step_avg:39.74ms
step:2203/2330 train_time:87542ms step_avg:39.74ms
step:2204/2330 train_time:87598ms step_avg:39.75ms
step:2205/2330 train_time:87621ms step_avg:39.74ms
step:2206/2330 train_time:87677ms step_avg:39.74ms
step:2207/2330 train_time:87700ms step_avg:39.74ms
step:2208/2330 train_time:87757ms step_avg:39.74ms
step:2209/2330 train_time:87779ms step_avg:39.74ms
step:2210/2330 train_time:87837ms step_avg:39.75ms
step:2211/2330 train_time:87860ms step_avg:39.74ms
step:2212/2330 train_time:87916ms step_avg:39.75ms
step:2213/2330 train_time:87940ms step_avg:39.74ms
step:2214/2330 train_time:87997ms step_avg:39.75ms
step:2215/2330 train_time:88020ms step_avg:39.74ms
step:2216/2330 train_time:88076ms step_avg:39.75ms
step:2217/2330 train_time:88100ms step_avg:39.74ms
step:2218/2330 train_time:88157ms step_avg:39.75ms
step:2219/2330 train_time:88179ms step_avg:39.74ms
step:2220/2330 train_time:88237ms step_avg:39.75ms
step:2221/2330 train_time:88259ms step_avg:39.74ms
step:2222/2330 train_time:88316ms step_avg:39.75ms
step:2223/2330 train_time:88338ms step_avg:39.74ms
step:2224/2330 train_time:88395ms step_avg:39.75ms
step:2225/2330 train_time:88417ms step_avg:39.74ms
step:2226/2330 train_time:88474ms step_avg:39.75ms
step:2227/2330 train_time:88496ms step_avg:39.74ms
step:2228/2330 train_time:88552ms step_avg:39.75ms
step:2229/2330 train_time:88574ms step_avg:39.74ms
step:2230/2330 train_time:88630ms step_avg:39.74ms
step:2231/2330 train_time:88652ms step_avg:39.74ms
step:2232/2330 train_time:88709ms step_avg:39.74ms
step:2233/2330 train_time:88731ms step_avg:39.74ms
step:2234/2330 train_time:88787ms step_avg:39.74ms
step:2235/2330 train_time:88809ms step_avg:39.74ms
step:2236/2330 train_time:88866ms step_avg:39.74ms
step:2237/2330 train_time:88889ms step_avg:39.74ms
step:2238/2330 train_time:88945ms step_avg:39.74ms
step:2239/2330 train_time:88968ms step_avg:39.74ms
step:2240/2330 train_time:89025ms step_avg:39.74ms
step:2241/2330 train_time:89048ms step_avg:39.74ms
step:2242/2330 train_time:89104ms step_avg:39.74ms
step:2243/2330 train_time:89127ms step_avg:39.74ms
step:2244/2330 train_time:89183ms step_avg:39.74ms
step:2245/2330 train_time:89206ms step_avg:39.74ms
step:2246/2330 train_time:89263ms step_avg:39.74ms
step:2247/2330 train_time:89286ms step_avg:39.74ms
step:2248/2330 train_time:89342ms step_avg:39.74ms
step:2249/2330 train_time:89364ms step_avg:39.74ms
step:2250/2330 train_time:89421ms step_avg:39.74ms
step:2250/2330 val_loss:5.0773 train_time:89518ms step_avg:39.79ms
step:2251/2330 train_time:89532ms step_avg:39.77ms
step:2252/2330 train_time:89545ms step_avg:39.76ms
step:2253/2330 train_time:89557ms step_avg:39.75ms
step:2254/2330 train_time:89582ms step_avg:39.74ms
step:2255/2330 train_time:89603ms step_avg:39.74ms
step:2256/2330 train_time:89658ms step_avg:39.74ms
step:2257/2330 train_time:89680ms step_avg:39.73ms
step:2258/2330 train_time:89736ms step_avg:39.74ms
step:2259/2330 train_time:89758ms step_avg:39.73ms
step:2260/2330 train_time:89813ms step_avg:39.74ms
step:2261/2330 train_time:89838ms step_avg:39.73ms
step:2262/2330 train_time:89896ms step_avg:39.74ms
step:2263/2330 train_time:89921ms step_avg:39.74ms
step:2264/2330 train_time:89978ms step_avg:39.74ms
step:2265/2330 train_time:90000ms step_avg:39.74ms
step:2266/2330 train_time:90057ms step_avg:39.74ms
step:2267/2330 train_time:90078ms step_avg:39.73ms
step:2268/2330 train_time:90134ms step_avg:39.74ms
step:2269/2330 train_time:90156ms step_avg:39.73ms
step:2270/2330 train_time:90212ms step_avg:39.74ms
step:2271/2330 train_time:90235ms step_avg:39.73ms
step:2272/2330 train_time:90291ms step_avg:39.74ms
step:2273/2330 train_time:90314ms step_avg:39.73ms
step:2274/2330 train_time:90370ms step_avg:39.74ms
step:2275/2330 train_time:90393ms step_avg:39.73ms
step:2276/2330 train_time:90451ms step_avg:39.74ms
step:2277/2330 train_time:90476ms step_avg:39.73ms
step:2278/2330 train_time:90533ms step_avg:39.74ms
step:2279/2330 train_time:90556ms step_avg:39.74ms
step:2280/2330 train_time:90612ms step_avg:39.74ms
step:2281/2330 train_time:90635ms step_avg:39.73ms
step:2282/2330 train_time:90690ms step_avg:39.74ms
step:2283/2330 train_time:90713ms step_avg:39.73ms
step:2284/2330 train_time:90769ms step_avg:39.74ms
step:2285/2330 train_time:90792ms step_avg:39.73ms
step:2286/2330 train_time:90849ms step_avg:39.74ms
step:2287/2330 train_time:90872ms step_avg:39.73ms
step:2288/2330 train_time:90928ms step_avg:39.74ms
step:2289/2330 train_time:90952ms step_avg:39.73ms
step:2290/2330 train_time:91009ms step_avg:39.74ms
step:2291/2330 train_time:91032ms step_avg:39.73ms
step:2292/2330 train_time:91088ms step_avg:39.74ms
step:2293/2330 train_time:91111ms step_avg:39.73ms
step:2294/2330 train_time:91168ms step_avg:39.74ms
step:2295/2330 train_time:91190ms step_avg:39.73ms
step:2296/2330 train_time:91247ms step_avg:39.74ms
step:2297/2330 train_time:91269ms step_avg:39.73ms
step:2298/2330 train_time:91325ms step_avg:39.74ms
step:2299/2330 train_time:91348ms step_avg:39.73ms
step:2300/2330 train_time:91406ms step_avg:39.74ms
step:2301/2330 train_time:91428ms step_avg:39.73ms
step:2302/2330 train_time:91485ms step_avg:39.74ms
step:2303/2330 train_time:91507ms step_avg:39.73ms
step:2304/2330 train_time:91563ms step_avg:39.74ms
step:2305/2330 train_time:91585ms step_avg:39.73ms
step:2306/2330 train_time:91642ms step_avg:39.74ms
step:2307/2330 train_time:91664ms step_avg:39.73ms
step:2308/2330 train_time:91720ms step_avg:39.74ms
step:2309/2330 train_time:91743ms step_avg:39.73ms
step:2310/2330 train_time:91799ms step_avg:39.74ms
step:2311/2330 train_time:91821ms step_avg:39.73ms
step:2312/2330 train_time:91878ms step_avg:39.74ms
step:2313/2330 train_time:91900ms step_avg:39.73ms
step:2314/2330 train_time:91957ms step_avg:39.74ms
step:2315/2330 train_time:91980ms step_avg:39.73ms
step:2316/2330 train_time:92036ms step_avg:39.74ms
step:2317/2330 train_time:92058ms step_avg:39.73ms
step:2318/2330 train_time:92114ms step_avg:39.74ms
step:2319/2330 train_time:92137ms step_avg:39.73ms
step:2320/2330 train_time:92194ms step_avg:39.74ms
step:2321/2330 train_time:92216ms step_avg:39.73ms
step:2322/2330 train_time:92272ms step_avg:39.74ms
step:2323/2330 train_time:92296ms step_avg:39.73ms
step:2324/2330 train_time:92352ms step_avg:39.74ms
step:2325/2330 train_time:92375ms step_avg:39.73ms
step:2326/2330 train_time:92431ms step_avg:39.74ms
step:2327/2330 train_time:92455ms step_avg:39.73ms
step:2328/2330 train_time:92511ms step_avg:39.74ms
step:2329/2330 train_time:92534ms step_avg:39.73ms
step:2330/2330 train_time:92590ms step_avg:39.74ms
step:2330/2330 val_loss:5.0706 train_time:92688ms step_avg:39.78ms
peak memory allocated: 29712 MiB reserved: 38888 MiB
