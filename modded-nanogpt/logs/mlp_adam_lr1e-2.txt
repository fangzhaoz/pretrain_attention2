import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        # self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "mlp_adam_lr1e-2"
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
# embed_params = [p for n, p in model.named_parameters() if "embed" in n]
embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=1e-1, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizer2 = torch.optim.Adam(
    hidden_matrix_params + gate_params,
    lr=1e-2,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 04:47:00 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   33C    P0             112W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   31C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   31C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             115W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.05ms
step:1/2330 train_time:83ms step_avg:82.96ms
step:2/2330 train_time:145ms step_avg:72.56ms
step:3/2330 train_time:156ms step_avg:52.10ms
step:4/2330 train_time:168ms step_avg:42.02ms
step:5/2330 train_time:179ms step_avg:35.72ms
step:6/2330 train_time:189ms step_avg:31.56ms
step:7/2330 train_time:353ms step_avg:50.46ms
step:8/2330 train_time:393ms step_avg:49.06ms
step:9/2330 train_time:426ms step_avg:47.31ms
step:10/2330 train_time:466ms step_avg:46.57ms
step:11/2330 train_time:499ms step_avg:45.37ms
step:12/2330 train_time:539ms step_avg:44.91ms
step:13/2330 train_time:572ms step_avg:44.03ms
step:14/2330 train_time:612ms step_avg:43.73ms
step:15/2330 train_time:646ms step_avg:43.04ms
step:16/2330 train_time:685ms step_avg:42.84ms
step:17/2330 train_time:719ms step_avg:42.30ms
step:18/2330 train_time:759ms step_avg:42.17ms
step:19/2330 train_time:792ms step_avg:41.71ms
step:20/2330 train_time:832ms step_avg:41.61ms
step:21/2330 train_time:866ms step_avg:41.24ms
step:22/2330 train_time:906ms step_avg:41.19ms
step:23/2330 train_time:940ms step_avg:40.86ms
step:24/2330 train_time:980ms step_avg:40.82ms
step:25/2330 train_time:1013ms step_avg:40.54ms
step:26/2330 train_time:1053ms step_avg:40.51ms
step:27/2330 train_time:1087ms step_avg:40.26ms
step:28/2330 train_time:1127ms step_avg:40.25ms
step:29/2330 train_time:1161ms step_avg:40.02ms
step:30/2330 train_time:1201ms step_avg:40.03ms
step:31/2330 train_time:1236ms step_avg:39.87ms
step:32/2330 train_time:1276ms step_avg:39.88ms
step:33/2330 train_time:1314ms step_avg:39.81ms
step:34/2330 train_time:1354ms step_avg:39.81ms
step:35/2330 train_time:1389ms step_avg:39.70ms
step:36/2330 train_time:1430ms step_avg:39.72ms
step:37/2330 train_time:1464ms step_avg:39.57ms
step:38/2330 train_time:1504ms step_avg:39.58ms
step:39/2330 train_time:1538ms step_avg:39.45ms
step:40/2330 train_time:1579ms step_avg:39.47ms
step:41/2330 train_time:1614ms step_avg:39.36ms
step:42/2330 train_time:1654ms step_avg:39.38ms
step:43/2330 train_time:1688ms step_avg:39.26ms
step:44/2330 train_time:1729ms step_avg:39.29ms
step:45/2330 train_time:1763ms step_avg:39.18ms
step:46/2330 train_time:1804ms step_avg:39.21ms
step:47/2330 train_time:1838ms step_avg:39.10ms
step:48/2330 train_time:1878ms step_avg:39.13ms
step:49/2330 train_time:1912ms step_avg:39.02ms
step:50/2330 train_time:1952ms step_avg:39.04ms
step:51/2330 train_time:1986ms step_avg:38.94ms
step:52/2330 train_time:2027ms step_avg:38.98ms
step:53/2330 train_time:2061ms step_avg:38.88ms
step:54/2330 train_time:2101ms step_avg:38.91ms
step:55/2330 train_time:2135ms step_avg:38.82ms
step:56/2330 train_time:2175ms step_avg:38.84ms
step:57/2330 train_time:2209ms step_avg:38.76ms
step:58/2330 train_time:2250ms step_avg:38.79ms
step:59/2330 train_time:2285ms step_avg:38.73ms
step:60/2330 train_time:2326ms step_avg:38.76ms
step:61/2330 train_time:2360ms step_avg:38.70ms
step:62/2330 train_time:2401ms step_avg:38.72ms
step:63/2330 train_time:2435ms step_avg:38.66ms
step:64/2330 train_time:2476ms step_avg:38.68ms
step:65/2330 train_time:2510ms step_avg:38.62ms
step:66/2330 train_time:2550ms step_avg:38.64ms
step:67/2330 train_time:2585ms step_avg:38.58ms
step:68/2330 train_time:2625ms step_avg:38.60ms
step:69/2330 train_time:2659ms step_avg:38.54ms
step:70/2330 train_time:2699ms step_avg:38.56ms
step:71/2330 train_time:2734ms step_avg:38.51ms
step:72/2330 train_time:2774ms step_avg:38.53ms
step:73/2330 train_time:2809ms step_avg:38.48ms
step:74/2330 train_time:2849ms step_avg:38.50ms
step:75/2330 train_time:2885ms step_avg:38.46ms
step:76/2330 train_time:2925ms step_avg:38.48ms
step:77/2330 train_time:2959ms step_avg:38.43ms
step:78/2330 train_time:2999ms step_avg:38.45ms
step:79/2330 train_time:3034ms step_avg:38.41ms
step:80/2330 train_time:3074ms step_avg:38.43ms
step:81/2330 train_time:3108ms step_avg:38.38ms
step:82/2330 train_time:3149ms step_avg:38.40ms
step:83/2330 train_time:3184ms step_avg:38.36ms
step:84/2330 train_time:3224ms step_avg:38.39ms
step:85/2330 train_time:3259ms step_avg:38.34ms
step:86/2330 train_time:3300ms step_avg:38.37ms
step:87/2330 train_time:3334ms step_avg:38.33ms
step:88/2330 train_time:3375ms step_avg:38.35ms
step:89/2330 train_time:3409ms step_avg:38.30ms
step:90/2330 train_time:3449ms step_avg:38.33ms
step:91/2330 train_time:3485ms step_avg:38.30ms
step:92/2330 train_time:3526ms step_avg:38.33ms
step:93/2330 train_time:3560ms step_avg:38.28ms
step:94/2330 train_time:3601ms step_avg:38.30ms
step:95/2330 train_time:3635ms step_avg:38.26ms
step:96/2330 train_time:3676ms step_avg:38.29ms
step:97/2330 train_time:3710ms step_avg:38.25ms
step:98/2330 train_time:3750ms step_avg:38.27ms
step:99/2330 train_time:3786ms step_avg:38.24ms
step:100/2330 train_time:3826ms step_avg:38.26ms
step:101/2330 train_time:3861ms step_avg:38.23ms
step:102/2330 train_time:3902ms step_avg:38.25ms
step:103/2330 train_time:3937ms step_avg:38.22ms
step:104/2330 train_time:3977ms step_avg:38.24ms
step:105/2330 train_time:4011ms step_avg:38.20ms
step:106/2330 train_time:4051ms step_avg:38.22ms
step:107/2330 train_time:4086ms step_avg:38.19ms
step:108/2330 train_time:4127ms step_avg:38.21ms
step:109/2330 train_time:4161ms step_avg:38.18ms
step:110/2330 train_time:4202ms step_avg:38.20ms
step:111/2330 train_time:4236ms step_avg:38.17ms
step:112/2330 train_time:4277ms step_avg:38.18ms
step:113/2330 train_time:4311ms step_avg:38.15ms
step:114/2330 train_time:4352ms step_avg:38.17ms
step:115/2330 train_time:4386ms step_avg:38.14ms
step:116/2330 train_time:4427ms step_avg:38.16ms
step:117/2330 train_time:4461ms step_avg:38.13ms
step:118/2330 train_time:4501ms step_avg:38.15ms
step:119/2330 train_time:4536ms step_avg:38.12ms
step:120/2330 train_time:4577ms step_avg:38.14ms
step:121/2330 train_time:4612ms step_avg:38.11ms
step:122/2330 train_time:4652ms step_avg:38.13ms
step:123/2330 train_time:4687ms step_avg:38.10ms
step:124/2330 train_time:4727ms step_avg:38.12ms
step:125/2330 train_time:4761ms step_avg:38.09ms
step:126/2330 train_time:4802ms step_avg:38.11ms
step:127/2330 train_time:4837ms step_avg:38.08ms
step:128/2330 train_time:4877ms step_avg:38.10ms
step:129/2330 train_time:4912ms step_avg:38.08ms
step:130/2330 train_time:4952ms step_avg:38.09ms
step:131/2330 train_time:4987ms step_avg:38.07ms
step:132/2330 train_time:5027ms step_avg:38.09ms
step:133/2330 train_time:5062ms step_avg:38.06ms
step:134/2330 train_time:5102ms step_avg:38.08ms
step:135/2330 train_time:5137ms step_avg:38.05ms
step:136/2330 train_time:5178ms step_avg:38.07ms
step:137/2330 train_time:5212ms step_avg:38.04ms
step:138/2330 train_time:5253ms step_avg:38.06ms
step:139/2330 train_time:5287ms step_avg:38.04ms
step:140/2330 train_time:5328ms step_avg:38.06ms
step:141/2330 train_time:5363ms step_avg:38.03ms
step:142/2330 train_time:5403ms step_avg:38.05ms
step:143/2330 train_time:5438ms step_avg:38.03ms
step:144/2330 train_time:5479ms step_avg:38.05ms
step:145/2330 train_time:5513ms step_avg:38.02ms
step:146/2330 train_time:5554ms step_avg:38.04ms
step:147/2330 train_time:5588ms step_avg:38.02ms
step:148/2330 train_time:5629ms step_avg:38.03ms
step:149/2330 train_time:5663ms step_avg:38.01ms
step:150/2330 train_time:5704ms step_avg:38.02ms
step:151/2330 train_time:5738ms step_avg:38.00ms
step:152/2330 train_time:5779ms step_avg:38.02ms
step:153/2330 train_time:5813ms step_avg:38.00ms
step:154/2330 train_time:5854ms step_avg:38.01ms
step:155/2330 train_time:5888ms step_avg:37.99ms
step:156/2330 train_time:5929ms step_avg:38.00ms
step:157/2330 train_time:5963ms step_avg:37.98ms
step:158/2330 train_time:6004ms step_avg:38.00ms
step:159/2330 train_time:6038ms step_avg:37.98ms
step:160/2330 train_time:6078ms step_avg:37.99ms
step:161/2330 train_time:6113ms step_avg:37.97ms
step:162/2330 train_time:6154ms step_avg:37.98ms
step:163/2330 train_time:6188ms step_avg:37.96ms
step:164/2330 train_time:6229ms step_avg:37.98ms
step:165/2330 train_time:6263ms step_avg:37.96ms
step:166/2330 train_time:6304ms step_avg:37.98ms
step:167/2330 train_time:6339ms step_avg:37.96ms
step:168/2330 train_time:6380ms step_avg:37.98ms
step:169/2330 train_time:6415ms step_avg:37.96ms
step:170/2330 train_time:6455ms step_avg:37.97ms
step:171/2330 train_time:6491ms step_avg:37.96ms
step:172/2330 train_time:6531ms step_avg:37.97ms
step:173/2330 train_time:6566ms step_avg:37.95ms
step:174/2330 train_time:6607ms step_avg:37.97ms
step:175/2330 train_time:6641ms step_avg:37.95ms
step:176/2330 train_time:6682ms step_avg:37.96ms
step:177/2330 train_time:6717ms step_avg:37.95ms
step:178/2330 train_time:6758ms step_avg:37.96ms
step:179/2330 train_time:6793ms step_avg:37.95ms
step:180/2330 train_time:6833ms step_avg:37.96ms
step:181/2330 train_time:6867ms step_avg:37.94ms
step:182/2330 train_time:6908ms step_avg:37.95ms
step:183/2330 train_time:6943ms step_avg:37.94ms
step:184/2330 train_time:6983ms step_avg:37.95ms
step:185/2330 train_time:7018ms step_avg:37.93ms
step:186/2330 train_time:7059ms step_avg:37.95ms
step:187/2330 train_time:7093ms step_avg:37.93ms
step:188/2330 train_time:7134ms step_avg:37.95ms
step:189/2330 train_time:7168ms step_avg:37.93ms
step:190/2330 train_time:7208ms step_avg:37.94ms
step:191/2330 train_time:7244ms step_avg:37.93ms
step:192/2330 train_time:7284ms step_avg:37.94ms
step:193/2330 train_time:7320ms step_avg:37.93ms
step:194/2330 train_time:7361ms step_avg:37.94ms
step:195/2330 train_time:7396ms step_avg:37.93ms
step:196/2330 train_time:7437ms step_avg:37.94ms
step:197/2330 train_time:7472ms step_avg:37.93ms
step:198/2330 train_time:7512ms step_avg:37.94ms
step:199/2330 train_time:7547ms step_avg:37.93ms
step:200/2330 train_time:7588ms step_avg:37.94ms
step:201/2330 train_time:7623ms step_avg:37.93ms
step:202/2330 train_time:7663ms step_avg:37.94ms
step:203/2330 train_time:7698ms step_avg:37.92ms
step:204/2330 train_time:7739ms step_avg:37.93ms
step:205/2330 train_time:7773ms step_avg:37.92ms
step:206/2330 train_time:7814ms step_avg:37.93ms
step:207/2330 train_time:7848ms step_avg:37.91ms
step:208/2330 train_time:7889ms step_avg:37.93ms
step:209/2330 train_time:7924ms step_avg:37.91ms
step:210/2330 train_time:7964ms step_avg:37.92ms
step:211/2330 train_time:7999ms step_avg:37.91ms
step:212/2330 train_time:8039ms step_avg:37.92ms
step:213/2330 train_time:8074ms step_avg:37.91ms
step:214/2330 train_time:8114ms step_avg:37.92ms
step:215/2330 train_time:8149ms step_avg:37.90ms
step:216/2330 train_time:8189ms step_avg:37.91ms
step:217/2330 train_time:8224ms step_avg:37.90ms
step:218/2330 train_time:8265ms step_avg:37.91ms
step:219/2330 train_time:8300ms step_avg:37.90ms
step:220/2330 train_time:8341ms step_avg:37.91ms
step:221/2330 train_time:8376ms step_avg:37.90ms
step:222/2330 train_time:8416ms step_avg:37.91ms
step:223/2330 train_time:8451ms step_avg:37.90ms
step:224/2330 train_time:8491ms step_avg:37.91ms
step:225/2330 train_time:8526ms step_avg:37.90ms
step:226/2330 train_time:8567ms step_avg:37.91ms
step:227/2330 train_time:8602ms step_avg:37.89ms
step:228/2330 train_time:8643ms step_avg:37.91ms
step:229/2330 train_time:8678ms step_avg:37.90ms
step:230/2330 train_time:8719ms step_avg:37.91ms
step:231/2330 train_time:8754ms step_avg:37.90ms
step:232/2330 train_time:8794ms step_avg:37.91ms
step:233/2330 train_time:8829ms step_avg:37.89ms
step:234/2330 train_time:8870ms step_avg:37.91ms
step:235/2330 train_time:8904ms step_avg:37.89ms
step:236/2330 train_time:8945ms step_avg:37.90ms
step:237/2330 train_time:8980ms step_avg:37.89ms
step:238/2330 train_time:9020ms step_avg:37.90ms
step:239/2330 train_time:9056ms step_avg:37.89ms
step:240/2330 train_time:9097ms step_avg:37.90ms
step:241/2330 train_time:9131ms step_avg:37.89ms
step:242/2330 train_time:9172ms step_avg:37.90ms
step:243/2330 train_time:9207ms step_avg:37.89ms
step:244/2330 train_time:9248ms step_avg:37.90ms
step:245/2330 train_time:9282ms step_avg:37.89ms
step:246/2330 train_time:9323ms step_avg:37.90ms
step:247/2330 train_time:9358ms step_avg:37.89ms
step:248/2330 train_time:9399ms step_avg:37.90ms
step:249/2330 train_time:9434ms step_avg:37.89ms
step:250/2330 train_time:9475ms step_avg:37.90ms
step:250/2330 val_loss:5.8936 train_time:9586ms step_avg:38.34ms
step:251/2330 train_time:9598ms step_avg:38.24ms
step:252/2330 train_time:9608ms step_avg:38.13ms
step:253/2330 train_time:9617ms step_avg:38.01ms
step:254/2330 train_time:9628ms step_avg:37.91ms
step:255/2330 train_time:9662ms step_avg:37.89ms
step:256/2330 train_time:9702ms step_avg:37.90ms
step:257/2330 train_time:9737ms step_avg:37.89ms
step:258/2330 train_time:9777ms step_avg:37.89ms
step:259/2330 train_time:9811ms step_avg:37.88ms
step:260/2330 train_time:9851ms step_avg:37.89ms
step:261/2330 train_time:9886ms step_avg:37.88ms
step:262/2330 train_time:9926ms step_avg:37.89ms
step:263/2330 train_time:9965ms step_avg:37.89ms
step:264/2330 train_time:10006ms step_avg:37.90ms
step:265/2330 train_time:10043ms step_avg:37.90ms
step:266/2330 train_time:10083ms step_avg:37.91ms
step:267/2330 train_time:10118ms step_avg:37.90ms
step:268/2330 train_time:10159ms step_avg:37.91ms
step:269/2330 train_time:10195ms step_avg:37.90ms
step:270/2330 train_time:10235ms step_avg:37.91ms
step:271/2330 train_time:10271ms step_avg:37.90ms
step:272/2330 train_time:10311ms step_avg:37.91ms
step:273/2330 train_time:10346ms step_avg:37.90ms
step:274/2330 train_time:10386ms step_avg:37.91ms
step:275/2330 train_time:10421ms step_avg:37.89ms
step:276/2330 train_time:10462ms step_avg:37.90ms
step:277/2330 train_time:10496ms step_avg:37.89ms
step:278/2330 train_time:10538ms step_avg:37.91ms
step:279/2330 train_time:10573ms step_avg:37.90ms
step:280/2330 train_time:10615ms step_avg:37.91ms
step:281/2330 train_time:10650ms step_avg:37.90ms
step:282/2330 train_time:10691ms step_avg:37.91ms
step:283/2330 train_time:10725ms step_avg:37.90ms
step:284/2330 train_time:10766ms step_avg:37.91ms
step:285/2330 train_time:10801ms step_avg:37.90ms
step:286/2330 train_time:10842ms step_avg:37.91ms
step:287/2330 train_time:10877ms step_avg:37.90ms
step:288/2330 train_time:10918ms step_avg:37.91ms
step:289/2330 train_time:10953ms step_avg:37.90ms
step:290/2330 train_time:10994ms step_avg:37.91ms
step:291/2330 train_time:11029ms step_avg:37.90ms
step:292/2330 train_time:11069ms step_avg:37.91ms
step:293/2330 train_time:11105ms step_avg:37.90ms
step:294/2330 train_time:11145ms step_avg:37.91ms
step:295/2330 train_time:11182ms step_avg:37.90ms
step:296/2330 train_time:11222ms step_avg:37.91ms
step:297/2330 train_time:11258ms step_avg:37.91ms
step:298/2330 train_time:11299ms step_avg:37.92ms
step:299/2330 train_time:11334ms step_avg:37.90ms
step:300/2330 train_time:11374ms step_avg:37.91ms
step:301/2330 train_time:11409ms step_avg:37.90ms
step:302/2330 train_time:11450ms step_avg:37.91ms
step:303/2330 train_time:11485ms step_avg:37.90ms
step:304/2330 train_time:11526ms step_avg:37.91ms
step:305/2330 train_time:11562ms step_avg:37.91ms
step:306/2330 train_time:11603ms step_avg:37.92ms
step:307/2330 train_time:11639ms step_avg:37.91ms
step:308/2330 train_time:11680ms step_avg:37.92ms
step:309/2330 train_time:11714ms step_avg:37.91ms
step:310/2330 train_time:11755ms step_avg:37.92ms
step:311/2330 train_time:11790ms step_avg:37.91ms
step:312/2330 train_time:11831ms step_avg:37.92ms
step:313/2330 train_time:11866ms step_avg:37.91ms
step:314/2330 train_time:11906ms step_avg:37.92ms
step:315/2330 train_time:11942ms step_avg:37.91ms
step:316/2330 train_time:11983ms step_avg:37.92ms
step:317/2330 train_time:12018ms step_avg:37.91ms
step:318/2330 train_time:12058ms step_avg:37.92ms
step:319/2330 train_time:12094ms step_avg:37.91ms
step:320/2330 train_time:12134ms step_avg:37.92ms
step:321/2330 train_time:12169ms step_avg:37.91ms
step:322/2330 train_time:12210ms step_avg:37.92ms
step:323/2330 train_time:12245ms step_avg:37.91ms
step:324/2330 train_time:12286ms step_avg:37.92ms
step:325/2330 train_time:12321ms step_avg:37.91ms
step:326/2330 train_time:12362ms step_avg:37.92ms
step:327/2330 train_time:12397ms step_avg:37.91ms
step:328/2330 train_time:12438ms step_avg:37.92ms
step:329/2330 train_time:12472ms step_avg:37.91ms
step:330/2330 train_time:12513ms step_avg:37.92ms
step:331/2330 train_time:12548ms step_avg:37.91ms
step:332/2330 train_time:12589ms step_avg:37.92ms
step:333/2330 train_time:12624ms step_avg:37.91ms
step:334/2330 train_time:12664ms step_avg:37.92ms
step:335/2330 train_time:12700ms step_avg:37.91ms
step:336/2330 train_time:12741ms step_avg:37.92ms
step:337/2330 train_time:12776ms step_avg:37.91ms
step:338/2330 train_time:12816ms step_avg:37.92ms
step:339/2330 train_time:12851ms step_avg:37.91ms
step:340/2330 train_time:12892ms step_avg:37.92ms
step:341/2330 train_time:12927ms step_avg:37.91ms
step:342/2330 train_time:12968ms step_avg:37.92ms
step:343/2330 train_time:13003ms step_avg:37.91ms
step:344/2330 train_time:13044ms step_avg:37.92ms
step:345/2330 train_time:13080ms step_avg:37.91ms
step:346/2330 train_time:13121ms step_avg:37.92ms
step:347/2330 train_time:13156ms step_avg:37.91ms
step:348/2330 train_time:13198ms step_avg:37.92ms
step:349/2330 train_time:13232ms step_avg:37.91ms
step:350/2330 train_time:13273ms step_avg:37.92ms
step:351/2330 train_time:13307ms step_avg:37.91ms
step:352/2330 train_time:13349ms step_avg:37.92ms
step:353/2330 train_time:13384ms step_avg:37.92ms
step:354/2330 train_time:13425ms step_avg:37.92ms
step:355/2330 train_time:13460ms step_avg:37.92ms
step:356/2330 train_time:13501ms step_avg:37.93ms
step:357/2330 train_time:13536ms step_avg:37.92ms
step:358/2330 train_time:13577ms step_avg:37.92ms
step:359/2330 train_time:13611ms step_avg:37.91ms
step:360/2330 train_time:13652ms step_avg:37.92ms
step:361/2330 train_time:13688ms step_avg:37.92ms
step:362/2330 train_time:13728ms step_avg:37.92ms
step:363/2330 train_time:13764ms step_avg:37.92ms
step:364/2330 train_time:13805ms step_avg:37.92ms
step:365/2330 train_time:13840ms step_avg:37.92ms
step:366/2330 train_time:13881ms step_avg:37.93ms
step:367/2330 train_time:13916ms step_avg:37.92ms
step:368/2330 train_time:13957ms step_avg:37.93ms
step:369/2330 train_time:13992ms step_avg:37.92ms
step:370/2330 train_time:14033ms step_avg:37.93ms
step:371/2330 train_time:14068ms step_avg:37.92ms
step:372/2330 train_time:14109ms step_avg:37.93ms
step:373/2330 train_time:14144ms step_avg:37.92ms
step:374/2330 train_time:14185ms step_avg:37.93ms
step:375/2330 train_time:14220ms step_avg:37.92ms
step:376/2330 train_time:14261ms step_avg:37.93ms
step:377/2330 train_time:14297ms step_avg:37.92ms
step:378/2330 train_time:14338ms step_avg:37.93ms
step:379/2330 train_time:14373ms step_avg:37.92ms
step:380/2330 train_time:14413ms step_avg:37.93ms
step:381/2330 train_time:14449ms step_avg:37.92ms
step:382/2330 train_time:14490ms step_avg:37.93ms
step:383/2330 train_time:14525ms step_avg:37.92ms
step:384/2330 train_time:14566ms step_avg:37.93ms
step:385/2330 train_time:14601ms step_avg:37.93ms
step:386/2330 train_time:14643ms step_avg:37.93ms
step:387/2330 train_time:14677ms step_avg:37.93ms
step:388/2330 train_time:14718ms step_avg:37.93ms
step:389/2330 train_time:14753ms step_avg:37.92ms
step:390/2330 train_time:14793ms step_avg:37.93ms
step:391/2330 train_time:14828ms step_avg:37.92ms
step:392/2330 train_time:14869ms step_avg:37.93ms
step:393/2330 train_time:14904ms step_avg:37.92ms
step:394/2330 train_time:14945ms step_avg:37.93ms
step:395/2330 train_time:14981ms step_avg:37.93ms
step:396/2330 train_time:15022ms step_avg:37.93ms
step:397/2330 train_time:15058ms step_avg:37.93ms
step:398/2330 train_time:15098ms step_avg:37.94ms
step:399/2330 train_time:15135ms step_avg:37.93ms
step:400/2330 train_time:15175ms step_avg:37.94ms
step:401/2330 train_time:15210ms step_avg:37.93ms
step:402/2330 train_time:15251ms step_avg:37.94ms
step:403/2330 train_time:15286ms step_avg:37.93ms
step:404/2330 train_time:15327ms step_avg:37.94ms
step:405/2330 train_time:15363ms step_avg:37.93ms
step:406/2330 train_time:15404ms step_avg:37.94ms
step:407/2330 train_time:15440ms step_avg:37.94ms
step:408/2330 train_time:15481ms step_avg:37.94ms
step:409/2330 train_time:15517ms step_avg:37.94ms
step:410/2330 train_time:15557ms step_avg:37.94ms
step:411/2330 train_time:15592ms step_avg:37.94ms
step:412/2330 train_time:15633ms step_avg:37.94ms
step:413/2330 train_time:15668ms step_avg:37.94ms
step:414/2330 train_time:15709ms step_avg:37.95ms
step:415/2330 train_time:15744ms step_avg:37.94ms
step:416/2330 train_time:15786ms step_avg:37.95ms
step:417/2330 train_time:15820ms step_avg:37.94ms
step:418/2330 train_time:15862ms step_avg:37.95ms
step:419/2330 train_time:15897ms step_avg:37.94ms
step:420/2330 train_time:15937ms step_avg:37.95ms
step:421/2330 train_time:15972ms step_avg:37.94ms
step:422/2330 train_time:16013ms step_avg:37.95ms
step:423/2330 train_time:16048ms step_avg:37.94ms
step:424/2330 train_time:16090ms step_avg:37.95ms
step:425/2330 train_time:16124ms step_avg:37.94ms
step:426/2330 train_time:16165ms step_avg:37.95ms
step:427/2330 train_time:16201ms step_avg:37.94ms
step:428/2330 train_time:16243ms step_avg:37.95ms
step:429/2330 train_time:16278ms step_avg:37.94ms
step:430/2330 train_time:16319ms step_avg:37.95ms
step:431/2330 train_time:16354ms step_avg:37.94ms
step:432/2330 train_time:16395ms step_avg:37.95ms
step:433/2330 train_time:16431ms step_avg:37.95ms
step:434/2330 train_time:16472ms step_avg:37.95ms
step:435/2330 train_time:16507ms step_avg:37.95ms
step:436/2330 train_time:16547ms step_avg:37.95ms
step:437/2330 train_time:16583ms step_avg:37.95ms
step:438/2330 train_time:16624ms step_avg:37.95ms
step:439/2330 train_time:16660ms step_avg:37.95ms
step:440/2330 train_time:16700ms step_avg:37.96ms
step:441/2330 train_time:16736ms step_avg:37.95ms
step:442/2330 train_time:16777ms step_avg:37.96ms
step:443/2330 train_time:16812ms step_avg:37.95ms
step:444/2330 train_time:16853ms step_avg:37.96ms
step:445/2330 train_time:16888ms step_avg:37.95ms
step:446/2330 train_time:16929ms step_avg:37.96ms
step:447/2330 train_time:16965ms step_avg:37.95ms
step:448/2330 train_time:17005ms step_avg:37.96ms
step:449/2330 train_time:17041ms step_avg:37.95ms
step:450/2330 train_time:17081ms step_avg:37.96ms
step:451/2330 train_time:17116ms step_avg:37.95ms
step:452/2330 train_time:17157ms step_avg:37.96ms
step:453/2330 train_time:17192ms step_avg:37.95ms
step:454/2330 train_time:17233ms step_avg:37.96ms
step:455/2330 train_time:17268ms step_avg:37.95ms
step:456/2330 train_time:17309ms step_avg:37.96ms
step:457/2330 train_time:17344ms step_avg:37.95ms
step:458/2330 train_time:17386ms step_avg:37.96ms
step:459/2330 train_time:17421ms step_avg:37.95ms
step:460/2330 train_time:17462ms step_avg:37.96ms
step:461/2330 train_time:17498ms step_avg:37.96ms
step:462/2330 train_time:17539ms step_avg:37.96ms
step:463/2330 train_time:17575ms step_avg:37.96ms
step:464/2330 train_time:17615ms step_avg:37.96ms
step:465/2330 train_time:17650ms step_avg:37.96ms
step:466/2330 train_time:17692ms step_avg:37.96ms
step:467/2330 train_time:17726ms step_avg:37.96ms
step:468/2330 train_time:17767ms step_avg:37.96ms
step:469/2330 train_time:17803ms step_avg:37.96ms
step:470/2330 train_time:17843ms step_avg:37.96ms
step:471/2330 train_time:17880ms step_avg:37.96ms
step:472/2330 train_time:17921ms step_avg:37.97ms
step:473/2330 train_time:17957ms step_avg:37.96ms
step:474/2330 train_time:17998ms step_avg:37.97ms
step:475/2330 train_time:18034ms step_avg:37.97ms
step:476/2330 train_time:18074ms step_avg:37.97ms
step:477/2330 train_time:18109ms step_avg:37.97ms
step:478/2330 train_time:18150ms step_avg:37.97ms
step:479/2330 train_time:18185ms step_avg:37.96ms
step:480/2330 train_time:18227ms step_avg:37.97ms
step:481/2330 train_time:18262ms step_avg:37.97ms
step:482/2330 train_time:18303ms step_avg:37.97ms
step:483/2330 train_time:18339ms step_avg:37.97ms
step:484/2330 train_time:18379ms step_avg:37.97ms
step:485/2330 train_time:18415ms step_avg:37.97ms
step:486/2330 train_time:18455ms step_avg:37.97ms
step:487/2330 train_time:18490ms step_avg:37.97ms
step:488/2330 train_time:18532ms step_avg:37.97ms
step:489/2330 train_time:18567ms step_avg:37.97ms
step:490/2330 train_time:18608ms step_avg:37.97ms
step:491/2330 train_time:18644ms step_avg:37.97ms
step:492/2330 train_time:18685ms step_avg:37.98ms
step:493/2330 train_time:18720ms step_avg:37.97ms
step:494/2330 train_time:18761ms step_avg:37.98ms
step:495/2330 train_time:18796ms step_avg:37.97ms
step:496/2330 train_time:18836ms step_avg:37.98ms
step:497/2330 train_time:18872ms step_avg:37.97ms
step:498/2330 train_time:18912ms step_avg:37.98ms
step:499/2330 train_time:18949ms step_avg:37.97ms
step:500/2330 train_time:18990ms step_avg:37.98ms
step:500/2330 val_loss:5.6506 train_time:19101ms step_avg:38.20ms
step:501/2330 train_time:19113ms step_avg:38.15ms
step:502/2330 train_time:19123ms step_avg:38.09ms
step:503/2330 train_time:19134ms step_avg:38.04ms
step:504/2330 train_time:19144ms step_avg:37.98ms
step:505/2330 train_time:19178ms step_avg:37.98ms
step:506/2330 train_time:19218ms step_avg:37.98ms
step:507/2330 train_time:19252ms step_avg:37.97ms
step:508/2330 train_time:19293ms step_avg:37.98ms
step:509/2330 train_time:19327ms step_avg:37.97ms
step:510/2330 train_time:19368ms step_avg:37.98ms
step:511/2330 train_time:19402ms step_avg:37.97ms
step:512/2330 train_time:19443ms step_avg:37.97ms
step:513/2330 train_time:19479ms step_avg:37.97ms
step:514/2330 train_time:19520ms step_avg:37.98ms
step:515/2330 train_time:19556ms step_avg:37.97ms
step:516/2330 train_time:19597ms step_avg:37.98ms
step:517/2330 train_time:19631ms step_avg:37.97ms
step:518/2330 train_time:19671ms step_avg:37.98ms
step:519/2330 train_time:19707ms step_avg:37.97ms
step:520/2330 train_time:19748ms step_avg:37.98ms
step:521/2330 train_time:19784ms step_avg:37.97ms
step:522/2330 train_time:19824ms step_avg:37.98ms
step:523/2330 train_time:19859ms step_avg:37.97ms
step:524/2330 train_time:19899ms step_avg:37.98ms
step:525/2330 train_time:19934ms step_avg:37.97ms
step:526/2330 train_time:19975ms step_avg:37.98ms
step:527/2330 train_time:20011ms step_avg:37.97ms
step:528/2330 train_time:20052ms step_avg:37.98ms
step:529/2330 train_time:20089ms step_avg:37.97ms
step:530/2330 train_time:20130ms step_avg:37.98ms
step:531/2330 train_time:20166ms step_avg:37.98ms
step:532/2330 train_time:20207ms step_avg:37.98ms
step:533/2330 train_time:20242ms step_avg:37.98ms
step:534/2330 train_time:20283ms step_avg:37.98ms
step:535/2330 train_time:20317ms step_avg:37.98ms
step:536/2330 train_time:20358ms step_avg:37.98ms
step:537/2330 train_time:20393ms step_avg:37.98ms
step:538/2330 train_time:20434ms step_avg:37.98ms
step:539/2330 train_time:20470ms step_avg:37.98ms
step:540/2330 train_time:20511ms step_avg:37.98ms
step:541/2330 train_time:20547ms step_avg:37.98ms
step:542/2330 train_time:20588ms step_avg:37.99ms
step:543/2330 train_time:20623ms step_avg:37.98ms
step:544/2330 train_time:20664ms step_avg:37.99ms
step:545/2330 train_time:20699ms step_avg:37.98ms
step:546/2330 train_time:20740ms step_avg:37.98ms
step:547/2330 train_time:20775ms step_avg:37.98ms
step:548/2330 train_time:20816ms step_avg:37.99ms
step:549/2330 train_time:20851ms step_avg:37.98ms
step:550/2330 train_time:20892ms step_avg:37.99ms
step:551/2330 train_time:20927ms step_avg:37.98ms
step:552/2330 train_time:20968ms step_avg:37.99ms
step:553/2330 train_time:21003ms step_avg:37.98ms
step:554/2330 train_time:21045ms step_avg:37.99ms
step:555/2330 train_time:21080ms step_avg:37.98ms
step:556/2330 train_time:21121ms step_avg:37.99ms
step:557/2330 train_time:21157ms step_avg:37.98ms
step:558/2330 train_time:21198ms step_avg:37.99ms
step:559/2330 train_time:21232ms step_avg:37.98ms
step:560/2330 train_time:21273ms step_avg:37.99ms
step:561/2330 train_time:21308ms step_avg:37.98ms
step:562/2330 train_time:21349ms step_avg:37.99ms
step:563/2330 train_time:21385ms step_avg:37.98ms
step:564/2330 train_time:21426ms step_avg:37.99ms
step:565/2330 train_time:21462ms step_avg:37.99ms
step:566/2330 train_time:21503ms step_avg:37.99ms
step:567/2330 train_time:21538ms step_avg:37.99ms
step:568/2330 train_time:21579ms step_avg:37.99ms
step:569/2330 train_time:21614ms step_avg:37.99ms
step:570/2330 train_time:21656ms step_avg:37.99ms
step:571/2330 train_time:21691ms step_avg:37.99ms
step:572/2330 train_time:21732ms step_avg:37.99ms
step:573/2330 train_time:21768ms step_avg:37.99ms
step:574/2330 train_time:21809ms step_avg:37.99ms
step:575/2330 train_time:21844ms step_avg:37.99ms
step:576/2330 train_time:21885ms step_avg:37.99ms
step:577/2330 train_time:21920ms step_avg:37.99ms
step:578/2330 train_time:21960ms step_avg:37.99ms
step:579/2330 train_time:21995ms step_avg:37.99ms
step:580/2330 train_time:22037ms step_avg:37.99ms
step:581/2330 train_time:22072ms step_avg:37.99ms
step:582/2330 train_time:22114ms step_avg:38.00ms
step:583/2330 train_time:22149ms step_avg:37.99ms
step:584/2330 train_time:22190ms step_avg:38.00ms
step:585/2330 train_time:22226ms step_avg:37.99ms
step:586/2330 train_time:22267ms step_avg:38.00ms
step:587/2330 train_time:22302ms step_avg:37.99ms
step:588/2330 train_time:22343ms step_avg:38.00ms
step:589/2330 train_time:22379ms step_avg:37.99ms
step:590/2330 train_time:22419ms step_avg:38.00ms
step:591/2330 train_time:22455ms step_avg:37.99ms
step:592/2330 train_time:22496ms step_avg:38.00ms
step:593/2330 train_time:22531ms step_avg:37.99ms
step:594/2330 train_time:22571ms step_avg:38.00ms
step:595/2330 train_time:22607ms step_avg:37.99ms
step:596/2330 train_time:22648ms step_avg:38.00ms
step:597/2330 train_time:22683ms step_avg:37.99ms
step:598/2330 train_time:22723ms step_avg:38.00ms
step:599/2330 train_time:22759ms step_avg:38.00ms
step:600/2330 train_time:22800ms step_avg:38.00ms
step:601/2330 train_time:22835ms step_avg:38.00ms
step:602/2330 train_time:22876ms step_avg:38.00ms
step:603/2330 train_time:22912ms step_avg:38.00ms
step:604/2330 train_time:22953ms step_avg:38.00ms
step:605/2330 train_time:22989ms step_avg:38.00ms
step:606/2330 train_time:23029ms step_avg:38.00ms
step:607/2330 train_time:23065ms step_avg:38.00ms
step:608/2330 train_time:23106ms step_avg:38.00ms
step:609/2330 train_time:23141ms step_avg:38.00ms
step:610/2330 train_time:23182ms step_avg:38.00ms
step:611/2330 train_time:23217ms step_avg:38.00ms
step:612/2330 train_time:23258ms step_avg:38.00ms
step:613/2330 train_time:23293ms step_avg:38.00ms
step:614/2330 train_time:23335ms step_avg:38.00ms
step:615/2330 train_time:23370ms step_avg:38.00ms
step:616/2330 train_time:23411ms step_avg:38.00ms
step:617/2330 train_time:23447ms step_avg:38.00ms
step:618/2330 train_time:23488ms step_avg:38.01ms
step:619/2330 train_time:23523ms step_avg:38.00ms
step:620/2330 train_time:23564ms step_avg:38.01ms
step:621/2330 train_time:23599ms step_avg:38.00ms
step:622/2330 train_time:23640ms step_avg:38.01ms
step:623/2330 train_time:23675ms step_avg:38.00ms
step:624/2330 train_time:23716ms step_avg:38.01ms
step:625/2330 train_time:23751ms step_avg:38.00ms
step:626/2330 train_time:23792ms step_avg:38.01ms
step:627/2330 train_time:23828ms step_avg:38.00ms
step:628/2330 train_time:23869ms step_avg:38.01ms
step:629/2330 train_time:23904ms step_avg:38.00ms
step:630/2330 train_time:23944ms step_avg:38.01ms
step:631/2330 train_time:23980ms step_avg:38.00ms
step:632/2330 train_time:24020ms step_avg:38.01ms
step:633/2330 train_time:24056ms step_avg:38.00ms
step:634/2330 train_time:24098ms step_avg:38.01ms
step:635/2330 train_time:24132ms step_avg:38.00ms
step:636/2330 train_time:24173ms step_avg:38.01ms
step:637/2330 train_time:24209ms step_avg:38.00ms
step:638/2330 train_time:24250ms step_avg:38.01ms
step:639/2330 train_time:24286ms step_avg:38.01ms
step:640/2330 train_time:24326ms step_avg:38.01ms
step:641/2330 train_time:24362ms step_avg:38.01ms
step:642/2330 train_time:24402ms step_avg:38.01ms
step:643/2330 train_time:24437ms step_avg:38.01ms
step:644/2330 train_time:24478ms step_avg:38.01ms
step:645/2330 train_time:24514ms step_avg:38.01ms
step:646/2330 train_time:24555ms step_avg:38.01ms
step:647/2330 train_time:24590ms step_avg:38.01ms
step:648/2330 train_time:24631ms step_avg:38.01ms
step:649/2330 train_time:24666ms step_avg:38.01ms
step:650/2330 train_time:24707ms step_avg:38.01ms
step:651/2330 train_time:24743ms step_avg:38.01ms
step:652/2330 train_time:24784ms step_avg:38.01ms
step:653/2330 train_time:24819ms step_avg:38.01ms
step:654/2330 train_time:24860ms step_avg:38.01ms
step:655/2330 train_time:24895ms step_avg:38.01ms
step:656/2330 train_time:24936ms step_avg:38.01ms
step:657/2330 train_time:24972ms step_avg:38.01ms
step:658/2330 train_time:25012ms step_avg:38.01ms
step:659/2330 train_time:25048ms step_avg:38.01ms
step:660/2330 train_time:25089ms step_avg:38.01ms
step:661/2330 train_time:25125ms step_avg:38.01ms
step:662/2330 train_time:25166ms step_avg:38.01ms
step:663/2330 train_time:25202ms step_avg:38.01ms
step:664/2330 train_time:25242ms step_avg:38.02ms
step:665/2330 train_time:25277ms step_avg:38.01ms
step:666/2330 train_time:25318ms step_avg:38.02ms
step:667/2330 train_time:25353ms step_avg:38.01ms
step:668/2330 train_time:25394ms step_avg:38.01ms
step:669/2330 train_time:25430ms step_avg:38.01ms
step:670/2330 train_time:25471ms step_avg:38.02ms
step:671/2330 train_time:25506ms step_avg:38.01ms
step:672/2330 train_time:25547ms step_avg:38.02ms
step:673/2330 train_time:25582ms step_avg:38.01ms
step:674/2330 train_time:25623ms step_avg:38.02ms
step:675/2330 train_time:25658ms step_avg:38.01ms
step:676/2330 train_time:25698ms step_avg:38.02ms
step:677/2330 train_time:25734ms step_avg:38.01ms
step:678/2330 train_time:25775ms step_avg:38.02ms
step:679/2330 train_time:25811ms step_avg:38.01ms
step:680/2330 train_time:25852ms step_avg:38.02ms
step:681/2330 train_time:25887ms step_avg:38.01ms
step:682/2330 train_time:25928ms step_avg:38.02ms
step:683/2330 train_time:25964ms step_avg:38.02ms
step:684/2330 train_time:26005ms step_avg:38.02ms
step:685/2330 train_time:26041ms step_avg:38.02ms
step:686/2330 train_time:26082ms step_avg:38.02ms
step:687/2330 train_time:26117ms step_avg:38.02ms
step:688/2330 train_time:26158ms step_avg:38.02ms
step:689/2330 train_time:26193ms step_avg:38.02ms
step:690/2330 train_time:26233ms step_avg:38.02ms
step:691/2330 train_time:26269ms step_avg:38.02ms
step:692/2330 train_time:26309ms step_avg:38.02ms
step:693/2330 train_time:26346ms step_avg:38.02ms
step:694/2330 train_time:26387ms step_avg:38.02ms
step:695/2330 train_time:26422ms step_avg:38.02ms
step:696/2330 train_time:26463ms step_avg:38.02ms
step:697/2330 train_time:26498ms step_avg:38.02ms
step:698/2330 train_time:26538ms step_avg:38.02ms
step:699/2330 train_time:26574ms step_avg:38.02ms
step:700/2330 train_time:26615ms step_avg:38.02ms
step:701/2330 train_time:26650ms step_avg:38.02ms
step:702/2330 train_time:26691ms step_avg:38.02ms
step:703/2330 train_time:26726ms step_avg:38.02ms
step:704/2330 train_time:26767ms step_avg:38.02ms
step:705/2330 train_time:26802ms step_avg:38.02ms
step:706/2330 train_time:26843ms step_avg:38.02ms
step:707/2330 train_time:26878ms step_avg:38.02ms
step:708/2330 train_time:26919ms step_avg:38.02ms
step:709/2330 train_time:26954ms step_avg:38.02ms
step:710/2330 train_time:26995ms step_avg:38.02ms
step:711/2330 train_time:27031ms step_avg:38.02ms
step:712/2330 train_time:27072ms step_avg:38.02ms
step:713/2330 train_time:27107ms step_avg:38.02ms
step:714/2330 train_time:27149ms step_avg:38.02ms
step:715/2330 train_time:27184ms step_avg:38.02ms
step:716/2330 train_time:27224ms step_avg:38.02ms
step:717/2330 train_time:27260ms step_avg:38.02ms
step:718/2330 train_time:27300ms step_avg:38.02ms
step:719/2330 train_time:27335ms step_avg:38.02ms
step:720/2330 train_time:27377ms step_avg:38.02ms
step:721/2330 train_time:27412ms step_avg:38.02ms
step:722/2330 train_time:27453ms step_avg:38.02ms
step:723/2330 train_time:27488ms step_avg:38.02ms
step:724/2330 train_time:27529ms step_avg:38.02ms
step:725/2330 train_time:27565ms step_avg:38.02ms
step:726/2330 train_time:27607ms step_avg:38.03ms
step:727/2330 train_time:27641ms step_avg:38.02ms
step:728/2330 train_time:27682ms step_avg:38.03ms
step:729/2330 train_time:27717ms step_avg:38.02ms
step:730/2330 train_time:27758ms step_avg:38.02ms
step:731/2330 train_time:27794ms step_avg:38.02ms
step:732/2330 train_time:27835ms step_avg:38.03ms
step:733/2330 train_time:27870ms step_avg:38.02ms
step:734/2330 train_time:27912ms step_avg:38.03ms
step:735/2330 train_time:27947ms step_avg:38.02ms
step:736/2330 train_time:27988ms step_avg:38.03ms
step:737/2330 train_time:28023ms step_avg:38.02ms
step:738/2330 train_time:28064ms step_avg:38.03ms
step:739/2330 train_time:28101ms step_avg:38.03ms
step:740/2330 train_time:28141ms step_avg:38.03ms
step:741/2330 train_time:28177ms step_avg:38.03ms
step:742/2330 train_time:28218ms step_avg:38.03ms
step:743/2330 train_time:28253ms step_avg:38.03ms
step:744/2330 train_time:28294ms step_avg:38.03ms
step:745/2330 train_time:28329ms step_avg:38.02ms
step:746/2330 train_time:28369ms step_avg:38.03ms
step:747/2330 train_time:28405ms step_avg:38.03ms
step:748/2330 train_time:28445ms step_avg:38.03ms
step:749/2330 train_time:28482ms step_avg:38.03ms
step:750/2330 train_time:28522ms step_avg:38.03ms
step:750/2330 val_loss:5.5433 train_time:28634ms step_avg:38.18ms
step:751/2330 train_time:28647ms step_avg:38.14ms
step:752/2330 train_time:28659ms step_avg:38.11ms
step:753/2330 train_time:28670ms step_avg:38.07ms
step:754/2330 train_time:28681ms step_avg:38.04ms
step:755/2330 train_time:28711ms step_avg:38.03ms
step:756/2330 train_time:28751ms step_avg:38.03ms
step:757/2330 train_time:28786ms step_avg:38.03ms
step:758/2330 train_time:28826ms step_avg:38.03ms
step:759/2330 train_time:28861ms step_avg:38.03ms
step:760/2330 train_time:28902ms step_avg:38.03ms
step:761/2330 train_time:28937ms step_avg:38.03ms
step:762/2330 train_time:28980ms step_avg:38.03ms
step:763/2330 train_time:29016ms step_avg:38.03ms
step:764/2330 train_time:29057ms step_avg:38.03ms
step:765/2330 train_time:29093ms step_avg:38.03ms
step:766/2330 train_time:29134ms step_avg:38.03ms
step:767/2330 train_time:29168ms step_avg:38.03ms
step:768/2330 train_time:29208ms step_avg:38.03ms
step:769/2330 train_time:29244ms step_avg:38.03ms
step:770/2330 train_time:29285ms step_avg:38.03ms
step:771/2330 train_time:29320ms step_avg:38.03ms
step:772/2330 train_time:29360ms step_avg:38.03ms
step:773/2330 train_time:29395ms step_avg:38.03ms
step:774/2330 train_time:29436ms step_avg:38.03ms
step:775/2330 train_time:29471ms step_avg:38.03ms
step:776/2330 train_time:29511ms step_avg:38.03ms
step:777/2330 train_time:29547ms step_avg:38.03ms
step:778/2330 train_time:29588ms step_avg:38.03ms
step:779/2330 train_time:29625ms step_avg:38.03ms
step:780/2330 train_time:29666ms step_avg:38.03ms
step:781/2330 train_time:29701ms step_avg:38.03ms
step:782/2330 train_time:29743ms step_avg:38.03ms
step:783/2330 train_time:29779ms step_avg:38.03ms
step:784/2330 train_time:29819ms step_avg:38.03ms
step:785/2330 train_time:29854ms step_avg:38.03ms
step:786/2330 train_time:29895ms step_avg:38.03ms
step:787/2330 train_time:29930ms step_avg:38.03ms
step:788/2330 train_time:29971ms step_avg:38.03ms
step:789/2330 train_time:30007ms step_avg:38.03ms
step:790/2330 train_time:30048ms step_avg:38.04ms
step:791/2330 train_time:30084ms step_avg:38.03ms
step:792/2330 train_time:30125ms step_avg:38.04ms
step:793/2330 train_time:30160ms step_avg:38.03ms
step:794/2330 train_time:30201ms step_avg:38.04ms
step:795/2330 train_time:30237ms step_avg:38.03ms
step:796/2330 train_time:30277ms step_avg:38.04ms
step:797/2330 train_time:30313ms step_avg:38.03ms
step:798/2330 train_time:30354ms step_avg:38.04ms
step:799/2330 train_time:30389ms step_avg:38.03ms
step:800/2330 train_time:30430ms step_avg:38.04ms
step:801/2330 train_time:30465ms step_avg:38.03ms
step:802/2330 train_time:30505ms step_avg:38.04ms
step:803/2330 train_time:30541ms step_avg:38.03ms
step:804/2330 train_time:30581ms step_avg:38.04ms
step:805/2330 train_time:30618ms step_avg:38.03ms
step:806/2330 train_time:30658ms step_avg:38.04ms
step:807/2330 train_time:30694ms step_avg:38.03ms
step:808/2330 train_time:30735ms step_avg:38.04ms
step:809/2330 train_time:30770ms step_avg:38.03ms
step:810/2330 train_time:30811ms step_avg:38.04ms
step:811/2330 train_time:30846ms step_avg:38.03ms
step:812/2330 train_time:30887ms step_avg:38.04ms
step:813/2330 train_time:30923ms step_avg:38.04ms
step:814/2330 train_time:30964ms step_avg:38.04ms
step:815/2330 train_time:31000ms step_avg:38.04ms
step:816/2330 train_time:31040ms step_avg:38.04ms
step:817/2330 train_time:31076ms step_avg:38.04ms
step:818/2330 train_time:31116ms step_avg:38.04ms
step:819/2330 train_time:31151ms step_avg:38.04ms
step:820/2330 train_time:31192ms step_avg:38.04ms
step:821/2330 train_time:31227ms step_avg:38.04ms
step:822/2330 train_time:31268ms step_avg:38.04ms
step:823/2330 train_time:31303ms step_avg:38.04ms
step:824/2330 train_time:31344ms step_avg:38.04ms
step:825/2330 train_time:31379ms step_avg:38.04ms
step:826/2330 train_time:31420ms step_avg:38.04ms
step:827/2330 train_time:31455ms step_avg:38.04ms
step:828/2330 train_time:31496ms step_avg:38.04ms
step:829/2330 train_time:31531ms step_avg:38.04ms
step:830/2330 train_time:31572ms step_avg:38.04ms
step:831/2330 train_time:31607ms step_avg:38.04ms
step:832/2330 train_time:31648ms step_avg:38.04ms
step:833/2330 train_time:31684ms step_avg:38.04ms
step:834/2330 train_time:31725ms step_avg:38.04ms
step:835/2330 train_time:31761ms step_avg:38.04ms
step:836/2330 train_time:31801ms step_avg:38.04ms
step:837/2330 train_time:31836ms step_avg:38.04ms
step:838/2330 train_time:31877ms step_avg:38.04ms
step:839/2330 train_time:31912ms step_avg:38.04ms
step:840/2330 train_time:31952ms step_avg:38.04ms
step:841/2330 train_time:31988ms step_avg:38.04ms
step:842/2330 train_time:32029ms step_avg:38.04ms
step:843/2330 train_time:32064ms step_avg:38.04ms
step:844/2330 train_time:32105ms step_avg:38.04ms
step:845/2330 train_time:32141ms step_avg:38.04ms
step:846/2330 train_time:32182ms step_avg:38.04ms
step:847/2330 train_time:32218ms step_avg:38.04ms
step:848/2330 train_time:32258ms step_avg:38.04ms
step:849/2330 train_time:32294ms step_avg:38.04ms
step:850/2330 train_time:32335ms step_avg:38.04ms
step:851/2330 train_time:32370ms step_avg:38.04ms
step:852/2330 train_time:32411ms step_avg:38.04ms
step:853/2330 train_time:32446ms step_avg:38.04ms
step:854/2330 train_time:32487ms step_avg:38.04ms
step:855/2330 train_time:32523ms step_avg:38.04ms
step:856/2330 train_time:32564ms step_avg:38.04ms
step:857/2330 train_time:32600ms step_avg:38.04ms
step:858/2330 train_time:32641ms step_avg:38.04ms
step:859/2330 train_time:32676ms step_avg:38.04ms
step:860/2330 train_time:32716ms step_avg:38.04ms
step:861/2330 train_time:32752ms step_avg:38.04ms
step:862/2330 train_time:32792ms step_avg:38.04ms
step:863/2330 train_time:32828ms step_avg:38.04ms
step:864/2330 train_time:32868ms step_avg:38.04ms
step:865/2330 train_time:32904ms step_avg:38.04ms
step:866/2330 train_time:32945ms step_avg:38.04ms
step:867/2330 train_time:32981ms step_avg:38.04ms
step:868/2330 train_time:33022ms step_avg:38.04ms
step:869/2330 train_time:33058ms step_avg:38.04ms
step:870/2330 train_time:33098ms step_avg:38.04ms
step:871/2330 train_time:33134ms step_avg:38.04ms
step:872/2330 train_time:33175ms step_avg:38.04ms
step:873/2330 train_time:33211ms step_avg:38.04ms
step:874/2330 train_time:33252ms step_avg:38.05ms
step:875/2330 train_time:33287ms step_avg:38.04ms
step:876/2330 train_time:33328ms step_avg:38.05ms
step:877/2330 train_time:33363ms step_avg:38.04ms
step:878/2330 train_time:33404ms step_avg:38.05ms
step:879/2330 train_time:33439ms step_avg:38.04ms
step:880/2330 train_time:33480ms step_avg:38.05ms
step:881/2330 train_time:33515ms step_avg:38.04ms
step:882/2330 train_time:33555ms step_avg:38.04ms
step:883/2330 train_time:33591ms step_avg:38.04ms
step:884/2330 train_time:33632ms step_avg:38.05ms
step:885/2330 train_time:33667ms step_avg:38.04ms
step:886/2330 train_time:33707ms step_avg:38.04ms
step:887/2330 train_time:33743ms step_avg:38.04ms
step:888/2330 train_time:33784ms step_avg:38.05ms
step:889/2330 train_time:33820ms step_avg:38.04ms
step:890/2330 train_time:33861ms step_avg:38.05ms
step:891/2330 train_time:33897ms step_avg:38.04ms
step:892/2330 train_time:33937ms step_avg:38.05ms
step:893/2330 train_time:33973ms step_avg:38.04ms
step:894/2330 train_time:34014ms step_avg:38.05ms
step:895/2330 train_time:34049ms step_avg:38.04ms
step:896/2330 train_time:34090ms step_avg:38.05ms
step:897/2330 train_time:34125ms step_avg:38.04ms
step:898/2330 train_time:34166ms step_avg:38.05ms
step:899/2330 train_time:34201ms step_avg:38.04ms
step:900/2330 train_time:34242ms step_avg:38.05ms
step:901/2330 train_time:34278ms step_avg:38.04ms
step:902/2330 train_time:34319ms step_avg:38.05ms
step:903/2330 train_time:34354ms step_avg:38.04ms
step:904/2330 train_time:34394ms step_avg:38.05ms
step:905/2330 train_time:34429ms step_avg:38.04ms
step:906/2330 train_time:34470ms step_avg:38.05ms
step:907/2330 train_time:34507ms step_avg:38.05ms
step:908/2330 train_time:34548ms step_avg:38.05ms
step:909/2330 train_time:34583ms step_avg:38.05ms
step:910/2330 train_time:34623ms step_avg:38.05ms
step:911/2330 train_time:34660ms step_avg:38.05ms
step:912/2330 train_time:34700ms step_avg:38.05ms
step:913/2330 train_time:34736ms step_avg:38.05ms
step:914/2330 train_time:34776ms step_avg:38.05ms
step:915/2330 train_time:34811ms step_avg:38.05ms
step:916/2330 train_time:34852ms step_avg:38.05ms
step:917/2330 train_time:34887ms step_avg:38.04ms
step:918/2330 train_time:34928ms step_avg:38.05ms
step:919/2330 train_time:34964ms step_avg:38.05ms
step:920/2330 train_time:35005ms step_avg:38.05ms
step:921/2330 train_time:35041ms step_avg:38.05ms
step:922/2330 train_time:35082ms step_avg:38.05ms
step:923/2330 train_time:35118ms step_avg:38.05ms
step:924/2330 train_time:35159ms step_avg:38.05ms
step:925/2330 train_time:35194ms step_avg:38.05ms
step:926/2330 train_time:35235ms step_avg:38.05ms
step:927/2330 train_time:35270ms step_avg:38.05ms
step:928/2330 train_time:35311ms step_avg:38.05ms
step:929/2330 train_time:35347ms step_avg:38.05ms
step:930/2330 train_time:35387ms step_avg:38.05ms
step:931/2330 train_time:35424ms step_avg:38.05ms
step:932/2330 train_time:35465ms step_avg:38.05ms
step:933/2330 train_time:35500ms step_avg:38.05ms
step:934/2330 train_time:35541ms step_avg:38.05ms
step:935/2330 train_time:35577ms step_avg:38.05ms
step:936/2330 train_time:35618ms step_avg:38.05ms
step:937/2330 train_time:35654ms step_avg:38.05ms
step:938/2330 train_time:35694ms step_avg:38.05ms
step:939/2330 train_time:35729ms step_avg:38.05ms
step:940/2330 train_time:35770ms step_avg:38.05ms
step:941/2330 train_time:35805ms step_avg:38.05ms
step:942/2330 train_time:35847ms step_avg:38.05ms
step:943/2330 train_time:35882ms step_avg:38.05ms
step:944/2330 train_time:35922ms step_avg:38.05ms
step:945/2330 train_time:35958ms step_avg:38.05ms
step:946/2330 train_time:35999ms step_avg:38.05ms
step:947/2330 train_time:36035ms step_avg:38.05ms
step:948/2330 train_time:36075ms step_avg:38.05ms
step:949/2330 train_time:36110ms step_avg:38.05ms
step:950/2330 train_time:36152ms step_avg:38.05ms
step:951/2330 train_time:36186ms step_avg:38.05ms
step:952/2330 train_time:36228ms step_avg:38.05ms
step:953/2330 train_time:36263ms step_avg:38.05ms
step:954/2330 train_time:36304ms step_avg:38.05ms
step:955/2330 train_time:36340ms step_avg:38.05ms
step:956/2330 train_time:36381ms step_avg:38.06ms
step:957/2330 train_time:36417ms step_avg:38.05ms
step:958/2330 train_time:36458ms step_avg:38.06ms
step:959/2330 train_time:36493ms step_avg:38.05ms
step:960/2330 train_time:36534ms step_avg:38.06ms
step:961/2330 train_time:36569ms step_avg:38.05ms
step:962/2330 train_time:36611ms step_avg:38.06ms
step:963/2330 train_time:36646ms step_avg:38.05ms
step:964/2330 train_time:36687ms step_avg:38.06ms
step:965/2330 train_time:36722ms step_avg:38.05ms
step:966/2330 train_time:36763ms step_avg:38.06ms
step:967/2330 train_time:36799ms step_avg:38.05ms
step:968/2330 train_time:36839ms step_avg:38.06ms
step:969/2330 train_time:36874ms step_avg:38.05ms
step:970/2330 train_time:36915ms step_avg:38.06ms
step:971/2330 train_time:36950ms step_avg:38.05ms
step:972/2330 train_time:36991ms step_avg:38.06ms
step:973/2330 train_time:37027ms step_avg:38.05ms
step:974/2330 train_time:37067ms step_avg:38.06ms
step:975/2330 train_time:37103ms step_avg:38.05ms
step:976/2330 train_time:37144ms step_avg:38.06ms
step:977/2330 train_time:37180ms step_avg:38.06ms
step:978/2330 train_time:37221ms step_avg:38.06ms
step:979/2330 train_time:37255ms step_avg:38.05ms
step:980/2330 train_time:37296ms step_avg:38.06ms
step:981/2330 train_time:37331ms step_avg:38.05ms
step:982/2330 train_time:37372ms step_avg:38.06ms
step:983/2330 train_time:37408ms step_avg:38.05ms
step:984/2330 train_time:37449ms step_avg:38.06ms
step:985/2330 train_time:37485ms step_avg:38.06ms
step:986/2330 train_time:37526ms step_avg:38.06ms
step:987/2330 train_time:37561ms step_avg:38.06ms
step:988/2330 train_time:37602ms step_avg:38.06ms
step:989/2330 train_time:37637ms step_avg:38.06ms
step:990/2330 train_time:37677ms step_avg:38.06ms
step:991/2330 train_time:37713ms step_avg:38.06ms
step:992/2330 train_time:37753ms step_avg:38.06ms
step:993/2330 train_time:37790ms step_avg:38.06ms
step:994/2330 train_time:37832ms step_avg:38.06ms
step:995/2330 train_time:37867ms step_avg:38.06ms
step:996/2330 train_time:37907ms step_avg:38.06ms
step:997/2330 train_time:37942ms step_avg:38.06ms
step:998/2330 train_time:37983ms step_avg:38.06ms
step:999/2330 train_time:38020ms step_avg:38.06ms
step:1000/2330 train_time:38060ms step_avg:38.06ms
step:1000/2330 val_loss:5.4791 train_time:38171ms step_avg:38.17ms
step:1001/2330 train_time:38183ms step_avg:38.14ms
step:1002/2330 train_time:38194ms step_avg:38.12ms
step:1003/2330 train_time:38204ms step_avg:38.09ms
step:1004/2330 train_time:38215ms step_avg:38.06ms
step:1005/2330 train_time:38247ms step_avg:38.06ms
step:1006/2330 train_time:38288ms step_avg:38.06ms
step:1007/2330 train_time:38322ms step_avg:38.06ms
step:1008/2330 train_time:38362ms step_avg:38.06ms
step:1009/2330 train_time:38397ms step_avg:38.05ms
step:1010/2330 train_time:38437ms step_avg:38.06ms
step:1011/2330 train_time:38472ms step_avg:38.05ms
step:1012/2330 train_time:38513ms step_avg:38.06ms
step:1013/2330 train_time:38551ms step_avg:38.06ms
step:1014/2330 train_time:38591ms step_avg:38.06ms
step:1015/2330 train_time:38628ms step_avg:38.06ms
step:1016/2330 train_time:38668ms step_avg:38.06ms
step:1017/2330 train_time:38705ms step_avg:38.06ms
step:1018/2330 train_time:38745ms step_avg:38.06ms
step:1019/2330 train_time:38781ms step_avg:38.06ms
step:1020/2330 train_time:38821ms step_avg:38.06ms
step:1021/2330 train_time:38857ms step_avg:38.06ms
step:1022/2330 train_time:38898ms step_avg:38.06ms
step:1023/2330 train_time:38933ms step_avg:38.06ms
step:1024/2330 train_time:38974ms step_avg:38.06ms
step:1025/2330 train_time:39008ms step_avg:38.06ms
step:1026/2330 train_time:39048ms step_avg:38.06ms
step:1027/2330 train_time:39085ms step_avg:38.06ms
step:1028/2330 train_time:39126ms step_avg:38.06ms
step:1029/2330 train_time:39165ms step_avg:38.06ms
step:1030/2330 train_time:39206ms step_avg:38.06ms
step:1031/2330 train_time:39243ms step_avg:38.06ms
step:1032/2330 train_time:39283ms step_avg:38.07ms
step:1033/2330 train_time:39319ms step_avg:38.06ms
step:1034/2330 train_time:39359ms step_avg:38.07ms
step:1035/2330 train_time:39395ms step_avg:38.06ms
step:1036/2330 train_time:39435ms step_avg:38.06ms
step:1037/2330 train_time:39470ms step_avg:38.06ms
step:1038/2330 train_time:39511ms step_avg:38.06ms
step:1039/2330 train_time:39546ms step_avg:38.06ms
step:1040/2330 train_time:39587ms step_avg:38.06ms
step:1041/2330 train_time:39622ms step_avg:38.06ms
step:1042/2330 train_time:39663ms step_avg:38.06ms
step:1043/2330 train_time:39698ms step_avg:38.06ms
step:1044/2330 train_time:39739ms step_avg:38.06ms
step:1045/2330 train_time:39775ms step_avg:38.06ms
step:1046/2330 train_time:39815ms step_avg:38.06ms
step:1047/2330 train_time:39851ms step_avg:38.06ms
step:1048/2330 train_time:39892ms step_avg:38.06ms
step:1049/2330 train_time:39926ms step_avg:38.06ms
step:1050/2330 train_time:39966ms step_avg:38.06ms
step:1051/2330 train_time:40001ms step_avg:38.06ms
step:1052/2330 train_time:40042ms step_avg:38.06ms
step:1053/2330 train_time:40079ms step_avg:38.06ms
step:1054/2330 train_time:40121ms step_avg:38.07ms
step:1055/2330 train_time:40157ms step_avg:38.06ms
step:1056/2330 train_time:40198ms step_avg:38.07ms
step:1057/2330 train_time:40234ms step_avg:38.06ms
step:1058/2330 train_time:40276ms step_avg:38.07ms
step:1059/2330 train_time:40310ms step_avg:38.06ms
step:1060/2330 train_time:40351ms step_avg:38.07ms
step:1061/2330 train_time:40385ms step_avg:38.06ms
step:1062/2330 train_time:40426ms step_avg:38.07ms
step:1063/2330 train_time:40461ms step_avg:38.06ms
step:1064/2330 train_time:40502ms step_avg:38.07ms
step:1065/2330 train_time:40538ms step_avg:38.06ms
step:1066/2330 train_time:40579ms step_avg:38.07ms
step:1067/2330 train_time:40614ms step_avg:38.06ms
step:1068/2330 train_time:40655ms step_avg:38.07ms
step:1069/2330 train_time:40689ms step_avg:38.06ms
step:1070/2330 train_time:40730ms step_avg:38.07ms
step:1071/2330 train_time:40765ms step_avg:38.06ms
step:1072/2330 train_time:40806ms step_avg:38.07ms
step:1073/2330 train_time:40840ms step_avg:38.06ms
step:1074/2330 train_time:40881ms step_avg:38.06ms
step:1075/2330 train_time:40916ms step_avg:38.06ms
step:1076/2330 train_time:40956ms step_avg:38.06ms
step:1077/2330 train_time:40993ms step_avg:38.06ms
step:1078/2330 train_time:41034ms step_avg:38.07ms
step:1079/2330 train_time:41069ms step_avg:38.06ms
step:1080/2330 train_time:41110ms step_avg:38.06ms
step:1081/2330 train_time:41144ms step_avg:38.06ms
step:1082/2330 train_time:41185ms step_avg:38.06ms
step:1083/2330 train_time:41221ms step_avg:38.06ms
step:1084/2330 train_time:41262ms step_avg:38.06ms
step:1085/2330 train_time:41297ms step_avg:38.06ms
step:1086/2330 train_time:41339ms step_avg:38.07ms
step:1087/2330 train_time:41374ms step_avg:38.06ms
step:1088/2330 train_time:41416ms step_avg:38.07ms
step:1089/2330 train_time:41451ms step_avg:38.06ms
step:1090/2330 train_time:41492ms step_avg:38.07ms
step:1091/2330 train_time:41527ms step_avg:38.06ms
step:1092/2330 train_time:41567ms step_avg:38.07ms
step:1093/2330 train_time:41602ms step_avg:38.06ms
step:1094/2330 train_time:41644ms step_avg:38.07ms
step:1095/2330 train_time:41678ms step_avg:38.06ms
step:1096/2330 train_time:41720ms step_avg:38.07ms
step:1097/2330 train_time:41754ms step_avg:38.06ms
step:1098/2330 train_time:41796ms step_avg:38.07ms
step:1099/2330 train_time:41830ms step_avg:38.06ms
step:1100/2330 train_time:41872ms step_avg:38.07ms
step:1101/2330 train_time:41906ms step_avg:38.06ms
step:1102/2330 train_time:41946ms step_avg:38.06ms
step:1103/2330 train_time:41983ms step_avg:38.06ms
step:1104/2330 train_time:42024ms step_avg:38.06ms
step:1105/2330 train_time:42059ms step_avg:38.06ms
step:1106/2330 train_time:42100ms step_avg:38.06ms
step:1107/2330 train_time:42136ms step_avg:38.06ms
step:1108/2330 train_time:42176ms step_avg:38.07ms
step:1109/2330 train_time:42212ms step_avg:38.06ms
step:1110/2330 train_time:42253ms step_avg:38.07ms
step:1111/2330 train_time:42289ms step_avg:38.06ms
step:1112/2330 train_time:42330ms step_avg:38.07ms
step:1113/2330 train_time:42365ms step_avg:38.06ms
step:1114/2330 train_time:42406ms step_avg:38.07ms
step:1115/2330 train_time:42441ms step_avg:38.06ms
step:1116/2330 train_time:42482ms step_avg:38.07ms
step:1117/2330 train_time:42517ms step_avg:38.06ms
step:1118/2330 train_time:42558ms step_avg:38.07ms
step:1119/2330 train_time:42593ms step_avg:38.06ms
step:1120/2330 train_time:42634ms step_avg:38.07ms
step:1121/2330 train_time:42669ms step_avg:38.06ms
step:1122/2330 train_time:42710ms step_avg:38.07ms
step:1123/2330 train_time:42744ms step_avg:38.06ms
step:1124/2330 train_time:42785ms step_avg:38.07ms
step:1125/2330 train_time:42820ms step_avg:38.06ms
step:1126/2330 train_time:42861ms step_avg:38.06ms
step:1127/2330 train_time:42896ms step_avg:38.06ms
step:1128/2330 train_time:42937ms step_avg:38.06ms
step:1129/2330 train_time:42972ms step_avg:38.06ms
step:1130/2330 train_time:43013ms step_avg:38.06ms
step:1131/2330 train_time:43048ms step_avg:38.06ms
step:1132/2330 train_time:43089ms step_avg:38.06ms
step:1133/2330 train_time:43123ms step_avg:38.06ms
step:1134/2330 train_time:43164ms step_avg:38.06ms
step:1135/2330 train_time:43200ms step_avg:38.06ms
step:1136/2330 train_time:43241ms step_avg:38.06ms
step:1137/2330 train_time:43277ms step_avg:38.06ms
step:1138/2330 train_time:43318ms step_avg:38.07ms
step:1139/2330 train_time:43353ms step_avg:38.06ms
step:1140/2330 train_time:43394ms step_avg:38.06ms
step:1141/2330 train_time:43429ms step_avg:38.06ms
step:1142/2330 train_time:43470ms step_avg:38.07ms
step:1143/2330 train_time:43505ms step_avg:38.06ms
step:1144/2330 train_time:43546ms step_avg:38.06ms
step:1145/2330 train_time:43582ms step_avg:38.06ms
step:1146/2330 train_time:43623ms step_avg:38.07ms
step:1147/2330 train_time:43658ms step_avg:38.06ms
step:1148/2330 train_time:43699ms step_avg:38.07ms
step:1149/2330 train_time:43734ms step_avg:38.06ms
step:1150/2330 train_time:43775ms step_avg:38.07ms
step:1151/2330 train_time:43812ms step_avg:38.06ms
step:1152/2330 train_time:43852ms step_avg:38.07ms
step:1153/2330 train_time:43888ms step_avg:38.06ms
step:1154/2330 train_time:43929ms step_avg:38.07ms
step:1155/2330 train_time:43963ms step_avg:38.06ms
step:1156/2330 train_time:44004ms step_avg:38.07ms
step:1157/2330 train_time:44039ms step_avg:38.06ms
step:1158/2330 train_time:44081ms step_avg:38.07ms
step:1159/2330 train_time:44115ms step_avg:38.06ms
step:1160/2330 train_time:44156ms step_avg:38.07ms
step:1161/2330 train_time:44190ms step_avg:38.06ms
step:1162/2330 train_time:44231ms step_avg:38.06ms
step:1163/2330 train_time:44266ms step_avg:38.06ms
step:1164/2330 train_time:44307ms step_avg:38.06ms
step:1165/2330 train_time:44342ms step_avg:38.06ms
step:1166/2330 train_time:44384ms step_avg:38.06ms
step:1167/2330 train_time:44418ms step_avg:38.06ms
step:1168/2330 train_time:44460ms step_avg:38.06ms
step:1169/2330 train_time:44494ms step_avg:38.06ms
step:1170/2330 train_time:44536ms step_avg:38.07ms
step:1171/2330 train_time:44571ms step_avg:38.06ms
step:1172/2330 train_time:44612ms step_avg:38.07ms
step:1173/2330 train_time:44646ms step_avg:38.06ms
step:1174/2330 train_time:44687ms step_avg:38.06ms
step:1175/2330 train_time:44722ms step_avg:38.06ms
step:1176/2330 train_time:44763ms step_avg:38.06ms
step:1177/2330 train_time:44798ms step_avg:38.06ms
step:1178/2330 train_time:44839ms step_avg:38.06ms
step:1179/2330 train_time:44874ms step_avg:38.06ms
step:1180/2330 train_time:44915ms step_avg:38.06ms
step:1181/2330 train_time:44950ms step_avg:38.06ms
step:1182/2330 train_time:44991ms step_avg:38.06ms
step:1183/2330 train_time:45026ms step_avg:38.06ms
step:1184/2330 train_time:45066ms step_avg:38.06ms
step:1185/2330 train_time:45102ms step_avg:38.06ms
step:1186/2330 train_time:45143ms step_avg:38.06ms
step:1187/2330 train_time:45177ms step_avg:38.06ms
step:1188/2330 train_time:45219ms step_avg:38.06ms
step:1189/2330 train_time:45254ms step_avg:38.06ms
step:1190/2330 train_time:45296ms step_avg:38.06ms
step:1191/2330 train_time:45331ms step_avg:38.06ms
step:1192/2330 train_time:45372ms step_avg:38.06ms
step:1193/2330 train_time:45408ms step_avg:38.06ms
step:1194/2330 train_time:45448ms step_avg:38.06ms
step:1195/2330 train_time:45484ms step_avg:38.06ms
step:1196/2330 train_time:45525ms step_avg:38.06ms
step:1197/2330 train_time:45560ms step_avg:38.06ms
step:1198/2330 train_time:45601ms step_avg:38.06ms
step:1199/2330 train_time:45636ms step_avg:38.06ms
step:1200/2330 train_time:45676ms step_avg:38.06ms
step:1201/2330 train_time:45711ms step_avg:38.06ms
step:1202/2330 train_time:45753ms step_avg:38.06ms
step:1203/2330 train_time:45788ms step_avg:38.06ms
step:1204/2330 train_time:45829ms step_avg:38.06ms
step:1205/2330 train_time:45864ms step_avg:38.06ms
step:1206/2330 train_time:45905ms step_avg:38.06ms
step:1207/2330 train_time:45941ms step_avg:38.06ms
step:1208/2330 train_time:45981ms step_avg:38.06ms
step:1209/2330 train_time:46017ms step_avg:38.06ms
step:1210/2330 train_time:46057ms step_avg:38.06ms
step:1211/2330 train_time:46092ms step_avg:38.06ms
step:1212/2330 train_time:46133ms step_avg:38.06ms
step:1213/2330 train_time:46168ms step_avg:38.06ms
step:1214/2330 train_time:46209ms step_avg:38.06ms
step:1215/2330 train_time:46244ms step_avg:38.06ms
step:1216/2330 train_time:46285ms step_avg:38.06ms
step:1217/2330 train_time:46320ms step_avg:38.06ms
step:1218/2330 train_time:46361ms step_avg:38.06ms
step:1219/2330 train_time:46396ms step_avg:38.06ms
step:1220/2330 train_time:46437ms step_avg:38.06ms
step:1221/2330 train_time:46473ms step_avg:38.06ms
step:1222/2330 train_time:46514ms step_avg:38.06ms
step:1223/2330 train_time:46549ms step_avg:38.06ms
step:1224/2330 train_time:46590ms step_avg:38.06ms
step:1225/2330 train_time:46624ms step_avg:38.06ms
step:1226/2330 train_time:46665ms step_avg:38.06ms
step:1227/2330 train_time:46700ms step_avg:38.06ms
step:1228/2330 train_time:46741ms step_avg:38.06ms
step:1229/2330 train_time:46776ms step_avg:38.06ms
step:1230/2330 train_time:46818ms step_avg:38.06ms
step:1231/2330 train_time:46853ms step_avg:38.06ms
step:1232/2330 train_time:46894ms step_avg:38.06ms
step:1233/2330 train_time:46929ms step_avg:38.06ms
step:1234/2330 train_time:46970ms step_avg:38.06ms
step:1235/2330 train_time:47005ms step_avg:38.06ms
step:1236/2330 train_time:47045ms step_avg:38.06ms
step:1237/2330 train_time:47081ms step_avg:38.06ms
step:1238/2330 train_time:47121ms step_avg:38.06ms
step:1239/2330 train_time:47156ms step_avg:38.06ms
step:1240/2330 train_time:47197ms step_avg:38.06ms
step:1241/2330 train_time:47233ms step_avg:38.06ms
step:1242/2330 train_time:47274ms step_avg:38.06ms
step:1243/2330 train_time:47308ms step_avg:38.06ms
step:1244/2330 train_time:47349ms step_avg:38.06ms
step:1245/2330 train_time:47384ms step_avg:38.06ms
step:1246/2330 train_time:47425ms step_avg:38.06ms
step:1247/2330 train_time:47460ms step_avg:38.06ms
step:1248/2330 train_time:47502ms step_avg:38.06ms
step:1249/2330 train_time:47537ms step_avg:38.06ms
step:1250/2330 train_time:47578ms step_avg:38.06ms
step:1250/2330 val_loss:5.4334 train_time:47690ms step_avg:38.15ms
step:1251/2330 train_time:47702ms step_avg:38.13ms
step:1252/2330 train_time:47714ms step_avg:38.11ms
step:1253/2330 train_time:47723ms step_avg:38.09ms
step:1254/2330 train_time:47734ms step_avg:38.07ms
step:1255/2330 train_time:47768ms step_avg:38.06ms
step:1256/2330 train_time:47809ms step_avg:38.06ms
step:1257/2330 train_time:47843ms step_avg:38.06ms
step:1258/2330 train_time:47883ms step_avg:38.06ms
step:1259/2330 train_time:47918ms step_avg:38.06ms
step:1260/2330 train_time:47959ms step_avg:38.06ms
step:1261/2330 train_time:47994ms step_avg:38.06ms
step:1262/2330 train_time:48036ms step_avg:38.06ms
step:1263/2330 train_time:48075ms step_avg:38.06ms
step:1264/2330 train_time:48116ms step_avg:38.07ms
step:1265/2330 train_time:48153ms step_avg:38.07ms
step:1266/2330 train_time:48194ms step_avg:38.07ms
step:1267/2330 train_time:48229ms step_avg:38.07ms
step:1268/2330 train_time:48271ms step_avg:38.07ms
step:1269/2330 train_time:48305ms step_avg:38.07ms
step:1270/2330 train_time:48346ms step_avg:38.07ms
step:1271/2330 train_time:48380ms step_avg:38.06ms
step:1272/2330 train_time:48421ms step_avg:38.07ms
step:1273/2330 train_time:48455ms step_avg:38.06ms
step:1274/2330 train_time:48496ms step_avg:38.07ms
step:1275/2330 train_time:48531ms step_avg:38.06ms
step:1276/2330 train_time:48572ms step_avg:38.07ms
step:1277/2330 train_time:48607ms step_avg:38.06ms
step:1278/2330 train_time:48647ms step_avg:38.07ms
step:1279/2330 train_time:48683ms step_avg:38.06ms
step:1280/2330 train_time:48723ms step_avg:38.06ms
step:1281/2330 train_time:48760ms step_avg:38.06ms
step:1282/2330 train_time:48800ms step_avg:38.07ms
step:1283/2330 train_time:48836ms step_avg:38.06ms
step:1284/2330 train_time:48876ms step_avg:38.07ms
step:1285/2330 train_time:48912ms step_avg:38.06ms
step:1286/2330 train_time:48952ms step_avg:38.07ms
step:1287/2330 train_time:48987ms step_avg:38.06ms
step:1288/2330 train_time:49028ms step_avg:38.07ms
step:1289/2330 train_time:49065ms step_avg:38.06ms
step:1290/2330 train_time:49105ms step_avg:38.07ms
step:1291/2330 train_time:49141ms step_avg:38.06ms
step:1292/2330 train_time:49181ms step_avg:38.07ms
step:1293/2330 train_time:49218ms step_avg:38.06ms
step:1294/2330 train_time:49259ms step_avg:38.07ms
step:1295/2330 train_time:49293ms step_avg:38.06ms
step:1296/2330 train_time:49334ms step_avg:38.07ms
step:1297/2330 train_time:49369ms step_avg:38.06ms
step:1298/2330 train_time:49409ms step_avg:38.07ms
step:1299/2330 train_time:49444ms step_avg:38.06ms
step:1300/2330 train_time:49484ms step_avg:38.06ms
step:1301/2330 train_time:49519ms step_avg:38.06ms
step:1302/2330 train_time:49560ms step_avg:38.06ms
step:1303/2330 train_time:49595ms step_avg:38.06ms
step:1304/2330 train_time:49636ms step_avg:38.06ms
step:1305/2330 train_time:49671ms step_avg:38.06ms
step:1306/2330 train_time:49713ms step_avg:38.06ms
step:1307/2330 train_time:49747ms step_avg:38.06ms
step:1308/2330 train_time:49789ms step_avg:38.06ms
step:1309/2330 train_time:49824ms step_avg:38.06ms
step:1310/2330 train_time:49865ms step_avg:38.06ms
step:1311/2330 train_time:49899ms step_avg:38.06ms
step:1312/2330 train_time:49940ms step_avg:38.06ms
step:1313/2330 train_time:49975ms step_avg:38.06ms
step:1314/2330 train_time:50016ms step_avg:38.06ms
step:1315/2330 train_time:50053ms step_avg:38.06ms
step:1316/2330 train_time:50094ms step_avg:38.07ms
step:1317/2330 train_time:50129ms step_avg:38.06ms
step:1318/2330 train_time:50170ms step_avg:38.07ms
step:1319/2330 train_time:50206ms step_avg:38.06ms
step:1320/2330 train_time:50247ms step_avg:38.07ms
step:1321/2330 train_time:50282ms step_avg:38.06ms
step:1322/2330 train_time:50323ms step_avg:38.07ms
step:1323/2330 train_time:50358ms step_avg:38.06ms
step:1324/2330 train_time:50399ms step_avg:38.07ms
step:1325/2330 train_time:50434ms step_avg:38.06ms
step:1326/2330 train_time:50475ms step_avg:38.07ms
step:1327/2330 train_time:50510ms step_avg:38.06ms
step:1328/2330 train_time:50551ms step_avg:38.07ms
step:1329/2330 train_time:50586ms step_avg:38.06ms
step:1330/2330 train_time:50627ms step_avg:38.07ms
step:1331/2330 train_time:50661ms step_avg:38.06ms
step:1332/2330 train_time:50702ms step_avg:38.06ms
step:1333/2330 train_time:50737ms step_avg:38.06ms
step:1334/2330 train_time:50778ms step_avg:38.06ms
step:1335/2330 train_time:50814ms step_avg:38.06ms
step:1336/2330 train_time:50855ms step_avg:38.07ms
step:1337/2330 train_time:50889ms step_avg:38.06ms
step:1338/2330 train_time:50930ms step_avg:38.06ms
step:1339/2330 train_time:50965ms step_avg:38.06ms
step:1340/2330 train_time:51006ms step_avg:38.06ms
step:1341/2330 train_time:51041ms step_avg:38.06ms
step:1342/2330 train_time:51082ms step_avg:38.06ms
step:1343/2330 train_time:51117ms step_avg:38.06ms
step:1344/2330 train_time:51158ms step_avg:38.06ms
step:1345/2330 train_time:51193ms step_avg:38.06ms
step:1346/2330 train_time:51234ms step_avg:38.06ms
step:1347/2330 train_time:51270ms step_avg:38.06ms
step:1348/2330 train_time:51311ms step_avg:38.06ms
step:1349/2330 train_time:51346ms step_avg:38.06ms
step:1350/2330 train_time:51387ms step_avg:38.06ms
step:1351/2330 train_time:51422ms step_avg:38.06ms
step:1352/2330 train_time:51462ms step_avg:38.06ms
step:1353/2330 train_time:51497ms step_avg:38.06ms
step:1354/2330 train_time:51538ms step_avg:38.06ms
step:1355/2330 train_time:51572ms step_avg:38.06ms
step:1356/2330 train_time:51613ms step_avg:38.06ms
step:1357/2330 train_time:51649ms step_avg:38.06ms
step:1358/2330 train_time:51690ms step_avg:38.06ms
step:1359/2330 train_time:51726ms step_avg:38.06ms
step:1360/2330 train_time:51767ms step_avg:38.06ms
step:1361/2330 train_time:51801ms step_avg:38.06ms
step:1362/2330 train_time:51841ms step_avg:38.06ms
step:1363/2330 train_time:51876ms step_avg:38.06ms
step:1364/2330 train_time:51917ms step_avg:38.06ms
step:1365/2330 train_time:51952ms step_avg:38.06ms
step:1366/2330 train_time:51993ms step_avg:38.06ms
step:1367/2330 train_time:52028ms step_avg:38.06ms
step:1368/2330 train_time:52069ms step_avg:38.06ms
step:1369/2330 train_time:52105ms step_avg:38.06ms
step:1370/2330 train_time:52146ms step_avg:38.06ms
step:1371/2330 train_time:52181ms step_avg:38.06ms
step:1372/2330 train_time:52222ms step_avg:38.06ms
step:1373/2330 train_time:52256ms step_avg:38.06ms
step:1374/2330 train_time:52297ms step_avg:38.06ms
step:1375/2330 train_time:52333ms step_avg:38.06ms
step:1376/2330 train_time:52374ms step_avg:38.06ms
step:1377/2330 train_time:52408ms step_avg:38.06ms
step:1378/2330 train_time:52450ms step_avg:38.06ms
step:1379/2330 train_time:52485ms step_avg:38.06ms
step:1380/2330 train_time:52526ms step_avg:38.06ms
step:1381/2330 train_time:52560ms step_avg:38.06ms
step:1382/2330 train_time:52601ms step_avg:38.06ms
step:1383/2330 train_time:52637ms step_avg:38.06ms
step:1384/2330 train_time:52677ms step_avg:38.06ms
step:1385/2330 train_time:52713ms step_avg:38.06ms
step:1386/2330 train_time:52754ms step_avg:38.06ms
step:1387/2330 train_time:52790ms step_avg:38.06ms
step:1388/2330 train_time:52831ms step_avg:38.06ms
step:1389/2330 train_time:52866ms step_avg:38.06ms
step:1390/2330 train_time:52907ms step_avg:38.06ms
step:1391/2330 train_time:52942ms step_avg:38.06ms
step:1392/2330 train_time:52982ms step_avg:38.06ms
step:1393/2330 train_time:53018ms step_avg:38.06ms
step:1394/2330 train_time:53059ms step_avg:38.06ms
step:1395/2330 train_time:53094ms step_avg:38.06ms
step:1396/2330 train_time:53135ms step_avg:38.06ms
step:1397/2330 train_time:53171ms step_avg:38.06ms
step:1398/2330 train_time:53212ms step_avg:38.06ms
step:1399/2330 train_time:53248ms step_avg:38.06ms
step:1400/2330 train_time:53289ms step_avg:38.06ms
step:1401/2330 train_time:53324ms step_avg:38.06ms
step:1402/2330 train_time:53365ms step_avg:38.06ms
step:1403/2330 train_time:53400ms step_avg:38.06ms
step:1404/2330 train_time:53440ms step_avg:38.06ms
step:1405/2330 train_time:53476ms step_avg:38.06ms
step:1406/2330 train_time:53517ms step_avg:38.06ms
step:1407/2330 train_time:53552ms step_avg:38.06ms
step:1408/2330 train_time:53594ms step_avg:38.06ms
step:1409/2330 train_time:53629ms step_avg:38.06ms
step:1410/2330 train_time:53671ms step_avg:38.06ms
step:1411/2330 train_time:53706ms step_avg:38.06ms
step:1412/2330 train_time:53747ms step_avg:38.06ms
step:1413/2330 train_time:53782ms step_avg:38.06ms
step:1414/2330 train_time:53823ms step_avg:38.06ms
step:1415/2330 train_time:53858ms step_avg:38.06ms
step:1416/2330 train_time:53899ms step_avg:38.06ms
step:1417/2330 train_time:53934ms step_avg:38.06ms
step:1418/2330 train_time:53976ms step_avg:38.06ms
step:1419/2330 train_time:54010ms step_avg:38.06ms
step:1420/2330 train_time:54052ms step_avg:38.06ms
step:1421/2330 train_time:54087ms step_avg:38.06ms
step:1422/2330 train_time:54128ms step_avg:38.06ms
step:1423/2330 train_time:54162ms step_avg:38.06ms
step:1424/2330 train_time:54203ms step_avg:38.06ms
step:1425/2330 train_time:54238ms step_avg:38.06ms
step:1426/2330 train_time:54280ms step_avg:38.06ms
step:1427/2330 train_time:54315ms step_avg:38.06ms
step:1428/2330 train_time:54357ms step_avg:38.06ms
step:1429/2330 train_time:54392ms step_avg:38.06ms
step:1430/2330 train_time:54433ms step_avg:38.07ms
step:1431/2330 train_time:54468ms step_avg:38.06ms
step:1432/2330 train_time:54509ms step_avg:38.07ms
step:1433/2330 train_time:54544ms step_avg:38.06ms
step:1434/2330 train_time:54585ms step_avg:38.07ms
step:1435/2330 train_time:54620ms step_avg:38.06ms
step:1436/2330 train_time:54661ms step_avg:38.06ms
step:1437/2330 train_time:54696ms step_avg:38.06ms
step:1438/2330 train_time:54737ms step_avg:38.06ms
step:1439/2330 train_time:54773ms step_avg:38.06ms
step:1440/2330 train_time:54813ms step_avg:38.06ms
step:1441/2330 train_time:54849ms step_avg:38.06ms
step:1442/2330 train_time:54890ms step_avg:38.07ms
step:1443/2330 train_time:54926ms step_avg:38.06ms
step:1444/2330 train_time:54967ms step_avg:38.07ms
step:1445/2330 train_time:55001ms step_avg:38.06ms
step:1446/2330 train_time:55042ms step_avg:38.06ms
step:1447/2330 train_time:55077ms step_avg:38.06ms
step:1448/2330 train_time:55118ms step_avg:38.07ms
step:1449/2330 train_time:55153ms step_avg:38.06ms
step:1450/2330 train_time:55194ms step_avg:38.07ms
step:1451/2330 train_time:55230ms step_avg:38.06ms
step:1452/2330 train_time:55271ms step_avg:38.07ms
step:1453/2330 train_time:55306ms step_avg:38.06ms
step:1454/2330 train_time:55348ms step_avg:38.07ms
step:1455/2330 train_time:55382ms step_avg:38.06ms
step:1456/2330 train_time:55423ms step_avg:38.07ms
step:1457/2330 train_time:55458ms step_avg:38.06ms
step:1458/2330 train_time:55499ms step_avg:38.07ms
step:1459/2330 train_time:55534ms step_avg:38.06ms
step:1460/2330 train_time:55576ms step_avg:38.07ms
step:1461/2330 train_time:55611ms step_avg:38.06ms
step:1462/2330 train_time:55653ms step_avg:38.07ms
step:1463/2330 train_time:55687ms step_avg:38.06ms
step:1464/2330 train_time:55729ms step_avg:38.07ms
step:1465/2330 train_time:55763ms step_avg:38.06ms
step:1466/2330 train_time:55804ms step_avg:38.07ms
step:1467/2330 train_time:55839ms step_avg:38.06ms
step:1468/2330 train_time:55880ms step_avg:38.07ms
step:1469/2330 train_time:55915ms step_avg:38.06ms
step:1470/2330 train_time:55957ms step_avg:38.07ms
step:1471/2330 train_time:55991ms step_avg:38.06ms
step:1472/2330 train_time:56031ms step_avg:38.06ms
step:1473/2330 train_time:56067ms step_avg:38.06ms
step:1474/2330 train_time:56108ms step_avg:38.06ms
step:1475/2330 train_time:56143ms step_avg:38.06ms
step:1476/2330 train_time:56184ms step_avg:38.06ms
step:1477/2330 train_time:56219ms step_avg:38.06ms
step:1478/2330 train_time:56260ms step_avg:38.06ms
step:1479/2330 train_time:56294ms step_avg:38.06ms
step:1480/2330 train_time:56335ms step_avg:38.06ms
step:1481/2330 train_time:56371ms step_avg:38.06ms
step:1482/2330 train_time:56411ms step_avg:38.06ms
step:1483/2330 train_time:56446ms step_avg:38.06ms
step:1484/2330 train_time:56487ms step_avg:38.06ms
step:1485/2330 train_time:56522ms step_avg:38.06ms
step:1486/2330 train_time:56563ms step_avg:38.06ms
step:1487/2330 train_time:56598ms step_avg:38.06ms
step:1488/2330 train_time:56639ms step_avg:38.06ms
step:1489/2330 train_time:56675ms step_avg:38.06ms
step:1490/2330 train_time:56716ms step_avg:38.06ms
step:1491/2330 train_time:56752ms step_avg:38.06ms
step:1492/2330 train_time:56793ms step_avg:38.07ms
step:1493/2330 train_time:56829ms step_avg:38.06ms
step:1494/2330 train_time:56870ms step_avg:38.07ms
step:1495/2330 train_time:56905ms step_avg:38.06ms
step:1496/2330 train_time:56946ms step_avg:38.07ms
step:1497/2330 train_time:56981ms step_avg:38.06ms
step:1498/2330 train_time:57022ms step_avg:38.07ms
step:1499/2330 train_time:57056ms step_avg:38.06ms
step:1500/2330 train_time:57098ms step_avg:38.07ms
step:1500/2330 val_loss:5.3808 train_time:57209ms step_avg:38.14ms
step:1501/2330 train_time:57222ms step_avg:38.12ms
step:1502/2330 train_time:57233ms step_avg:38.10ms
step:1503/2330 train_time:57243ms step_avg:38.09ms
step:1504/2330 train_time:57255ms step_avg:38.07ms
step:1505/2330 train_time:57286ms step_avg:38.06ms
step:1506/2330 train_time:57327ms step_avg:38.07ms
step:1507/2330 train_time:57361ms step_avg:38.06ms
step:1508/2330 train_time:57401ms step_avg:38.06ms
step:1509/2330 train_time:57436ms step_avg:38.06ms
step:1510/2330 train_time:57477ms step_avg:38.06ms
step:1511/2330 train_time:57512ms step_avg:38.06ms
step:1512/2330 train_time:57554ms step_avg:38.06ms
step:1513/2330 train_time:57590ms step_avg:38.06ms
step:1514/2330 train_time:57631ms step_avg:38.07ms
step:1515/2330 train_time:57667ms step_avg:38.06ms
step:1516/2330 train_time:57708ms step_avg:38.07ms
step:1517/2330 train_time:57743ms step_avg:38.06ms
step:1518/2330 train_time:57784ms step_avg:38.07ms
step:1519/2330 train_time:57819ms step_avg:38.06ms
step:1520/2330 train_time:57860ms step_avg:38.07ms
step:1521/2330 train_time:57895ms step_avg:38.06ms
step:1522/2330 train_time:57935ms step_avg:38.07ms
step:1523/2330 train_time:57970ms step_avg:38.06ms
step:1524/2330 train_time:58010ms step_avg:38.06ms
step:1525/2330 train_time:58045ms step_avg:38.06ms
step:1526/2330 train_time:58086ms step_avg:38.06ms
step:1527/2330 train_time:58121ms step_avg:38.06ms
step:1528/2330 train_time:58163ms step_avg:38.06ms
step:1529/2330 train_time:58198ms step_avg:38.06ms
step:1530/2330 train_time:58241ms step_avg:38.07ms
step:1531/2330 train_time:58275ms step_avg:38.06ms
step:1532/2330 train_time:58316ms step_avg:38.07ms
step:1533/2330 train_time:58351ms step_avg:38.06ms
step:1534/2330 train_time:58392ms step_avg:38.06ms
step:1535/2330 train_time:58426ms step_avg:38.06ms
step:1536/2330 train_time:58467ms step_avg:38.06ms
step:1537/2330 train_time:58502ms step_avg:38.06ms
step:1538/2330 train_time:58544ms step_avg:38.07ms
step:1539/2330 train_time:58579ms step_avg:38.06ms
step:1540/2330 train_time:58620ms step_avg:38.07ms
step:1541/2330 train_time:58657ms step_avg:38.06ms
step:1542/2330 train_time:58698ms step_avg:38.07ms
step:1543/2330 train_time:58732ms step_avg:38.06ms
step:1544/2330 train_time:58773ms step_avg:38.07ms
step:1545/2330 train_time:58808ms step_avg:38.06ms
step:1546/2330 train_time:58849ms step_avg:38.07ms
step:1547/2330 train_time:58885ms step_avg:38.06ms
step:1548/2330 train_time:58926ms step_avg:38.07ms
step:1549/2330 train_time:58960ms step_avg:38.06ms
step:1550/2330 train_time:59001ms step_avg:38.07ms
step:1551/2330 train_time:59037ms step_avg:38.06ms
step:1552/2330 train_time:59077ms step_avg:38.07ms
step:1553/2330 train_time:59114ms step_avg:38.06ms
step:1554/2330 train_time:59155ms step_avg:38.07ms
step:1555/2330 train_time:59190ms step_avg:38.06ms
step:1556/2330 train_time:59231ms step_avg:38.07ms
step:1557/2330 train_time:59265ms step_avg:38.06ms
step:1558/2330 train_time:59306ms step_avg:38.07ms
step:1559/2330 train_time:59341ms step_avg:38.06ms
step:1560/2330 train_time:59383ms step_avg:38.07ms
step:1561/2330 train_time:59418ms step_avg:38.06ms
step:1562/2330 train_time:59459ms step_avg:38.07ms
step:1563/2330 train_time:59494ms step_avg:38.06ms
step:1564/2330 train_time:59535ms step_avg:38.07ms
step:1565/2330 train_time:59571ms step_avg:38.06ms
step:1566/2330 train_time:59611ms step_avg:38.07ms
step:1567/2330 train_time:59647ms step_avg:38.06ms
step:1568/2330 train_time:59688ms step_avg:38.07ms
step:1569/2330 train_time:59723ms step_avg:38.06ms
step:1570/2330 train_time:59764ms step_avg:38.07ms
step:1571/2330 train_time:59799ms step_avg:38.06ms
step:1572/2330 train_time:59840ms step_avg:38.07ms
step:1573/2330 train_time:59874ms step_avg:38.06ms
step:1574/2330 train_time:59915ms step_avg:38.07ms
step:1575/2330 train_time:59950ms step_avg:38.06ms
step:1576/2330 train_time:59991ms step_avg:38.07ms
step:1577/2330 train_time:60026ms step_avg:38.06ms
step:1578/2330 train_time:60067ms step_avg:38.07ms
step:1579/2330 train_time:60103ms step_avg:38.06ms
step:1580/2330 train_time:60144ms step_avg:38.07ms
step:1581/2330 train_time:60179ms step_avg:38.06ms
step:1582/2330 train_time:60220ms step_avg:38.07ms
step:1583/2330 train_time:60256ms step_avg:38.06ms
step:1584/2330 train_time:60297ms step_avg:38.07ms
step:1585/2330 train_time:60334ms step_avg:38.07ms
step:1586/2330 train_time:60374ms step_avg:38.07ms
step:1587/2330 train_time:60409ms step_avg:38.06ms
step:1588/2330 train_time:60449ms step_avg:38.07ms
step:1589/2330 train_time:60484ms step_avg:38.06ms
step:1590/2330 train_time:60526ms step_avg:38.07ms
step:1591/2330 train_time:60561ms step_avg:38.06ms
step:1592/2330 train_time:60601ms step_avg:38.07ms
step:1593/2330 train_time:60637ms step_avg:38.06ms
step:1594/2330 train_time:60678ms step_avg:38.07ms
step:1595/2330 train_time:60714ms step_avg:38.07ms
step:1596/2330 train_time:60755ms step_avg:38.07ms
step:1597/2330 train_time:60791ms step_avg:38.07ms
step:1598/2330 train_time:60831ms step_avg:38.07ms
step:1599/2330 train_time:60866ms step_avg:38.07ms
step:1600/2330 train_time:60907ms step_avg:38.07ms
step:1601/2330 train_time:60942ms step_avg:38.07ms
step:1602/2330 train_time:60983ms step_avg:38.07ms
step:1603/2330 train_time:61019ms step_avg:38.07ms
step:1604/2330 train_time:61060ms step_avg:38.07ms
step:1605/2330 train_time:61096ms step_avg:38.07ms
step:1606/2330 train_time:61137ms step_avg:38.07ms
step:1607/2330 train_time:61171ms step_avg:38.07ms
step:1608/2330 train_time:61212ms step_avg:38.07ms
step:1609/2330 train_time:61247ms step_avg:38.07ms
step:1610/2330 train_time:61288ms step_avg:38.07ms
step:1611/2330 train_time:61324ms step_avg:38.07ms
step:1612/2330 train_time:61365ms step_avg:38.07ms
step:1613/2330 train_time:61401ms step_avg:38.07ms
step:1614/2330 train_time:61441ms step_avg:38.07ms
step:1615/2330 train_time:61476ms step_avg:38.07ms
step:1616/2330 train_time:61517ms step_avg:38.07ms
step:1617/2330 train_time:61552ms step_avg:38.07ms
step:1618/2330 train_time:61593ms step_avg:38.07ms
step:1619/2330 train_time:61627ms step_avg:38.06ms
step:1620/2330 train_time:61668ms step_avg:38.07ms
step:1621/2330 train_time:61703ms step_avg:38.06ms
step:1622/2330 train_time:61744ms step_avg:38.07ms
step:1623/2330 train_time:61779ms step_avg:38.06ms
step:1624/2330 train_time:61820ms step_avg:38.07ms
step:1625/2330 train_time:61856ms step_avg:38.07ms
step:1626/2330 train_time:61897ms step_avg:38.07ms
step:1627/2330 train_time:61932ms step_avg:38.07ms
step:1628/2330 train_time:61973ms step_avg:38.07ms
step:1629/2330 train_time:62008ms step_avg:38.07ms
step:1630/2330 train_time:62049ms step_avg:38.07ms
step:1631/2330 train_time:62084ms step_avg:38.06ms
step:1632/2330 train_time:62125ms step_avg:38.07ms
step:1633/2330 train_time:62160ms step_avg:38.06ms
step:1634/2330 train_time:62201ms step_avg:38.07ms
step:1635/2330 train_time:62237ms step_avg:38.07ms
step:1636/2330 train_time:62278ms step_avg:38.07ms
step:1637/2330 train_time:62313ms step_avg:38.07ms
step:1638/2330 train_time:62354ms step_avg:38.07ms
step:1639/2330 train_time:62389ms step_avg:38.07ms
step:1640/2330 train_time:62430ms step_avg:38.07ms
step:1641/2330 train_time:62464ms step_avg:38.06ms
step:1642/2330 train_time:62505ms step_avg:38.07ms
step:1643/2330 train_time:62541ms step_avg:38.06ms
step:1644/2330 train_time:62582ms step_avg:38.07ms
step:1645/2330 train_time:62617ms step_avg:38.07ms
step:1646/2330 train_time:62658ms step_avg:38.07ms
step:1647/2330 train_time:62694ms step_avg:38.07ms
step:1648/2330 train_time:62735ms step_avg:38.07ms
step:1649/2330 train_time:62770ms step_avg:38.07ms
step:1650/2330 train_time:62811ms step_avg:38.07ms
step:1651/2330 train_time:62845ms step_avg:38.06ms
step:1652/2330 train_time:62886ms step_avg:38.07ms
step:1653/2330 train_time:62921ms step_avg:38.06ms
step:1654/2330 train_time:62962ms step_avg:38.07ms
step:1655/2330 train_time:62998ms step_avg:38.07ms
step:1656/2330 train_time:63039ms step_avg:38.07ms
step:1657/2330 train_time:63074ms step_avg:38.07ms
step:1658/2330 train_time:63116ms step_avg:38.07ms
step:1659/2330 train_time:63150ms step_avg:38.07ms
step:1660/2330 train_time:63191ms step_avg:38.07ms
step:1661/2330 train_time:63227ms step_avg:38.07ms
step:1662/2330 train_time:63267ms step_avg:38.07ms
step:1663/2330 train_time:63302ms step_avg:38.06ms
step:1664/2330 train_time:63343ms step_avg:38.07ms
step:1665/2330 train_time:63378ms step_avg:38.06ms
step:1666/2330 train_time:63419ms step_avg:38.07ms
step:1667/2330 train_time:63455ms step_avg:38.07ms
step:1668/2330 train_time:63496ms step_avg:38.07ms
step:1669/2330 train_time:63531ms step_avg:38.07ms
step:1670/2330 train_time:63572ms step_avg:38.07ms
step:1671/2330 train_time:63607ms step_avg:38.07ms
step:1672/2330 train_time:63648ms step_avg:38.07ms
step:1673/2330 train_time:63683ms step_avg:38.06ms
step:1674/2330 train_time:63724ms step_avg:38.07ms
step:1675/2330 train_time:63758ms step_avg:38.06ms
step:1676/2330 train_time:63799ms step_avg:38.07ms
step:1677/2330 train_time:63836ms step_avg:38.07ms
step:1678/2330 train_time:63877ms step_avg:38.07ms
step:1679/2330 train_time:63912ms step_avg:38.07ms
step:1680/2330 train_time:63953ms step_avg:38.07ms
step:1681/2330 train_time:63987ms step_avg:38.06ms
step:1682/2330 train_time:64028ms step_avg:38.07ms
step:1683/2330 train_time:64062ms step_avg:38.06ms
step:1684/2330 train_time:64103ms step_avg:38.07ms
step:1685/2330 train_time:64139ms step_avg:38.06ms
step:1686/2330 train_time:64181ms step_avg:38.07ms
step:1687/2330 train_time:64216ms step_avg:38.07ms
step:1688/2330 train_time:64257ms step_avg:38.07ms
step:1689/2330 train_time:64292ms step_avg:38.07ms
step:1690/2330 train_time:64333ms step_avg:38.07ms
step:1691/2330 train_time:64369ms step_avg:38.07ms
step:1692/2330 train_time:64409ms step_avg:38.07ms
step:1693/2330 train_time:64445ms step_avg:38.07ms
step:1694/2330 train_time:64486ms step_avg:38.07ms
step:1695/2330 train_time:64521ms step_avg:38.07ms
step:1696/2330 train_time:64562ms step_avg:38.07ms
step:1697/2330 train_time:64597ms step_avg:38.07ms
step:1698/2330 train_time:64638ms step_avg:38.07ms
step:1699/2330 train_time:64673ms step_avg:38.07ms
step:1700/2330 train_time:64714ms step_avg:38.07ms
step:1701/2330 train_time:64749ms step_avg:38.07ms
step:1702/2330 train_time:64790ms step_avg:38.07ms
step:1703/2330 train_time:64825ms step_avg:38.06ms
step:1704/2330 train_time:64866ms step_avg:38.07ms
step:1705/2330 train_time:64901ms step_avg:38.06ms
step:1706/2330 train_time:64942ms step_avg:38.07ms
step:1707/2330 train_time:64977ms step_avg:38.06ms
step:1708/2330 train_time:65018ms step_avg:38.07ms
step:1709/2330 train_time:65053ms step_avg:38.06ms
step:1710/2330 train_time:65094ms step_avg:38.07ms
step:1711/2330 train_time:65129ms step_avg:38.06ms
step:1712/2330 train_time:65170ms step_avg:38.07ms
step:1713/2330 train_time:65204ms step_avg:38.06ms
step:1714/2330 train_time:65246ms step_avg:38.07ms
step:1715/2330 train_time:65280ms step_avg:38.06ms
step:1716/2330 train_time:65322ms step_avg:38.07ms
step:1717/2330 train_time:65357ms step_avg:38.06ms
step:1718/2330 train_time:65398ms step_avg:38.07ms
step:1719/2330 train_time:65433ms step_avg:38.06ms
step:1720/2330 train_time:65475ms step_avg:38.07ms
step:1721/2330 train_time:65509ms step_avg:38.06ms
step:1722/2330 train_time:65550ms step_avg:38.07ms
step:1723/2330 train_time:65585ms step_avg:38.06ms
step:1724/2330 train_time:65626ms step_avg:38.07ms
step:1725/2330 train_time:65661ms step_avg:38.06ms
step:1726/2330 train_time:65702ms step_avg:38.07ms
step:1727/2330 train_time:65737ms step_avg:38.06ms
step:1728/2330 train_time:65779ms step_avg:38.07ms
step:1729/2330 train_time:65813ms step_avg:38.06ms
step:1730/2330 train_time:65854ms step_avg:38.07ms
step:1731/2330 train_time:65888ms step_avg:38.06ms
step:1732/2330 train_time:65929ms step_avg:38.07ms
step:1733/2330 train_time:65963ms step_avg:38.06ms
step:1734/2330 train_time:66005ms step_avg:38.06ms
step:1735/2330 train_time:66040ms step_avg:38.06ms
step:1736/2330 train_time:66080ms step_avg:38.06ms
step:1737/2330 train_time:66117ms step_avg:38.06ms
step:1738/2330 train_time:66158ms step_avg:38.07ms
step:1739/2330 train_time:66193ms step_avg:38.06ms
step:1740/2330 train_time:66234ms step_avg:38.07ms
step:1741/2330 train_time:66269ms step_avg:38.06ms
step:1742/2330 train_time:66310ms step_avg:38.07ms
step:1743/2330 train_time:66345ms step_avg:38.06ms
step:1744/2330 train_time:66386ms step_avg:38.07ms
step:1745/2330 train_time:66421ms step_avg:38.06ms
step:1746/2330 train_time:66462ms step_avg:38.07ms
step:1747/2330 train_time:66497ms step_avg:38.06ms
step:1748/2330 train_time:66538ms step_avg:38.07ms
step:1749/2330 train_time:66574ms step_avg:38.06ms
step:1750/2330 train_time:66615ms step_avg:38.07ms
step:1750/2330 val_loss:5.3362 train_time:66725ms step_avg:38.13ms
step:1751/2330 train_time:66737ms step_avg:38.11ms
step:1752/2330 train_time:66748ms step_avg:38.10ms
step:1753/2330 train_time:66757ms step_avg:38.08ms
step:1754/2330 train_time:66768ms step_avg:38.07ms
step:1755/2330 train_time:66802ms step_avg:38.06ms
step:1756/2330 train_time:66843ms step_avg:38.07ms
step:1757/2330 train_time:66877ms step_avg:38.06ms
step:1758/2330 train_time:66918ms step_avg:38.06ms
step:1759/2330 train_time:66952ms step_avg:38.06ms
step:1760/2330 train_time:66993ms step_avg:38.06ms
step:1761/2330 train_time:67028ms step_avg:38.06ms
step:1762/2330 train_time:67069ms step_avg:38.06ms
step:1763/2330 train_time:67108ms step_avg:38.06ms
step:1764/2330 train_time:67149ms step_avg:38.07ms
step:1765/2330 train_time:67185ms step_avg:38.07ms
step:1766/2330 train_time:67226ms step_avg:38.07ms
step:1767/2330 train_time:67261ms step_avg:38.07ms
step:1768/2330 train_time:67302ms step_avg:38.07ms
step:1769/2330 train_time:67337ms step_avg:38.06ms
step:1770/2330 train_time:67378ms step_avg:38.07ms
step:1771/2330 train_time:67412ms step_avg:38.06ms
step:1772/2330 train_time:67452ms step_avg:38.07ms
step:1773/2330 train_time:67487ms step_avg:38.06ms
step:1774/2330 train_time:67527ms step_avg:38.06ms
step:1775/2330 train_time:67562ms step_avg:38.06ms
step:1776/2330 train_time:67603ms step_avg:38.06ms
step:1777/2330 train_time:67638ms step_avg:38.06ms
step:1778/2330 train_time:67680ms step_avg:38.07ms
step:1779/2330 train_time:67715ms step_avg:38.06ms
step:1780/2330 train_time:67756ms step_avg:38.07ms
step:1781/2330 train_time:67791ms step_avg:38.06ms
step:1782/2330 train_time:67831ms step_avg:38.06ms
step:1783/2330 train_time:67866ms step_avg:38.06ms
step:1784/2330 train_time:67907ms step_avg:38.06ms
step:1785/2330 train_time:67941ms step_avg:38.06ms
step:1786/2330 train_time:67982ms step_avg:38.06ms
step:1787/2330 train_time:68018ms step_avg:38.06ms
step:1788/2330 train_time:68059ms step_avg:38.06ms
step:1789/2330 train_time:68096ms step_avg:38.06ms
step:1790/2330 train_time:68137ms step_avg:38.07ms
step:1791/2330 train_time:68174ms step_avg:38.06ms
step:1792/2330 train_time:68215ms step_avg:38.07ms
step:1793/2330 train_time:68250ms step_avg:38.06ms
step:1794/2330 train_time:68291ms step_avg:38.07ms
step:1795/2330 train_time:68326ms step_avg:38.06ms
step:1796/2330 train_time:68367ms step_avg:38.07ms
step:1797/2330 train_time:68402ms step_avg:38.06ms
step:1798/2330 train_time:68443ms step_avg:38.07ms
step:1799/2330 train_time:68478ms step_avg:38.06ms
step:1800/2330 train_time:68519ms step_avg:38.07ms
step:1801/2330 train_time:68553ms step_avg:38.06ms
step:1802/2330 train_time:68594ms step_avg:38.07ms
step:1803/2330 train_time:68628ms step_avg:38.06ms
step:1804/2330 train_time:68669ms step_avg:38.06ms
step:1805/2330 train_time:68704ms step_avg:38.06ms
step:1806/2330 train_time:68745ms step_avg:38.06ms
step:1807/2330 train_time:68780ms step_avg:38.06ms
step:1808/2330 train_time:68821ms step_avg:38.06ms
step:1809/2330 train_time:68856ms step_avg:38.06ms
step:1810/2330 train_time:68896ms step_avg:38.06ms
step:1811/2330 train_time:68931ms step_avg:38.06ms
step:1812/2330 train_time:68972ms step_avg:38.06ms
step:1813/2330 train_time:69008ms step_avg:38.06ms
step:1814/2330 train_time:69048ms step_avg:38.06ms
step:1815/2330 train_time:69085ms step_avg:38.06ms
step:1816/2330 train_time:69126ms step_avg:38.06ms
step:1817/2330 train_time:69162ms step_avg:38.06ms
step:1818/2330 train_time:69203ms step_avg:38.07ms
step:1819/2330 train_time:69239ms step_avg:38.06ms
step:1820/2330 train_time:69279ms step_avg:38.07ms
step:1821/2330 train_time:69315ms step_avg:38.06ms
step:1822/2330 train_time:69356ms step_avg:38.07ms
step:1823/2330 train_time:69391ms step_avg:38.06ms
step:1824/2330 train_time:69431ms step_avg:38.07ms
step:1825/2330 train_time:69467ms step_avg:38.06ms
step:1826/2330 train_time:69508ms step_avg:38.07ms
step:1827/2330 train_time:69542ms step_avg:38.06ms
step:1828/2330 train_time:69582ms step_avg:38.06ms
step:1829/2330 train_time:69618ms step_avg:38.06ms
step:1830/2330 train_time:69659ms step_avg:38.06ms
step:1831/2330 train_time:69694ms step_avg:38.06ms
step:1832/2330 train_time:69735ms step_avg:38.06ms
step:1833/2330 train_time:69769ms step_avg:38.06ms
step:1834/2330 train_time:69809ms step_avg:38.06ms
step:1835/2330 train_time:69844ms step_avg:38.06ms
step:1836/2330 train_time:69885ms step_avg:38.06ms
step:1837/2330 train_time:69920ms step_avg:38.06ms
step:1838/2330 train_time:69961ms step_avg:38.06ms
step:1839/2330 train_time:69996ms step_avg:38.06ms
step:1840/2330 train_time:70038ms step_avg:38.06ms
step:1841/2330 train_time:70073ms step_avg:38.06ms
step:1842/2330 train_time:70114ms step_avg:38.06ms
step:1843/2330 train_time:70150ms step_avg:38.06ms
step:1844/2330 train_time:70190ms step_avg:38.06ms
step:1845/2330 train_time:70226ms step_avg:38.06ms
step:1846/2330 train_time:70267ms step_avg:38.06ms
step:1847/2330 train_time:70302ms step_avg:38.06ms
step:1848/2330 train_time:70344ms step_avg:38.06ms
step:1849/2330 train_time:70378ms step_avg:38.06ms
step:1850/2330 train_time:70419ms step_avg:38.06ms
step:1851/2330 train_time:70454ms step_avg:38.06ms
step:1852/2330 train_time:70494ms step_avg:38.06ms
step:1853/2330 train_time:70529ms step_avg:38.06ms
step:1854/2330 train_time:70570ms step_avg:38.06ms
step:1855/2330 train_time:70605ms step_avg:38.06ms
step:1856/2330 train_time:70646ms step_avg:38.06ms
step:1857/2330 train_time:70681ms step_avg:38.06ms
step:1858/2330 train_time:70722ms step_avg:38.06ms
step:1859/2330 train_time:70758ms step_avg:38.06ms
step:1860/2330 train_time:70798ms step_avg:38.06ms
step:1861/2330 train_time:70833ms step_avg:38.06ms
step:1862/2330 train_time:70874ms step_avg:38.06ms
step:1863/2330 train_time:70909ms step_avg:38.06ms
step:1864/2330 train_time:70949ms step_avg:38.06ms
step:1865/2330 train_time:70985ms step_avg:38.06ms
step:1866/2330 train_time:71026ms step_avg:38.06ms
step:1867/2330 train_time:71061ms step_avg:38.06ms
step:1868/2330 train_time:71102ms step_avg:38.06ms
step:1869/2330 train_time:71137ms step_avg:38.06ms
step:1870/2330 train_time:71179ms step_avg:38.06ms
step:1871/2330 train_time:71213ms step_avg:38.06ms
step:1872/2330 train_time:71254ms step_avg:38.06ms
step:1873/2330 train_time:71288ms step_avg:38.06ms
step:1874/2330 train_time:71329ms step_avg:38.06ms
step:1875/2330 train_time:71364ms step_avg:38.06ms
step:1876/2330 train_time:71405ms step_avg:38.06ms
step:1877/2330 train_time:71440ms step_avg:38.06ms
step:1878/2330 train_time:71481ms step_avg:38.06ms
step:1879/2330 train_time:71516ms step_avg:38.06ms
step:1880/2330 train_time:71557ms step_avg:38.06ms
step:1881/2330 train_time:71592ms step_avg:38.06ms
step:1882/2330 train_time:71633ms step_avg:38.06ms
step:1883/2330 train_time:71668ms step_avg:38.06ms
step:1884/2330 train_time:71708ms step_avg:38.06ms
step:1885/2330 train_time:71743ms step_avg:38.06ms
step:1886/2330 train_time:71784ms step_avg:38.06ms
step:1887/2330 train_time:71820ms step_avg:38.06ms
step:1888/2330 train_time:71861ms step_avg:38.06ms
step:1889/2330 train_time:71896ms step_avg:38.06ms
step:1890/2330 train_time:71937ms step_avg:38.06ms
step:1891/2330 train_time:71972ms step_avg:38.06ms
step:1892/2330 train_time:72013ms step_avg:38.06ms
step:1893/2330 train_time:72048ms step_avg:38.06ms
step:1894/2330 train_time:72089ms step_avg:38.06ms
step:1895/2330 train_time:72124ms step_avg:38.06ms
step:1896/2330 train_time:72165ms step_avg:38.06ms
step:1897/2330 train_time:72201ms step_avg:38.06ms
step:1898/2330 train_time:72241ms step_avg:38.06ms
step:1899/2330 train_time:72278ms step_avg:38.06ms
step:1900/2330 train_time:72319ms step_avg:38.06ms
step:1901/2330 train_time:72355ms step_avg:38.06ms
step:1902/2330 train_time:72396ms step_avg:38.06ms
step:1903/2330 train_time:72431ms step_avg:38.06ms
step:1904/2330 train_time:72472ms step_avg:38.06ms
step:1905/2330 train_time:72507ms step_avg:38.06ms
step:1906/2330 train_time:72548ms step_avg:38.06ms
step:1907/2330 train_time:72582ms step_avg:38.06ms
step:1908/2330 train_time:72623ms step_avg:38.06ms
step:1909/2330 train_time:72659ms step_avg:38.06ms
step:1910/2330 train_time:72700ms step_avg:38.06ms
step:1911/2330 train_time:72735ms step_avg:38.06ms
step:1912/2330 train_time:72776ms step_avg:38.06ms
step:1913/2330 train_time:72812ms step_avg:38.06ms
step:1914/2330 train_time:72852ms step_avg:38.06ms
step:1915/2330 train_time:72888ms step_avg:38.06ms
step:1916/2330 train_time:72928ms step_avg:38.06ms
step:1917/2330 train_time:72964ms step_avg:38.06ms
step:1918/2330 train_time:73004ms step_avg:38.06ms
step:1919/2330 train_time:73040ms step_avg:38.06ms
step:1920/2330 train_time:73080ms step_avg:38.06ms
step:1921/2330 train_time:73116ms step_avg:38.06ms
step:1922/2330 train_time:73156ms step_avg:38.06ms
step:1923/2330 train_time:73192ms step_avg:38.06ms
step:1924/2330 train_time:73232ms step_avg:38.06ms
step:1925/2330 train_time:73267ms step_avg:38.06ms
step:1926/2330 train_time:73308ms step_avg:38.06ms
step:1927/2330 train_time:73344ms step_avg:38.06ms
step:1928/2330 train_time:73385ms step_avg:38.06ms
step:1929/2330 train_time:73420ms step_avg:38.06ms
step:1930/2330 train_time:73461ms step_avg:38.06ms
step:1931/2330 train_time:73497ms step_avg:38.06ms
step:1932/2330 train_time:73538ms step_avg:38.06ms
step:1933/2330 train_time:73573ms step_avg:38.06ms
step:1934/2330 train_time:73614ms step_avg:38.06ms
step:1935/2330 train_time:73649ms step_avg:38.06ms
step:1936/2330 train_time:73689ms step_avg:38.06ms
step:1937/2330 train_time:73725ms step_avg:38.06ms
step:1938/2330 train_time:73766ms step_avg:38.06ms
step:1939/2330 train_time:73801ms step_avg:38.06ms
step:1940/2330 train_time:73841ms step_avg:38.06ms
step:1941/2330 train_time:73877ms step_avg:38.06ms
step:1942/2330 train_time:73918ms step_avg:38.06ms
step:1943/2330 train_time:73953ms step_avg:38.06ms
step:1944/2330 train_time:73994ms step_avg:38.06ms
step:1945/2330 train_time:74029ms step_avg:38.06ms
step:1946/2330 train_time:74069ms step_avg:38.06ms
step:1947/2330 train_time:74105ms step_avg:38.06ms
step:1948/2330 train_time:74146ms step_avg:38.06ms
step:1949/2330 train_time:74181ms step_avg:38.06ms
step:1950/2330 train_time:74222ms step_avg:38.06ms
step:1951/2330 train_time:74258ms step_avg:38.06ms
step:1952/2330 train_time:74299ms step_avg:38.06ms
step:1953/2330 train_time:74335ms step_avg:38.06ms
step:1954/2330 train_time:74375ms step_avg:38.06ms
step:1955/2330 train_time:74411ms step_avg:38.06ms
step:1956/2330 train_time:74452ms step_avg:38.06ms
step:1957/2330 train_time:74487ms step_avg:38.06ms
step:1958/2330 train_time:74528ms step_avg:38.06ms
step:1959/2330 train_time:74562ms step_avg:38.06ms
step:1960/2330 train_time:74604ms step_avg:38.06ms
step:1961/2330 train_time:74638ms step_avg:38.06ms
step:1962/2330 train_time:74679ms step_avg:38.06ms
step:1963/2330 train_time:74714ms step_avg:38.06ms
step:1964/2330 train_time:74755ms step_avg:38.06ms
step:1965/2330 train_time:74791ms step_avg:38.06ms
step:1966/2330 train_time:74831ms step_avg:38.06ms
step:1967/2330 train_time:74866ms step_avg:38.06ms
step:1968/2330 train_time:74907ms step_avg:38.06ms
step:1969/2330 train_time:74942ms step_avg:38.06ms
step:1970/2330 train_time:74982ms step_avg:38.06ms
step:1971/2330 train_time:75018ms step_avg:38.06ms
step:1972/2330 train_time:75059ms step_avg:38.06ms
step:1973/2330 train_time:75094ms step_avg:38.06ms
step:1974/2330 train_time:75135ms step_avg:38.06ms
step:1975/2330 train_time:75170ms step_avg:38.06ms
step:1976/2330 train_time:75210ms step_avg:38.06ms
step:1977/2330 train_time:75246ms step_avg:38.06ms
step:1978/2330 train_time:75286ms step_avg:38.06ms
step:1979/2330 train_time:75322ms step_avg:38.06ms
step:1980/2330 train_time:75362ms step_avg:38.06ms
step:1981/2330 train_time:75399ms step_avg:38.06ms
step:1982/2330 train_time:75440ms step_avg:38.06ms
step:1983/2330 train_time:75475ms step_avg:38.06ms
step:1984/2330 train_time:75516ms step_avg:38.06ms
step:1985/2330 train_time:75551ms step_avg:38.06ms
step:1986/2330 train_time:75592ms step_avg:38.06ms
step:1987/2330 train_time:75627ms step_avg:38.06ms
step:1988/2330 train_time:75668ms step_avg:38.06ms
step:1989/2330 train_time:75702ms step_avg:38.06ms
step:1990/2330 train_time:75743ms step_avg:38.06ms
step:1991/2330 train_time:75778ms step_avg:38.06ms
step:1992/2330 train_time:75819ms step_avg:38.06ms
step:1993/2330 train_time:75854ms step_avg:38.06ms
step:1994/2330 train_time:75895ms step_avg:38.06ms
step:1995/2330 train_time:75929ms step_avg:38.06ms
step:1996/2330 train_time:75969ms step_avg:38.06ms
step:1997/2330 train_time:76005ms step_avg:38.06ms
step:1998/2330 train_time:76045ms step_avg:38.06ms
step:1999/2330 train_time:76081ms step_avg:38.06ms
step:2000/2330 train_time:76122ms step_avg:38.06ms
step:2000/2330 val_loss:5.2978 train_time:76235ms step_avg:38.12ms
step:2001/2330 train_time:76247ms step_avg:38.10ms
step:2002/2330 train_time:76260ms step_avg:38.09ms
step:2003/2330 train_time:76269ms step_avg:38.08ms
step:2004/2330 train_time:76279ms step_avg:38.06ms
step:2005/2330 train_time:76312ms step_avg:38.06ms
step:2006/2330 train_time:76352ms step_avg:38.06ms
step:2007/2330 train_time:76387ms step_avg:38.06ms
step:2008/2330 train_time:76427ms step_avg:38.06ms
step:2009/2330 train_time:76462ms step_avg:38.06ms
step:2010/2330 train_time:76502ms step_avg:38.06ms
step:2011/2330 train_time:76538ms step_avg:38.06ms
step:2012/2330 train_time:76579ms step_avg:38.06ms
step:2013/2330 train_time:76617ms step_avg:38.06ms
step:2014/2330 train_time:76658ms step_avg:38.06ms
step:2015/2330 train_time:76695ms step_avg:38.06ms
step:2016/2330 train_time:76735ms step_avg:38.06ms
step:2017/2330 train_time:76772ms step_avg:38.06ms
step:2018/2330 train_time:76812ms step_avg:38.06ms
step:2019/2330 train_time:76848ms step_avg:38.06ms
step:2020/2330 train_time:76888ms step_avg:38.06ms
step:2021/2330 train_time:76923ms step_avg:38.06ms
step:2022/2330 train_time:76964ms step_avg:38.06ms
step:2023/2330 train_time:76998ms step_avg:38.06ms
step:2024/2330 train_time:77038ms step_avg:38.06ms
step:2025/2330 train_time:77073ms step_avg:38.06ms
step:2026/2330 train_time:77114ms step_avg:38.06ms
step:2027/2330 train_time:77149ms step_avg:38.06ms
step:2028/2330 train_time:77191ms step_avg:38.06ms
step:2029/2330 train_time:77226ms step_avg:38.06ms
step:2030/2330 train_time:77268ms step_avg:38.06ms
step:2031/2330 train_time:77303ms step_avg:38.06ms
step:2032/2330 train_time:77344ms step_avg:38.06ms
step:2033/2330 train_time:77378ms step_avg:38.06ms
step:2034/2330 train_time:77419ms step_avg:38.06ms
step:2035/2330 train_time:77454ms step_avg:38.06ms
step:2036/2330 train_time:77495ms step_avg:38.06ms
step:2037/2330 train_time:77531ms step_avg:38.06ms
step:2038/2330 train_time:77573ms step_avg:38.06ms
step:2039/2330 train_time:77609ms step_avg:38.06ms
step:2040/2330 train_time:77650ms step_avg:38.06ms
step:2041/2330 train_time:77686ms step_avg:38.06ms
step:2042/2330 train_time:77727ms step_avg:38.06ms
step:2043/2330 train_time:77762ms step_avg:38.06ms
step:2044/2330 train_time:77803ms step_avg:38.06ms
step:2045/2330 train_time:77838ms step_avg:38.06ms
step:2046/2330 train_time:77879ms step_avg:38.06ms
step:2047/2330 train_time:77914ms step_avg:38.06ms
step:2048/2330 train_time:77955ms step_avg:38.06ms
step:2049/2330 train_time:77991ms step_avg:38.06ms
step:2050/2330 train_time:78031ms step_avg:38.06ms
step:2051/2330 train_time:78067ms step_avg:38.06ms
step:2052/2330 train_time:78107ms step_avg:38.06ms
step:2053/2330 train_time:78143ms step_avg:38.06ms
step:2054/2330 train_time:78183ms step_avg:38.06ms
step:2055/2330 train_time:78218ms step_avg:38.06ms
step:2056/2330 train_time:78259ms step_avg:38.06ms
step:2057/2330 train_time:78294ms step_avg:38.06ms
step:2058/2330 train_time:78336ms step_avg:38.06ms
step:2059/2330 train_time:78370ms step_avg:38.06ms
step:2060/2330 train_time:78411ms step_avg:38.06ms
step:2061/2330 train_time:78446ms step_avg:38.06ms
step:2062/2330 train_time:78487ms step_avg:38.06ms
step:2063/2330 train_time:78522ms step_avg:38.06ms
step:2064/2330 train_time:78563ms step_avg:38.06ms
step:2065/2330 train_time:78598ms step_avg:38.06ms
step:2066/2330 train_time:78639ms step_avg:38.06ms
step:2067/2330 train_time:78674ms step_avg:38.06ms
step:2068/2330 train_time:78715ms step_avg:38.06ms
step:2069/2330 train_time:78751ms step_avg:38.06ms
step:2070/2330 train_time:78792ms step_avg:38.06ms
step:2071/2330 train_time:78828ms step_avg:38.06ms
step:2072/2330 train_time:78868ms step_avg:38.06ms
step:2073/2330 train_time:78904ms step_avg:38.06ms
step:2074/2330 train_time:78945ms step_avg:38.06ms
step:2075/2330 train_time:78980ms step_avg:38.06ms
step:2076/2330 train_time:79020ms step_avg:38.06ms
step:2077/2330 train_time:79055ms step_avg:38.06ms
step:2078/2330 train_time:79096ms step_avg:38.06ms
step:2079/2330 train_time:79131ms step_avg:38.06ms
step:2080/2330 train_time:79172ms step_avg:38.06ms
step:2081/2330 train_time:79206ms step_avg:38.06ms
step:2082/2330 train_time:79247ms step_avg:38.06ms
step:2083/2330 train_time:79283ms step_avg:38.06ms
step:2084/2330 train_time:79324ms step_avg:38.06ms
step:2085/2330 train_time:79360ms step_avg:38.06ms
step:2086/2330 train_time:79400ms step_avg:38.06ms
step:2087/2330 train_time:79437ms step_avg:38.06ms
step:2088/2330 train_time:79478ms step_avg:38.06ms
step:2089/2330 train_time:79513ms step_avg:38.06ms
step:2090/2330 train_time:79554ms step_avg:38.06ms
step:2091/2330 train_time:79589ms step_avg:38.06ms
step:2092/2330 train_time:79630ms step_avg:38.06ms
step:2093/2330 train_time:79666ms step_avg:38.06ms
step:2094/2330 train_time:79707ms step_avg:38.06ms
step:2095/2330 train_time:79743ms step_avg:38.06ms
step:2096/2330 train_time:79784ms step_avg:38.06ms
step:2097/2330 train_time:79820ms step_avg:38.06ms
step:2098/2330 train_time:79860ms step_avg:38.06ms
step:2099/2330 train_time:79895ms step_avg:38.06ms
step:2100/2330 train_time:79936ms step_avg:38.06ms
step:2101/2330 train_time:79971ms step_avg:38.06ms
step:2102/2330 train_time:80012ms step_avg:38.06ms
step:2103/2330 train_time:80046ms step_avg:38.06ms
step:2104/2330 train_time:80087ms step_avg:38.06ms
step:2105/2330 train_time:80122ms step_avg:38.06ms
step:2106/2330 train_time:80163ms step_avg:38.06ms
step:2107/2330 train_time:80197ms step_avg:38.06ms
step:2108/2330 train_time:80238ms step_avg:38.06ms
step:2109/2330 train_time:80273ms step_avg:38.06ms
step:2110/2330 train_time:80313ms step_avg:38.06ms
step:2111/2330 train_time:80349ms step_avg:38.06ms
step:2112/2330 train_time:80390ms step_avg:38.06ms
step:2113/2330 train_time:80426ms step_avg:38.06ms
step:2114/2330 train_time:80467ms step_avg:38.06ms
step:2115/2330 train_time:80503ms step_avg:38.06ms
step:2116/2330 train_time:80545ms step_avg:38.06ms
step:2117/2330 train_time:80580ms step_avg:38.06ms
step:2118/2330 train_time:80621ms step_avg:38.06ms
step:2119/2330 train_time:80656ms step_avg:38.06ms
step:2120/2330 train_time:80697ms step_avg:38.06ms
step:2121/2330 train_time:80732ms step_avg:38.06ms
step:2122/2330 train_time:80773ms step_avg:38.06ms
step:2123/2330 train_time:80808ms step_avg:38.06ms
step:2124/2330 train_time:80849ms step_avg:38.06ms
step:2125/2330 train_time:80885ms step_avg:38.06ms
step:2126/2330 train_time:80925ms step_avg:38.06ms
step:2127/2330 train_time:80961ms step_avg:38.06ms
step:2128/2330 train_time:81001ms step_avg:38.06ms
step:2129/2330 train_time:81037ms step_avg:38.06ms
step:2130/2330 train_time:81077ms step_avg:38.06ms
step:2131/2330 train_time:81112ms step_avg:38.06ms
step:2132/2330 train_time:81152ms step_avg:38.06ms
step:2133/2330 train_time:81188ms step_avg:38.06ms
step:2134/2330 train_time:81228ms step_avg:38.06ms
step:2135/2330 train_time:81264ms step_avg:38.06ms
step:2136/2330 train_time:81305ms step_avg:38.06ms
step:2137/2330 train_time:81340ms step_avg:38.06ms
step:2138/2330 train_time:81380ms step_avg:38.06ms
step:2139/2330 train_time:81416ms step_avg:38.06ms
step:2140/2330 train_time:81457ms step_avg:38.06ms
step:2141/2330 train_time:81491ms step_avg:38.06ms
step:2142/2330 train_time:81533ms step_avg:38.06ms
step:2143/2330 train_time:81568ms step_avg:38.06ms
step:2144/2330 train_time:81610ms step_avg:38.06ms
step:2145/2330 train_time:81645ms step_avg:38.06ms
step:2146/2330 train_time:81686ms step_avg:38.06ms
step:2147/2330 train_time:81721ms step_avg:38.06ms
step:2148/2330 train_time:81761ms step_avg:38.06ms
step:2149/2330 train_time:81797ms step_avg:38.06ms
step:2150/2330 train_time:81838ms step_avg:38.06ms
step:2151/2330 train_time:81872ms step_avg:38.06ms
step:2152/2330 train_time:81913ms step_avg:38.06ms
step:2153/2330 train_time:81949ms step_avg:38.06ms
step:2154/2330 train_time:81990ms step_avg:38.06ms
step:2155/2330 train_time:82026ms step_avg:38.06ms
step:2156/2330 train_time:82067ms step_avg:38.06ms
step:2157/2330 train_time:82102ms step_avg:38.06ms
step:2158/2330 train_time:82142ms step_avg:38.06ms
step:2159/2330 train_time:82177ms step_avg:38.06ms
step:2160/2330 train_time:82217ms step_avg:38.06ms
step:2161/2330 train_time:82252ms step_avg:38.06ms
step:2162/2330 train_time:82293ms step_avg:38.06ms
step:2163/2330 train_time:82329ms step_avg:38.06ms
step:2164/2330 train_time:82370ms step_avg:38.06ms
step:2165/2330 train_time:82404ms step_avg:38.06ms
step:2166/2330 train_time:82445ms step_avg:38.06ms
step:2167/2330 train_time:82480ms step_avg:38.06ms
step:2168/2330 train_time:82520ms step_avg:38.06ms
step:2169/2330 train_time:82556ms step_avg:38.06ms
step:2170/2330 train_time:82597ms step_avg:38.06ms
step:2171/2330 train_time:82632ms step_avg:38.06ms
step:2172/2330 train_time:82672ms step_avg:38.06ms
step:2173/2330 train_time:82708ms step_avg:38.06ms
step:2174/2330 train_time:82748ms step_avg:38.06ms
step:2175/2330 train_time:82784ms step_avg:38.06ms
step:2176/2330 train_time:82825ms step_avg:38.06ms
step:2177/2330 train_time:82859ms step_avg:38.06ms
step:2178/2330 train_time:82900ms step_avg:38.06ms
step:2179/2330 train_time:82936ms step_avg:38.06ms
step:2180/2330 train_time:82977ms step_avg:38.06ms
step:2181/2330 train_time:83011ms step_avg:38.06ms
step:2182/2330 train_time:83052ms step_avg:38.06ms
step:2183/2330 train_time:83088ms step_avg:38.06ms
step:2184/2330 train_time:83129ms step_avg:38.06ms
step:2185/2330 train_time:83165ms step_avg:38.06ms
step:2186/2330 train_time:83206ms step_avg:38.06ms
step:2187/2330 train_time:83241ms step_avg:38.06ms
step:2188/2330 train_time:83281ms step_avg:38.06ms
step:2189/2330 train_time:83316ms step_avg:38.06ms
step:2190/2330 train_time:83357ms step_avg:38.06ms
step:2191/2330 train_time:83392ms step_avg:38.06ms
step:2192/2330 train_time:83433ms step_avg:38.06ms
step:2193/2330 train_time:83468ms step_avg:38.06ms
step:2194/2330 train_time:83509ms step_avg:38.06ms
step:2195/2330 train_time:83545ms step_avg:38.06ms
step:2196/2330 train_time:83586ms step_avg:38.06ms
step:2197/2330 train_time:83622ms step_avg:38.06ms
step:2198/2330 train_time:83662ms step_avg:38.06ms
step:2199/2330 train_time:83697ms step_avg:38.06ms
step:2200/2330 train_time:83739ms step_avg:38.06ms
step:2201/2330 train_time:83773ms step_avg:38.06ms
step:2202/2330 train_time:83814ms step_avg:38.06ms
step:2203/2330 train_time:83849ms step_avg:38.06ms
step:2204/2330 train_time:83890ms step_avg:38.06ms
step:2205/2330 train_time:83926ms step_avg:38.06ms
step:2206/2330 train_time:83967ms step_avg:38.06ms
step:2207/2330 train_time:84003ms step_avg:38.06ms
step:2208/2330 train_time:84044ms step_avg:38.06ms
step:2209/2330 train_time:84079ms step_avg:38.06ms
step:2210/2330 train_time:84119ms step_avg:38.06ms
step:2211/2330 train_time:84155ms step_avg:38.06ms
step:2212/2330 train_time:84196ms step_avg:38.06ms
step:2213/2330 train_time:84232ms step_avg:38.06ms
step:2214/2330 train_time:84273ms step_avg:38.06ms
step:2215/2330 train_time:84308ms step_avg:38.06ms
step:2216/2330 train_time:84349ms step_avg:38.06ms
step:2217/2330 train_time:84385ms step_avg:38.06ms
step:2218/2330 train_time:84425ms step_avg:38.06ms
step:2219/2330 train_time:84460ms step_avg:38.06ms
step:2220/2330 train_time:84501ms step_avg:38.06ms
step:2221/2330 train_time:84536ms step_avg:38.06ms
step:2222/2330 train_time:84577ms step_avg:38.06ms
step:2223/2330 train_time:84612ms step_avg:38.06ms
step:2224/2330 train_time:84653ms step_avg:38.06ms
step:2225/2330 train_time:84689ms step_avg:38.06ms
step:2226/2330 train_time:84729ms step_avg:38.06ms
step:2227/2330 train_time:84765ms step_avg:38.06ms
step:2228/2330 train_time:84806ms step_avg:38.06ms
step:2229/2330 train_time:84842ms step_avg:38.06ms
step:2230/2330 train_time:84882ms step_avg:38.06ms
step:2231/2330 train_time:84917ms step_avg:38.06ms
step:2232/2330 train_time:84958ms step_avg:38.06ms
step:2233/2330 train_time:84994ms step_avg:38.06ms
step:2234/2330 train_time:85035ms step_avg:38.06ms
step:2235/2330 train_time:85071ms step_avg:38.06ms
step:2236/2330 train_time:85112ms step_avg:38.06ms
step:2237/2330 train_time:85147ms step_avg:38.06ms
step:2238/2330 train_time:85188ms step_avg:38.06ms
step:2239/2330 train_time:85224ms step_avg:38.06ms
step:2240/2330 train_time:85264ms step_avg:38.06ms
step:2241/2330 train_time:85299ms step_avg:38.06ms
step:2242/2330 train_time:85340ms step_avg:38.06ms
step:2243/2330 train_time:85376ms step_avg:38.06ms
step:2244/2330 train_time:85417ms step_avg:38.06ms
step:2245/2330 train_time:85453ms step_avg:38.06ms
step:2246/2330 train_time:85494ms step_avg:38.06ms
step:2247/2330 train_time:85529ms step_avg:38.06ms
step:2248/2330 train_time:85570ms step_avg:38.06ms
step:2249/2330 train_time:85605ms step_avg:38.06ms
step:2250/2330 train_time:85645ms step_avg:38.06ms
step:2250/2330 val_loss:5.2660 train_time:85757ms step_avg:38.11ms
step:2251/2330 train_time:85768ms step_avg:38.10ms
step:2252/2330 train_time:85779ms step_avg:38.09ms
step:2253/2330 train_time:85789ms step_avg:38.08ms
step:2254/2330 train_time:85800ms step_avg:38.07ms
step:2255/2330 train_time:85834ms step_avg:38.06ms
step:2256/2330 train_time:85874ms step_avg:38.06ms
step:2257/2330 train_time:85909ms step_avg:38.06ms
step:2258/2330 train_time:85949ms step_avg:38.06ms
step:2259/2330 train_time:85983ms step_avg:38.06ms
step:2260/2330 train_time:86024ms step_avg:38.06ms
step:2261/2330 train_time:86060ms step_avg:38.06ms
step:2262/2330 train_time:86102ms step_avg:38.06ms
step:2263/2330 train_time:86142ms step_avg:38.07ms
step:2264/2330 train_time:86183ms step_avg:38.07ms
step:2265/2330 train_time:86220ms step_avg:38.07ms
step:2266/2330 train_time:86261ms step_avg:38.07ms
step:2267/2330 train_time:86296ms step_avg:38.07ms
step:2268/2330 train_time:86337ms step_avg:38.07ms
step:2269/2330 train_time:86373ms step_avg:38.07ms
step:2270/2330 train_time:86413ms step_avg:38.07ms
step:2271/2330 train_time:86448ms step_avg:38.07ms
step:2272/2330 train_time:86489ms step_avg:38.07ms
step:2273/2330 train_time:86523ms step_avg:38.07ms
step:2274/2330 train_time:86564ms step_avg:38.07ms
step:2275/2330 train_time:86599ms step_avg:38.07ms
step:2276/2330 train_time:86639ms step_avg:38.07ms
step:2277/2330 train_time:86674ms step_avg:38.07ms
step:2278/2330 train_time:86715ms step_avg:38.07ms
step:2279/2330 train_time:86750ms step_avg:38.07ms
step:2280/2330 train_time:86791ms step_avg:38.07ms
step:2281/2330 train_time:86826ms step_avg:38.06ms
step:2282/2330 train_time:86866ms step_avg:38.07ms
step:2283/2330 train_time:86902ms step_avg:38.06ms
step:2284/2330 train_time:86943ms step_avg:38.07ms
step:2285/2330 train_time:86977ms step_avg:38.06ms
step:2286/2330 train_time:87018ms step_avg:38.07ms
step:2287/2330 train_time:87055ms step_avg:38.06ms
step:2288/2330 train_time:87096ms step_avg:38.07ms
step:2289/2330 train_time:87132ms step_avg:38.07ms
step:2290/2330 train_time:87173ms step_avg:38.07ms
step:2291/2330 train_time:87209ms step_avg:38.07ms
step:2292/2330 train_time:87250ms step_avg:38.07ms
step:2293/2330 train_time:87285ms step_avg:38.07ms
step:2294/2330 train_time:87325ms step_avg:38.07ms
step:2295/2330 train_time:87361ms step_avg:38.07ms
step:2296/2330 train_time:87401ms step_avg:38.07ms
step:2297/2330 train_time:87437ms step_avg:38.07ms
step:2298/2330 train_time:87477ms step_avg:38.07ms
step:2299/2330 train_time:87513ms step_avg:38.07ms
step:2300/2330 train_time:87553ms step_avg:38.07ms
step:2301/2330 train_time:87589ms step_avg:38.07ms
step:2302/2330 train_time:87629ms step_avg:38.07ms
step:2303/2330 train_time:87664ms step_avg:38.07ms
step:2304/2330 train_time:87705ms step_avg:38.07ms
step:2305/2330 train_time:87739ms step_avg:38.06ms
step:2306/2330 train_time:87780ms step_avg:38.07ms
step:2307/2330 train_time:87815ms step_avg:38.06ms
step:2308/2330 train_time:87856ms step_avg:38.07ms
step:2309/2330 train_time:87891ms step_avg:38.06ms
step:2310/2330 train_time:87931ms step_avg:38.07ms
step:2311/2330 train_time:87967ms step_avg:38.06ms
step:2312/2330 train_time:88007ms step_avg:38.07ms
step:2313/2330 train_time:88042ms step_avg:38.06ms
step:2314/2330 train_time:88083ms step_avg:38.07ms
step:2315/2330 train_time:88119ms step_avg:38.06ms
step:2316/2330 train_time:88160ms step_avg:38.07ms
step:2317/2330 train_time:88196ms step_avg:38.06ms
step:2318/2330 train_time:88237ms step_avg:38.07ms
step:2319/2330 train_time:88272ms step_avg:38.06ms
step:2320/2330 train_time:88313ms step_avg:38.07ms
step:2321/2330 train_time:88349ms step_avg:38.07ms
step:2322/2330 train_time:88389ms step_avg:38.07ms
step:2323/2330 train_time:88425ms step_avg:38.07ms
step:2324/2330 train_time:88466ms step_avg:38.07ms
step:2325/2330 train_time:88501ms step_avg:38.06ms
step:2326/2330 train_time:88542ms step_avg:38.07ms
step:2327/2330 train_time:88577ms step_avg:38.06ms
step:2328/2330 train_time:88617ms step_avg:38.07ms
step:2329/2330 train_time:88654ms step_avg:38.07ms
step:2330/2330 train_time:88694ms step_avg:38.07ms
step:2330/2330 val_loss:5.2576 train_time:88807ms step_avg:38.11ms
peak memory allocated: 27165 MiB reserved: 38888 MiB
