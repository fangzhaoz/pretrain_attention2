import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = divide_by_norm(batched)
                # v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = divide_by_norm(batched_update_grads)
                # v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        # self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "mlp_gd_lr1e-1"
    os.makedirs("logs3", exist_ok=True)
    logfile = f"logs3/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
# embed_params = [p for n, p in model.named_parameters() if "embed" in n]
embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=1e-1, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    return 1.0

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum_min + 0.5 * (momentum_max - momentum_min)

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs3/{run_id}", exist_ok=True)
            torch.save(log, f"logs3/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 08:14:39 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   30C    P0             110W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   28C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   27C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   27C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   28C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   27C    P0             108W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.05ms
step:1/2330 train_time:94ms step_avg:93.53ms
step:2/2330 train_time:164ms step_avg:81.87ms
step:3/2330 train_time:176ms step_avg:58.75ms
step:4/2330 train_time:189ms step_avg:47.15ms
step:5/2330 train_time:199ms step_avg:39.77ms
step:6/2330 train_time:224ms step_avg:37.30ms
step:7/2330 train_time:245ms step_avg:35.01ms
step:8/2330 train_time:300ms step_avg:37.46ms
step:9/2330 train_time:322ms step_avg:35.72ms
step:10/2330 train_time:377ms step_avg:37.72ms
step:11/2330 train_time:399ms step_avg:36.30ms
step:12/2330 train_time:455ms step_avg:37.95ms
step:13/2330 train_time:477ms step_avg:36.68ms
step:14/2330 train_time:533ms step_avg:38.04ms
step:15/2330 train_time:554ms step_avg:36.94ms
step:16/2330 train_time:609ms step_avg:38.09ms
step:17/2330 train_time:632ms step_avg:37.16ms
step:18/2330 train_time:687ms step_avg:38.17ms
step:19/2330 train_time:709ms step_avg:37.33ms
step:20/2330 train_time:765ms step_avg:38.24ms
step:21/2330 train_time:787ms step_avg:37.47ms
step:22/2330 train_time:842ms step_avg:38.27ms
step:23/2330 train_time:864ms step_avg:37.57ms
step:24/2330 train_time:920ms step_avg:38.32ms
step:25/2330 train_time:942ms step_avg:37.66ms
step:26/2330 train_time:998ms step_avg:38.39ms
step:27/2330 train_time:1023ms step_avg:37.87ms
step:28/2330 train_time:1085ms step_avg:38.76ms
step:29/2330 train_time:1110ms step_avg:38.29ms
step:30/2330 train_time:1168ms step_avg:38.93ms
step:31/2330 train_time:1191ms step_avg:38.42ms
step:32/2330 train_time:1247ms step_avg:38.98ms
step:33/2330 train_time:1271ms step_avg:38.50ms
step:34/2330 train_time:1327ms step_avg:39.02ms
step:35/2330 train_time:1350ms step_avg:38.56ms
step:36/2330 train_time:1405ms step_avg:39.04ms
step:37/2330 train_time:1428ms step_avg:38.60ms
step:38/2330 train_time:1483ms step_avg:39.03ms
step:39/2330 train_time:1505ms step_avg:38.60ms
step:40/2330 train_time:1561ms step_avg:39.02ms
step:41/2330 train_time:1583ms step_avg:38.61ms
step:42/2330 train_time:1639ms step_avg:39.01ms
step:43/2330 train_time:1660ms step_avg:38.61ms
step:44/2330 train_time:1716ms step_avg:39.00ms
step:45/2330 train_time:1738ms step_avg:38.62ms
step:46/2330 train_time:1792ms step_avg:38.97ms
step:47/2330 train_time:1814ms step_avg:38.60ms
step:48/2330 train_time:1869ms step_avg:38.93ms
step:49/2330 train_time:1891ms step_avg:38.58ms
step:50/2330 train_time:1946ms step_avg:38.93ms
step:51/2330 train_time:1969ms step_avg:38.61ms
step:52/2330 train_time:2026ms step_avg:38.97ms
step:53/2330 train_time:2049ms step_avg:38.67ms
step:54/2330 train_time:2106ms step_avg:39.01ms
step:55/2330 train_time:2130ms step_avg:38.72ms
step:56/2330 train_time:2186ms step_avg:39.04ms
step:57/2330 train_time:2209ms step_avg:38.75ms
step:58/2330 train_time:2265ms step_avg:39.05ms
step:59/2330 train_time:2287ms step_avg:38.76ms
step:60/2330 train_time:2343ms step_avg:39.05ms
step:61/2330 train_time:2366ms step_avg:38.78ms
step:62/2330 train_time:2421ms step_avg:39.05ms
step:63/2330 train_time:2443ms step_avg:38.78ms
step:64/2330 train_time:2498ms step_avg:39.04ms
step:65/2330 train_time:2521ms step_avg:38.78ms
step:66/2330 train_time:2576ms step_avg:39.03ms
step:67/2330 train_time:2598ms step_avg:38.77ms
step:68/2330 train_time:2654ms step_avg:39.02ms
step:69/2330 train_time:2675ms step_avg:38.77ms
step:70/2330 train_time:2730ms step_avg:39.00ms
step:71/2330 train_time:2752ms step_avg:38.76ms
step:72/2330 train_time:2807ms step_avg:38.98ms
step:73/2330 train_time:2829ms step_avg:38.75ms
step:74/2330 train_time:2885ms step_avg:38.98ms
step:75/2330 train_time:2907ms step_avg:38.76ms
step:76/2330 train_time:2963ms step_avg:38.98ms
step:77/2330 train_time:2986ms step_avg:38.77ms
step:78/2330 train_time:3042ms step_avg:39.00ms
step:79/2330 train_time:3065ms step_avg:38.80ms
step:80/2330 train_time:3122ms step_avg:39.03ms
step:81/2330 train_time:3144ms step_avg:38.82ms
step:82/2330 train_time:3201ms step_avg:39.03ms
step:83/2330 train_time:3223ms step_avg:38.83ms
step:84/2330 train_time:3280ms step_avg:39.05ms
step:85/2330 train_time:3302ms step_avg:38.85ms
step:86/2330 train_time:3358ms step_avg:39.05ms
step:87/2330 train_time:3380ms step_avg:38.85ms
step:88/2330 train_time:3436ms step_avg:39.04ms
step:89/2330 train_time:3458ms step_avg:38.85ms
step:90/2330 train_time:3514ms step_avg:39.05ms
step:91/2330 train_time:3536ms step_avg:38.85ms
step:92/2330 train_time:3591ms step_avg:39.03ms
step:93/2330 train_time:3613ms step_avg:38.85ms
step:94/2330 train_time:3668ms step_avg:39.02ms
step:95/2330 train_time:3690ms step_avg:38.84ms
step:96/2330 train_time:3745ms step_avg:39.01ms
step:97/2330 train_time:3767ms step_avg:38.84ms
step:98/2330 train_time:3823ms step_avg:39.01ms
step:99/2330 train_time:3845ms step_avg:38.84ms
step:100/2330 train_time:3900ms step_avg:39.00ms
step:101/2330 train_time:3922ms step_avg:38.83ms
step:102/2330 train_time:3978ms step_avg:39.00ms
step:103/2330 train_time:4000ms step_avg:38.84ms
step:104/2330 train_time:4057ms step_avg:39.01ms
step:105/2330 train_time:4079ms step_avg:38.85ms
step:106/2330 train_time:4136ms step_avg:39.02ms
step:107/2330 train_time:4159ms step_avg:38.87ms
step:108/2330 train_time:4216ms step_avg:39.03ms
step:109/2330 train_time:4238ms step_avg:38.88ms
step:110/2330 train_time:4294ms step_avg:39.04ms
step:111/2330 train_time:4316ms step_avg:38.89ms
step:112/2330 train_time:4373ms step_avg:39.04ms
step:113/2330 train_time:4395ms step_avg:38.89ms
step:114/2330 train_time:4451ms step_avg:39.04ms
step:115/2330 train_time:4473ms step_avg:38.89ms
step:116/2330 train_time:4528ms step_avg:39.04ms
step:117/2330 train_time:4551ms step_avg:38.90ms
step:118/2330 train_time:4606ms step_avg:39.04ms
step:119/2330 train_time:4629ms step_avg:38.90ms
step:120/2330 train_time:4685ms step_avg:39.04ms
step:121/2330 train_time:4707ms step_avg:38.90ms
step:122/2330 train_time:4763ms step_avg:39.04ms
step:123/2330 train_time:4785ms step_avg:38.90ms
step:124/2330 train_time:4840ms step_avg:39.03ms
step:125/2330 train_time:4862ms step_avg:38.90ms
step:126/2330 train_time:4918ms step_avg:39.04ms
step:127/2330 train_time:4940ms step_avg:38.90ms
step:128/2330 train_time:4996ms step_avg:39.03ms
step:129/2330 train_time:5018ms step_avg:38.90ms
step:130/2330 train_time:5074ms step_avg:39.03ms
step:131/2330 train_time:5097ms step_avg:38.91ms
step:132/2330 train_time:5153ms step_avg:39.04ms
step:133/2330 train_time:5175ms step_avg:38.91ms
step:134/2330 train_time:5232ms step_avg:39.04ms
step:135/2330 train_time:5253ms step_avg:38.91ms
step:136/2330 train_time:5309ms step_avg:39.04ms
step:137/2330 train_time:5332ms step_avg:38.92ms
step:138/2330 train_time:5387ms step_avg:39.04ms
step:139/2330 train_time:5410ms step_avg:38.92ms
step:140/2330 train_time:5465ms step_avg:39.04ms
step:141/2330 train_time:5488ms step_avg:38.92ms
step:142/2330 train_time:5544ms step_avg:39.04ms
step:143/2330 train_time:5567ms step_avg:38.93ms
step:144/2330 train_time:5623ms step_avg:39.05ms
step:145/2330 train_time:5645ms step_avg:38.93ms
step:146/2330 train_time:5700ms step_avg:39.04ms
step:147/2330 train_time:5722ms step_avg:38.93ms
step:148/2330 train_time:5778ms step_avg:39.04ms
step:149/2330 train_time:5800ms step_avg:38.93ms
step:150/2330 train_time:5856ms step_avg:39.04ms
step:151/2330 train_time:5878ms step_avg:38.93ms
step:152/2330 train_time:5934ms step_avg:39.04ms
step:153/2330 train_time:5956ms step_avg:38.93ms
step:154/2330 train_time:6012ms step_avg:39.04ms
step:155/2330 train_time:6034ms step_avg:38.93ms
step:156/2330 train_time:6090ms step_avg:39.04ms
step:157/2330 train_time:6112ms step_avg:38.93ms
step:158/2330 train_time:6168ms step_avg:39.04ms
step:159/2330 train_time:6191ms step_avg:38.94ms
step:160/2330 train_time:6247ms step_avg:39.05ms
step:161/2330 train_time:6270ms step_avg:38.94ms
step:162/2330 train_time:6325ms step_avg:39.05ms
step:163/2330 train_time:6349ms step_avg:38.95ms
step:164/2330 train_time:6404ms step_avg:39.05ms
step:165/2330 train_time:6427ms step_avg:38.95ms
step:166/2330 train_time:6483ms step_avg:39.05ms
step:167/2330 train_time:6505ms step_avg:38.95ms
step:168/2330 train_time:6561ms step_avg:39.05ms
step:169/2330 train_time:6583ms step_avg:38.95ms
step:170/2330 train_time:6639ms step_avg:39.05ms
step:171/2330 train_time:6661ms step_avg:38.95ms
step:172/2330 train_time:6717ms step_avg:39.05ms
step:173/2330 train_time:6740ms step_avg:38.96ms
step:174/2330 train_time:6796ms step_avg:39.06ms
step:175/2330 train_time:6817ms step_avg:38.96ms
step:176/2330 train_time:6874ms step_avg:39.06ms
step:177/2330 train_time:6896ms step_avg:38.96ms
step:178/2330 train_time:6952ms step_avg:39.05ms
step:179/2330 train_time:6973ms step_avg:38.96ms
step:180/2330 train_time:7030ms step_avg:39.05ms
step:181/2330 train_time:7052ms step_avg:38.96ms
step:182/2330 train_time:7108ms step_avg:39.05ms
step:183/2330 train_time:7131ms step_avg:38.97ms
step:184/2330 train_time:7187ms step_avg:39.06ms
step:185/2330 train_time:7209ms step_avg:38.97ms
step:186/2330 train_time:7265ms step_avg:39.06ms
step:187/2330 train_time:7288ms step_avg:38.97ms
step:188/2330 train_time:7343ms step_avg:39.06ms
step:189/2330 train_time:7366ms step_avg:38.97ms
step:190/2330 train_time:7422ms step_avg:39.06ms
step:191/2330 train_time:7444ms step_avg:38.97ms
step:192/2330 train_time:7500ms step_avg:39.06ms
step:193/2330 train_time:7522ms step_avg:38.97ms
step:194/2330 train_time:7579ms step_avg:39.06ms
step:195/2330 train_time:7601ms step_avg:38.98ms
step:196/2330 train_time:7657ms step_avg:39.06ms
step:197/2330 train_time:7679ms step_avg:38.98ms
step:198/2330 train_time:7735ms step_avg:39.07ms
step:199/2330 train_time:7757ms step_avg:38.98ms
step:200/2330 train_time:7814ms step_avg:39.07ms
step:201/2330 train_time:7836ms step_avg:38.98ms
step:202/2330 train_time:7891ms step_avg:39.06ms
step:203/2330 train_time:7914ms step_avg:38.98ms
step:204/2330 train_time:7970ms step_avg:39.07ms
step:205/2330 train_time:7992ms step_avg:38.98ms
step:206/2330 train_time:8048ms step_avg:39.07ms
step:207/2330 train_time:8070ms step_avg:38.99ms
step:208/2330 train_time:8126ms step_avg:39.07ms
step:209/2330 train_time:8149ms step_avg:38.99ms
step:210/2330 train_time:8205ms step_avg:39.07ms
step:211/2330 train_time:8227ms step_avg:38.99ms
step:212/2330 train_time:8283ms step_avg:39.07ms
step:213/2330 train_time:8305ms step_avg:38.99ms
step:214/2330 train_time:8361ms step_avg:39.07ms
step:215/2330 train_time:8384ms step_avg:38.99ms
step:216/2330 train_time:8439ms step_avg:39.07ms
step:217/2330 train_time:8461ms step_avg:38.99ms
step:218/2330 train_time:8517ms step_avg:39.07ms
step:219/2330 train_time:8540ms step_avg:38.99ms
step:220/2330 train_time:8596ms step_avg:39.07ms
step:221/2330 train_time:8618ms step_avg:39.00ms
step:222/2330 train_time:8674ms step_avg:39.07ms
step:223/2330 train_time:8697ms step_avg:39.00ms
step:224/2330 train_time:8752ms step_avg:39.07ms
step:225/2330 train_time:8775ms step_avg:39.00ms
step:226/2330 train_time:8831ms step_avg:39.07ms
step:227/2330 train_time:8853ms step_avg:39.00ms
step:228/2330 train_time:8909ms step_avg:39.07ms
step:229/2330 train_time:8931ms step_avg:39.00ms
step:230/2330 train_time:8987ms step_avg:39.08ms
step:231/2330 train_time:9010ms step_avg:39.00ms
step:232/2330 train_time:9066ms step_avg:39.08ms
step:233/2330 train_time:9088ms step_avg:39.00ms
step:234/2330 train_time:9144ms step_avg:39.08ms
step:235/2330 train_time:9167ms step_avg:39.01ms
step:236/2330 train_time:9223ms step_avg:39.08ms
step:237/2330 train_time:9245ms step_avg:39.01ms
step:238/2330 train_time:9300ms step_avg:39.08ms
step:239/2330 train_time:9322ms step_avg:39.01ms
step:240/2330 train_time:9378ms step_avg:39.08ms
step:241/2330 train_time:9401ms step_avg:39.01ms
step:242/2330 train_time:9457ms step_avg:39.08ms
step:243/2330 train_time:9478ms step_avg:39.01ms
step:244/2330 train_time:9534ms step_avg:39.08ms
step:245/2330 train_time:9557ms step_avg:39.01ms
step:246/2330 train_time:9613ms step_avg:39.08ms
step:247/2330 train_time:9635ms step_avg:39.01ms
step:248/2330 train_time:9691ms step_avg:39.08ms
step:249/2330 train_time:9713ms step_avg:39.01ms
step:250/2330 train_time:9769ms step_avg:39.08ms
step:250/2330 val_loss:5.5079 train_time:9864ms step_avg:39.46ms
step:251/2330 train_time:9878ms step_avg:39.35ms
step:252/2330 train_time:9890ms step_avg:39.25ms
step:253/2330 train_time:9901ms step_avg:39.13ms
step:254/2330 train_time:9926ms step_avg:39.08ms
step:255/2330 train_time:9948ms step_avg:39.01ms
step:256/2330 train_time:10002ms step_avg:39.07ms
step:257/2330 train_time:10024ms step_avg:39.00ms
step:258/2330 train_time:10079ms step_avg:39.07ms
step:259/2330 train_time:10101ms step_avg:39.00ms
step:260/2330 train_time:10157ms step_avg:39.06ms
step:261/2330 train_time:10183ms step_avg:39.02ms
step:262/2330 train_time:10244ms step_avg:39.10ms
step:263/2330 train_time:10268ms step_avg:39.04ms
step:264/2330 train_time:10325ms step_avg:39.11ms
step:265/2330 train_time:10348ms step_avg:39.05ms
step:266/2330 train_time:10404ms step_avg:39.11ms
step:267/2330 train_time:10426ms step_avg:39.05ms
step:268/2330 train_time:10482ms step_avg:39.11ms
step:269/2330 train_time:10504ms step_avg:39.05ms
step:270/2330 train_time:10560ms step_avg:39.11ms
step:271/2330 train_time:10581ms step_avg:39.04ms
step:272/2330 train_time:10637ms step_avg:39.11ms
step:273/2330 train_time:10659ms step_avg:39.04ms
step:274/2330 train_time:10714ms step_avg:39.10ms
step:275/2330 train_time:10736ms step_avg:39.04ms
step:276/2330 train_time:10798ms step_avg:39.12ms
step:277/2330 train_time:10821ms step_avg:39.07ms
step:278/2330 train_time:10880ms step_avg:39.14ms
step:279/2330 train_time:10902ms step_avg:39.08ms
step:280/2330 train_time:10959ms step_avg:39.14ms
step:281/2330 train_time:10980ms step_avg:39.08ms
step:282/2330 train_time:11036ms step_avg:39.13ms
step:283/2330 train_time:11058ms step_avg:39.07ms
step:284/2330 train_time:11114ms step_avg:39.14ms
step:285/2330 train_time:11137ms step_avg:39.08ms
step:286/2330 train_time:11193ms step_avg:39.14ms
step:287/2330 train_time:11217ms step_avg:39.08ms
step:288/2330 train_time:11274ms step_avg:39.14ms
step:289/2330 train_time:11297ms step_avg:39.09ms
step:290/2330 train_time:11353ms step_avg:39.15ms
step:291/2330 train_time:11375ms step_avg:39.09ms
step:292/2330 train_time:11431ms step_avg:39.15ms
step:293/2330 train_time:11453ms step_avg:39.09ms
step:294/2330 train_time:11509ms step_avg:39.15ms
step:295/2330 train_time:11532ms step_avg:39.09ms
step:296/2330 train_time:11588ms step_avg:39.15ms
step:297/2330 train_time:11610ms step_avg:39.09ms
step:298/2330 train_time:11666ms step_avg:39.15ms
step:299/2330 train_time:11688ms step_avg:39.09ms
step:300/2330 train_time:11745ms step_avg:39.15ms
step:301/2330 train_time:11767ms step_avg:39.09ms
step:302/2330 train_time:11824ms step_avg:39.15ms
step:303/2330 train_time:11847ms step_avg:39.10ms
step:304/2330 train_time:11903ms step_avg:39.15ms
step:305/2330 train_time:11924ms step_avg:39.10ms
step:306/2330 train_time:11981ms step_avg:39.15ms
step:307/2330 train_time:12002ms step_avg:39.10ms
step:308/2330 train_time:12059ms step_avg:39.15ms
step:309/2330 train_time:12082ms step_avg:39.10ms
step:310/2330 train_time:12139ms step_avg:39.16ms
step:311/2330 train_time:12162ms step_avg:39.11ms
step:312/2330 train_time:12220ms step_avg:39.17ms
step:313/2330 train_time:12243ms step_avg:39.11ms
step:314/2330 train_time:12299ms step_avg:39.17ms
step:315/2330 train_time:12322ms step_avg:39.12ms
step:316/2330 train_time:12378ms step_avg:39.17ms
step:317/2330 train_time:12401ms step_avg:39.12ms
step:318/2330 train_time:12457ms step_avg:39.17ms
step:319/2330 train_time:12479ms step_avg:39.12ms
step:320/2330 train_time:12536ms step_avg:39.17ms
step:321/2330 train_time:12558ms step_avg:39.12ms
step:322/2330 train_time:12614ms step_avg:39.17ms
step:323/2330 train_time:12635ms step_avg:39.12ms
step:324/2330 train_time:12691ms step_avg:39.17ms
step:325/2330 train_time:12714ms step_avg:39.12ms
step:326/2330 train_time:12771ms step_avg:39.17ms
step:327/2330 train_time:12794ms step_avg:39.13ms
step:328/2330 train_time:12851ms step_avg:39.18ms
step:329/2330 train_time:12874ms step_avg:39.13ms
step:330/2330 train_time:12930ms step_avg:39.18ms
step:331/2330 train_time:12953ms step_avg:39.13ms
step:332/2330 train_time:13008ms step_avg:39.18ms
step:333/2330 train_time:13031ms step_avg:39.13ms
step:334/2330 train_time:13088ms step_avg:39.18ms
step:335/2330 train_time:13110ms step_avg:39.14ms
step:336/2330 train_time:13166ms step_avg:39.18ms
step:337/2330 train_time:13189ms step_avg:39.14ms
step:338/2330 train_time:13244ms step_avg:39.18ms
step:339/2330 train_time:13267ms step_avg:39.14ms
step:340/2330 train_time:13324ms step_avg:39.19ms
step:341/2330 train_time:13347ms step_avg:39.14ms
step:342/2330 train_time:13403ms step_avg:39.19ms
step:343/2330 train_time:13425ms step_avg:39.14ms
step:344/2330 train_time:13481ms step_avg:39.19ms
step:345/2330 train_time:13504ms step_avg:39.14ms
step:346/2330 train_time:13561ms step_avg:39.19ms
step:347/2330 train_time:13583ms step_avg:39.14ms
step:348/2330 train_time:13640ms step_avg:39.19ms
step:349/2330 train_time:13663ms step_avg:39.15ms
step:350/2330 train_time:13720ms step_avg:39.20ms
step:351/2330 train_time:13741ms step_avg:39.15ms
step:352/2330 train_time:13798ms step_avg:39.20ms
step:353/2330 train_time:13821ms step_avg:39.15ms
step:354/2330 train_time:13878ms step_avg:39.20ms
step:355/2330 train_time:13900ms step_avg:39.16ms
step:356/2330 train_time:13957ms step_avg:39.20ms
step:357/2330 train_time:13978ms step_avg:39.15ms
step:358/2330 train_time:14035ms step_avg:39.20ms
step:359/2330 train_time:14057ms step_avg:39.16ms
step:360/2330 train_time:14113ms step_avg:39.20ms
step:361/2330 train_time:14135ms step_avg:39.15ms
step:362/2330 train_time:14191ms step_avg:39.20ms
step:363/2330 train_time:14214ms step_avg:39.16ms
step:364/2330 train_time:14270ms step_avg:39.20ms
step:365/2330 train_time:14293ms step_avg:39.16ms
step:366/2330 train_time:14349ms step_avg:39.21ms
step:367/2330 train_time:14372ms step_avg:39.16ms
step:368/2330 train_time:14428ms step_avg:39.21ms
step:369/2330 train_time:14451ms step_avg:39.16ms
step:370/2330 train_time:14507ms step_avg:39.21ms
step:371/2330 train_time:14530ms step_avg:39.16ms
step:372/2330 train_time:14586ms step_avg:39.21ms
step:373/2330 train_time:14609ms step_avg:39.17ms
step:374/2330 train_time:14665ms step_avg:39.21ms
step:375/2330 train_time:14688ms step_avg:39.17ms
step:376/2330 train_time:14743ms step_avg:39.21ms
step:377/2330 train_time:14766ms step_avg:39.17ms
step:378/2330 train_time:14822ms step_avg:39.21ms
step:379/2330 train_time:14844ms step_avg:39.17ms
step:380/2330 train_time:14901ms step_avg:39.21ms
step:381/2330 train_time:14924ms step_avg:39.17ms
step:382/2330 train_time:14981ms step_avg:39.22ms
step:383/2330 train_time:15003ms step_avg:39.17ms
step:384/2330 train_time:15060ms step_avg:39.22ms
step:385/2330 train_time:15082ms step_avg:39.17ms
step:386/2330 train_time:15139ms step_avg:39.22ms
step:387/2330 train_time:15161ms step_avg:39.18ms
step:388/2330 train_time:15217ms step_avg:39.22ms
step:389/2330 train_time:15240ms step_avg:39.18ms
step:390/2330 train_time:15297ms step_avg:39.22ms
step:391/2330 train_time:15319ms step_avg:39.18ms
step:392/2330 train_time:15375ms step_avg:39.22ms
step:393/2330 train_time:15397ms step_avg:39.18ms
step:394/2330 train_time:15453ms step_avg:39.22ms
step:395/2330 train_time:15475ms step_avg:39.18ms
step:396/2330 train_time:15531ms step_avg:39.22ms
step:397/2330 train_time:15554ms step_avg:39.18ms
step:398/2330 train_time:15610ms step_avg:39.22ms
step:399/2330 train_time:15633ms step_avg:39.18ms
step:400/2330 train_time:15689ms step_avg:39.22ms
step:401/2330 train_time:15712ms step_avg:39.18ms
step:402/2330 train_time:15768ms step_avg:39.22ms
step:403/2330 train_time:15791ms step_avg:39.18ms
step:404/2330 train_time:15848ms step_avg:39.23ms
step:405/2330 train_time:15871ms step_avg:39.19ms
step:406/2330 train_time:15927ms step_avg:39.23ms
step:407/2330 train_time:15950ms step_avg:39.19ms
step:408/2330 train_time:16006ms step_avg:39.23ms
step:409/2330 train_time:16029ms step_avg:39.19ms
step:410/2330 train_time:16085ms step_avg:39.23ms
step:411/2330 train_time:16107ms step_avg:39.19ms
step:412/2330 train_time:16163ms step_avg:39.23ms
step:413/2330 train_time:16186ms step_avg:39.19ms
step:414/2330 train_time:16242ms step_avg:39.23ms
step:415/2330 train_time:16264ms step_avg:39.19ms
step:416/2330 train_time:16320ms step_avg:39.23ms
step:417/2330 train_time:16343ms step_avg:39.19ms
step:418/2330 train_time:16399ms step_avg:39.23ms
step:419/2330 train_time:16422ms step_avg:39.19ms
step:420/2330 train_time:16478ms step_avg:39.23ms
step:421/2330 train_time:16500ms step_avg:39.19ms
step:422/2330 train_time:16556ms step_avg:39.23ms
step:423/2330 train_time:16578ms step_avg:39.19ms
step:424/2330 train_time:16635ms step_avg:39.23ms
step:425/2330 train_time:16657ms step_avg:39.19ms
step:426/2330 train_time:16714ms step_avg:39.23ms
step:427/2330 train_time:16736ms step_avg:39.19ms
step:428/2330 train_time:16792ms step_avg:39.23ms
step:429/2330 train_time:16814ms step_avg:39.19ms
step:430/2330 train_time:16870ms step_avg:39.23ms
step:431/2330 train_time:16893ms step_avg:39.19ms
step:432/2330 train_time:16948ms step_avg:39.23ms
step:433/2330 train_time:16971ms step_avg:39.19ms
step:434/2330 train_time:17027ms step_avg:39.23ms
step:435/2330 train_time:17049ms step_avg:39.19ms
step:436/2330 train_time:17106ms step_avg:39.23ms
step:437/2330 train_time:17128ms step_avg:39.19ms
step:438/2330 train_time:17185ms step_avg:39.23ms
step:439/2330 train_time:17207ms step_avg:39.20ms
step:440/2330 train_time:17263ms step_avg:39.23ms
step:441/2330 train_time:17286ms step_avg:39.20ms
step:442/2330 train_time:17342ms step_avg:39.23ms
step:443/2330 train_time:17365ms step_avg:39.20ms
step:444/2330 train_time:17422ms step_avg:39.24ms
step:445/2330 train_time:17444ms step_avg:39.20ms
step:446/2330 train_time:17500ms step_avg:39.24ms
step:447/2330 train_time:17523ms step_avg:39.20ms
step:448/2330 train_time:17579ms step_avg:39.24ms
step:449/2330 train_time:17601ms step_avg:39.20ms
step:450/2330 train_time:17658ms step_avg:39.24ms
step:451/2330 train_time:17680ms step_avg:39.20ms
step:452/2330 train_time:17736ms step_avg:39.24ms
step:453/2330 train_time:17759ms step_avg:39.20ms
step:454/2330 train_time:17815ms step_avg:39.24ms
step:455/2330 train_time:17838ms step_avg:39.20ms
step:456/2330 train_time:17895ms step_avg:39.24ms
step:457/2330 train_time:17917ms step_avg:39.21ms
step:458/2330 train_time:17973ms step_avg:39.24ms
step:459/2330 train_time:17996ms step_avg:39.21ms
step:460/2330 train_time:18052ms step_avg:39.24ms
step:461/2330 train_time:18075ms step_avg:39.21ms
step:462/2330 train_time:18130ms step_avg:39.24ms
step:463/2330 train_time:18153ms step_avg:39.21ms
step:464/2330 train_time:18210ms step_avg:39.25ms
step:465/2330 train_time:18233ms step_avg:39.21ms
step:466/2330 train_time:18289ms step_avg:39.25ms
step:467/2330 train_time:18312ms step_avg:39.21ms
step:468/2330 train_time:18368ms step_avg:39.25ms
step:469/2330 train_time:18391ms step_avg:39.21ms
step:470/2330 train_time:18447ms step_avg:39.25ms
step:471/2330 train_time:18470ms step_avg:39.21ms
step:472/2330 train_time:18526ms step_avg:39.25ms
step:473/2330 train_time:18549ms step_avg:39.21ms
step:474/2330 train_time:18604ms step_avg:39.25ms
step:475/2330 train_time:18627ms step_avg:39.21ms
step:476/2330 train_time:18683ms step_avg:39.25ms
step:477/2330 train_time:18706ms step_avg:39.22ms
step:478/2330 train_time:18763ms step_avg:39.25ms
step:479/2330 train_time:18785ms step_avg:39.22ms
step:480/2330 train_time:18842ms step_avg:39.25ms
step:481/2330 train_time:18865ms step_avg:39.22ms
step:482/2330 train_time:18922ms step_avg:39.26ms
step:483/2330 train_time:18945ms step_avg:39.22ms
step:484/2330 train_time:19001ms step_avg:39.26ms
step:485/2330 train_time:19023ms step_avg:39.22ms
step:486/2330 train_time:19080ms step_avg:39.26ms
step:487/2330 train_time:19102ms step_avg:39.22ms
step:488/2330 train_time:19158ms step_avg:39.26ms
step:489/2330 train_time:19180ms step_avg:39.22ms
step:490/2330 train_time:19236ms step_avg:39.26ms
step:491/2330 train_time:19258ms step_avg:39.22ms
step:492/2330 train_time:19315ms step_avg:39.26ms
step:493/2330 train_time:19337ms step_avg:39.22ms
step:494/2330 train_time:19393ms step_avg:39.26ms
step:495/2330 train_time:19414ms step_avg:39.22ms
step:496/2330 train_time:19470ms step_avg:39.25ms
step:497/2330 train_time:19493ms step_avg:39.22ms
step:498/2330 train_time:19550ms step_avg:39.26ms
step:499/2330 train_time:19573ms step_avg:39.22ms
step:500/2330 train_time:19629ms step_avg:39.26ms
step:500/2330 val_loss:5.3724 train_time:19725ms step_avg:39.45ms
step:501/2330 train_time:19738ms step_avg:39.40ms
step:502/2330 train_time:19750ms step_avg:39.34ms
step:503/2330 train_time:19761ms step_avg:39.29ms
step:504/2330 train_time:19787ms step_avg:39.26ms
step:505/2330 train_time:19808ms step_avg:39.22ms
step:506/2330 train_time:19863ms step_avg:39.26ms
step:507/2330 train_time:19885ms step_avg:39.22ms
step:508/2330 train_time:19941ms step_avg:39.25ms
step:509/2330 train_time:19963ms step_avg:39.22ms
step:510/2330 train_time:20020ms step_avg:39.25ms
step:511/2330 train_time:20044ms step_avg:39.22ms
step:512/2330 train_time:20104ms step_avg:39.27ms
step:513/2330 train_time:20128ms step_avg:39.24ms
step:514/2330 train_time:20185ms step_avg:39.27ms
step:515/2330 train_time:20207ms step_avg:39.24ms
step:516/2330 train_time:20262ms step_avg:39.27ms
step:517/2330 train_time:20284ms step_avg:39.23ms
step:518/2330 train_time:20341ms step_avg:39.27ms
step:519/2330 train_time:20363ms step_avg:39.23ms
step:520/2330 train_time:20418ms step_avg:39.27ms
step:521/2330 train_time:20441ms step_avg:39.23ms
step:522/2330 train_time:20496ms step_avg:39.26ms
step:523/2330 train_time:20518ms step_avg:39.23ms
step:524/2330 train_time:20573ms step_avg:39.26ms
step:525/2330 train_time:20595ms step_avg:39.23ms
step:526/2330 train_time:20653ms step_avg:39.26ms
step:527/2330 train_time:20676ms step_avg:39.23ms
step:528/2330 train_time:20732ms step_avg:39.26ms
step:529/2330 train_time:20754ms step_avg:39.23ms
step:530/2330 train_time:20809ms step_avg:39.26ms
step:531/2330 train_time:20832ms step_avg:39.23ms
step:532/2330 train_time:20888ms step_avg:39.26ms
step:533/2330 train_time:20911ms step_avg:39.23ms
step:534/2330 train_time:20967ms step_avg:39.26ms
step:535/2330 train_time:20990ms step_avg:39.23ms
step:536/2330 train_time:21047ms step_avg:39.27ms
step:537/2330 train_time:21070ms step_avg:39.24ms
step:538/2330 train_time:21127ms step_avg:39.27ms
step:539/2330 train_time:21149ms step_avg:39.24ms
step:540/2330 train_time:21205ms step_avg:39.27ms
step:541/2330 train_time:21227ms step_avg:39.24ms
step:542/2330 train_time:21284ms step_avg:39.27ms
step:543/2330 train_time:21306ms step_avg:39.24ms
step:544/2330 train_time:21362ms step_avg:39.27ms
step:545/2330 train_time:21384ms step_avg:39.24ms
step:546/2330 train_time:21440ms step_avg:39.27ms
step:547/2330 train_time:21461ms step_avg:39.23ms
step:548/2330 train_time:21516ms step_avg:39.26ms
step:549/2330 train_time:21539ms step_avg:39.23ms
step:550/2330 train_time:21595ms step_avg:39.26ms
step:551/2330 train_time:21618ms step_avg:39.23ms
step:552/2330 train_time:21673ms step_avg:39.26ms
step:553/2330 train_time:21696ms step_avg:39.23ms
step:554/2330 train_time:21752ms step_avg:39.26ms
step:555/2330 train_time:21775ms step_avg:39.23ms
step:556/2330 train_time:21831ms step_avg:39.26ms
step:557/2330 train_time:21854ms step_avg:39.23ms
step:558/2330 train_time:21909ms step_avg:39.26ms
step:559/2330 train_time:21931ms step_avg:39.23ms
step:560/2330 train_time:21988ms step_avg:39.26ms
step:561/2330 train_time:22010ms step_avg:39.23ms
step:562/2330 train_time:22066ms step_avg:39.26ms
step:563/2330 train_time:22089ms step_avg:39.23ms
step:564/2330 train_time:22145ms step_avg:39.26ms
step:565/2330 train_time:22168ms step_avg:39.24ms
step:566/2330 train_time:22224ms step_avg:39.27ms
step:567/2330 train_time:22247ms step_avg:39.24ms
step:568/2330 train_time:22303ms step_avg:39.27ms
step:569/2330 train_time:22325ms step_avg:39.24ms
step:570/2330 train_time:22381ms step_avg:39.27ms
step:571/2330 train_time:22403ms step_avg:39.23ms
step:572/2330 train_time:22459ms step_avg:39.26ms
step:573/2330 train_time:22481ms step_avg:39.23ms
step:574/2330 train_time:22537ms step_avg:39.26ms
step:575/2330 train_time:22559ms step_avg:39.23ms
step:576/2330 train_time:22614ms step_avg:39.26ms
step:577/2330 train_time:22636ms step_avg:39.23ms
step:578/2330 train_time:22692ms step_avg:39.26ms
step:579/2330 train_time:22715ms step_avg:39.23ms
step:580/2330 train_time:22770ms step_avg:39.26ms
step:581/2330 train_time:22793ms step_avg:39.23ms
step:582/2330 train_time:22849ms step_avg:39.26ms
step:583/2330 train_time:22871ms step_avg:39.23ms
step:584/2330 train_time:22927ms step_avg:39.26ms
step:585/2330 train_time:22949ms step_avg:39.23ms
step:586/2330 train_time:23006ms step_avg:39.26ms
step:587/2330 train_time:23028ms step_avg:39.23ms
step:588/2330 train_time:23085ms step_avg:39.26ms
step:589/2330 train_time:23107ms step_avg:39.23ms
step:590/2330 train_time:23163ms step_avg:39.26ms
step:591/2330 train_time:23185ms step_avg:39.23ms
step:592/2330 train_time:23242ms step_avg:39.26ms
step:593/2330 train_time:23264ms step_avg:39.23ms
step:594/2330 train_time:23320ms step_avg:39.26ms
step:595/2330 train_time:23342ms step_avg:39.23ms
step:596/2330 train_time:23397ms step_avg:39.26ms
step:597/2330 train_time:23420ms step_avg:39.23ms
step:598/2330 train_time:23475ms step_avg:39.26ms
step:599/2330 train_time:23498ms step_avg:39.23ms
step:600/2330 train_time:23554ms step_avg:39.26ms
step:601/2330 train_time:23576ms step_avg:39.23ms
step:602/2330 train_time:23632ms step_avg:39.26ms
step:603/2330 train_time:23654ms step_avg:39.23ms
step:604/2330 train_time:23710ms step_avg:39.25ms
step:605/2330 train_time:23733ms step_avg:39.23ms
step:606/2330 train_time:23789ms step_avg:39.26ms
step:607/2330 train_time:23811ms step_avg:39.23ms
step:608/2330 train_time:23866ms step_avg:39.25ms
step:609/2330 train_time:23889ms step_avg:39.23ms
step:610/2330 train_time:23945ms step_avg:39.25ms
step:611/2330 train_time:23968ms step_avg:39.23ms
step:612/2330 train_time:24024ms step_avg:39.25ms
step:613/2330 train_time:24046ms step_avg:39.23ms
step:614/2330 train_time:24102ms step_avg:39.25ms
step:615/2330 train_time:24124ms step_avg:39.23ms
step:616/2330 train_time:24181ms step_avg:39.25ms
step:617/2330 train_time:24202ms step_avg:39.23ms
step:618/2330 train_time:24258ms step_avg:39.25ms
step:619/2330 train_time:24280ms step_avg:39.22ms
step:620/2330 train_time:24336ms step_avg:39.25ms
step:621/2330 train_time:24358ms step_avg:39.22ms
step:622/2330 train_time:24414ms step_avg:39.25ms
step:623/2330 train_time:24437ms step_avg:39.22ms
step:624/2330 train_time:24493ms step_avg:39.25ms
step:625/2330 train_time:24516ms step_avg:39.23ms
step:626/2330 train_time:24572ms step_avg:39.25ms
step:627/2330 train_time:24595ms step_avg:39.23ms
step:628/2330 train_time:24650ms step_avg:39.25ms
step:629/2330 train_time:24672ms step_avg:39.22ms
step:630/2330 train_time:24728ms step_avg:39.25ms
step:631/2330 train_time:24750ms step_avg:39.22ms
step:632/2330 train_time:24805ms step_avg:39.25ms
step:633/2330 train_time:24828ms step_avg:39.22ms
step:634/2330 train_time:24884ms step_avg:39.25ms
step:635/2330 train_time:24906ms step_avg:39.22ms
step:636/2330 train_time:24962ms step_avg:39.25ms
step:637/2330 train_time:24984ms step_avg:39.22ms
step:638/2330 train_time:25041ms step_avg:39.25ms
step:639/2330 train_time:25063ms step_avg:39.22ms
step:640/2330 train_time:25119ms step_avg:39.25ms
step:641/2330 train_time:25141ms step_avg:39.22ms
step:642/2330 train_time:25197ms step_avg:39.25ms
step:643/2330 train_time:25220ms step_avg:39.22ms
step:644/2330 train_time:25275ms step_avg:39.25ms
step:645/2330 train_time:25298ms step_avg:39.22ms
step:646/2330 train_time:25354ms step_avg:39.25ms
step:647/2330 train_time:25376ms step_avg:39.22ms
step:648/2330 train_time:25432ms step_avg:39.25ms
step:649/2330 train_time:25454ms step_avg:39.22ms
step:650/2330 train_time:25510ms step_avg:39.25ms
step:651/2330 train_time:25532ms step_avg:39.22ms
step:652/2330 train_time:25588ms step_avg:39.25ms
step:653/2330 train_time:25610ms step_avg:39.22ms
step:654/2330 train_time:25666ms step_avg:39.24ms
step:655/2330 train_time:25688ms step_avg:39.22ms
step:656/2330 train_time:25744ms step_avg:39.24ms
step:657/2330 train_time:25766ms step_avg:39.22ms
step:658/2330 train_time:25822ms step_avg:39.24ms
step:659/2330 train_time:25844ms step_avg:39.22ms
step:660/2330 train_time:25900ms step_avg:39.24ms
step:661/2330 train_time:25922ms step_avg:39.22ms
step:662/2330 train_time:25978ms step_avg:39.24ms
step:663/2330 train_time:26001ms step_avg:39.22ms
step:664/2330 train_time:26057ms step_avg:39.24ms
step:665/2330 train_time:26080ms step_avg:39.22ms
step:666/2330 train_time:26135ms step_avg:39.24ms
step:667/2330 train_time:26158ms step_avg:39.22ms
step:668/2330 train_time:26214ms step_avg:39.24ms
step:669/2330 train_time:26236ms step_avg:39.22ms
step:670/2330 train_time:26293ms step_avg:39.24ms
step:671/2330 train_time:26315ms step_avg:39.22ms
step:672/2330 train_time:26370ms step_avg:39.24ms
step:673/2330 train_time:26392ms step_avg:39.22ms
step:674/2330 train_time:26449ms step_avg:39.24ms
step:675/2330 train_time:26470ms step_avg:39.22ms
step:676/2330 train_time:26526ms step_avg:39.24ms
step:677/2330 train_time:26548ms step_avg:39.21ms
step:678/2330 train_time:26604ms step_avg:39.24ms
step:679/2330 train_time:26626ms step_avg:39.21ms
step:680/2330 train_time:26683ms step_avg:39.24ms
step:681/2330 train_time:26705ms step_avg:39.22ms
step:682/2330 train_time:26762ms step_avg:39.24ms
step:683/2330 train_time:26784ms step_avg:39.21ms
step:684/2330 train_time:26839ms step_avg:39.24ms
step:685/2330 train_time:26861ms step_avg:39.21ms
step:686/2330 train_time:26917ms step_avg:39.24ms
step:687/2330 train_time:26940ms step_avg:39.21ms
step:688/2330 train_time:26996ms step_avg:39.24ms
step:689/2330 train_time:27019ms step_avg:39.21ms
step:690/2330 train_time:27074ms step_avg:39.24ms
step:691/2330 train_time:27097ms step_avg:39.21ms
step:692/2330 train_time:27153ms step_avg:39.24ms
step:693/2330 train_time:27175ms step_avg:39.21ms
step:694/2330 train_time:27232ms step_avg:39.24ms
step:695/2330 train_time:27254ms step_avg:39.21ms
step:696/2330 train_time:27311ms step_avg:39.24ms
step:697/2330 train_time:27333ms step_avg:39.21ms
step:698/2330 train_time:27389ms step_avg:39.24ms
step:699/2330 train_time:27411ms step_avg:39.21ms
step:700/2330 train_time:27467ms step_avg:39.24ms
step:701/2330 train_time:27489ms step_avg:39.21ms
step:702/2330 train_time:27545ms step_avg:39.24ms
step:703/2330 train_time:27567ms step_avg:39.21ms
step:704/2330 train_time:27623ms step_avg:39.24ms
step:705/2330 train_time:27644ms step_avg:39.21ms
step:706/2330 train_time:27700ms step_avg:39.24ms
step:707/2330 train_time:27722ms step_avg:39.21ms
step:708/2330 train_time:27778ms step_avg:39.23ms
step:709/2330 train_time:27800ms step_avg:39.21ms
step:710/2330 train_time:27856ms step_avg:39.23ms
step:711/2330 train_time:27878ms step_avg:39.21ms
step:712/2330 train_time:27934ms step_avg:39.23ms
step:713/2330 train_time:27956ms step_avg:39.21ms
step:714/2330 train_time:28012ms step_avg:39.23ms
step:715/2330 train_time:28034ms step_avg:39.21ms
step:716/2330 train_time:28091ms step_avg:39.23ms
step:717/2330 train_time:28113ms step_avg:39.21ms
step:718/2330 train_time:28169ms step_avg:39.23ms
step:719/2330 train_time:28192ms step_avg:39.21ms
step:720/2330 train_time:28248ms step_avg:39.23ms
step:721/2330 train_time:28270ms step_avg:39.21ms
step:722/2330 train_time:28327ms step_avg:39.23ms
step:723/2330 train_time:28349ms step_avg:39.21ms
step:724/2330 train_time:28405ms step_avg:39.23ms
step:725/2330 train_time:28427ms step_avg:39.21ms
step:726/2330 train_time:28484ms step_avg:39.23ms
step:727/2330 train_time:28505ms step_avg:39.21ms
step:728/2330 train_time:28561ms step_avg:39.23ms
step:729/2330 train_time:28583ms step_avg:39.21ms
step:730/2330 train_time:28639ms step_avg:39.23ms
step:731/2330 train_time:28662ms step_avg:39.21ms
step:732/2330 train_time:28717ms step_avg:39.23ms
step:733/2330 train_time:28739ms step_avg:39.21ms
step:734/2330 train_time:28796ms step_avg:39.23ms
step:735/2330 train_time:28818ms step_avg:39.21ms
step:736/2330 train_time:28874ms step_avg:39.23ms
step:737/2330 train_time:28897ms step_avg:39.21ms
step:738/2330 train_time:28953ms step_avg:39.23ms
step:739/2330 train_time:28975ms step_avg:39.21ms
step:740/2330 train_time:29032ms step_avg:39.23ms
step:741/2330 train_time:29054ms step_avg:39.21ms
step:742/2330 train_time:29110ms step_avg:39.23ms
step:743/2330 train_time:29132ms step_avg:39.21ms
step:744/2330 train_time:29188ms step_avg:39.23ms
step:745/2330 train_time:29211ms step_avg:39.21ms
step:746/2330 train_time:29267ms step_avg:39.23ms
step:747/2330 train_time:29289ms step_avg:39.21ms
step:748/2330 train_time:29345ms step_avg:39.23ms
step:749/2330 train_time:29368ms step_avg:39.21ms
step:750/2330 train_time:29424ms step_avg:39.23ms
step:750/2330 val_loss:5.2896 train_time:29518ms step_avg:39.36ms
step:751/2330 train_time:29532ms step_avg:39.32ms
step:752/2330 train_time:29544ms step_avg:39.29ms
step:753/2330 train_time:29555ms step_avg:39.25ms
step:754/2330 train_time:29581ms step_avg:39.23ms
step:755/2330 train_time:29603ms step_avg:39.21ms
step:756/2330 train_time:29659ms step_avg:39.23ms
step:757/2330 train_time:29681ms step_avg:39.21ms
step:758/2330 train_time:29736ms step_avg:39.23ms
step:759/2330 train_time:29758ms step_avg:39.21ms
step:760/2330 train_time:29813ms step_avg:39.23ms
step:761/2330 train_time:29836ms step_avg:39.21ms
step:762/2330 train_time:29896ms step_avg:39.23ms
step:763/2330 train_time:29920ms step_avg:39.21ms
step:764/2330 train_time:29978ms step_avg:39.24ms
step:765/2330 train_time:30003ms step_avg:39.22ms
step:766/2330 train_time:30060ms step_avg:39.24ms
step:767/2330 train_time:30082ms step_avg:39.22ms
step:768/2330 train_time:30139ms step_avg:39.24ms
step:769/2330 train_time:30161ms step_avg:39.22ms
step:770/2330 train_time:30217ms step_avg:39.24ms
step:771/2330 train_time:30239ms step_avg:39.22ms
step:772/2330 train_time:30295ms step_avg:39.24ms
step:773/2330 train_time:30317ms step_avg:39.22ms
step:774/2330 train_time:30372ms step_avg:39.24ms
step:775/2330 train_time:30394ms step_avg:39.22ms
step:776/2330 train_time:30450ms step_avg:39.24ms
step:777/2330 train_time:30473ms step_avg:39.22ms
step:778/2330 train_time:30529ms step_avg:39.24ms
step:779/2330 train_time:30552ms step_avg:39.22ms
step:780/2330 train_time:30607ms step_avg:39.24ms
step:781/2330 train_time:30630ms step_avg:39.22ms
step:782/2330 train_time:30685ms step_avg:39.24ms
step:783/2330 train_time:30707ms step_avg:39.22ms
step:784/2330 train_time:30763ms step_avg:39.24ms
step:785/2330 train_time:30785ms step_avg:39.22ms
step:786/2330 train_time:30841ms step_avg:39.24ms
step:787/2330 train_time:30865ms step_avg:39.22ms
step:788/2330 train_time:30921ms step_avg:39.24ms
step:789/2330 train_time:30944ms step_avg:39.22ms
step:790/2330 train_time:31002ms step_avg:39.24ms
step:791/2330 train_time:31026ms step_avg:39.22ms
step:792/2330 train_time:31082ms step_avg:39.25ms
step:793/2330 train_time:31105ms step_avg:39.22ms
step:794/2330 train_time:31162ms step_avg:39.25ms
step:795/2330 train_time:31184ms step_avg:39.23ms
step:796/2330 train_time:31241ms step_avg:39.25ms
step:797/2330 train_time:31264ms step_avg:39.23ms
step:798/2330 train_time:31319ms step_avg:39.25ms
step:799/2330 train_time:31342ms step_avg:39.23ms
step:800/2330 train_time:31399ms step_avg:39.25ms
step:801/2330 train_time:31421ms step_avg:39.23ms
step:802/2330 train_time:31477ms step_avg:39.25ms
step:803/2330 train_time:31499ms step_avg:39.23ms
step:804/2330 train_time:31556ms step_avg:39.25ms
step:805/2330 train_time:31577ms step_avg:39.23ms
step:806/2330 train_time:31634ms step_avg:39.25ms
step:807/2330 train_time:31656ms step_avg:39.23ms
step:808/2330 train_time:31712ms step_avg:39.25ms
step:809/2330 train_time:31734ms step_avg:39.23ms
step:810/2330 train_time:31790ms step_avg:39.25ms
step:811/2330 train_time:31813ms step_avg:39.23ms
step:812/2330 train_time:31869ms step_avg:39.25ms
step:813/2330 train_time:31893ms step_avg:39.23ms
step:814/2330 train_time:31950ms step_avg:39.25ms
step:815/2330 train_time:31974ms step_avg:39.23ms
step:816/2330 train_time:32031ms step_avg:39.25ms
step:817/2330 train_time:32054ms step_avg:39.23ms
step:818/2330 train_time:32110ms step_avg:39.25ms
step:819/2330 train_time:32133ms step_avg:39.23ms
step:820/2330 train_time:32190ms step_avg:39.26ms
step:821/2330 train_time:32213ms step_avg:39.24ms
step:822/2330 train_time:32269ms step_avg:39.26ms
step:823/2330 train_time:32292ms step_avg:39.24ms
step:824/2330 train_time:32348ms step_avg:39.26ms
step:825/2330 train_time:32370ms step_avg:39.24ms
step:826/2330 train_time:32426ms step_avg:39.26ms
step:827/2330 train_time:32449ms step_avg:39.24ms
step:828/2330 train_time:32504ms step_avg:39.26ms
step:829/2330 train_time:32527ms step_avg:39.24ms
step:830/2330 train_time:32582ms step_avg:39.26ms
step:831/2330 train_time:32605ms step_avg:39.24ms
step:832/2330 train_time:32661ms step_avg:39.26ms
step:833/2330 train_time:32684ms step_avg:39.24ms
step:834/2330 train_time:32739ms step_avg:39.26ms
step:835/2330 train_time:32762ms step_avg:39.24ms
step:836/2330 train_time:32819ms step_avg:39.26ms
step:837/2330 train_time:32840ms step_avg:39.24ms
step:838/2330 train_time:32897ms step_avg:39.26ms
step:839/2330 train_time:32920ms step_avg:39.24ms
step:840/2330 train_time:32976ms step_avg:39.26ms
step:841/2330 train_time:32998ms step_avg:39.24ms
step:842/2330 train_time:33056ms step_avg:39.26ms
step:843/2330 train_time:33078ms step_avg:39.24ms
step:844/2330 train_time:33134ms step_avg:39.26ms
step:845/2330 train_time:33157ms step_avg:39.24ms
step:846/2330 train_time:33213ms step_avg:39.26ms
step:847/2330 train_time:33235ms step_avg:39.24ms
step:848/2330 train_time:33291ms step_avg:39.26ms
step:849/2330 train_time:33313ms step_avg:39.24ms
step:850/2330 train_time:33369ms step_avg:39.26ms
step:851/2330 train_time:33392ms step_avg:39.24ms
step:852/2330 train_time:33447ms step_avg:39.26ms
step:853/2330 train_time:33470ms step_avg:39.24ms
step:854/2330 train_time:33526ms step_avg:39.26ms
step:855/2330 train_time:33549ms step_avg:39.24ms
step:856/2330 train_time:33604ms step_avg:39.26ms
step:857/2330 train_time:33626ms step_avg:39.24ms
step:858/2330 train_time:33682ms step_avg:39.26ms
step:859/2330 train_time:33704ms step_avg:39.24ms
step:860/2330 train_time:33760ms step_avg:39.26ms
step:861/2330 train_time:33783ms step_avg:39.24ms
step:862/2330 train_time:33840ms step_avg:39.26ms
step:863/2330 train_time:33863ms step_avg:39.24ms
step:864/2330 train_time:33919ms step_avg:39.26ms
step:865/2330 train_time:33942ms step_avg:39.24ms
step:866/2330 train_time:33999ms step_avg:39.26ms
step:867/2330 train_time:34021ms step_avg:39.24ms
step:868/2330 train_time:34077ms step_avg:39.26ms
step:869/2330 train_time:34100ms step_avg:39.24ms
step:870/2330 train_time:34158ms step_avg:39.26ms
step:871/2330 train_time:34181ms step_avg:39.24ms
step:872/2330 train_time:34237ms step_avg:39.26ms
step:873/2330 train_time:34259ms step_avg:39.24ms
step:874/2330 train_time:34316ms step_avg:39.26ms
step:875/2330 train_time:34338ms step_avg:39.24ms
step:876/2330 train_time:34396ms step_avg:39.26ms
step:877/2330 train_time:34418ms step_avg:39.24ms
step:878/2330 train_time:34474ms step_avg:39.26ms
step:879/2330 train_time:34496ms step_avg:39.24ms
step:880/2330 train_time:34552ms step_avg:39.26ms
step:881/2330 train_time:34574ms step_avg:39.24ms
step:882/2330 train_time:34630ms step_avg:39.26ms
step:883/2330 train_time:34654ms step_avg:39.25ms
step:884/2330 train_time:34710ms step_avg:39.26ms
step:885/2330 train_time:34733ms step_avg:39.25ms
step:886/2330 train_time:34789ms step_avg:39.27ms
step:887/2330 train_time:34812ms step_avg:39.25ms
step:888/2330 train_time:34868ms step_avg:39.27ms
step:889/2330 train_time:34891ms step_avg:39.25ms
step:890/2330 train_time:34947ms step_avg:39.27ms
step:891/2330 train_time:34970ms step_avg:39.25ms
step:892/2330 train_time:35026ms step_avg:39.27ms
step:893/2330 train_time:35049ms step_avg:39.25ms
step:894/2330 train_time:35105ms step_avg:39.27ms
step:895/2330 train_time:35128ms step_avg:39.25ms
step:896/2330 train_time:35183ms step_avg:39.27ms
step:897/2330 train_time:35206ms step_avg:39.25ms
step:898/2330 train_time:35263ms step_avg:39.27ms
step:899/2330 train_time:35286ms step_avg:39.25ms
step:900/2330 train_time:35342ms step_avg:39.27ms
step:901/2330 train_time:35364ms step_avg:39.25ms
step:902/2330 train_time:35421ms step_avg:39.27ms
step:903/2330 train_time:35445ms step_avg:39.25ms
step:904/2330 train_time:35501ms step_avg:39.27ms
step:905/2330 train_time:35524ms step_avg:39.25ms
step:906/2330 train_time:35581ms step_avg:39.27ms
step:907/2330 train_time:35603ms step_avg:39.25ms
step:908/2330 train_time:35660ms step_avg:39.27ms
step:909/2330 train_time:35682ms step_avg:39.25ms
step:910/2330 train_time:35738ms step_avg:39.27ms
step:911/2330 train_time:35760ms step_avg:39.25ms
step:912/2330 train_time:35817ms step_avg:39.27ms
step:913/2330 train_time:35839ms step_avg:39.25ms
step:914/2330 train_time:35896ms step_avg:39.27ms
step:915/2330 train_time:35918ms step_avg:39.25ms
step:916/2330 train_time:35974ms step_avg:39.27ms
step:917/2330 train_time:35996ms step_avg:39.25ms
step:918/2330 train_time:36052ms step_avg:39.27ms
step:919/2330 train_time:36074ms step_avg:39.25ms
step:920/2330 train_time:36130ms step_avg:39.27ms
step:921/2330 train_time:36154ms step_avg:39.26ms
step:922/2330 train_time:36211ms step_avg:39.27ms
step:923/2330 train_time:36234ms step_avg:39.26ms
step:924/2330 train_time:36290ms step_avg:39.28ms
step:925/2330 train_time:36314ms step_avg:39.26ms
step:926/2330 train_time:36371ms step_avg:39.28ms
step:927/2330 train_time:36395ms step_avg:39.26ms
step:928/2330 train_time:36451ms step_avg:39.28ms
step:929/2330 train_time:36474ms step_avg:39.26ms
step:930/2330 train_time:36530ms step_avg:39.28ms
step:931/2330 train_time:36553ms step_avg:39.26ms
step:932/2330 train_time:36610ms step_avg:39.28ms
step:933/2330 train_time:36633ms step_avg:39.26ms
step:934/2330 train_time:36689ms step_avg:39.28ms
step:935/2330 train_time:36713ms step_avg:39.26ms
step:936/2330 train_time:36769ms step_avg:39.28ms
step:937/2330 train_time:36792ms step_avg:39.27ms
step:938/2330 train_time:36848ms step_avg:39.28ms
step:939/2330 train_time:36870ms step_avg:39.27ms
step:940/2330 train_time:36926ms step_avg:39.28ms
step:941/2330 train_time:36948ms step_avg:39.27ms
step:942/2330 train_time:37004ms step_avg:39.28ms
step:943/2330 train_time:37026ms step_avg:39.26ms
step:944/2330 train_time:37082ms step_avg:39.28ms
step:945/2330 train_time:37105ms step_avg:39.26ms
step:946/2330 train_time:37161ms step_avg:39.28ms
step:947/2330 train_time:37183ms step_avg:39.26ms
step:948/2330 train_time:37240ms step_avg:39.28ms
step:949/2330 train_time:37262ms step_avg:39.26ms
step:950/2330 train_time:37320ms step_avg:39.28ms
step:951/2330 train_time:37343ms step_avg:39.27ms
step:952/2330 train_time:37401ms step_avg:39.29ms
step:953/2330 train_time:37423ms step_avg:39.27ms
step:954/2330 train_time:37480ms step_avg:39.29ms
step:955/2330 train_time:37502ms step_avg:39.27ms
step:956/2330 train_time:37560ms step_avg:39.29ms
step:957/2330 train_time:37582ms step_avg:39.27ms
step:958/2330 train_time:37640ms step_avg:39.29ms
step:959/2330 train_time:37662ms step_avg:39.27ms
step:960/2330 train_time:37719ms step_avg:39.29ms
step:961/2330 train_time:37742ms step_avg:39.27ms
step:962/2330 train_time:37799ms step_avg:39.29ms
step:963/2330 train_time:37820ms step_avg:39.27ms
step:964/2330 train_time:37877ms step_avg:39.29ms
step:965/2330 train_time:37899ms step_avg:39.27ms
step:966/2330 train_time:37954ms step_avg:39.29ms
step:967/2330 train_time:37976ms step_avg:39.27ms
step:968/2330 train_time:38032ms step_avg:39.29ms
step:969/2330 train_time:38054ms step_avg:39.27ms
step:970/2330 train_time:38110ms step_avg:39.29ms
step:971/2330 train_time:38133ms step_avg:39.27ms
step:972/2330 train_time:38188ms step_avg:39.29ms
step:973/2330 train_time:38211ms step_avg:39.27ms
step:974/2330 train_time:38267ms step_avg:39.29ms
step:975/2330 train_time:38290ms step_avg:39.27ms
step:976/2330 train_time:38346ms step_avg:39.29ms
step:977/2330 train_time:38369ms step_avg:39.27ms
step:978/2330 train_time:38425ms step_avg:39.29ms
step:979/2330 train_time:38447ms step_avg:39.27ms
step:980/2330 train_time:38503ms step_avg:39.29ms
step:981/2330 train_time:38526ms step_avg:39.27ms
step:982/2330 train_time:38583ms step_avg:39.29ms
step:983/2330 train_time:38606ms step_avg:39.27ms
step:984/2330 train_time:38662ms step_avg:39.29ms
step:985/2330 train_time:38684ms step_avg:39.27ms
step:986/2330 train_time:38741ms step_avg:39.29ms
step:987/2330 train_time:38763ms step_avg:39.27ms
step:988/2330 train_time:38819ms step_avg:39.29ms
step:989/2330 train_time:38841ms step_avg:39.27ms
step:990/2330 train_time:38897ms step_avg:39.29ms
step:991/2330 train_time:38919ms step_avg:39.27ms
step:992/2330 train_time:38975ms step_avg:39.29ms
step:993/2330 train_time:38997ms step_avg:39.27ms
step:994/2330 train_time:39054ms step_avg:39.29ms
step:995/2330 train_time:39075ms step_avg:39.27ms
step:996/2330 train_time:39131ms step_avg:39.29ms
step:997/2330 train_time:39153ms step_avg:39.27ms
step:998/2330 train_time:39210ms step_avg:39.29ms
step:999/2330 train_time:39232ms step_avg:39.27ms
step:1000/2330 train_time:39289ms step_avg:39.29ms
step:1000/2330 val_loss:5.2450 train_time:39386ms step_avg:39.39ms
step:1001/2330 train_time:39398ms step_avg:39.36ms
step:1002/2330 train_time:39410ms step_avg:39.33ms
step:1003/2330 train_time:39421ms step_avg:39.30ms
step:1004/2330 train_time:39449ms step_avg:39.29ms
step:1005/2330 train_time:39470ms step_avg:39.27ms
step:1006/2330 train_time:39525ms step_avg:39.29ms
step:1007/2330 train_time:39547ms step_avg:39.27ms
step:1008/2330 train_time:39602ms step_avg:39.29ms
step:1009/2330 train_time:39624ms step_avg:39.27ms
step:1010/2330 train_time:39679ms step_avg:39.29ms
step:1011/2330 train_time:39705ms step_avg:39.27ms
step:1012/2330 train_time:39765ms step_avg:39.29ms
step:1013/2330 train_time:39789ms step_avg:39.28ms
step:1014/2330 train_time:39846ms step_avg:39.30ms
step:1015/2330 train_time:39870ms step_avg:39.28ms
step:1016/2330 train_time:39926ms step_avg:39.30ms
step:1017/2330 train_time:39948ms step_avg:39.28ms
step:1018/2330 train_time:40005ms step_avg:39.30ms
step:1019/2330 train_time:40027ms step_avg:39.28ms
step:1020/2330 train_time:40083ms step_avg:39.30ms
step:1021/2330 train_time:40105ms step_avg:39.28ms
step:1022/2330 train_time:40161ms step_avg:39.30ms
step:1023/2330 train_time:40183ms step_avg:39.28ms
step:1024/2330 train_time:40239ms step_avg:39.30ms
step:1025/2330 train_time:40261ms step_avg:39.28ms
step:1026/2330 train_time:40320ms step_avg:39.30ms
step:1027/2330 train_time:40343ms step_avg:39.28ms
step:1028/2330 train_time:40400ms step_avg:39.30ms
step:1029/2330 train_time:40422ms step_avg:39.28ms
step:1030/2330 train_time:40478ms step_avg:39.30ms
step:1031/2330 train_time:40500ms step_avg:39.28ms
step:1032/2330 train_time:40555ms step_avg:39.30ms
step:1033/2330 train_time:40577ms step_avg:39.28ms
step:1034/2330 train_time:40633ms step_avg:39.30ms
step:1035/2330 train_time:40656ms step_avg:39.28ms
step:1036/2330 train_time:40714ms step_avg:39.30ms
step:1037/2330 train_time:40738ms step_avg:39.28ms
step:1038/2330 train_time:40795ms step_avg:39.30ms
step:1039/2330 train_time:40818ms step_avg:39.29ms
step:1040/2330 train_time:40876ms step_avg:39.30ms
step:1041/2330 train_time:40900ms step_avg:39.29ms
step:1042/2330 train_time:40956ms step_avg:39.31ms
step:1043/2330 train_time:40979ms step_avg:39.29ms
step:1044/2330 train_time:41035ms step_avg:39.31ms
step:1045/2330 train_time:41057ms step_avg:39.29ms
step:1046/2330 train_time:41113ms step_avg:39.31ms
step:1047/2330 train_time:41136ms step_avg:39.29ms
step:1048/2330 train_time:41191ms step_avg:39.30ms
step:1049/2330 train_time:41214ms step_avg:39.29ms
step:1050/2330 train_time:41270ms step_avg:39.30ms
step:1051/2330 train_time:41292ms step_avg:39.29ms
step:1052/2330 train_time:41348ms step_avg:39.30ms
step:1053/2330 train_time:41371ms step_avg:39.29ms
step:1054/2330 train_time:41426ms step_avg:39.30ms
step:1055/2330 train_time:41449ms step_avg:39.29ms
step:1056/2330 train_time:41504ms step_avg:39.30ms
step:1057/2330 train_time:41527ms step_avg:39.29ms
step:1058/2330 train_time:41584ms step_avg:39.30ms
step:1059/2330 train_time:41606ms step_avg:39.29ms
step:1060/2330 train_time:41663ms step_avg:39.30ms
step:1061/2330 train_time:41685ms step_avg:39.29ms
step:1062/2330 train_time:41743ms step_avg:39.31ms
step:1063/2330 train_time:41765ms step_avg:39.29ms
step:1064/2330 train_time:41822ms step_avg:39.31ms
step:1065/2330 train_time:41844ms step_avg:39.29ms
step:1066/2330 train_time:41902ms step_avg:39.31ms
step:1067/2330 train_time:41924ms step_avg:39.29ms
step:1068/2330 train_time:41981ms step_avg:39.31ms
step:1069/2330 train_time:42003ms step_avg:39.29ms
step:1070/2330 train_time:42059ms step_avg:39.31ms
step:1071/2330 train_time:42081ms step_avg:39.29ms
step:1072/2330 train_time:42137ms step_avg:39.31ms
step:1073/2330 train_time:42159ms step_avg:39.29ms
step:1074/2330 train_time:42214ms step_avg:39.31ms
step:1075/2330 train_time:42237ms step_avg:39.29ms
step:1076/2330 train_time:42292ms step_avg:39.31ms
step:1077/2330 train_time:42316ms step_avg:39.29ms
step:1078/2330 train_time:42372ms step_avg:39.31ms
step:1079/2330 train_time:42394ms step_avg:39.29ms
step:1080/2330 train_time:42450ms step_avg:39.31ms
step:1081/2330 train_time:42473ms step_avg:39.29ms
step:1082/2330 train_time:42528ms step_avg:39.31ms
step:1083/2330 train_time:42551ms step_avg:39.29ms
step:1084/2330 train_time:42608ms step_avg:39.31ms
step:1085/2330 train_time:42630ms step_avg:39.29ms
step:1086/2330 train_time:42686ms step_avg:39.31ms
step:1087/2330 train_time:42708ms step_avg:39.29ms
step:1088/2330 train_time:42765ms step_avg:39.31ms
step:1089/2330 train_time:42787ms step_avg:39.29ms
step:1090/2330 train_time:42844ms step_avg:39.31ms
step:1091/2330 train_time:42867ms step_avg:39.29ms
step:1092/2330 train_time:42924ms step_avg:39.31ms
step:1093/2330 train_time:42946ms step_avg:39.29ms
step:1094/2330 train_time:43003ms step_avg:39.31ms
step:1095/2330 train_time:43026ms step_avg:39.29ms
step:1096/2330 train_time:43082ms step_avg:39.31ms
step:1097/2330 train_time:43105ms step_avg:39.29ms
step:1098/2330 train_time:43162ms step_avg:39.31ms
step:1099/2330 train_time:43184ms step_avg:39.29ms
step:1100/2330 train_time:43241ms step_avg:39.31ms
step:1101/2330 train_time:43263ms step_avg:39.29ms
step:1102/2330 train_time:43319ms step_avg:39.31ms
step:1103/2330 train_time:43342ms step_avg:39.29ms
step:1104/2330 train_time:43398ms step_avg:39.31ms
step:1105/2330 train_time:43420ms step_avg:39.29ms
step:1106/2330 train_time:43475ms step_avg:39.31ms
step:1107/2330 train_time:43498ms step_avg:39.29ms
step:1108/2330 train_time:43554ms step_avg:39.31ms
step:1109/2330 train_time:43576ms step_avg:39.29ms
step:1110/2330 train_time:43632ms step_avg:39.31ms
step:1111/2330 train_time:43655ms step_avg:39.29ms
step:1112/2330 train_time:43711ms step_avg:39.31ms
step:1113/2330 train_time:43734ms step_avg:39.29ms
step:1114/2330 train_time:43790ms step_avg:39.31ms
step:1115/2330 train_time:43813ms step_avg:39.29ms
step:1116/2330 train_time:43870ms step_avg:39.31ms
step:1117/2330 train_time:43892ms step_avg:39.29ms
step:1118/2330 train_time:43948ms step_avg:39.31ms
step:1119/2330 train_time:43970ms step_avg:39.29ms
step:1120/2330 train_time:44027ms step_avg:39.31ms
step:1121/2330 train_time:44049ms step_avg:39.29ms
step:1122/2330 train_time:44106ms step_avg:39.31ms
step:1123/2330 train_time:44128ms step_avg:39.30ms
step:1124/2330 train_time:44185ms step_avg:39.31ms
step:1125/2330 train_time:44208ms step_avg:39.30ms
step:1126/2330 train_time:44264ms step_avg:39.31ms
step:1127/2330 train_time:44286ms step_avg:39.30ms
step:1128/2330 train_time:44343ms step_avg:39.31ms
step:1129/2330 train_time:44365ms step_avg:39.30ms
step:1130/2330 train_time:44421ms step_avg:39.31ms
step:1131/2330 train_time:44443ms step_avg:39.30ms
step:1132/2330 train_time:44499ms step_avg:39.31ms
step:1133/2330 train_time:44521ms step_avg:39.30ms
step:1134/2330 train_time:44577ms step_avg:39.31ms
step:1135/2330 train_time:44599ms step_avg:39.29ms
step:1136/2330 train_time:44656ms step_avg:39.31ms
step:1137/2330 train_time:44678ms step_avg:39.29ms
step:1138/2330 train_time:44734ms step_avg:39.31ms
step:1139/2330 train_time:44757ms step_avg:39.29ms
step:1140/2330 train_time:44813ms step_avg:39.31ms
step:1141/2330 train_time:44836ms step_avg:39.30ms
step:1142/2330 train_time:44892ms step_avg:39.31ms
step:1143/2330 train_time:44915ms step_avg:39.30ms
step:1144/2330 train_time:44971ms step_avg:39.31ms
step:1145/2330 train_time:44994ms step_avg:39.30ms
step:1146/2330 train_time:45050ms step_avg:39.31ms
step:1147/2330 train_time:45073ms step_avg:39.30ms
step:1148/2330 train_time:45129ms step_avg:39.31ms
step:1149/2330 train_time:45152ms step_avg:39.30ms
step:1150/2330 train_time:45208ms step_avg:39.31ms
step:1151/2330 train_time:45230ms step_avg:39.30ms
step:1152/2330 train_time:45286ms step_avg:39.31ms
step:1153/2330 train_time:45309ms step_avg:39.30ms
step:1154/2330 train_time:45366ms step_avg:39.31ms
step:1155/2330 train_time:45389ms step_avg:39.30ms
step:1156/2330 train_time:45445ms step_avg:39.31ms
step:1157/2330 train_time:45468ms step_avg:39.30ms
step:1158/2330 train_time:45525ms step_avg:39.31ms
step:1159/2330 train_time:45547ms step_avg:39.30ms
step:1160/2330 train_time:45604ms step_avg:39.31ms
step:1161/2330 train_time:45627ms step_avg:39.30ms
step:1162/2330 train_time:45683ms step_avg:39.31ms
step:1163/2330 train_time:45706ms step_avg:39.30ms
step:1164/2330 train_time:45764ms step_avg:39.32ms
step:1165/2330 train_time:45786ms step_avg:39.30ms
step:1166/2330 train_time:45843ms step_avg:39.32ms
step:1167/2330 train_time:45865ms step_avg:39.30ms
step:1168/2330 train_time:45921ms step_avg:39.32ms
step:1169/2330 train_time:45943ms step_avg:39.30ms
step:1170/2330 train_time:46000ms step_avg:39.32ms
step:1171/2330 train_time:46022ms step_avg:39.30ms
step:1172/2330 train_time:46078ms step_avg:39.32ms
step:1173/2330 train_time:46100ms step_avg:39.30ms
step:1174/2330 train_time:46156ms step_avg:39.31ms
step:1175/2330 train_time:46178ms step_avg:39.30ms
step:1176/2330 train_time:46234ms step_avg:39.31ms
step:1177/2330 train_time:46257ms step_avg:39.30ms
step:1178/2330 train_time:46313ms step_avg:39.32ms
step:1179/2330 train_time:46336ms step_avg:39.30ms
step:1180/2330 train_time:46392ms step_avg:39.32ms
step:1181/2330 train_time:46415ms step_avg:39.30ms
step:1182/2330 train_time:46471ms step_avg:39.32ms
step:1183/2330 train_time:46494ms step_avg:39.30ms
step:1184/2330 train_time:46550ms step_avg:39.32ms
step:1185/2330 train_time:46573ms step_avg:39.30ms
step:1186/2330 train_time:46629ms step_avg:39.32ms
step:1187/2330 train_time:46651ms step_avg:39.30ms
step:1188/2330 train_time:46707ms step_avg:39.32ms
step:1189/2330 train_time:46730ms step_avg:39.30ms
step:1190/2330 train_time:46787ms step_avg:39.32ms
step:1191/2330 train_time:46809ms step_avg:39.30ms
step:1192/2330 train_time:46865ms step_avg:39.32ms
step:1193/2330 train_time:46888ms step_avg:39.30ms
step:1194/2330 train_time:46944ms step_avg:39.32ms
step:1195/2330 train_time:46967ms step_avg:39.30ms
step:1196/2330 train_time:47024ms step_avg:39.32ms
step:1197/2330 train_time:47046ms step_avg:39.30ms
step:1198/2330 train_time:47102ms step_avg:39.32ms
step:1199/2330 train_time:47124ms step_avg:39.30ms
step:1200/2330 train_time:47180ms step_avg:39.32ms
step:1201/2330 train_time:47202ms step_avg:39.30ms
step:1202/2330 train_time:47259ms step_avg:39.32ms
step:1203/2330 train_time:47281ms step_avg:39.30ms
step:1204/2330 train_time:47337ms step_avg:39.32ms
step:1205/2330 train_time:47359ms step_avg:39.30ms
step:1206/2330 train_time:47415ms step_avg:39.32ms
step:1207/2330 train_time:47437ms step_avg:39.30ms
step:1208/2330 train_time:47494ms step_avg:39.32ms
step:1209/2330 train_time:47518ms step_avg:39.30ms
step:1210/2330 train_time:47574ms step_avg:39.32ms
step:1211/2330 train_time:47598ms step_avg:39.30ms
step:1212/2330 train_time:47654ms step_avg:39.32ms
step:1213/2330 train_time:47677ms step_avg:39.31ms
step:1214/2330 train_time:47734ms step_avg:39.32ms
step:1215/2330 train_time:47758ms step_avg:39.31ms
step:1216/2330 train_time:47813ms step_avg:39.32ms
step:1217/2330 train_time:47836ms step_avg:39.31ms
step:1218/2330 train_time:47892ms step_avg:39.32ms
step:1219/2330 train_time:47915ms step_avg:39.31ms
step:1220/2330 train_time:47971ms step_avg:39.32ms
step:1221/2330 train_time:47994ms step_avg:39.31ms
step:1222/2330 train_time:48049ms step_avg:39.32ms
step:1223/2330 train_time:48072ms step_avg:39.31ms
step:1224/2330 train_time:48128ms step_avg:39.32ms
step:1225/2330 train_time:48151ms step_avg:39.31ms
step:1226/2330 train_time:48207ms step_avg:39.32ms
step:1227/2330 train_time:48230ms step_avg:39.31ms
step:1228/2330 train_time:48286ms step_avg:39.32ms
step:1229/2330 train_time:48309ms step_avg:39.31ms
step:1230/2330 train_time:48365ms step_avg:39.32ms
step:1231/2330 train_time:48388ms step_avg:39.31ms
step:1232/2330 train_time:48444ms step_avg:39.32ms
step:1233/2330 train_time:48466ms step_avg:39.31ms
step:1234/2330 train_time:48524ms step_avg:39.32ms
step:1235/2330 train_time:48546ms step_avg:39.31ms
step:1236/2330 train_time:48603ms step_avg:39.32ms
step:1237/2330 train_time:48625ms step_avg:39.31ms
step:1238/2330 train_time:48682ms step_avg:39.32ms
step:1239/2330 train_time:48703ms step_avg:39.31ms
step:1240/2330 train_time:48760ms step_avg:39.32ms
step:1241/2330 train_time:48782ms step_avg:39.31ms
step:1242/2330 train_time:48838ms step_avg:39.32ms
step:1243/2330 train_time:48860ms step_avg:39.31ms
step:1244/2330 train_time:48916ms step_avg:39.32ms
step:1245/2330 train_time:48939ms step_avg:39.31ms
step:1246/2330 train_time:48994ms step_avg:39.32ms
step:1247/2330 train_time:49017ms step_avg:39.31ms
step:1248/2330 train_time:49073ms step_avg:39.32ms
step:1249/2330 train_time:49096ms step_avg:39.31ms
step:1250/2330 train_time:49152ms step_avg:39.32ms
step:1250/2330 val_loss:5.2131 train_time:49249ms step_avg:39.40ms
step:1251/2330 train_time:49263ms step_avg:39.38ms
step:1252/2330 train_time:49275ms step_avg:39.36ms
step:1253/2330 train_time:49286ms step_avg:39.33ms
step:1254/2330 train_time:49312ms step_avg:39.32ms
step:1255/2330 train_time:49333ms step_avg:39.31ms
step:1256/2330 train_time:49388ms step_avg:39.32ms
step:1257/2330 train_time:49410ms step_avg:39.31ms
step:1258/2330 train_time:49466ms step_avg:39.32ms
step:1259/2330 train_time:49488ms step_avg:39.31ms
step:1260/2330 train_time:49544ms step_avg:39.32ms
step:1261/2330 train_time:49568ms step_avg:39.31ms
step:1262/2330 train_time:49628ms step_avg:39.32ms
step:1263/2330 train_time:49652ms step_avg:39.31ms
step:1264/2330 train_time:49708ms step_avg:39.33ms
step:1265/2330 train_time:49730ms step_avg:39.31ms
step:1266/2330 train_time:49786ms step_avg:39.33ms
step:1267/2330 train_time:49808ms step_avg:39.31ms
step:1268/2330 train_time:49864ms step_avg:39.32ms
step:1269/2330 train_time:49886ms step_avg:39.31ms
step:1270/2330 train_time:49941ms step_avg:39.32ms
step:1271/2330 train_time:49964ms step_avg:39.31ms
step:1272/2330 train_time:50019ms step_avg:39.32ms
step:1273/2330 train_time:50041ms step_avg:39.31ms
step:1274/2330 train_time:50096ms step_avg:39.32ms
step:1275/2330 train_time:50118ms step_avg:39.31ms
step:1276/2330 train_time:50175ms step_avg:39.32ms
step:1277/2330 train_time:50199ms step_avg:39.31ms
step:1278/2330 train_time:50255ms step_avg:39.32ms
step:1279/2330 train_time:50277ms step_avg:39.31ms
step:1280/2330 train_time:50334ms step_avg:39.32ms
step:1281/2330 train_time:50356ms step_avg:39.31ms
step:1282/2330 train_time:50411ms step_avg:39.32ms
step:1283/2330 train_time:50434ms step_avg:39.31ms
step:1284/2330 train_time:50490ms step_avg:39.32ms
step:1285/2330 train_time:50514ms step_avg:39.31ms
step:1286/2330 train_time:50572ms step_avg:39.33ms
step:1287/2330 train_time:50596ms step_avg:39.31ms
step:1288/2330 train_time:50653ms step_avg:39.33ms
step:1289/2330 train_time:50676ms step_avg:39.31ms
step:1290/2330 train_time:50733ms step_avg:39.33ms
step:1291/2330 train_time:50756ms step_avg:39.32ms
step:1292/2330 train_time:50812ms step_avg:39.33ms
step:1293/2330 train_time:50835ms step_avg:39.32ms
step:1294/2330 train_time:50891ms step_avg:39.33ms
step:1295/2330 train_time:50913ms step_avg:39.32ms
step:1296/2330 train_time:50970ms step_avg:39.33ms
step:1297/2330 train_time:50991ms step_avg:39.31ms
step:1298/2330 train_time:51047ms step_avg:39.33ms
step:1299/2330 train_time:51069ms step_avg:39.31ms
step:1300/2330 train_time:51125ms step_avg:39.33ms
step:1301/2330 train_time:51148ms step_avg:39.31ms
step:1302/2330 train_time:51203ms step_avg:39.33ms
step:1303/2330 train_time:51225ms step_avg:39.31ms
step:1304/2330 train_time:51281ms step_avg:39.33ms
step:1305/2330 train_time:51304ms step_avg:39.31ms
step:1306/2330 train_time:51359ms step_avg:39.33ms
step:1307/2330 train_time:51382ms step_avg:39.31ms
step:1308/2330 train_time:51439ms step_avg:39.33ms
step:1309/2330 train_time:51462ms step_avg:39.31ms
step:1310/2330 train_time:51518ms step_avg:39.33ms
step:1311/2330 train_time:51541ms step_avg:39.31ms
step:1312/2330 train_time:51598ms step_avg:39.33ms
step:1313/2330 train_time:51621ms step_avg:39.32ms
step:1314/2330 train_time:51677ms step_avg:39.33ms
step:1315/2330 train_time:51700ms step_avg:39.32ms
step:1316/2330 train_time:51756ms step_avg:39.33ms
step:1317/2330 train_time:51778ms step_avg:39.32ms
step:1318/2330 train_time:51835ms step_avg:39.33ms
step:1319/2330 train_time:51858ms step_avg:39.32ms
step:1320/2330 train_time:51913ms step_avg:39.33ms
step:1321/2330 train_time:51936ms step_avg:39.32ms
step:1322/2330 train_time:51992ms step_avg:39.33ms
step:1323/2330 train_time:52014ms step_avg:39.32ms
step:1324/2330 train_time:52070ms step_avg:39.33ms
step:1325/2330 train_time:52092ms step_avg:39.31ms
step:1326/2330 train_time:52148ms step_avg:39.33ms
step:1327/2330 train_time:52170ms step_avg:39.31ms
step:1328/2330 train_time:52226ms step_avg:39.33ms
step:1329/2330 train_time:52248ms step_avg:39.31ms
step:1330/2330 train_time:52305ms step_avg:39.33ms
step:1331/2330 train_time:52326ms step_avg:39.31ms
step:1332/2330 train_time:52382ms step_avg:39.33ms
step:1333/2330 train_time:52405ms step_avg:39.31ms
step:1334/2330 train_time:52461ms step_avg:39.33ms
step:1335/2330 train_time:52484ms step_avg:39.31ms
step:1336/2330 train_time:52540ms step_avg:39.33ms
step:1337/2330 train_time:52563ms step_avg:39.31ms
step:1338/2330 train_time:52619ms step_avg:39.33ms
step:1339/2330 train_time:52643ms step_avg:39.31ms
step:1340/2330 train_time:52699ms step_avg:39.33ms
step:1341/2330 train_time:52722ms step_avg:39.32ms
step:1342/2330 train_time:52779ms step_avg:39.33ms
step:1343/2330 train_time:52802ms step_avg:39.32ms
step:1344/2330 train_time:52858ms step_avg:39.33ms
step:1345/2330 train_time:52881ms step_avg:39.32ms
step:1346/2330 train_time:52936ms step_avg:39.33ms
step:1347/2330 train_time:52959ms step_avg:39.32ms
step:1348/2330 train_time:53015ms step_avg:39.33ms
step:1349/2330 train_time:53038ms step_avg:39.32ms
step:1350/2330 train_time:53093ms step_avg:39.33ms
step:1351/2330 train_time:53115ms step_avg:39.32ms
step:1352/2330 train_time:53171ms step_avg:39.33ms
step:1353/2330 train_time:53194ms step_avg:39.32ms
step:1354/2330 train_time:53250ms step_avg:39.33ms
step:1355/2330 train_time:53272ms step_avg:39.32ms
step:1356/2330 train_time:53329ms step_avg:39.33ms
step:1357/2330 train_time:53351ms step_avg:39.32ms
step:1358/2330 train_time:53407ms step_avg:39.33ms
step:1359/2330 train_time:53429ms step_avg:39.31ms
step:1360/2330 train_time:53485ms step_avg:39.33ms
step:1361/2330 train_time:53507ms step_avg:39.31ms
step:1362/2330 train_time:53563ms step_avg:39.33ms
step:1363/2330 train_time:53585ms step_avg:39.31ms
step:1364/2330 train_time:53642ms step_avg:39.33ms
step:1365/2330 train_time:53666ms step_avg:39.32ms
step:1366/2330 train_time:53723ms step_avg:39.33ms
step:1367/2330 train_time:53746ms step_avg:39.32ms
step:1368/2330 train_time:53802ms step_avg:39.33ms
step:1369/2330 train_time:53826ms step_avg:39.32ms
step:1370/2330 train_time:53881ms step_avg:39.33ms
step:1371/2330 train_time:53904ms step_avg:39.32ms
step:1372/2330 train_time:53960ms step_avg:39.33ms
step:1373/2330 train_time:53985ms step_avg:39.32ms
step:1374/2330 train_time:54041ms step_avg:39.33ms
step:1375/2330 train_time:54064ms step_avg:39.32ms
step:1376/2330 train_time:54121ms step_avg:39.33ms
step:1377/2330 train_time:54143ms step_avg:39.32ms
step:1378/2330 train_time:54199ms step_avg:39.33ms
step:1379/2330 train_time:54222ms step_avg:39.32ms
step:1380/2330 train_time:54278ms step_avg:39.33ms
step:1381/2330 train_time:54300ms step_avg:39.32ms
step:1382/2330 train_time:54355ms step_avg:39.33ms
step:1383/2330 train_time:54378ms step_avg:39.32ms
step:1384/2330 train_time:54434ms step_avg:39.33ms
step:1385/2330 train_time:54456ms step_avg:39.32ms
step:1386/2330 train_time:54512ms step_avg:39.33ms
step:1387/2330 train_time:54535ms step_avg:39.32ms
step:1388/2330 train_time:54591ms step_avg:39.33ms
step:1389/2330 train_time:54614ms step_avg:39.32ms
step:1390/2330 train_time:54670ms step_avg:39.33ms
step:1391/2330 train_time:54692ms step_avg:39.32ms
step:1392/2330 train_time:54749ms step_avg:39.33ms
step:1393/2330 train_time:54771ms step_avg:39.32ms
step:1394/2330 train_time:54827ms step_avg:39.33ms
step:1395/2330 train_time:54850ms step_avg:39.32ms
step:1396/2330 train_time:54907ms step_avg:39.33ms
step:1397/2330 train_time:54928ms step_avg:39.32ms
step:1398/2330 train_time:54985ms step_avg:39.33ms
step:1399/2330 train_time:55007ms step_avg:39.32ms
step:1400/2330 train_time:55063ms step_avg:39.33ms
step:1401/2330 train_time:55086ms step_avg:39.32ms
step:1402/2330 train_time:55141ms step_avg:39.33ms
step:1403/2330 train_time:55164ms step_avg:39.32ms
step:1404/2330 train_time:55219ms step_avg:39.33ms
step:1405/2330 train_time:55242ms step_avg:39.32ms
step:1406/2330 train_time:55299ms step_avg:39.33ms
step:1407/2330 train_time:55321ms step_avg:39.32ms
step:1408/2330 train_time:55377ms step_avg:39.33ms
step:1409/2330 train_time:55399ms step_avg:39.32ms
step:1410/2330 train_time:55455ms step_avg:39.33ms
step:1411/2330 train_time:55478ms step_avg:39.32ms
step:1412/2330 train_time:55533ms step_avg:39.33ms
step:1413/2330 train_time:55556ms step_avg:39.32ms
step:1414/2330 train_time:55612ms step_avg:39.33ms
step:1415/2330 train_time:55634ms step_avg:39.32ms
step:1416/2330 train_time:55690ms step_avg:39.33ms
step:1417/2330 train_time:55713ms step_avg:39.32ms
step:1418/2330 train_time:55769ms step_avg:39.33ms
step:1419/2330 train_time:55791ms step_avg:39.32ms
step:1420/2330 train_time:55848ms step_avg:39.33ms
step:1421/2330 train_time:55870ms step_avg:39.32ms
step:1422/2330 train_time:55927ms step_avg:39.33ms
step:1423/2330 train_time:55949ms step_avg:39.32ms
step:1424/2330 train_time:56006ms step_avg:39.33ms
step:1425/2330 train_time:56028ms step_avg:39.32ms
step:1426/2330 train_time:56084ms step_avg:39.33ms
step:1427/2330 train_time:56106ms step_avg:39.32ms
step:1428/2330 train_time:56162ms step_avg:39.33ms
step:1429/2330 train_time:56185ms step_avg:39.32ms
step:1430/2330 train_time:56241ms step_avg:39.33ms
step:1431/2330 train_time:56263ms step_avg:39.32ms
step:1432/2330 train_time:56319ms step_avg:39.33ms
step:1433/2330 train_time:56342ms step_avg:39.32ms
step:1434/2330 train_time:56398ms step_avg:39.33ms
step:1435/2330 train_time:56420ms step_avg:39.32ms
step:1436/2330 train_time:56476ms step_avg:39.33ms
step:1437/2330 train_time:56499ms step_avg:39.32ms
step:1438/2330 train_time:56555ms step_avg:39.33ms
step:1439/2330 train_time:56578ms step_avg:39.32ms
step:1440/2330 train_time:56634ms step_avg:39.33ms
step:1441/2330 train_time:56657ms step_avg:39.32ms
step:1442/2330 train_time:56713ms step_avg:39.33ms
step:1443/2330 train_time:56735ms step_avg:39.32ms
step:1444/2330 train_time:56791ms step_avg:39.33ms
step:1445/2330 train_time:56814ms step_avg:39.32ms
step:1446/2330 train_time:56873ms step_avg:39.33ms
step:1447/2330 train_time:56896ms step_avg:39.32ms
step:1448/2330 train_time:56953ms step_avg:39.33ms
step:1449/2330 train_time:56977ms step_avg:39.32ms
step:1450/2330 train_time:57033ms step_avg:39.33ms
step:1451/2330 train_time:57056ms step_avg:39.32ms
step:1452/2330 train_time:57113ms step_avg:39.33ms
step:1453/2330 train_time:57135ms step_avg:39.32ms
step:1454/2330 train_time:57192ms step_avg:39.33ms
step:1455/2330 train_time:57214ms step_avg:39.32ms
step:1456/2330 train_time:57271ms step_avg:39.33ms
step:1457/2330 train_time:57293ms step_avg:39.32ms
step:1458/2330 train_time:57349ms step_avg:39.33ms
step:1459/2330 train_time:57371ms step_avg:39.32ms
step:1460/2330 train_time:57427ms step_avg:39.33ms
step:1461/2330 train_time:57450ms step_avg:39.32ms
step:1462/2330 train_time:57505ms step_avg:39.33ms
step:1463/2330 train_time:57527ms step_avg:39.32ms
step:1464/2330 train_time:57583ms step_avg:39.33ms
step:1465/2330 train_time:57606ms step_avg:39.32ms
step:1466/2330 train_time:57661ms step_avg:39.33ms
step:1467/2330 train_time:57684ms step_avg:39.32ms
step:1468/2330 train_time:57740ms step_avg:39.33ms
step:1469/2330 train_time:57764ms step_avg:39.32ms
step:1470/2330 train_time:57820ms step_avg:39.33ms
step:1471/2330 train_time:57843ms step_avg:39.32ms
step:1472/2330 train_time:57899ms step_avg:39.33ms
step:1473/2330 train_time:57922ms step_avg:39.32ms
step:1474/2330 train_time:57980ms step_avg:39.33ms
step:1475/2330 train_time:58003ms step_avg:39.32ms
step:1476/2330 train_time:58059ms step_avg:39.34ms
step:1477/2330 train_time:58083ms step_avg:39.32ms
step:1478/2330 train_time:58139ms step_avg:39.34ms
step:1479/2330 train_time:58162ms step_avg:39.33ms
step:1480/2330 train_time:58218ms step_avg:39.34ms
step:1481/2330 train_time:58241ms step_avg:39.33ms
step:1482/2330 train_time:58297ms step_avg:39.34ms
step:1483/2330 train_time:58319ms step_avg:39.33ms
step:1484/2330 train_time:58375ms step_avg:39.34ms
step:1485/2330 train_time:58397ms step_avg:39.32ms
step:1486/2330 train_time:58453ms step_avg:39.34ms
step:1487/2330 train_time:58476ms step_avg:39.32ms
step:1488/2330 train_time:58532ms step_avg:39.34ms
step:1489/2330 train_time:58555ms step_avg:39.32ms
step:1490/2330 train_time:58610ms step_avg:39.34ms
step:1491/2330 train_time:58633ms step_avg:39.32ms
step:1492/2330 train_time:58689ms step_avg:39.34ms
step:1493/2330 train_time:58711ms step_avg:39.32ms
step:1494/2330 train_time:58767ms step_avg:39.34ms
step:1495/2330 train_time:58789ms step_avg:39.32ms
step:1496/2330 train_time:58845ms step_avg:39.33ms
step:1497/2330 train_time:58868ms step_avg:39.32ms
step:1498/2330 train_time:58924ms step_avg:39.34ms
step:1499/2330 train_time:58946ms step_avg:39.32ms
step:1500/2330 train_time:59003ms step_avg:39.34ms
step:1500/2330 val_loss:5.1878 train_time:59098ms step_avg:39.40ms
step:1501/2330 train_time:59111ms step_avg:39.38ms
step:1502/2330 train_time:59123ms step_avg:39.36ms
step:1503/2330 train_time:59134ms step_avg:39.34ms
step:1504/2330 train_time:59161ms step_avg:39.34ms
step:1505/2330 train_time:59182ms step_avg:39.32ms
step:1506/2330 train_time:59237ms step_avg:39.33ms
step:1507/2330 train_time:59258ms step_avg:39.32ms
step:1508/2330 train_time:59313ms step_avg:39.33ms
step:1509/2330 train_time:59335ms step_avg:39.32ms
step:1510/2330 train_time:59391ms step_avg:39.33ms
step:1511/2330 train_time:59415ms step_avg:39.32ms
step:1512/2330 train_time:59474ms step_avg:39.33ms
step:1513/2330 train_time:59500ms step_avg:39.33ms
step:1514/2330 train_time:59557ms step_avg:39.34ms
step:1515/2330 train_time:59581ms step_avg:39.33ms
step:1516/2330 train_time:59637ms step_avg:39.34ms
step:1517/2330 train_time:59659ms step_avg:39.33ms
step:1518/2330 train_time:59715ms step_avg:39.34ms
step:1519/2330 train_time:59738ms step_avg:39.33ms
step:1520/2330 train_time:59793ms step_avg:39.34ms
step:1521/2330 train_time:59815ms step_avg:39.33ms
step:1522/2330 train_time:59870ms step_avg:39.34ms
step:1523/2330 train_time:59892ms step_avg:39.33ms
step:1524/2330 train_time:59948ms step_avg:39.34ms
step:1525/2330 train_time:59970ms step_avg:39.32ms
step:1526/2330 train_time:60026ms step_avg:39.34ms
step:1527/2330 train_time:60049ms step_avg:39.32ms
step:1528/2330 train_time:60106ms step_avg:39.34ms
step:1529/2330 train_time:60130ms step_avg:39.33ms
step:1530/2330 train_time:60185ms step_avg:39.34ms
step:1531/2330 train_time:60206ms step_avg:39.32ms
step:1532/2330 train_time:60262ms step_avg:39.34ms
step:1533/2330 train_time:60285ms step_avg:39.32ms
step:1534/2330 train_time:60341ms step_avg:39.34ms
step:1535/2330 train_time:60364ms step_avg:39.32ms
step:1536/2330 train_time:60421ms step_avg:39.34ms
step:1537/2330 train_time:60443ms step_avg:39.33ms
step:1538/2330 train_time:60500ms step_avg:39.34ms
step:1539/2330 train_time:60523ms step_avg:39.33ms
step:1540/2330 train_time:60579ms step_avg:39.34ms
step:1541/2330 train_time:60601ms step_avg:39.33ms
step:1542/2330 train_time:60657ms step_avg:39.34ms
step:1543/2330 train_time:60679ms step_avg:39.33ms
step:1544/2330 train_time:60735ms step_avg:39.34ms
step:1545/2330 train_time:60757ms step_avg:39.32ms
step:1546/2330 train_time:60813ms step_avg:39.34ms
step:1547/2330 train_time:60835ms step_avg:39.32ms
step:1548/2330 train_time:60890ms step_avg:39.33ms
step:1549/2330 train_time:60913ms step_avg:39.32ms
step:1550/2330 train_time:60968ms step_avg:39.33ms
step:1551/2330 train_time:60991ms step_avg:39.32ms
step:1552/2330 train_time:61047ms step_avg:39.33ms
step:1553/2330 train_time:61070ms step_avg:39.32ms
step:1554/2330 train_time:61126ms step_avg:39.33ms
step:1555/2330 train_time:61148ms step_avg:39.32ms
step:1556/2330 train_time:61204ms step_avg:39.33ms
step:1557/2330 train_time:61226ms step_avg:39.32ms
step:1558/2330 train_time:61282ms step_avg:39.33ms
step:1559/2330 train_time:61305ms step_avg:39.32ms
step:1560/2330 train_time:61361ms step_avg:39.33ms
step:1561/2330 train_time:61383ms step_avg:39.32ms
step:1562/2330 train_time:61439ms step_avg:39.33ms
step:1563/2330 train_time:61461ms step_avg:39.32ms
step:1564/2330 train_time:61518ms step_avg:39.33ms
step:1565/2330 train_time:61540ms step_avg:39.32ms
step:1566/2330 train_time:61597ms step_avg:39.33ms
step:1567/2330 train_time:61619ms step_avg:39.32ms
step:1568/2330 train_time:61675ms step_avg:39.33ms
step:1569/2330 train_time:61697ms step_avg:39.32ms
step:1570/2330 train_time:61752ms step_avg:39.33ms
step:1571/2330 train_time:61775ms step_avg:39.32ms
step:1572/2330 train_time:61830ms step_avg:39.33ms
step:1573/2330 train_time:61853ms step_avg:39.32ms
step:1574/2330 train_time:61909ms step_avg:39.33ms
step:1575/2330 train_time:61931ms step_avg:39.32ms
step:1576/2330 train_time:61987ms step_avg:39.33ms
step:1577/2330 train_time:62009ms step_avg:39.32ms
step:1578/2330 train_time:62065ms step_avg:39.33ms
step:1579/2330 train_time:62087ms step_avg:39.32ms
step:1580/2330 train_time:62142ms step_avg:39.33ms
step:1581/2330 train_time:62164ms step_avg:39.32ms
step:1582/2330 train_time:62220ms step_avg:39.33ms
step:1583/2330 train_time:62242ms step_avg:39.32ms
step:1584/2330 train_time:62299ms step_avg:39.33ms
step:1585/2330 train_time:62322ms step_avg:39.32ms
step:1586/2330 train_time:62378ms step_avg:39.33ms
step:1587/2330 train_time:62400ms step_avg:39.32ms
step:1588/2330 train_time:62456ms step_avg:39.33ms
step:1589/2330 train_time:62479ms step_avg:39.32ms
step:1590/2330 train_time:62534ms step_avg:39.33ms
step:1591/2330 train_time:62557ms step_avg:39.32ms
step:1592/2330 train_time:62613ms step_avg:39.33ms
step:1593/2330 train_time:62635ms step_avg:39.32ms
step:1594/2330 train_time:62691ms step_avg:39.33ms
step:1595/2330 train_time:62714ms step_avg:39.32ms
step:1596/2330 train_time:62770ms step_avg:39.33ms
step:1597/2330 train_time:62792ms step_avg:39.32ms
step:1598/2330 train_time:62848ms step_avg:39.33ms
step:1599/2330 train_time:62869ms step_avg:39.32ms
step:1600/2330 train_time:62925ms step_avg:39.33ms
step:1601/2330 train_time:62947ms step_avg:39.32ms
step:1602/2330 train_time:63003ms step_avg:39.33ms
step:1603/2330 train_time:63025ms step_avg:39.32ms
step:1604/2330 train_time:63081ms step_avg:39.33ms
step:1605/2330 train_time:63103ms step_avg:39.32ms
step:1606/2330 train_time:63158ms step_avg:39.33ms
step:1607/2330 train_time:63181ms step_avg:39.32ms
step:1608/2330 train_time:63237ms step_avg:39.33ms
step:1609/2330 train_time:63259ms step_avg:39.32ms
step:1610/2330 train_time:63315ms step_avg:39.33ms
step:1611/2330 train_time:63337ms step_avg:39.32ms
step:1612/2330 train_time:63393ms step_avg:39.33ms
step:1613/2330 train_time:63416ms step_avg:39.32ms
step:1614/2330 train_time:63472ms step_avg:39.33ms
step:1615/2330 train_time:63495ms step_avg:39.32ms
step:1616/2330 train_time:63550ms step_avg:39.33ms
step:1617/2330 train_time:63573ms step_avg:39.32ms
step:1618/2330 train_time:63629ms step_avg:39.33ms
step:1619/2330 train_time:63651ms step_avg:39.32ms
step:1620/2330 train_time:63707ms step_avg:39.33ms
step:1621/2330 train_time:63730ms step_avg:39.31ms
step:1622/2330 train_time:63785ms step_avg:39.33ms
step:1623/2330 train_time:63807ms step_avg:39.31ms
step:1624/2330 train_time:63863ms step_avg:39.32ms
step:1625/2330 train_time:63885ms step_avg:39.31ms
step:1626/2330 train_time:63941ms step_avg:39.32ms
step:1627/2330 train_time:63963ms step_avg:39.31ms
step:1628/2330 train_time:64019ms step_avg:39.32ms
step:1629/2330 train_time:64041ms step_avg:39.31ms
step:1630/2330 train_time:64096ms step_avg:39.32ms
step:1631/2330 train_time:64118ms step_avg:39.31ms
step:1632/2330 train_time:64174ms step_avg:39.32ms
step:1633/2330 train_time:64197ms step_avg:39.31ms
step:1634/2330 train_time:64252ms step_avg:39.32ms
step:1635/2330 train_time:64275ms step_avg:39.31ms
step:1636/2330 train_time:64331ms step_avg:39.32ms
step:1637/2330 train_time:64353ms step_avg:39.31ms
step:1638/2330 train_time:64409ms step_avg:39.32ms
step:1639/2330 train_time:64432ms step_avg:39.31ms
step:1640/2330 train_time:64488ms step_avg:39.32ms
step:1641/2330 train_time:64510ms step_avg:39.31ms
step:1642/2330 train_time:64566ms step_avg:39.32ms
step:1643/2330 train_time:64589ms step_avg:39.31ms
step:1644/2330 train_time:64645ms step_avg:39.32ms
step:1645/2330 train_time:64667ms step_avg:39.31ms
step:1646/2330 train_time:64723ms step_avg:39.32ms
step:1647/2330 train_time:64745ms step_avg:39.31ms
step:1648/2330 train_time:64801ms step_avg:39.32ms
step:1649/2330 train_time:64824ms step_avg:39.31ms
step:1650/2330 train_time:64880ms step_avg:39.32ms
step:1651/2330 train_time:64901ms step_avg:39.31ms
step:1652/2330 train_time:64957ms step_avg:39.32ms
step:1653/2330 train_time:64979ms step_avg:39.31ms
step:1654/2330 train_time:65035ms step_avg:39.32ms
step:1655/2330 train_time:65057ms step_avg:39.31ms
step:1656/2330 train_time:65112ms step_avg:39.32ms
step:1657/2330 train_time:65135ms step_avg:39.31ms
step:1658/2330 train_time:65190ms step_avg:39.32ms
step:1659/2330 train_time:65213ms step_avg:39.31ms
step:1660/2330 train_time:65269ms step_avg:39.32ms
step:1661/2330 train_time:65291ms step_avg:39.31ms
step:1662/2330 train_time:65347ms step_avg:39.32ms
step:1663/2330 train_time:65370ms step_avg:39.31ms
step:1664/2330 train_time:65425ms step_avg:39.32ms
step:1665/2330 train_time:65448ms step_avg:39.31ms
step:1666/2330 train_time:65504ms step_avg:39.32ms
step:1667/2330 train_time:65526ms step_avg:39.31ms
step:1668/2330 train_time:65582ms step_avg:39.32ms
step:1669/2330 train_time:65605ms step_avg:39.31ms
step:1670/2330 train_time:65661ms step_avg:39.32ms
step:1671/2330 train_time:65683ms step_avg:39.31ms
step:1672/2330 train_time:65739ms step_avg:39.32ms
step:1673/2330 train_time:65761ms step_avg:39.31ms
step:1674/2330 train_time:65817ms step_avg:39.32ms
step:1675/2330 train_time:65839ms step_avg:39.31ms
step:1676/2330 train_time:65894ms step_avg:39.32ms
step:1677/2330 train_time:65916ms step_avg:39.31ms
step:1678/2330 train_time:65972ms step_avg:39.32ms
step:1679/2330 train_time:65994ms step_avg:39.31ms
step:1680/2330 train_time:66050ms step_avg:39.32ms
step:1681/2330 train_time:66072ms step_avg:39.31ms
step:1682/2330 train_time:66127ms step_avg:39.31ms
step:1683/2330 train_time:66149ms step_avg:39.30ms
step:1684/2330 train_time:66205ms step_avg:39.31ms
step:1685/2330 train_time:66227ms step_avg:39.30ms
step:1686/2330 train_time:66283ms step_avg:39.31ms
step:1687/2330 train_time:66306ms step_avg:39.30ms
step:1688/2330 train_time:66362ms step_avg:39.31ms
step:1689/2330 train_time:66384ms step_avg:39.30ms
step:1690/2330 train_time:66440ms step_avg:39.31ms
step:1691/2330 train_time:66463ms step_avg:39.30ms
step:1692/2330 train_time:66518ms step_avg:39.31ms
step:1693/2330 train_time:66541ms step_avg:39.30ms
step:1694/2330 train_time:66597ms step_avg:39.31ms
step:1695/2330 train_time:66619ms step_avg:39.30ms
step:1696/2330 train_time:66674ms step_avg:39.31ms
step:1697/2330 train_time:66697ms step_avg:39.30ms
step:1698/2330 train_time:66753ms step_avg:39.31ms
step:1699/2330 train_time:66776ms step_avg:39.30ms
step:1700/2330 train_time:66831ms step_avg:39.31ms
step:1701/2330 train_time:66853ms step_avg:39.30ms
step:1702/2330 train_time:66909ms step_avg:39.31ms
step:1703/2330 train_time:66931ms step_avg:39.30ms
step:1704/2330 train_time:66986ms step_avg:39.31ms
step:1705/2330 train_time:67008ms step_avg:39.30ms
step:1706/2330 train_time:67064ms step_avg:39.31ms
step:1707/2330 train_time:67087ms step_avg:39.30ms
step:1708/2330 train_time:67143ms step_avg:39.31ms
step:1709/2330 train_time:67165ms step_avg:39.30ms
step:1710/2330 train_time:67221ms step_avg:39.31ms
step:1711/2330 train_time:67243ms step_avg:39.30ms
step:1712/2330 train_time:67299ms step_avg:39.31ms
step:1713/2330 train_time:67321ms step_avg:39.30ms
step:1714/2330 train_time:67377ms step_avg:39.31ms
step:1715/2330 train_time:67399ms step_avg:39.30ms
step:1716/2330 train_time:67456ms step_avg:39.31ms
step:1717/2330 train_time:67478ms step_avg:39.30ms
step:1718/2330 train_time:67533ms step_avg:39.31ms
step:1719/2330 train_time:67556ms step_avg:39.30ms
step:1720/2330 train_time:67612ms step_avg:39.31ms
step:1721/2330 train_time:67634ms step_avg:39.30ms
step:1722/2330 train_time:67690ms step_avg:39.31ms
step:1723/2330 train_time:67712ms step_avg:39.30ms
step:1724/2330 train_time:67768ms step_avg:39.31ms
step:1725/2330 train_time:67790ms step_avg:39.30ms
step:1726/2330 train_time:67846ms step_avg:39.31ms
step:1727/2330 train_time:67868ms step_avg:39.30ms
step:1728/2330 train_time:67923ms step_avg:39.31ms
step:1729/2330 train_time:67945ms step_avg:39.30ms
step:1730/2330 train_time:68001ms step_avg:39.31ms
step:1731/2330 train_time:68023ms step_avg:39.30ms
step:1732/2330 train_time:68079ms step_avg:39.31ms
step:1733/2330 train_time:68101ms step_avg:39.30ms
step:1734/2330 train_time:68156ms step_avg:39.31ms
step:1735/2330 train_time:68178ms step_avg:39.30ms
step:1736/2330 train_time:68234ms step_avg:39.31ms
step:1737/2330 train_time:68256ms step_avg:39.30ms
step:1738/2330 train_time:68311ms step_avg:39.30ms
step:1739/2330 train_time:68334ms step_avg:39.30ms
step:1740/2330 train_time:68390ms step_avg:39.30ms
step:1741/2330 train_time:68412ms step_avg:39.29ms
step:1742/2330 train_time:68468ms step_avg:39.30ms
step:1743/2330 train_time:68491ms step_avg:39.29ms
step:1744/2330 train_time:68547ms step_avg:39.30ms
step:1745/2330 train_time:68569ms step_avg:39.29ms
step:1746/2330 train_time:68625ms step_avg:39.30ms
step:1747/2330 train_time:68648ms step_avg:39.29ms
step:1748/2330 train_time:68705ms step_avg:39.30ms
step:1749/2330 train_time:68726ms step_avg:39.29ms
step:1750/2330 train_time:68782ms step_avg:39.30ms
step:1750/2330 val_loss:5.1701 train_time:68876ms step_avg:39.36ms
step:1751/2330 train_time:68888ms step_avg:39.34ms
step:1752/2330 train_time:68899ms step_avg:39.33ms
step:1753/2330 train_time:68909ms step_avg:39.31ms
step:1754/2330 train_time:68939ms step_avg:39.30ms
step:1755/2330 train_time:68960ms step_avg:39.29ms
step:1756/2330 train_time:69015ms step_avg:39.30ms
step:1757/2330 train_time:69037ms step_avg:39.29ms
step:1758/2330 train_time:69092ms step_avg:39.30ms
step:1759/2330 train_time:69114ms step_avg:39.29ms
step:1760/2330 train_time:69169ms step_avg:39.30ms
step:1761/2330 train_time:69191ms step_avg:39.29ms
step:1762/2330 train_time:69250ms step_avg:39.30ms
step:1763/2330 train_time:69276ms step_avg:39.29ms
step:1764/2330 train_time:69334ms step_avg:39.30ms
step:1765/2330 train_time:69358ms step_avg:39.30ms
step:1766/2330 train_time:69414ms step_avg:39.31ms
step:1767/2330 train_time:69437ms step_avg:39.30ms
step:1768/2330 train_time:69492ms step_avg:39.31ms
step:1769/2330 train_time:69514ms step_avg:39.30ms
step:1770/2330 train_time:69570ms step_avg:39.30ms
step:1771/2330 train_time:69592ms step_avg:39.30ms
step:1772/2330 train_time:69648ms step_avg:39.30ms
step:1773/2330 train_time:69671ms step_avg:39.30ms
step:1774/2330 train_time:69727ms step_avg:39.30ms
step:1775/2330 train_time:69748ms step_avg:39.29ms
step:1776/2330 train_time:69805ms step_avg:39.30ms
step:1777/2330 train_time:69827ms step_avg:39.30ms
step:1778/2330 train_time:69884ms step_avg:39.30ms
step:1779/2330 train_time:69906ms step_avg:39.30ms
step:1780/2330 train_time:69963ms step_avg:39.30ms
step:1781/2330 train_time:69984ms step_avg:39.29ms
step:1782/2330 train_time:70039ms step_avg:39.30ms
step:1783/2330 train_time:70061ms step_avg:39.29ms
step:1784/2330 train_time:70117ms step_avg:39.30ms
step:1785/2330 train_time:70140ms step_avg:39.29ms
step:1786/2330 train_time:70197ms step_avg:39.30ms
step:1787/2330 train_time:70220ms step_avg:39.29ms
step:1788/2330 train_time:70276ms step_avg:39.30ms
step:1789/2330 train_time:70299ms step_avg:39.29ms
step:1790/2330 train_time:70355ms step_avg:39.30ms
step:1791/2330 train_time:70378ms step_avg:39.30ms
step:1792/2330 train_time:70434ms step_avg:39.30ms
step:1793/2330 train_time:70456ms step_avg:39.30ms
step:1794/2330 train_time:70512ms step_avg:39.30ms
step:1795/2330 train_time:70536ms step_avg:39.30ms
step:1796/2330 train_time:70592ms step_avg:39.30ms
step:1797/2330 train_time:70614ms step_avg:39.30ms
step:1798/2330 train_time:70670ms step_avg:39.30ms
step:1799/2330 train_time:70693ms step_avg:39.30ms
step:1800/2330 train_time:70749ms step_avg:39.30ms
step:1801/2330 train_time:70771ms step_avg:39.30ms
step:1802/2330 train_time:70828ms step_avg:39.31ms
step:1803/2330 train_time:70851ms step_avg:39.30ms
step:1804/2330 train_time:70908ms step_avg:39.31ms
step:1805/2330 train_time:70930ms step_avg:39.30ms
step:1806/2330 train_time:70986ms step_avg:39.31ms
step:1807/2330 train_time:71009ms step_avg:39.30ms
step:1808/2330 train_time:71064ms step_avg:39.31ms
step:1809/2330 train_time:71086ms step_avg:39.30ms
step:1810/2330 train_time:71143ms step_avg:39.31ms
step:1811/2330 train_time:71165ms step_avg:39.30ms
step:1812/2330 train_time:71221ms step_avg:39.31ms
step:1813/2330 train_time:71243ms step_avg:39.30ms
step:1814/2330 train_time:71299ms step_avg:39.30ms
step:1815/2330 train_time:71322ms step_avg:39.30ms
step:1816/2330 train_time:71380ms step_avg:39.31ms
step:1817/2330 train_time:71403ms step_avg:39.30ms
step:1818/2330 train_time:71459ms step_avg:39.31ms
step:1819/2330 train_time:71482ms step_avg:39.30ms
step:1820/2330 train_time:71539ms step_avg:39.31ms
step:1821/2330 train_time:71562ms step_avg:39.30ms
step:1822/2330 train_time:71618ms step_avg:39.31ms
step:1823/2330 train_time:71641ms step_avg:39.30ms
step:1824/2330 train_time:71698ms step_avg:39.31ms
step:1825/2330 train_time:71721ms step_avg:39.30ms
step:1826/2330 train_time:71776ms step_avg:39.31ms
step:1827/2330 train_time:71799ms step_avg:39.30ms
step:1828/2330 train_time:71855ms step_avg:39.31ms
step:1829/2330 train_time:71878ms step_avg:39.30ms
step:1830/2330 train_time:71934ms step_avg:39.31ms
step:1831/2330 train_time:71956ms step_avg:39.30ms
step:1832/2330 train_time:72011ms step_avg:39.31ms
step:1833/2330 train_time:72035ms step_avg:39.30ms
step:1834/2330 train_time:72092ms step_avg:39.31ms
step:1835/2330 train_time:72115ms step_avg:39.30ms
step:1836/2330 train_time:72171ms step_avg:39.31ms
step:1837/2330 train_time:72194ms step_avg:39.30ms
step:1838/2330 train_time:72251ms step_avg:39.31ms
step:1839/2330 train_time:72274ms step_avg:39.30ms
step:1840/2330 train_time:72331ms step_avg:39.31ms
step:1841/2330 train_time:72354ms step_avg:39.30ms
step:1842/2330 train_time:72411ms step_avg:39.31ms
step:1843/2330 train_time:72434ms step_avg:39.30ms
step:1844/2330 train_time:72490ms step_avg:39.31ms
step:1845/2330 train_time:72512ms step_avg:39.30ms
step:1846/2330 train_time:72569ms step_avg:39.31ms
step:1847/2330 train_time:72591ms step_avg:39.30ms
step:1848/2330 train_time:72647ms step_avg:39.31ms
step:1849/2330 train_time:72670ms step_avg:39.30ms
step:1850/2330 train_time:72726ms step_avg:39.31ms
step:1851/2330 train_time:72748ms step_avg:39.30ms
step:1852/2330 train_time:72804ms step_avg:39.31ms
step:1853/2330 train_time:72826ms step_avg:39.30ms
step:1854/2330 train_time:72881ms step_avg:39.31ms
step:1855/2330 train_time:72903ms step_avg:39.30ms
step:1856/2330 train_time:72958ms step_avg:39.31ms
step:1857/2330 train_time:72981ms step_avg:39.30ms
step:1858/2330 train_time:73036ms step_avg:39.31ms
step:1859/2330 train_time:73059ms step_avg:39.30ms
step:1860/2330 train_time:73115ms step_avg:39.31ms
step:1861/2330 train_time:73137ms step_avg:39.30ms
step:1862/2330 train_time:73193ms step_avg:39.31ms
step:1863/2330 train_time:73216ms step_avg:39.30ms
step:1864/2330 train_time:73271ms step_avg:39.31ms
step:1865/2330 train_time:73294ms step_avg:39.30ms
step:1866/2330 train_time:73351ms step_avg:39.31ms
step:1867/2330 train_time:73374ms step_avg:39.30ms
step:1868/2330 train_time:73431ms step_avg:39.31ms
step:1869/2330 train_time:73454ms step_avg:39.30ms
step:1870/2330 train_time:73511ms step_avg:39.31ms
step:1871/2330 train_time:73533ms step_avg:39.30ms
step:1872/2330 train_time:73589ms step_avg:39.31ms
step:1873/2330 train_time:73612ms step_avg:39.30ms
step:1874/2330 train_time:73668ms step_avg:39.31ms
step:1875/2330 train_time:73690ms step_avg:39.30ms
step:1876/2330 train_time:73747ms step_avg:39.31ms
step:1877/2330 train_time:73769ms step_avg:39.30ms
step:1878/2330 train_time:73825ms step_avg:39.31ms
step:1879/2330 train_time:73847ms step_avg:39.30ms
step:1880/2330 train_time:73904ms step_avg:39.31ms
step:1881/2330 train_time:73925ms step_avg:39.30ms
step:1882/2330 train_time:73981ms step_avg:39.31ms
step:1883/2330 train_time:74003ms step_avg:39.30ms
step:1884/2330 train_time:74058ms step_avg:39.31ms
step:1885/2330 train_time:74081ms step_avg:39.30ms
step:1886/2330 train_time:74137ms step_avg:39.31ms
step:1887/2330 train_time:74160ms step_avg:39.30ms
step:1888/2330 train_time:74215ms step_avg:39.31ms
step:1889/2330 train_time:74238ms step_avg:39.30ms
step:1890/2330 train_time:74294ms step_avg:39.31ms
step:1891/2330 train_time:74317ms step_avg:39.30ms
step:1892/2330 train_time:74373ms step_avg:39.31ms
step:1893/2330 train_time:74395ms step_avg:39.30ms
step:1894/2330 train_time:74451ms step_avg:39.31ms
step:1895/2330 train_time:74473ms step_avg:39.30ms
step:1896/2330 train_time:74529ms step_avg:39.31ms
step:1897/2330 train_time:74552ms step_avg:39.30ms
step:1898/2330 train_time:74609ms step_avg:39.31ms
step:1899/2330 train_time:74632ms step_avg:39.30ms
step:1900/2330 train_time:74688ms step_avg:39.31ms
step:1901/2330 train_time:74710ms step_avg:39.30ms
step:1902/2330 train_time:74767ms step_avg:39.31ms
step:1903/2330 train_time:74789ms step_avg:39.30ms
step:1904/2330 train_time:74845ms step_avg:39.31ms
step:1905/2330 train_time:74867ms step_avg:39.30ms
step:1906/2330 train_time:74923ms step_avg:39.31ms
step:1907/2330 train_time:74945ms step_avg:39.30ms
step:1908/2330 train_time:75002ms step_avg:39.31ms
step:1909/2330 train_time:75023ms step_avg:39.30ms
step:1910/2330 train_time:75078ms step_avg:39.31ms
step:1911/2330 train_time:75100ms step_avg:39.30ms
step:1912/2330 train_time:75157ms step_avg:39.31ms
step:1913/2330 train_time:75179ms step_avg:39.30ms
step:1914/2330 train_time:75236ms step_avg:39.31ms
step:1915/2330 train_time:75259ms step_avg:39.30ms
step:1916/2330 train_time:75315ms step_avg:39.31ms
step:1917/2330 train_time:75337ms step_avg:39.30ms
step:1918/2330 train_time:75393ms step_avg:39.31ms
step:1919/2330 train_time:75416ms step_avg:39.30ms
step:1920/2330 train_time:75472ms step_avg:39.31ms
step:1921/2330 train_time:75494ms step_avg:39.30ms
step:1922/2330 train_time:75550ms step_avg:39.31ms
step:1923/2330 train_time:75573ms step_avg:39.30ms
step:1924/2330 train_time:75629ms step_avg:39.31ms
step:1925/2330 train_time:75653ms step_avg:39.30ms
step:1926/2330 train_time:75709ms step_avg:39.31ms
step:1927/2330 train_time:75731ms step_avg:39.30ms
step:1928/2330 train_time:75789ms step_avg:39.31ms
step:1929/2330 train_time:75811ms step_avg:39.30ms
step:1930/2330 train_time:75868ms step_avg:39.31ms
step:1931/2330 train_time:75890ms step_avg:39.30ms
step:1932/2330 train_time:75947ms step_avg:39.31ms
step:1933/2330 train_time:75969ms step_avg:39.30ms
step:1934/2330 train_time:76027ms step_avg:39.31ms
step:1935/2330 train_time:76049ms step_avg:39.30ms
step:1936/2330 train_time:76106ms step_avg:39.31ms
step:1937/2330 train_time:76128ms step_avg:39.30ms
step:1938/2330 train_time:76184ms step_avg:39.31ms
step:1939/2330 train_time:76207ms step_avg:39.30ms
step:1940/2330 train_time:76263ms step_avg:39.31ms
step:1941/2330 train_time:76285ms step_avg:39.30ms
step:1942/2330 train_time:76341ms step_avg:39.31ms
step:1943/2330 train_time:76364ms step_avg:39.30ms
step:1944/2330 train_time:76420ms step_avg:39.31ms
step:1945/2330 train_time:76443ms step_avg:39.30ms
step:1946/2330 train_time:76499ms step_avg:39.31ms
step:1947/2330 train_time:76522ms step_avg:39.30ms
step:1948/2330 train_time:76578ms step_avg:39.31ms
step:1949/2330 train_time:76601ms step_avg:39.30ms
step:1950/2330 train_time:76657ms step_avg:39.31ms
step:1951/2330 train_time:76679ms step_avg:39.30ms
step:1952/2330 train_time:76736ms step_avg:39.31ms
step:1953/2330 train_time:76758ms step_avg:39.30ms
step:1954/2330 train_time:76814ms step_avg:39.31ms
step:1955/2330 train_time:76837ms step_avg:39.30ms
step:1956/2330 train_time:76893ms step_avg:39.31ms
step:1957/2330 train_time:76916ms step_avg:39.30ms
step:1958/2330 train_time:76971ms step_avg:39.31ms
step:1959/2330 train_time:76994ms step_avg:39.30ms
step:1960/2330 train_time:77051ms step_avg:39.31ms
step:1961/2330 train_time:77074ms step_avg:39.30ms
step:1962/2330 train_time:77131ms step_avg:39.31ms
step:1963/2330 train_time:77154ms step_avg:39.30ms
step:1964/2330 train_time:77211ms step_avg:39.31ms
step:1965/2330 train_time:77235ms step_avg:39.31ms
step:1966/2330 train_time:77291ms step_avg:39.31ms
step:1967/2330 train_time:77314ms step_avg:39.31ms
step:1968/2330 train_time:77370ms step_avg:39.31ms
step:1969/2330 train_time:77393ms step_avg:39.31ms
step:1970/2330 train_time:77449ms step_avg:39.31ms
step:1971/2330 train_time:77471ms step_avg:39.31ms
step:1972/2330 train_time:77528ms step_avg:39.31ms
step:1973/2330 train_time:77550ms step_avg:39.31ms
step:1974/2330 train_time:77606ms step_avg:39.31ms
step:1975/2330 train_time:77628ms step_avg:39.31ms
step:1976/2330 train_time:77684ms step_avg:39.31ms
step:1977/2330 train_time:77706ms step_avg:39.31ms
step:1978/2330 train_time:77763ms step_avg:39.31ms
step:1979/2330 train_time:77785ms step_avg:39.31ms
step:1980/2330 train_time:77840ms step_avg:39.31ms
step:1981/2330 train_time:77863ms step_avg:39.30ms
step:1982/2330 train_time:77918ms step_avg:39.31ms
step:1983/2330 train_time:77941ms step_avg:39.30ms
step:1984/2330 train_time:77997ms step_avg:39.31ms
step:1985/2330 train_time:78020ms step_avg:39.30ms
step:1986/2330 train_time:78076ms step_avg:39.31ms
step:1987/2330 train_time:78099ms step_avg:39.30ms
step:1988/2330 train_time:78155ms step_avg:39.31ms
step:1989/2330 train_time:78179ms step_avg:39.31ms
step:1990/2330 train_time:78234ms step_avg:39.31ms
step:1991/2330 train_time:78257ms step_avg:39.31ms
step:1992/2330 train_time:78314ms step_avg:39.31ms
step:1993/2330 train_time:78337ms step_avg:39.31ms
step:1994/2330 train_time:78393ms step_avg:39.31ms
step:1995/2330 train_time:78416ms step_avg:39.31ms
step:1996/2330 train_time:78472ms step_avg:39.31ms
step:1997/2330 train_time:78494ms step_avg:39.31ms
step:1998/2330 train_time:78550ms step_avg:39.31ms
step:1999/2330 train_time:78573ms step_avg:39.31ms
step:2000/2330 train_time:78629ms step_avg:39.31ms
step:2000/2330 val_loss:5.1541 train_time:78728ms step_avg:39.36ms
step:2001/2330 train_time:78740ms step_avg:39.35ms
step:2002/2330 train_time:78751ms step_avg:39.34ms
step:2003/2330 train_time:78761ms step_avg:39.32ms
step:2004/2330 train_time:78791ms step_avg:39.32ms
step:2005/2330 train_time:78812ms step_avg:39.31ms
step:2006/2330 train_time:78867ms step_avg:39.32ms
step:2007/2330 train_time:78889ms step_avg:39.31ms
step:2008/2330 train_time:78944ms step_avg:39.31ms
step:2009/2330 train_time:78966ms step_avg:39.31ms
step:2010/2330 train_time:79021ms step_avg:39.31ms
step:2011/2330 train_time:79045ms step_avg:39.31ms
step:2012/2330 train_time:79105ms step_avg:39.32ms
step:2013/2330 train_time:79129ms step_avg:39.31ms
step:2014/2330 train_time:79186ms step_avg:39.32ms
step:2015/2330 train_time:79209ms step_avg:39.31ms
step:2016/2330 train_time:79265ms step_avg:39.32ms
step:2017/2330 train_time:79288ms step_avg:39.31ms
step:2018/2330 train_time:79343ms step_avg:39.32ms
step:2019/2330 train_time:79365ms step_avg:39.31ms
step:2020/2330 train_time:79420ms step_avg:39.32ms
step:2021/2330 train_time:79443ms step_avg:39.31ms
step:2022/2330 train_time:79498ms step_avg:39.32ms
step:2023/2330 train_time:79520ms step_avg:39.31ms
step:2024/2330 train_time:79575ms step_avg:39.32ms
step:2025/2330 train_time:79596ms step_avg:39.31ms
step:2026/2330 train_time:79653ms step_avg:39.32ms
step:2027/2330 train_time:79675ms step_avg:39.31ms
step:2028/2330 train_time:79731ms step_avg:39.31ms
step:2029/2330 train_time:79752ms step_avg:39.31ms
step:2030/2330 train_time:79807ms step_avg:39.31ms
step:2031/2330 train_time:79830ms step_avg:39.31ms
step:2032/2330 train_time:79885ms step_avg:39.31ms
step:2033/2330 train_time:79907ms step_avg:39.30ms
step:2034/2330 train_time:79963ms step_avg:39.31ms
step:2035/2330 train_time:79985ms step_avg:39.30ms
step:2036/2330 train_time:80041ms step_avg:39.31ms
step:2037/2330 train_time:80064ms step_avg:39.30ms
step:2038/2330 train_time:80122ms step_avg:39.31ms
step:2039/2330 train_time:80145ms step_avg:39.31ms
step:2040/2330 train_time:80201ms step_avg:39.31ms
step:2041/2330 train_time:80224ms step_avg:39.31ms
step:2042/2330 train_time:80281ms step_avg:39.31ms
step:2043/2330 train_time:80303ms step_avg:39.31ms
step:2044/2330 train_time:80359ms step_avg:39.31ms
step:2045/2330 train_time:80381ms step_avg:39.31ms
step:2046/2330 train_time:80436ms step_avg:39.31ms
step:2047/2330 train_time:80458ms step_avg:39.31ms
step:2048/2330 train_time:80513ms step_avg:39.31ms
step:2049/2330 train_time:80536ms step_avg:39.30ms
step:2050/2330 train_time:80591ms step_avg:39.31ms
step:2051/2330 train_time:80613ms step_avg:39.30ms
step:2052/2330 train_time:80668ms step_avg:39.31ms
step:2053/2330 train_time:80691ms step_avg:39.30ms
step:2054/2330 train_time:80746ms step_avg:39.31ms
step:2055/2330 train_time:80768ms step_avg:39.30ms
step:2056/2330 train_time:80824ms step_avg:39.31ms
step:2057/2330 train_time:80846ms step_avg:39.30ms
step:2058/2330 train_time:80901ms step_avg:39.31ms
step:2059/2330 train_time:80923ms step_avg:39.30ms
step:2060/2330 train_time:80979ms step_avg:39.31ms
step:2061/2330 train_time:81001ms step_avg:39.30ms
step:2062/2330 train_time:81058ms step_avg:39.31ms
step:2063/2330 train_time:81080ms step_avg:39.30ms
step:2064/2330 train_time:81137ms step_avg:39.31ms
step:2065/2330 train_time:81159ms step_avg:39.30ms
step:2066/2330 train_time:81216ms step_avg:39.31ms
step:2067/2330 train_time:81238ms step_avg:39.30ms
step:2068/2330 train_time:81294ms step_avg:39.31ms
step:2069/2330 train_time:81316ms step_avg:39.30ms
step:2070/2330 train_time:81371ms step_avg:39.31ms
step:2071/2330 train_time:81394ms step_avg:39.30ms
step:2072/2330 train_time:81449ms step_avg:39.31ms
step:2073/2330 train_time:81472ms step_avg:39.30ms
step:2074/2330 train_time:81527ms step_avg:39.31ms
step:2075/2330 train_time:81549ms step_avg:39.30ms
step:2076/2330 train_time:81605ms step_avg:39.31ms
step:2077/2330 train_time:81628ms step_avg:39.30ms
step:2078/2330 train_time:81683ms step_avg:39.31ms
step:2079/2330 train_time:81705ms step_avg:39.30ms
step:2080/2330 train_time:81761ms step_avg:39.31ms
step:2081/2330 train_time:81783ms step_avg:39.30ms
step:2082/2330 train_time:81839ms step_avg:39.31ms
step:2083/2330 train_time:81861ms step_avg:39.30ms
step:2084/2330 train_time:81917ms step_avg:39.31ms
step:2085/2330 train_time:81939ms step_avg:39.30ms
step:2086/2330 train_time:81996ms step_avg:39.31ms
step:2087/2330 train_time:82018ms step_avg:39.30ms
step:2088/2330 train_time:82075ms step_avg:39.31ms
step:2089/2330 train_time:82097ms step_avg:39.30ms
step:2090/2330 train_time:82154ms step_avg:39.31ms
step:2091/2330 train_time:82176ms step_avg:39.30ms
step:2092/2330 train_time:82232ms step_avg:39.31ms
step:2093/2330 train_time:82254ms step_avg:39.30ms
step:2094/2330 train_time:82310ms step_avg:39.31ms
step:2095/2330 train_time:82332ms step_avg:39.30ms
step:2096/2330 train_time:82388ms step_avg:39.31ms
step:2097/2330 train_time:82410ms step_avg:39.30ms
step:2098/2330 train_time:82466ms step_avg:39.31ms
step:2099/2330 train_time:82488ms step_avg:39.30ms
step:2100/2330 train_time:82543ms step_avg:39.31ms
step:2101/2330 train_time:82565ms step_avg:39.30ms
step:2102/2330 train_time:82622ms step_avg:39.31ms
step:2103/2330 train_time:82644ms step_avg:39.30ms
step:2104/2330 train_time:82699ms step_avg:39.31ms
step:2105/2330 train_time:82721ms step_avg:39.30ms
step:2106/2330 train_time:82777ms step_avg:39.31ms
step:2107/2330 train_time:82798ms step_avg:39.30ms
step:2108/2330 train_time:82855ms step_avg:39.31ms
step:2109/2330 train_time:82877ms step_avg:39.30ms
step:2110/2330 train_time:82933ms step_avg:39.30ms
step:2111/2330 train_time:82955ms step_avg:39.30ms
step:2112/2330 train_time:83010ms step_avg:39.30ms
step:2113/2330 train_time:83033ms step_avg:39.30ms
step:2114/2330 train_time:83088ms step_avg:39.30ms
step:2115/2330 train_time:83112ms step_avg:39.30ms
step:2116/2330 train_time:83168ms step_avg:39.30ms
step:2117/2330 train_time:83190ms step_avg:39.30ms
step:2118/2330 train_time:83246ms step_avg:39.30ms
step:2119/2330 train_time:83268ms step_avg:39.30ms
step:2120/2330 train_time:83324ms step_avg:39.30ms
step:2121/2330 train_time:83347ms step_avg:39.30ms
step:2122/2330 train_time:83403ms step_avg:39.30ms
step:2123/2330 train_time:83425ms step_avg:39.30ms
step:2124/2330 train_time:83481ms step_avg:39.30ms
step:2125/2330 train_time:83503ms step_avg:39.30ms
step:2126/2330 train_time:83559ms step_avg:39.30ms
step:2127/2330 train_time:83581ms step_avg:39.30ms
step:2128/2330 train_time:83637ms step_avg:39.30ms
step:2129/2330 train_time:83659ms step_avg:39.30ms
step:2130/2330 train_time:83715ms step_avg:39.30ms
step:2131/2330 train_time:83737ms step_avg:39.29ms
step:2132/2330 train_time:83792ms step_avg:39.30ms
step:2133/2330 train_time:83815ms step_avg:39.29ms
step:2134/2330 train_time:83870ms step_avg:39.30ms
step:2135/2330 train_time:83893ms step_avg:39.29ms
step:2136/2330 train_time:83948ms step_avg:39.30ms
step:2137/2330 train_time:83971ms step_avg:39.29ms
step:2138/2330 train_time:84027ms step_avg:39.30ms
step:2139/2330 train_time:84049ms step_avg:39.29ms
step:2140/2330 train_time:84105ms step_avg:39.30ms
step:2141/2330 train_time:84128ms step_avg:39.29ms
step:2142/2330 train_time:84184ms step_avg:39.30ms
step:2143/2330 train_time:84206ms step_avg:39.29ms
step:2144/2330 train_time:84262ms step_avg:39.30ms
step:2145/2330 train_time:84284ms step_avg:39.29ms
step:2146/2330 train_time:84341ms step_avg:39.30ms
step:2147/2330 train_time:84363ms step_avg:39.29ms
step:2148/2330 train_time:84419ms step_avg:39.30ms
step:2149/2330 train_time:84442ms step_avg:39.29ms
step:2150/2330 train_time:84497ms step_avg:39.30ms
step:2151/2330 train_time:84520ms step_avg:39.29ms
step:2152/2330 train_time:84576ms step_avg:39.30ms
step:2153/2330 train_time:84597ms step_avg:39.29ms
step:2154/2330 train_time:84653ms step_avg:39.30ms
step:2155/2330 train_time:84675ms step_avg:39.29ms
step:2156/2330 train_time:84731ms step_avg:39.30ms
step:2157/2330 train_time:84753ms step_avg:39.29ms
step:2158/2330 train_time:84809ms step_avg:39.30ms
step:2159/2330 train_time:84831ms step_avg:39.29ms
step:2160/2330 train_time:84887ms step_avg:39.30ms
step:2161/2330 train_time:84909ms step_avg:39.29ms
step:2162/2330 train_time:84965ms step_avg:39.30ms
step:2163/2330 train_time:84987ms step_avg:39.29ms
step:2164/2330 train_time:85043ms step_avg:39.30ms
step:2165/2330 train_time:85066ms step_avg:39.29ms
step:2166/2330 train_time:85122ms step_avg:39.30ms
step:2167/2330 train_time:85144ms step_avg:39.29ms
step:2168/2330 train_time:85200ms step_avg:39.30ms
step:2169/2330 train_time:85222ms step_avg:39.29ms
step:2170/2330 train_time:85278ms step_avg:39.30ms
step:2171/2330 train_time:85300ms step_avg:39.29ms
step:2172/2330 train_time:85356ms step_avg:39.30ms
step:2173/2330 train_time:85379ms step_avg:39.29ms
step:2174/2330 train_time:85434ms step_avg:39.30ms
step:2175/2330 train_time:85456ms step_avg:39.29ms
step:2176/2330 train_time:85512ms step_avg:39.30ms
step:2177/2330 train_time:85534ms step_avg:39.29ms
step:2178/2330 train_time:85589ms step_avg:39.30ms
step:2179/2330 train_time:85612ms step_avg:39.29ms
step:2180/2330 train_time:85668ms step_avg:39.30ms
step:2181/2330 train_time:85690ms step_avg:39.29ms
step:2182/2330 train_time:85746ms step_avg:39.30ms
step:2183/2330 train_time:85768ms step_avg:39.29ms
step:2184/2330 train_time:85824ms step_avg:39.30ms
step:2185/2330 train_time:85846ms step_avg:39.29ms
step:2186/2330 train_time:85902ms step_avg:39.30ms
step:2187/2330 train_time:85924ms step_avg:39.29ms
step:2188/2330 train_time:85980ms step_avg:39.30ms
step:2189/2330 train_time:86002ms step_avg:39.29ms
step:2190/2330 train_time:86058ms step_avg:39.30ms
step:2191/2330 train_time:86080ms step_avg:39.29ms
step:2192/2330 train_time:86136ms step_avg:39.30ms
step:2193/2330 train_time:86158ms step_avg:39.29ms
step:2194/2330 train_time:86214ms step_avg:39.30ms
step:2195/2330 train_time:86236ms step_avg:39.29ms
step:2196/2330 train_time:86292ms step_avg:39.30ms
step:2197/2330 train_time:86315ms step_avg:39.29ms
step:2198/2330 train_time:86371ms step_avg:39.30ms
step:2199/2330 train_time:86394ms step_avg:39.29ms
step:2200/2330 train_time:86449ms step_avg:39.30ms
step:2201/2330 train_time:86472ms step_avg:39.29ms
step:2202/2330 train_time:86528ms step_avg:39.29ms
step:2203/2330 train_time:86550ms step_avg:39.29ms
step:2204/2330 train_time:86606ms step_avg:39.29ms
step:2205/2330 train_time:86628ms step_avg:39.29ms
step:2206/2330 train_time:86684ms step_avg:39.29ms
step:2207/2330 train_time:86706ms step_avg:39.29ms
step:2208/2330 train_time:86762ms step_avg:39.29ms
step:2209/2330 train_time:86784ms step_avg:39.29ms
step:2210/2330 train_time:86840ms step_avg:39.29ms
step:2211/2330 train_time:86862ms step_avg:39.29ms
step:2212/2330 train_time:86918ms step_avg:39.29ms
step:2213/2330 train_time:86940ms step_avg:39.29ms
step:2214/2330 train_time:86996ms step_avg:39.29ms
step:2215/2330 train_time:87018ms step_avg:39.29ms
step:2216/2330 train_time:87074ms step_avg:39.29ms
step:2217/2330 train_time:87096ms step_avg:39.29ms
step:2218/2330 train_time:87153ms step_avg:39.29ms
step:2219/2330 train_time:87175ms step_avg:39.29ms
step:2220/2330 train_time:87231ms step_avg:39.29ms
step:2221/2330 train_time:87254ms step_avg:39.29ms
step:2222/2330 train_time:87309ms step_avg:39.29ms
step:2223/2330 train_time:87332ms step_avg:39.29ms
step:2224/2330 train_time:87388ms step_avg:39.29ms
step:2225/2330 train_time:87410ms step_avg:39.29ms
step:2226/2330 train_time:87466ms step_avg:39.29ms
step:2227/2330 train_time:87489ms step_avg:39.29ms
step:2228/2330 train_time:87544ms step_avg:39.29ms
step:2229/2330 train_time:87566ms step_avg:39.29ms
step:2230/2330 train_time:87621ms step_avg:39.29ms
step:2231/2330 train_time:87644ms step_avg:39.28ms
step:2232/2330 train_time:87700ms step_avg:39.29ms
step:2233/2330 train_time:87722ms step_avg:39.28ms
step:2234/2330 train_time:87778ms step_avg:39.29ms
step:2235/2330 train_time:87799ms step_avg:39.28ms
step:2236/2330 train_time:87856ms step_avg:39.29ms
step:2237/2330 train_time:87878ms step_avg:39.28ms
step:2238/2330 train_time:87934ms step_avg:39.29ms
step:2239/2330 train_time:87956ms step_avg:39.28ms
step:2240/2330 train_time:88011ms step_avg:39.29ms
step:2241/2330 train_time:88034ms step_avg:39.28ms
step:2242/2330 train_time:88089ms step_avg:39.29ms
step:2243/2330 train_time:88112ms step_avg:39.28ms
step:2244/2330 train_time:88167ms step_avg:39.29ms
step:2245/2330 train_time:88189ms step_avg:39.28ms
step:2246/2330 train_time:88245ms step_avg:39.29ms
step:2247/2330 train_time:88267ms step_avg:39.28ms
step:2248/2330 train_time:88324ms step_avg:39.29ms
step:2249/2330 train_time:88346ms step_avg:39.28ms
step:2250/2330 train_time:88402ms step_avg:39.29ms
step:2250/2330 val_loss:5.1808 train_time:88497ms step_avg:39.33ms
step:2251/2330 train_time:88510ms step_avg:39.32ms
step:2252/2330 train_time:88522ms step_avg:39.31ms
step:2253/2330 train_time:88532ms step_avg:39.30ms
step:2254/2330 train_time:88560ms step_avg:39.29ms
step:2255/2330 train_time:88582ms step_avg:39.28ms
step:2256/2330 train_time:88636ms step_avg:39.29ms
step:2257/2330 train_time:88658ms step_avg:39.28ms
step:2258/2330 train_time:88713ms step_avg:39.29ms
step:2259/2330 train_time:88734ms step_avg:39.28ms
step:2260/2330 train_time:88790ms step_avg:39.29ms
step:2261/2330 train_time:88815ms step_avg:39.28ms
step:2262/2330 train_time:88875ms step_avg:39.29ms
step:2263/2330 train_time:88899ms step_avg:39.28ms
step:2264/2330 train_time:88956ms step_avg:39.29ms
step:2265/2330 train_time:88979ms step_avg:39.28ms
step:2266/2330 train_time:89035ms step_avg:39.29ms
step:2267/2330 train_time:89057ms step_avg:39.28ms
step:2268/2330 train_time:89113ms step_avg:39.29ms
step:2269/2330 train_time:89135ms step_avg:39.28ms
step:2270/2330 train_time:89191ms step_avg:39.29ms
step:2271/2330 train_time:89213ms step_avg:39.28ms
step:2272/2330 train_time:89268ms step_avg:39.29ms
step:2273/2330 train_time:89290ms step_avg:39.28ms
step:2274/2330 train_time:89345ms step_avg:39.29ms
step:2275/2330 train_time:89366ms step_avg:39.28ms
step:2276/2330 train_time:89422ms step_avg:39.29ms
step:2277/2330 train_time:89445ms step_avg:39.28ms
step:2278/2330 train_time:89500ms step_avg:39.29ms
step:2279/2330 train_time:89522ms step_avg:39.28ms
step:2280/2330 train_time:89578ms step_avg:39.29ms
step:2281/2330 train_time:89600ms step_avg:39.28ms
step:2282/2330 train_time:89655ms step_avg:39.29ms
step:2283/2330 train_time:89677ms step_avg:39.28ms
step:2284/2330 train_time:89733ms step_avg:39.29ms
step:2285/2330 train_time:89756ms step_avg:39.28ms
step:2286/2330 train_time:89813ms step_avg:39.29ms
step:2287/2330 train_time:89837ms step_avg:39.28ms
step:2288/2330 train_time:89895ms step_avg:39.29ms
step:2289/2330 train_time:89918ms step_avg:39.28ms
step:2290/2330 train_time:89975ms step_avg:39.29ms
step:2291/2330 train_time:89997ms step_avg:39.28ms
step:2292/2330 train_time:90053ms step_avg:39.29ms
step:2293/2330 train_time:90075ms step_avg:39.28ms
step:2294/2330 train_time:90131ms step_avg:39.29ms
step:2295/2330 train_time:90153ms step_avg:39.28ms
step:2296/2330 train_time:90209ms step_avg:39.29ms
step:2297/2330 train_time:90231ms step_avg:39.28ms
step:2298/2330 train_time:90286ms step_avg:39.29ms
step:2299/2330 train_time:90308ms step_avg:39.28ms
step:2300/2330 train_time:90363ms step_avg:39.29ms
step:2301/2330 train_time:90385ms step_avg:39.28ms
step:2302/2330 train_time:90441ms step_avg:39.29ms
step:2303/2330 train_time:90463ms step_avg:39.28ms
step:2304/2330 train_time:90518ms step_avg:39.29ms
step:2305/2330 train_time:90540ms step_avg:39.28ms
step:2306/2330 train_time:90597ms step_avg:39.29ms
step:2307/2330 train_time:90618ms step_avg:39.28ms
step:2308/2330 train_time:90674ms step_avg:39.29ms
step:2309/2330 train_time:90696ms step_avg:39.28ms
step:2310/2330 train_time:90753ms step_avg:39.29ms
step:2311/2330 train_time:90776ms step_avg:39.28ms
step:2312/2330 train_time:90833ms step_avg:39.29ms
step:2313/2330 train_time:90855ms step_avg:39.28ms
step:2314/2330 train_time:90911ms step_avg:39.29ms
step:2315/2330 train_time:90934ms step_avg:39.28ms
step:2316/2330 train_time:90990ms step_avg:39.29ms
step:2317/2330 train_time:91013ms step_avg:39.28ms
step:2318/2330 train_time:91069ms step_avg:39.29ms
step:2319/2330 train_time:91091ms step_avg:39.28ms
step:2320/2330 train_time:91146ms step_avg:39.29ms
step:2321/2330 train_time:91168ms step_avg:39.28ms
step:2322/2330 train_time:91223ms step_avg:39.29ms
step:2323/2330 train_time:91246ms step_avg:39.28ms
step:2324/2330 train_time:91301ms step_avg:39.29ms
step:2325/2330 train_time:91323ms step_avg:39.28ms
step:2326/2330 train_time:91379ms step_avg:39.29ms
step:2327/2330 train_time:91401ms step_avg:39.28ms
step:2328/2330 train_time:91456ms step_avg:39.29ms
step:2329/2330 train_time:91479ms step_avg:39.28ms
step:2330/2330 train_time:91535ms step_avg:39.29ms
step:2330/2330 val_loss:5.1377 train_time:91629ms step_avg:39.33ms
peak memory allocated: 29226 MiB reserved: 38888 MiB
