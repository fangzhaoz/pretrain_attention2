import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                # v_chunk = divide_by_norm(batched)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                # v_chunk = divide_by_norm(batched_update_grads)
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "muon_lr5e-2"
    os.makedirs("logs3", exist_ok=True)
    logfile = f"logs3/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
# embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=5e-2, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    return 1.0

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum_min + 0.5 * (momentum_max - momentum_min)

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs3/{run_id}", exist_ok=True)
            torch.save(log, f"logs3/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 07:36:48 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   34C    P0             113W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   32C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   30C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   32C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   30C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   34C    P0             116W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.05ms
step:1/2330 train_time:126ms step_avg:125.58ms
step:2/2330 train_time:214ms step_avg:106.83ms
step:3/2330 train_time:235ms step_avg:78.48ms
step:4/2330 train_time:272ms step_avg:67.93ms
step:5/2330 train_time:329ms step_avg:65.82ms
step:6/2330 train_time:391ms step_avg:65.08ms
step:7/2330 train_time:449ms step_avg:64.14ms
step:8/2330 train_time:511ms step_avg:63.84ms
step:9/2330 train_time:569ms step_avg:63.25ms
step:10/2330 train_time:631ms step_avg:63.10ms
step:11/2330 train_time:689ms step_avg:62.67ms
step:12/2330 train_time:751ms step_avg:62.59ms
step:13/2330 train_time:810ms step_avg:62.29ms
step:14/2330 train_time:872ms step_avg:62.25ms
step:15/2330 train_time:931ms step_avg:62.04ms
step:16/2330 train_time:993ms step_avg:62.03ms
step:17/2330 train_time:1053ms step_avg:61.97ms
step:18/2330 train_time:1118ms step_avg:62.12ms
step:19/2330 train_time:1181ms step_avg:62.18ms
step:20/2330 train_time:1245ms step_avg:62.26ms
step:21/2330 train_time:1305ms step_avg:62.12ms
step:22/2330 train_time:1368ms step_avg:62.17ms
step:23/2330 train_time:1427ms step_avg:62.04ms
step:24/2330 train_time:1489ms step_avg:62.05ms
step:25/2330 train_time:1548ms step_avg:61.92ms
step:26/2330 train_time:1610ms step_avg:61.94ms
step:27/2330 train_time:1669ms step_avg:61.82ms
step:28/2330 train_time:1731ms step_avg:61.84ms
step:29/2330 train_time:1790ms step_avg:61.74ms
step:30/2330 train_time:1853ms step_avg:61.76ms
step:31/2330 train_time:1911ms step_avg:61.65ms
step:32/2330 train_time:1974ms step_avg:61.68ms
step:33/2330 train_time:2033ms step_avg:61.60ms
step:34/2330 train_time:2098ms step_avg:61.71ms
step:35/2330 train_time:2158ms step_avg:61.66ms
step:36/2330 train_time:2221ms step_avg:61.71ms
step:37/2330 train_time:2281ms step_avg:61.65ms
step:38/2330 train_time:2344ms step_avg:61.68ms
step:39/2330 train_time:2404ms step_avg:61.64ms
step:40/2330 train_time:2467ms step_avg:61.67ms
step:41/2330 train_time:2526ms step_avg:61.61ms
step:42/2330 train_time:2588ms step_avg:61.62ms
step:43/2330 train_time:2647ms step_avg:61.56ms
step:44/2330 train_time:2709ms step_avg:61.57ms
step:45/2330 train_time:2768ms step_avg:61.51ms
step:46/2330 train_time:2830ms step_avg:61.53ms
step:47/2330 train_time:2889ms step_avg:61.47ms
step:48/2330 train_time:2951ms step_avg:61.49ms
step:49/2330 train_time:3011ms step_avg:61.45ms
step:50/2330 train_time:3075ms step_avg:61.51ms
step:51/2330 train_time:3135ms step_avg:61.48ms
step:52/2330 train_time:3198ms step_avg:61.50ms
step:53/2330 train_time:3258ms step_avg:61.47ms
step:54/2330 train_time:3321ms step_avg:61.50ms
step:55/2330 train_time:3381ms step_avg:61.47ms
step:56/2330 train_time:3443ms step_avg:61.48ms
step:57/2330 train_time:3504ms step_avg:61.47ms
step:58/2330 train_time:3566ms step_avg:61.48ms
step:59/2330 train_time:3625ms step_avg:61.43ms
step:60/2330 train_time:3687ms step_avg:61.44ms
step:61/2330 train_time:3746ms step_avg:61.40ms
step:62/2330 train_time:3809ms step_avg:61.43ms
step:63/2330 train_time:3867ms step_avg:61.39ms
step:64/2330 train_time:3930ms step_avg:61.41ms
step:65/2330 train_time:3989ms step_avg:61.38ms
step:66/2330 train_time:4052ms step_avg:61.40ms
step:67/2330 train_time:4112ms step_avg:61.38ms
step:68/2330 train_time:4175ms step_avg:61.40ms
step:69/2330 train_time:4235ms step_avg:61.38ms
step:70/2330 train_time:4298ms step_avg:61.39ms
step:71/2330 train_time:4358ms step_avg:61.39ms
step:72/2330 train_time:4421ms step_avg:61.40ms
step:73/2330 train_time:4481ms step_avg:61.38ms
step:74/2330 train_time:4543ms step_avg:61.40ms
step:75/2330 train_time:4603ms step_avg:61.37ms
step:76/2330 train_time:4665ms step_avg:61.38ms
step:77/2330 train_time:4724ms step_avg:61.35ms
step:78/2330 train_time:4786ms step_avg:61.35ms
step:79/2330 train_time:4845ms step_avg:61.33ms
step:80/2330 train_time:4908ms step_avg:61.35ms
step:81/2330 train_time:4967ms step_avg:61.32ms
step:82/2330 train_time:5029ms step_avg:61.33ms
step:83/2330 train_time:5088ms step_avg:61.30ms
step:84/2330 train_time:5151ms step_avg:61.32ms
step:85/2330 train_time:5210ms step_avg:61.30ms
step:86/2330 train_time:5273ms step_avg:61.32ms
step:87/2330 train_time:5333ms step_avg:61.30ms
step:88/2330 train_time:5397ms step_avg:61.33ms
step:89/2330 train_time:5457ms step_avg:61.31ms
step:90/2330 train_time:5518ms step_avg:61.32ms
step:91/2330 train_time:5578ms step_avg:61.29ms
step:92/2330 train_time:5641ms step_avg:61.31ms
step:93/2330 train_time:5700ms step_avg:61.29ms
step:94/2330 train_time:5763ms step_avg:61.31ms
step:95/2330 train_time:5823ms step_avg:61.29ms
step:96/2330 train_time:5885ms step_avg:61.30ms
step:97/2330 train_time:5943ms step_avg:61.27ms
step:98/2330 train_time:6006ms step_avg:61.29ms
step:99/2330 train_time:6065ms step_avg:61.26ms
step:100/2330 train_time:6128ms step_avg:61.28ms
step:101/2330 train_time:6187ms step_avg:61.26ms
step:102/2330 train_time:6250ms step_avg:61.27ms
step:103/2330 train_time:6310ms step_avg:61.26ms
step:104/2330 train_time:6372ms step_avg:61.27ms
step:105/2330 train_time:6432ms step_avg:61.25ms
step:106/2330 train_time:6494ms step_avg:61.27ms
step:107/2330 train_time:6554ms step_avg:61.26ms
step:108/2330 train_time:6617ms step_avg:61.26ms
step:109/2330 train_time:6675ms step_avg:61.24ms
step:110/2330 train_time:6738ms step_avg:61.26ms
step:111/2330 train_time:6797ms step_avg:61.23ms
step:112/2330 train_time:6859ms step_avg:61.24ms
step:113/2330 train_time:6919ms step_avg:61.23ms
step:114/2330 train_time:6981ms step_avg:61.24ms
step:115/2330 train_time:7041ms step_avg:61.23ms
step:116/2330 train_time:7104ms step_avg:61.24ms
step:117/2330 train_time:7163ms step_avg:61.22ms
step:118/2330 train_time:7225ms step_avg:61.23ms
step:119/2330 train_time:7284ms step_avg:61.21ms
step:120/2330 train_time:7347ms step_avg:61.22ms
step:121/2330 train_time:7406ms step_avg:61.20ms
step:122/2330 train_time:7467ms step_avg:61.21ms
step:123/2330 train_time:7527ms step_avg:61.19ms
step:124/2330 train_time:7589ms step_avg:61.20ms
step:125/2330 train_time:7649ms step_avg:61.19ms
step:126/2330 train_time:7712ms step_avg:61.20ms
step:127/2330 train_time:7771ms step_avg:61.19ms
step:128/2330 train_time:7833ms step_avg:61.19ms
step:129/2330 train_time:7892ms step_avg:61.18ms
step:130/2330 train_time:7954ms step_avg:61.19ms
step:131/2330 train_time:8014ms step_avg:61.17ms
step:132/2330 train_time:8077ms step_avg:61.19ms
step:133/2330 train_time:8135ms step_avg:61.17ms
step:134/2330 train_time:8197ms step_avg:61.17ms
step:135/2330 train_time:8257ms step_avg:61.16ms
step:136/2330 train_time:8319ms step_avg:61.17ms
step:137/2330 train_time:8378ms step_avg:61.16ms
step:138/2330 train_time:8440ms step_avg:61.16ms
step:139/2330 train_time:8500ms step_avg:61.15ms
step:140/2330 train_time:8563ms step_avg:61.16ms
step:141/2330 train_time:8622ms step_avg:61.15ms
step:142/2330 train_time:8685ms step_avg:61.16ms
step:143/2330 train_time:8744ms step_avg:61.14ms
step:144/2330 train_time:8806ms step_avg:61.15ms
step:145/2330 train_time:8864ms step_avg:61.13ms
step:146/2330 train_time:8926ms step_avg:61.14ms
step:147/2330 train_time:8986ms step_avg:61.13ms
step:148/2330 train_time:9048ms step_avg:61.14ms
step:149/2330 train_time:9108ms step_avg:61.13ms
step:150/2330 train_time:9171ms step_avg:61.14ms
step:151/2330 train_time:9230ms step_avg:61.13ms
step:152/2330 train_time:9292ms step_avg:61.13ms
step:153/2330 train_time:9353ms step_avg:61.13ms
step:154/2330 train_time:9415ms step_avg:61.14ms
step:155/2330 train_time:9474ms step_avg:61.13ms
step:156/2330 train_time:9536ms step_avg:61.13ms
step:157/2330 train_time:9596ms step_avg:61.12ms
step:158/2330 train_time:9659ms step_avg:61.13ms
step:159/2330 train_time:9718ms step_avg:61.12ms
step:160/2330 train_time:9781ms step_avg:61.13ms
step:161/2330 train_time:9840ms step_avg:61.12ms
step:162/2330 train_time:9903ms step_avg:61.13ms
step:163/2330 train_time:9961ms step_avg:61.11ms
step:164/2330 train_time:10023ms step_avg:61.12ms
step:165/2330 train_time:10082ms step_avg:61.10ms
step:166/2330 train_time:10145ms step_avg:61.11ms
step:167/2330 train_time:10204ms step_avg:61.10ms
step:168/2330 train_time:10266ms step_avg:61.11ms
step:169/2330 train_time:10326ms step_avg:61.10ms
step:170/2330 train_time:10388ms step_avg:61.11ms
step:171/2330 train_time:10448ms step_avg:61.10ms
step:172/2330 train_time:10510ms step_avg:61.11ms
step:173/2330 train_time:10569ms step_avg:61.10ms
step:174/2330 train_time:10631ms step_avg:61.10ms
step:175/2330 train_time:10691ms step_avg:61.09ms
step:176/2330 train_time:10754ms step_avg:61.10ms
step:177/2330 train_time:10814ms step_avg:61.10ms
step:178/2330 train_time:10876ms step_avg:61.10ms
step:179/2330 train_time:10935ms step_avg:61.09ms
step:180/2330 train_time:10997ms step_avg:61.09ms
step:181/2330 train_time:11056ms step_avg:61.08ms
step:182/2330 train_time:11118ms step_avg:61.09ms
step:183/2330 train_time:11178ms step_avg:61.08ms
step:184/2330 train_time:11241ms step_avg:61.09ms
step:185/2330 train_time:11301ms step_avg:61.09ms
step:186/2330 train_time:11363ms step_avg:61.09ms
step:187/2330 train_time:11422ms step_avg:61.08ms
step:188/2330 train_time:11485ms step_avg:61.09ms
step:189/2330 train_time:11544ms step_avg:61.08ms
step:190/2330 train_time:11607ms step_avg:61.09ms
step:191/2330 train_time:11667ms step_avg:61.08ms
step:192/2330 train_time:11729ms step_avg:61.09ms
step:193/2330 train_time:11788ms step_avg:61.08ms
step:194/2330 train_time:11850ms step_avg:61.08ms
step:195/2330 train_time:11909ms step_avg:61.07ms
step:196/2330 train_time:11971ms step_avg:61.08ms
step:197/2330 train_time:12031ms step_avg:61.07ms
step:198/2330 train_time:12094ms step_avg:61.08ms
step:199/2330 train_time:12154ms step_avg:61.07ms
step:200/2330 train_time:12216ms step_avg:61.08ms
step:201/2330 train_time:12275ms step_avg:61.07ms
step:202/2330 train_time:12337ms step_avg:61.07ms
step:203/2330 train_time:12396ms step_avg:61.06ms
step:204/2330 train_time:12458ms step_avg:61.07ms
step:205/2330 train_time:12517ms step_avg:61.06ms
step:206/2330 train_time:12580ms step_avg:61.07ms
step:207/2330 train_time:12640ms step_avg:61.06ms
step:208/2330 train_time:12703ms step_avg:61.07ms
step:209/2330 train_time:12763ms step_avg:61.07ms
step:210/2330 train_time:12825ms step_avg:61.07ms
step:211/2330 train_time:12884ms step_avg:61.06ms
step:212/2330 train_time:12946ms step_avg:61.07ms
step:213/2330 train_time:13005ms step_avg:61.06ms
step:214/2330 train_time:13068ms step_avg:61.06ms
step:215/2330 train_time:13127ms step_avg:61.06ms
step:216/2330 train_time:13189ms step_avg:61.06ms
step:217/2330 train_time:13249ms step_avg:61.05ms
step:218/2330 train_time:13311ms step_avg:61.06ms
step:219/2330 train_time:13372ms step_avg:61.06ms
step:220/2330 train_time:13434ms step_avg:61.06ms
step:221/2330 train_time:13493ms step_avg:61.05ms
step:222/2330 train_time:13556ms step_avg:61.06ms
step:223/2330 train_time:13616ms step_avg:61.06ms
step:224/2330 train_time:13677ms step_avg:61.06ms
step:225/2330 train_time:13737ms step_avg:61.05ms
step:226/2330 train_time:13800ms step_avg:61.06ms
step:227/2330 train_time:13860ms step_avg:61.06ms
step:228/2330 train_time:13922ms step_avg:61.06ms
step:229/2330 train_time:13982ms step_avg:61.06ms
step:230/2330 train_time:14045ms step_avg:61.07ms
step:231/2330 train_time:14105ms step_avg:61.06ms
step:232/2330 train_time:14166ms step_avg:61.06ms
step:233/2330 train_time:14225ms step_avg:61.05ms
step:234/2330 train_time:14287ms step_avg:61.06ms
step:235/2330 train_time:14347ms step_avg:61.05ms
step:236/2330 train_time:14409ms step_avg:61.06ms
step:237/2330 train_time:14469ms step_avg:61.05ms
step:238/2330 train_time:14531ms step_avg:61.05ms
step:239/2330 train_time:14591ms step_avg:61.05ms
step:240/2330 train_time:14654ms step_avg:61.06ms
step:241/2330 train_time:14713ms step_avg:61.05ms
step:242/2330 train_time:14775ms step_avg:61.05ms
step:243/2330 train_time:14835ms step_avg:61.05ms
step:244/2330 train_time:14897ms step_avg:61.05ms
step:245/2330 train_time:14956ms step_avg:61.05ms
step:246/2330 train_time:15019ms step_avg:61.05ms
step:247/2330 train_time:15079ms step_avg:61.05ms
step:248/2330 train_time:15142ms step_avg:61.06ms
step:249/2330 train_time:15201ms step_avg:61.05ms
step:250/2330 train_time:15264ms step_avg:61.06ms
step:250/2330 val_loss:4.0837 train_time:15328ms step_avg:61.31ms
step:251/2330 train_time:15352ms step_avg:61.17ms
step:252/2330 train_time:15389ms step_avg:61.07ms
step:253/2330 train_time:15453ms step_avg:61.08ms
step:254/2330 train_time:15519ms step_avg:61.10ms
step:255/2330 train_time:15578ms step_avg:61.09ms
step:256/2330 train_time:15641ms step_avg:61.10ms
step:257/2330 train_time:15700ms step_avg:61.09ms
step:258/2330 train_time:15761ms step_avg:61.09ms
step:259/2330 train_time:15820ms step_avg:61.08ms
step:260/2330 train_time:15882ms step_avg:61.09ms
step:261/2330 train_time:15941ms step_avg:61.08ms
step:262/2330 train_time:16003ms step_avg:61.08ms
step:263/2330 train_time:16061ms step_avg:61.07ms
step:264/2330 train_time:16122ms step_avg:61.07ms
step:265/2330 train_time:16181ms step_avg:61.06ms
step:266/2330 train_time:16242ms step_avg:61.06ms
step:267/2330 train_time:16302ms step_avg:61.06ms
step:268/2330 train_time:16365ms step_avg:61.06ms
step:269/2330 train_time:16426ms step_avg:61.06ms
step:270/2330 train_time:16489ms step_avg:61.07ms
step:271/2330 train_time:16549ms step_avg:61.07ms
step:272/2330 train_time:16612ms step_avg:61.07ms
step:273/2330 train_time:16671ms step_avg:61.06ms
step:274/2330 train_time:16733ms step_avg:61.07ms
step:275/2330 train_time:16793ms step_avg:61.07ms
step:276/2330 train_time:16855ms step_avg:61.07ms
step:277/2330 train_time:16915ms step_avg:61.06ms
step:278/2330 train_time:16977ms step_avg:61.07ms
step:279/2330 train_time:17036ms step_avg:61.06ms
step:280/2330 train_time:17097ms step_avg:61.06ms
step:281/2330 train_time:17156ms step_avg:61.05ms
step:282/2330 train_time:17218ms step_avg:61.06ms
step:283/2330 train_time:17278ms step_avg:61.05ms
step:284/2330 train_time:17341ms step_avg:61.06ms
step:285/2330 train_time:17401ms step_avg:61.06ms
step:286/2330 train_time:17465ms step_avg:61.07ms
step:287/2330 train_time:17525ms step_avg:61.06ms
step:288/2330 train_time:17588ms step_avg:61.07ms
step:289/2330 train_time:17647ms step_avg:61.06ms
step:290/2330 train_time:17709ms step_avg:61.07ms
step:291/2330 train_time:17768ms step_avg:61.06ms
step:292/2330 train_time:17830ms step_avg:61.06ms
step:293/2330 train_time:17889ms step_avg:61.06ms
step:294/2330 train_time:17951ms step_avg:61.06ms
step:295/2330 train_time:18011ms step_avg:61.05ms
step:296/2330 train_time:18074ms step_avg:61.06ms
step:297/2330 train_time:18134ms step_avg:61.06ms
step:298/2330 train_time:18195ms step_avg:61.06ms
step:299/2330 train_time:18254ms step_avg:61.05ms
step:300/2330 train_time:18316ms step_avg:61.05ms
step:301/2330 train_time:18375ms step_avg:61.05ms
step:302/2330 train_time:18437ms step_avg:61.05ms
step:303/2330 train_time:18497ms step_avg:61.04ms
step:304/2330 train_time:18560ms step_avg:61.05ms
step:305/2330 train_time:18619ms step_avg:61.05ms
step:306/2330 train_time:18682ms step_avg:61.05ms
step:307/2330 train_time:18742ms step_avg:61.05ms
step:308/2330 train_time:18805ms step_avg:61.05ms
step:309/2330 train_time:18864ms step_avg:61.05ms
step:310/2330 train_time:18926ms step_avg:61.05ms
step:311/2330 train_time:18985ms step_avg:61.04ms
step:312/2330 train_time:19047ms step_avg:61.05ms
step:313/2330 train_time:19106ms step_avg:61.04ms
step:314/2330 train_time:19168ms step_avg:61.05ms
step:315/2330 train_time:19228ms step_avg:61.04ms
step:316/2330 train_time:19291ms step_avg:61.05ms
step:317/2330 train_time:19351ms step_avg:61.04ms
step:318/2330 train_time:19413ms step_avg:61.05ms
step:319/2330 train_time:19472ms step_avg:61.04ms
step:320/2330 train_time:19535ms step_avg:61.05ms
step:321/2330 train_time:19595ms step_avg:61.04ms
step:322/2330 train_time:19657ms step_avg:61.05ms
step:323/2330 train_time:19716ms step_avg:61.04ms
step:324/2330 train_time:19779ms step_avg:61.05ms
step:325/2330 train_time:19839ms step_avg:61.04ms
step:326/2330 train_time:19902ms step_avg:61.05ms
step:327/2330 train_time:19961ms step_avg:61.04ms
step:328/2330 train_time:20023ms step_avg:61.05ms
step:329/2330 train_time:20083ms step_avg:61.04ms
step:330/2330 train_time:20145ms step_avg:61.04ms
step:331/2330 train_time:20204ms step_avg:61.04ms
step:332/2330 train_time:20266ms step_avg:61.04ms
step:333/2330 train_time:20325ms step_avg:61.04ms
step:334/2330 train_time:20388ms step_avg:61.04ms
step:335/2330 train_time:20447ms step_avg:61.04ms
step:336/2330 train_time:20510ms step_avg:61.04ms
step:337/2330 train_time:20569ms step_avg:61.04ms
step:338/2330 train_time:20631ms step_avg:61.04ms
step:339/2330 train_time:20691ms step_avg:61.04ms
step:340/2330 train_time:20753ms step_avg:61.04ms
step:341/2330 train_time:20813ms step_avg:61.04ms
step:342/2330 train_time:20876ms step_avg:61.04ms
step:343/2330 train_time:20936ms step_avg:61.04ms
step:344/2330 train_time:20998ms step_avg:61.04ms
step:345/2330 train_time:21058ms step_avg:61.04ms
step:346/2330 train_time:21120ms step_avg:61.04ms
step:347/2330 train_time:21179ms step_avg:61.04ms
step:348/2330 train_time:21242ms step_avg:61.04ms
step:349/2330 train_time:21303ms step_avg:61.04ms
step:350/2330 train_time:21366ms step_avg:61.05ms
step:351/2330 train_time:21424ms step_avg:61.04ms
step:352/2330 train_time:21486ms step_avg:61.04ms
step:353/2330 train_time:21545ms step_avg:61.03ms
step:354/2330 train_time:21607ms step_avg:61.04ms
step:355/2330 train_time:21666ms step_avg:61.03ms
step:356/2330 train_time:21729ms step_avg:61.04ms
step:357/2330 train_time:21789ms step_avg:61.03ms
step:358/2330 train_time:21850ms step_avg:61.03ms
step:359/2330 train_time:21910ms step_avg:61.03ms
step:360/2330 train_time:21972ms step_avg:61.03ms
step:361/2330 train_time:22032ms step_avg:61.03ms
step:362/2330 train_time:22094ms step_avg:61.03ms
step:363/2330 train_time:22154ms step_avg:61.03ms
step:364/2330 train_time:22216ms step_avg:61.03ms
step:365/2330 train_time:22276ms step_avg:61.03ms
step:366/2330 train_time:22338ms step_avg:61.03ms
step:367/2330 train_time:22397ms step_avg:61.03ms
step:368/2330 train_time:22460ms step_avg:61.03ms
step:369/2330 train_time:22519ms step_avg:61.03ms
step:370/2330 train_time:22582ms step_avg:61.03ms
step:371/2330 train_time:22641ms step_avg:61.03ms
step:372/2330 train_time:22704ms step_avg:61.03ms
step:373/2330 train_time:22763ms step_avg:61.03ms
step:374/2330 train_time:22825ms step_avg:61.03ms
step:375/2330 train_time:22885ms step_avg:61.03ms
step:376/2330 train_time:22948ms step_avg:61.03ms
step:377/2330 train_time:23007ms step_avg:61.03ms
step:378/2330 train_time:23068ms step_avg:61.03ms
step:379/2330 train_time:23129ms step_avg:61.03ms
step:380/2330 train_time:23191ms step_avg:61.03ms
step:381/2330 train_time:23250ms step_avg:61.02ms
step:382/2330 train_time:23313ms step_avg:61.03ms
step:383/2330 train_time:23373ms step_avg:61.03ms
step:384/2330 train_time:23436ms step_avg:61.03ms
step:385/2330 train_time:23496ms step_avg:61.03ms
step:386/2330 train_time:23558ms step_avg:61.03ms
step:387/2330 train_time:23617ms step_avg:61.03ms
step:388/2330 train_time:23680ms step_avg:61.03ms
step:389/2330 train_time:23739ms step_avg:61.03ms
step:390/2330 train_time:23803ms step_avg:61.03ms
step:391/2330 train_time:23863ms step_avg:61.03ms
step:392/2330 train_time:23925ms step_avg:61.03ms
step:393/2330 train_time:23984ms step_avg:61.03ms
step:394/2330 train_time:24046ms step_avg:61.03ms
step:395/2330 train_time:24106ms step_avg:61.03ms
step:396/2330 train_time:24168ms step_avg:61.03ms
step:397/2330 train_time:24227ms step_avg:61.02ms
step:398/2330 train_time:24289ms step_avg:61.03ms
step:399/2330 train_time:24348ms step_avg:61.02ms
step:400/2330 train_time:24411ms step_avg:61.03ms
step:401/2330 train_time:24471ms step_avg:61.02ms
step:402/2330 train_time:24533ms step_avg:61.03ms
step:403/2330 train_time:24592ms step_avg:61.02ms
step:404/2330 train_time:24655ms step_avg:61.03ms
step:405/2330 train_time:24715ms step_avg:61.02ms
step:406/2330 train_time:24777ms step_avg:61.03ms
step:407/2330 train_time:24837ms step_avg:61.02ms
step:408/2330 train_time:24899ms step_avg:61.03ms
step:409/2330 train_time:24958ms step_avg:61.02ms
step:410/2330 train_time:25021ms step_avg:61.03ms
step:411/2330 train_time:25081ms step_avg:61.03ms
step:412/2330 train_time:25144ms step_avg:61.03ms
step:413/2330 train_time:25204ms step_avg:61.03ms
step:414/2330 train_time:25266ms step_avg:61.03ms
step:415/2330 train_time:25326ms step_avg:61.03ms
step:416/2330 train_time:25388ms step_avg:61.03ms
step:417/2330 train_time:25447ms step_avg:61.02ms
step:418/2330 train_time:25510ms step_avg:61.03ms
step:419/2330 train_time:25569ms step_avg:61.02ms
step:420/2330 train_time:25631ms step_avg:61.03ms
step:421/2330 train_time:25691ms step_avg:61.02ms
step:422/2330 train_time:25753ms step_avg:61.03ms
step:423/2330 train_time:25813ms step_avg:61.02ms
step:424/2330 train_time:25876ms step_avg:61.03ms
step:425/2330 train_time:25936ms step_avg:61.03ms
step:426/2330 train_time:25998ms step_avg:61.03ms
step:427/2330 train_time:26057ms step_avg:61.02ms
step:428/2330 train_time:26120ms step_avg:61.03ms
step:429/2330 train_time:26180ms step_avg:61.03ms
step:430/2330 train_time:26243ms step_avg:61.03ms
step:431/2330 train_time:26302ms step_avg:61.03ms
step:432/2330 train_time:26365ms step_avg:61.03ms
step:433/2330 train_time:26424ms step_avg:61.02ms
step:434/2330 train_time:26486ms step_avg:61.03ms
step:435/2330 train_time:26546ms step_avg:61.02ms
step:436/2330 train_time:26608ms step_avg:61.03ms
step:437/2330 train_time:26668ms step_avg:61.02ms
step:438/2330 train_time:26730ms step_avg:61.03ms
step:439/2330 train_time:26790ms step_avg:61.02ms
step:440/2330 train_time:26852ms step_avg:61.03ms
step:441/2330 train_time:26912ms step_avg:61.02ms
step:442/2330 train_time:26975ms step_avg:61.03ms
step:443/2330 train_time:27035ms step_avg:61.03ms
step:444/2330 train_time:27097ms step_avg:61.03ms
step:445/2330 train_time:27156ms step_avg:61.02ms
step:446/2330 train_time:27218ms step_avg:61.03ms
step:447/2330 train_time:27277ms step_avg:61.02ms
step:448/2330 train_time:27340ms step_avg:61.03ms
step:449/2330 train_time:27400ms step_avg:61.02ms
step:450/2330 train_time:27463ms step_avg:61.03ms
step:451/2330 train_time:27522ms step_avg:61.02ms
step:452/2330 train_time:27585ms step_avg:61.03ms
step:453/2330 train_time:27644ms step_avg:61.02ms
step:454/2330 train_time:27706ms step_avg:61.03ms
step:455/2330 train_time:27766ms step_avg:61.02ms
step:456/2330 train_time:27828ms step_avg:61.03ms
step:457/2330 train_time:27888ms step_avg:61.02ms
step:458/2330 train_time:27951ms step_avg:61.03ms
step:459/2330 train_time:28010ms step_avg:61.02ms
step:460/2330 train_time:28073ms step_avg:61.03ms
step:461/2330 train_time:28133ms step_avg:61.03ms
step:462/2330 train_time:28196ms step_avg:61.03ms
step:463/2330 train_time:28256ms step_avg:61.03ms
step:464/2330 train_time:28318ms step_avg:61.03ms
step:465/2330 train_time:28378ms step_avg:61.03ms
step:466/2330 train_time:28440ms step_avg:61.03ms
step:467/2330 train_time:28500ms step_avg:61.03ms
step:468/2330 train_time:28563ms step_avg:61.03ms
step:469/2330 train_time:28624ms step_avg:61.03ms
step:470/2330 train_time:28687ms step_avg:61.04ms
step:471/2330 train_time:28745ms step_avg:61.03ms
step:472/2330 train_time:28807ms step_avg:61.03ms
step:473/2330 train_time:28866ms step_avg:61.03ms
step:474/2330 train_time:28928ms step_avg:61.03ms
step:475/2330 train_time:28988ms step_avg:61.03ms
step:476/2330 train_time:29050ms step_avg:61.03ms
step:477/2330 train_time:29110ms step_avg:61.03ms
step:478/2330 train_time:29173ms step_avg:61.03ms
step:479/2330 train_time:29233ms step_avg:61.03ms
step:480/2330 train_time:29296ms step_avg:61.03ms
step:481/2330 train_time:29355ms step_avg:61.03ms
step:482/2330 train_time:29418ms step_avg:61.03ms
step:483/2330 train_time:29478ms step_avg:61.03ms
step:484/2330 train_time:29539ms step_avg:61.03ms
step:485/2330 train_time:29599ms step_avg:61.03ms
step:486/2330 train_time:29661ms step_avg:61.03ms
step:487/2330 train_time:29721ms step_avg:61.03ms
step:488/2330 train_time:29784ms step_avg:61.03ms
step:489/2330 train_time:29843ms step_avg:61.03ms
step:490/2330 train_time:29905ms step_avg:61.03ms
step:491/2330 train_time:29965ms step_avg:61.03ms
step:492/2330 train_time:30027ms step_avg:61.03ms
step:493/2330 train_time:30086ms step_avg:61.03ms
step:494/2330 train_time:30148ms step_avg:61.03ms
step:495/2330 train_time:30208ms step_avg:61.03ms
step:496/2330 train_time:30270ms step_avg:61.03ms
step:497/2330 train_time:30330ms step_avg:61.03ms
step:498/2330 train_time:30393ms step_avg:61.03ms
step:499/2330 train_time:30453ms step_avg:61.03ms
step:500/2330 train_time:30516ms step_avg:61.03ms
step:500/2330 val_loss:3.8228 train_time:30581ms step_avg:61.16ms
step:501/2330 train_time:30605ms step_avg:61.09ms
step:502/2330 train_time:30641ms step_avg:61.04ms
step:503/2330 train_time:30705ms step_avg:61.04ms
step:504/2330 train_time:30770ms step_avg:61.05ms
step:505/2330 train_time:30829ms step_avg:61.05ms
step:506/2330 train_time:30891ms step_avg:61.05ms
step:507/2330 train_time:30950ms step_avg:61.05ms
step:508/2330 train_time:31012ms step_avg:61.05ms
step:509/2330 train_time:31071ms step_avg:61.04ms
step:510/2330 train_time:31132ms step_avg:61.04ms
step:511/2330 train_time:31191ms step_avg:61.04ms
step:512/2330 train_time:31253ms step_avg:61.04ms
step:513/2330 train_time:31312ms step_avg:61.04ms
step:514/2330 train_time:31374ms step_avg:61.04ms
step:515/2330 train_time:31433ms step_avg:61.03ms
step:516/2330 train_time:31495ms step_avg:61.04ms
step:517/2330 train_time:31555ms step_avg:61.04ms
step:518/2330 train_time:31619ms step_avg:61.04ms
step:519/2330 train_time:31679ms step_avg:61.04ms
step:520/2330 train_time:31742ms step_avg:61.04ms
step:521/2330 train_time:31801ms step_avg:61.04ms
step:522/2330 train_time:31864ms step_avg:61.04ms
step:523/2330 train_time:31924ms step_avg:61.04ms
step:524/2330 train_time:31986ms step_avg:61.04ms
step:525/2330 train_time:32045ms step_avg:61.04ms
step:526/2330 train_time:32107ms step_avg:61.04ms
step:527/2330 train_time:32166ms step_avg:61.04ms
step:528/2330 train_time:32228ms step_avg:61.04ms
step:529/2330 train_time:32287ms step_avg:61.03ms
step:530/2330 train_time:32348ms step_avg:61.03ms
step:531/2330 train_time:32407ms step_avg:61.03ms
step:532/2330 train_time:32470ms step_avg:61.03ms
step:533/2330 train_time:32530ms step_avg:61.03ms
step:534/2330 train_time:32593ms step_avg:61.04ms
step:535/2330 train_time:32654ms step_avg:61.03ms
step:536/2330 train_time:32716ms step_avg:61.04ms
step:537/2330 train_time:32776ms step_avg:61.04ms
step:538/2330 train_time:32839ms step_avg:61.04ms
step:539/2330 train_time:32899ms step_avg:61.04ms
step:540/2330 train_time:32961ms step_avg:61.04ms
step:541/2330 train_time:33021ms step_avg:61.04ms
step:542/2330 train_time:33083ms step_avg:61.04ms
step:543/2330 train_time:33143ms step_avg:61.04ms
step:544/2330 train_time:33205ms step_avg:61.04ms
step:545/2330 train_time:33264ms step_avg:61.03ms
step:546/2330 train_time:33327ms step_avg:61.04ms
step:547/2330 train_time:33386ms step_avg:61.03ms
step:548/2330 train_time:33448ms step_avg:61.04ms
step:549/2330 train_time:33507ms step_avg:61.03ms
step:550/2330 train_time:33570ms step_avg:61.04ms
step:551/2330 train_time:33631ms step_avg:61.04ms
step:552/2330 train_time:33693ms step_avg:61.04ms
step:553/2330 train_time:33753ms step_avg:61.04ms
step:554/2330 train_time:33816ms step_avg:61.04ms
step:555/2330 train_time:33876ms step_avg:61.04ms
step:556/2330 train_time:33939ms step_avg:61.04ms
step:557/2330 train_time:33999ms step_avg:61.04ms
step:558/2330 train_time:34061ms step_avg:61.04ms
step:559/2330 train_time:34120ms step_avg:61.04ms
step:560/2330 train_time:34182ms step_avg:61.04ms
step:561/2330 train_time:34242ms step_avg:61.04ms
step:562/2330 train_time:34304ms step_avg:61.04ms
step:563/2330 train_time:34363ms step_avg:61.04ms
step:564/2330 train_time:34426ms step_avg:61.04ms
step:565/2330 train_time:34486ms step_avg:61.04ms
step:566/2330 train_time:34548ms step_avg:61.04ms
step:567/2330 train_time:34608ms step_avg:61.04ms
step:568/2330 train_time:34670ms step_avg:61.04ms
step:569/2330 train_time:34730ms step_avg:61.04ms
step:570/2330 train_time:34792ms step_avg:61.04ms
step:571/2330 train_time:34853ms step_avg:61.04ms
step:572/2330 train_time:34915ms step_avg:61.04ms
step:573/2330 train_time:34974ms step_avg:61.04ms
step:574/2330 train_time:35036ms step_avg:61.04ms
step:575/2330 train_time:35096ms step_avg:61.04ms
step:576/2330 train_time:35158ms step_avg:61.04ms
step:577/2330 train_time:35218ms step_avg:61.04ms
step:578/2330 train_time:35280ms step_avg:61.04ms
step:579/2330 train_time:35340ms step_avg:61.04ms
step:580/2330 train_time:35402ms step_avg:61.04ms
step:581/2330 train_time:35462ms step_avg:61.04ms
step:582/2330 train_time:35526ms step_avg:61.04ms
step:583/2330 train_time:35585ms step_avg:61.04ms
step:584/2330 train_time:35647ms step_avg:61.04ms
step:585/2330 train_time:35707ms step_avg:61.04ms
step:586/2330 train_time:35769ms step_avg:61.04ms
step:587/2330 train_time:35829ms step_avg:61.04ms
step:588/2330 train_time:35891ms step_avg:61.04ms
step:589/2330 train_time:35950ms step_avg:61.04ms
step:590/2330 train_time:36012ms step_avg:61.04ms
step:591/2330 train_time:36072ms step_avg:61.04ms
step:592/2330 train_time:36135ms step_avg:61.04ms
step:593/2330 train_time:36195ms step_avg:61.04ms
step:594/2330 train_time:36257ms step_avg:61.04ms
step:595/2330 train_time:36317ms step_avg:61.04ms
step:596/2330 train_time:36380ms step_avg:61.04ms
step:597/2330 train_time:36439ms step_avg:61.04ms
step:598/2330 train_time:36502ms step_avg:61.04ms
step:599/2330 train_time:36561ms step_avg:61.04ms
step:600/2330 train_time:36625ms step_avg:61.04ms
step:601/2330 train_time:36684ms step_avg:61.04ms
step:602/2330 train_time:36746ms step_avg:61.04ms
step:603/2330 train_time:36806ms step_avg:61.04ms
step:604/2330 train_time:36868ms step_avg:61.04ms
step:605/2330 train_time:36927ms step_avg:61.04ms
step:606/2330 train_time:36989ms step_avg:61.04ms
step:607/2330 train_time:37048ms step_avg:61.03ms
step:608/2330 train_time:37111ms step_avg:61.04ms
step:609/2330 train_time:37170ms step_avg:61.03ms
step:610/2330 train_time:37233ms step_avg:61.04ms
step:611/2330 train_time:37292ms step_avg:61.03ms
step:612/2330 train_time:37356ms step_avg:61.04ms
step:613/2330 train_time:37415ms step_avg:61.04ms
step:614/2330 train_time:37479ms step_avg:61.04ms
step:615/2330 train_time:37539ms step_avg:61.04ms
step:616/2330 train_time:37601ms step_avg:61.04ms
step:617/2330 train_time:37660ms step_avg:61.04ms
step:618/2330 train_time:37723ms step_avg:61.04ms
step:619/2330 train_time:37783ms step_avg:61.04ms
step:620/2330 train_time:37846ms step_avg:61.04ms
step:621/2330 train_time:37905ms step_avg:61.04ms
step:622/2330 train_time:37967ms step_avg:61.04ms
step:623/2330 train_time:38026ms step_avg:61.04ms
step:624/2330 train_time:38088ms step_avg:61.04ms
step:625/2330 train_time:38147ms step_avg:61.04ms
step:626/2330 train_time:38210ms step_avg:61.04ms
step:627/2330 train_time:38270ms step_avg:61.04ms
step:628/2330 train_time:38332ms step_avg:61.04ms
step:629/2330 train_time:38392ms step_avg:61.04ms
step:630/2330 train_time:38454ms step_avg:61.04ms
step:631/2330 train_time:38514ms step_avg:61.04ms
step:632/2330 train_time:38578ms step_avg:61.04ms
step:633/2330 train_time:38637ms step_avg:61.04ms
step:634/2330 train_time:38700ms step_avg:61.04ms
step:635/2330 train_time:38759ms step_avg:61.04ms
step:636/2330 train_time:38821ms step_avg:61.04ms
step:637/2330 train_time:38880ms step_avg:61.04ms
step:638/2330 train_time:38943ms step_avg:61.04ms
step:639/2330 train_time:39003ms step_avg:61.04ms
step:640/2330 train_time:39065ms step_avg:61.04ms
step:641/2330 train_time:39125ms step_avg:61.04ms
step:642/2330 train_time:39187ms step_avg:61.04ms
step:643/2330 train_time:39246ms step_avg:61.04ms
step:644/2330 train_time:39308ms step_avg:61.04ms
step:645/2330 train_time:39367ms step_avg:61.03ms
step:646/2330 train_time:39430ms step_avg:61.04ms
step:647/2330 train_time:39490ms step_avg:61.03ms
step:648/2330 train_time:39553ms step_avg:61.04ms
step:649/2330 train_time:39613ms step_avg:61.04ms
step:650/2330 train_time:39675ms step_avg:61.04ms
step:651/2330 train_time:39735ms step_avg:61.04ms
step:652/2330 train_time:39798ms step_avg:61.04ms
step:653/2330 train_time:39857ms step_avg:61.04ms
step:654/2330 train_time:39919ms step_avg:61.04ms
step:655/2330 train_time:39978ms step_avg:61.04ms
step:656/2330 train_time:40041ms step_avg:61.04ms
step:657/2330 train_time:40100ms step_avg:61.04ms
step:658/2330 train_time:40164ms step_avg:61.04ms
step:659/2330 train_time:40224ms step_avg:61.04ms
step:660/2330 train_time:40286ms step_avg:61.04ms
step:661/2330 train_time:40346ms step_avg:61.04ms
step:662/2330 train_time:40408ms step_avg:61.04ms
step:663/2330 train_time:40468ms step_avg:61.04ms
step:664/2330 train_time:40530ms step_avg:61.04ms
step:665/2330 train_time:40590ms step_avg:61.04ms
step:666/2330 train_time:40652ms step_avg:61.04ms
step:667/2330 train_time:40712ms step_avg:61.04ms
step:668/2330 train_time:40775ms step_avg:61.04ms
step:669/2330 train_time:40834ms step_avg:61.04ms
step:670/2330 train_time:40897ms step_avg:61.04ms
step:671/2330 train_time:40957ms step_avg:61.04ms
step:672/2330 train_time:41018ms step_avg:61.04ms
step:673/2330 train_time:41078ms step_avg:61.04ms
step:674/2330 train_time:41141ms step_avg:61.04ms
step:675/2330 train_time:41200ms step_avg:61.04ms
step:676/2330 train_time:41264ms step_avg:61.04ms
step:677/2330 train_time:41324ms step_avg:61.04ms
step:678/2330 train_time:41386ms step_avg:61.04ms
step:679/2330 train_time:41445ms step_avg:61.04ms
step:680/2330 train_time:41508ms step_avg:61.04ms
step:681/2330 train_time:41569ms step_avg:61.04ms
step:682/2330 train_time:41631ms step_avg:61.04ms
step:683/2330 train_time:41690ms step_avg:61.04ms
step:684/2330 train_time:41753ms step_avg:61.04ms
step:685/2330 train_time:41812ms step_avg:61.04ms
step:686/2330 train_time:41875ms step_avg:61.04ms
step:687/2330 train_time:41934ms step_avg:61.04ms
step:688/2330 train_time:41997ms step_avg:61.04ms
step:689/2330 train_time:42057ms step_avg:61.04ms
step:690/2330 train_time:42119ms step_avg:61.04ms
step:691/2330 train_time:42178ms step_avg:61.04ms
step:692/2330 train_time:42241ms step_avg:61.04ms
step:693/2330 train_time:42300ms step_avg:61.04ms
step:694/2330 train_time:42363ms step_avg:61.04ms
step:695/2330 train_time:42423ms step_avg:61.04ms
step:696/2330 train_time:42487ms step_avg:61.04ms
step:697/2330 train_time:42546ms step_avg:61.04ms
step:698/2330 train_time:42608ms step_avg:61.04ms
step:699/2330 train_time:42668ms step_avg:61.04ms
step:700/2330 train_time:42730ms step_avg:61.04ms
step:701/2330 train_time:42790ms step_avg:61.04ms
step:702/2330 train_time:42852ms step_avg:61.04ms
step:703/2330 train_time:42911ms step_avg:61.04ms
step:704/2330 train_time:42973ms step_avg:61.04ms
step:705/2330 train_time:43033ms step_avg:61.04ms
step:706/2330 train_time:43096ms step_avg:61.04ms
step:707/2330 train_time:43156ms step_avg:61.04ms
step:708/2330 train_time:43219ms step_avg:61.04ms
step:709/2330 train_time:43278ms step_avg:61.04ms
step:710/2330 train_time:43341ms step_avg:61.04ms
step:711/2330 train_time:43400ms step_avg:61.04ms
step:712/2330 train_time:43463ms step_avg:61.04ms
step:713/2330 train_time:43524ms step_avg:61.04ms
step:714/2330 train_time:43586ms step_avg:61.05ms
step:715/2330 train_time:43646ms step_avg:61.04ms
step:716/2330 train_time:43709ms step_avg:61.05ms
step:717/2330 train_time:43768ms step_avg:61.04ms
step:718/2330 train_time:43831ms step_avg:61.05ms
step:719/2330 train_time:43890ms step_avg:61.04ms
step:720/2330 train_time:43952ms step_avg:61.05ms
step:721/2330 train_time:44012ms step_avg:61.04ms
step:722/2330 train_time:44074ms step_avg:61.04ms
step:723/2330 train_time:44134ms step_avg:61.04ms
step:724/2330 train_time:44196ms step_avg:61.04ms
step:725/2330 train_time:44256ms step_avg:61.04ms
step:726/2330 train_time:44318ms step_avg:61.04ms
step:727/2330 train_time:44378ms step_avg:61.04ms
step:728/2330 train_time:44440ms step_avg:61.04ms
step:729/2330 train_time:44500ms step_avg:61.04ms
step:730/2330 train_time:44562ms step_avg:61.04ms
step:731/2330 train_time:44622ms step_avg:61.04ms
step:732/2330 train_time:44685ms step_avg:61.05ms
step:733/2330 train_time:44745ms step_avg:61.04ms
step:734/2330 train_time:44808ms step_avg:61.05ms
step:735/2330 train_time:44868ms step_avg:61.05ms
step:736/2330 train_time:44931ms step_avg:61.05ms
step:737/2330 train_time:44990ms step_avg:61.04ms
step:738/2330 train_time:45052ms step_avg:61.05ms
step:739/2330 train_time:45110ms step_avg:61.04ms
step:740/2330 train_time:45173ms step_avg:61.04ms
step:741/2330 train_time:45232ms step_avg:61.04ms
step:742/2330 train_time:45295ms step_avg:61.04ms
step:743/2330 train_time:45355ms step_avg:61.04ms
step:744/2330 train_time:45418ms step_avg:61.05ms
step:745/2330 train_time:45478ms step_avg:61.04ms
step:746/2330 train_time:45540ms step_avg:61.05ms
step:747/2330 train_time:45599ms step_avg:61.04ms
step:748/2330 train_time:45661ms step_avg:61.04ms
step:749/2330 train_time:45721ms step_avg:61.04ms
step:750/2330 train_time:45784ms step_avg:61.05ms
step:750/2330 val_loss:3.6956 train_time:45849ms step_avg:61.13ms
step:751/2330 train_time:45873ms step_avg:61.08ms
step:752/2330 train_time:45910ms step_avg:61.05ms
step:753/2330 train_time:45972ms step_avg:61.05ms
step:754/2330 train_time:46036ms step_avg:61.06ms
step:755/2330 train_time:46095ms step_avg:61.05ms
step:756/2330 train_time:46158ms step_avg:61.06ms
step:757/2330 train_time:46217ms step_avg:61.05ms
step:758/2330 train_time:46279ms step_avg:61.05ms
step:759/2330 train_time:46337ms step_avg:61.05ms
step:760/2330 train_time:46399ms step_avg:61.05ms
step:761/2330 train_time:46457ms step_avg:61.05ms
step:762/2330 train_time:46519ms step_avg:61.05ms
step:763/2330 train_time:46577ms step_avg:61.04ms
step:764/2330 train_time:46639ms step_avg:61.05ms
step:765/2330 train_time:46698ms step_avg:61.04ms
step:766/2330 train_time:46761ms step_avg:61.05ms
step:767/2330 train_time:46822ms step_avg:61.05ms
step:768/2330 train_time:46886ms step_avg:61.05ms
step:769/2330 train_time:46947ms step_avg:61.05ms
step:770/2330 train_time:47011ms step_avg:61.05ms
step:771/2330 train_time:47071ms step_avg:61.05ms
step:772/2330 train_time:47135ms step_avg:61.06ms
step:773/2330 train_time:47194ms step_avg:61.05ms
step:774/2330 train_time:47257ms step_avg:61.06ms
step:775/2330 train_time:47317ms step_avg:61.05ms
step:776/2330 train_time:47379ms step_avg:61.06ms
step:777/2330 train_time:47439ms step_avg:61.05ms
step:778/2330 train_time:47501ms step_avg:61.06ms
step:779/2330 train_time:47561ms step_avg:61.05ms
step:780/2330 train_time:47623ms step_avg:61.06ms
step:781/2330 train_time:47682ms step_avg:61.05ms
step:782/2330 train_time:47745ms step_avg:61.06ms
step:783/2330 train_time:47805ms step_avg:61.05ms
step:784/2330 train_time:47869ms step_avg:61.06ms
step:785/2330 train_time:47929ms step_avg:61.06ms
step:786/2330 train_time:47993ms step_avg:61.06ms
step:787/2330 train_time:48053ms step_avg:61.06ms
step:788/2330 train_time:48117ms step_avg:61.06ms
step:789/2330 train_time:48177ms step_avg:61.06ms
step:790/2330 train_time:48240ms step_avg:61.06ms
step:791/2330 train_time:48300ms step_avg:61.06ms
step:792/2330 train_time:48363ms step_avg:61.06ms
step:793/2330 train_time:48422ms step_avg:61.06ms
step:794/2330 train_time:48485ms step_avg:61.06ms
step:795/2330 train_time:48544ms step_avg:61.06ms
step:796/2330 train_time:48607ms step_avg:61.06ms
step:797/2330 train_time:48667ms step_avg:61.06ms
step:798/2330 train_time:48730ms step_avg:61.06ms
step:799/2330 train_time:48790ms step_avg:61.06ms
step:800/2330 train_time:48853ms step_avg:61.07ms
step:801/2330 train_time:48913ms step_avg:61.06ms
step:802/2330 train_time:48977ms step_avg:61.07ms
step:803/2330 train_time:49037ms step_avg:61.07ms
step:804/2330 train_time:49100ms step_avg:61.07ms
step:805/2330 train_time:49160ms step_avg:61.07ms
step:806/2330 train_time:49224ms step_avg:61.07ms
step:807/2330 train_time:49283ms step_avg:61.07ms
step:808/2330 train_time:49346ms step_avg:61.07ms
step:809/2330 train_time:49406ms step_avg:61.07ms
step:810/2330 train_time:49469ms step_avg:61.07ms
step:811/2330 train_time:49529ms step_avg:61.07ms
step:812/2330 train_time:49592ms step_avg:61.07ms
step:813/2330 train_time:49652ms step_avg:61.07ms
step:814/2330 train_time:49716ms step_avg:61.08ms
step:815/2330 train_time:49776ms step_avg:61.07ms
step:816/2330 train_time:49839ms step_avg:61.08ms
step:817/2330 train_time:49899ms step_avg:61.08ms
step:818/2330 train_time:49962ms step_avg:61.08ms
step:819/2330 train_time:50023ms step_avg:61.08ms
step:820/2330 train_time:50085ms step_avg:61.08ms
step:821/2330 train_time:50146ms step_avg:61.08ms
step:822/2330 train_time:50209ms step_avg:61.08ms
step:823/2330 train_time:50269ms step_avg:61.08ms
step:824/2330 train_time:50332ms step_avg:61.08ms
step:825/2330 train_time:50392ms step_avg:61.08ms
step:826/2330 train_time:50456ms step_avg:61.08ms
step:827/2330 train_time:50515ms step_avg:61.08ms
step:828/2330 train_time:50578ms step_avg:61.08ms
step:829/2330 train_time:50637ms step_avg:61.08ms
step:830/2330 train_time:50700ms step_avg:61.08ms
step:831/2330 train_time:50760ms step_avg:61.08ms
step:832/2330 train_time:50823ms step_avg:61.09ms
step:833/2330 train_time:50882ms step_avg:61.08ms
step:834/2330 train_time:50945ms step_avg:61.09ms
step:835/2330 train_time:51005ms step_avg:61.08ms
step:836/2330 train_time:51069ms step_avg:61.09ms
step:837/2330 train_time:51129ms step_avg:61.09ms
step:838/2330 train_time:51192ms step_avg:61.09ms
step:839/2330 train_time:51251ms step_avg:61.09ms
step:840/2330 train_time:51314ms step_avg:61.09ms
step:841/2330 train_time:51374ms step_avg:61.09ms
step:842/2330 train_time:51437ms step_avg:61.09ms
step:843/2330 train_time:51497ms step_avg:61.09ms
step:844/2330 train_time:51560ms step_avg:61.09ms
step:845/2330 train_time:51620ms step_avg:61.09ms
step:846/2330 train_time:51683ms step_avg:61.09ms
step:847/2330 train_time:51743ms step_avg:61.09ms
step:848/2330 train_time:51805ms step_avg:61.09ms
step:849/2330 train_time:51864ms step_avg:61.09ms
step:850/2330 train_time:51927ms step_avg:61.09ms
step:851/2330 train_time:51987ms step_avg:61.09ms
step:852/2330 train_time:52051ms step_avg:61.09ms
step:853/2330 train_time:52111ms step_avg:61.09ms
step:854/2330 train_time:52174ms step_avg:61.09ms
step:855/2330 train_time:52233ms step_avg:61.09ms
step:856/2330 train_time:52296ms step_avg:61.09ms
step:857/2330 train_time:52356ms step_avg:61.09ms
step:858/2330 train_time:52419ms step_avg:61.09ms
step:859/2330 train_time:52478ms step_avg:61.09ms
step:860/2330 train_time:52542ms step_avg:61.09ms
step:861/2330 train_time:52601ms step_avg:61.09ms
step:862/2330 train_time:52665ms step_avg:61.10ms
step:863/2330 train_time:52724ms step_avg:61.09ms
step:864/2330 train_time:52787ms step_avg:61.10ms
step:865/2330 train_time:52847ms step_avg:61.09ms
step:866/2330 train_time:52910ms step_avg:61.10ms
step:867/2330 train_time:52970ms step_avg:61.10ms
step:868/2330 train_time:53033ms step_avg:61.10ms
step:869/2330 train_time:53093ms step_avg:61.10ms
step:870/2330 train_time:53156ms step_avg:61.10ms
step:871/2330 train_time:53216ms step_avg:61.10ms
step:872/2330 train_time:53279ms step_avg:61.10ms
step:873/2330 train_time:53339ms step_avg:61.10ms
step:874/2330 train_time:53402ms step_avg:61.10ms
step:875/2330 train_time:53463ms step_avg:61.10ms
step:876/2330 train_time:53526ms step_avg:61.10ms
step:877/2330 train_time:53586ms step_avg:61.10ms
step:878/2330 train_time:53648ms step_avg:61.10ms
step:879/2330 train_time:53708ms step_avg:61.10ms
step:880/2330 train_time:53771ms step_avg:61.10ms
step:881/2330 train_time:53830ms step_avg:61.10ms
step:882/2330 train_time:53893ms step_avg:61.10ms
step:883/2330 train_time:53953ms step_avg:61.10ms
step:884/2330 train_time:54016ms step_avg:61.10ms
step:885/2330 train_time:54076ms step_avg:61.10ms
step:886/2330 train_time:54139ms step_avg:61.10ms
step:887/2330 train_time:54199ms step_avg:61.10ms
step:888/2330 train_time:54262ms step_avg:61.11ms
step:889/2330 train_time:54322ms step_avg:61.10ms
step:890/2330 train_time:54384ms step_avg:61.11ms
step:891/2330 train_time:54444ms step_avg:61.10ms
step:892/2330 train_time:54507ms step_avg:61.11ms
step:893/2330 train_time:54567ms step_avg:61.11ms
step:894/2330 train_time:54630ms step_avg:61.11ms
step:895/2330 train_time:54690ms step_avg:61.11ms
step:896/2330 train_time:54753ms step_avg:61.11ms
step:897/2330 train_time:54813ms step_avg:61.11ms
step:898/2330 train_time:54876ms step_avg:61.11ms
step:899/2330 train_time:54936ms step_avg:61.11ms
step:900/2330 train_time:54999ms step_avg:61.11ms
step:901/2330 train_time:55059ms step_avg:61.11ms
step:902/2330 train_time:55122ms step_avg:61.11ms
step:903/2330 train_time:55182ms step_avg:61.11ms
step:904/2330 train_time:55245ms step_avg:61.11ms
step:905/2330 train_time:55305ms step_avg:61.11ms
step:906/2330 train_time:55367ms step_avg:61.11ms
step:907/2330 train_time:55427ms step_avg:61.11ms
step:908/2330 train_time:55490ms step_avg:61.11ms
step:909/2330 train_time:55551ms step_avg:61.11ms
step:910/2330 train_time:55614ms step_avg:61.11ms
step:911/2330 train_time:55674ms step_avg:61.11ms
step:912/2330 train_time:55738ms step_avg:61.12ms
step:913/2330 train_time:55797ms step_avg:61.11ms
step:914/2330 train_time:55861ms step_avg:61.12ms
step:915/2330 train_time:55920ms step_avg:61.12ms
step:916/2330 train_time:55983ms step_avg:61.12ms
step:917/2330 train_time:56043ms step_avg:61.12ms
step:918/2330 train_time:56106ms step_avg:61.12ms
step:919/2330 train_time:56166ms step_avg:61.12ms
step:920/2330 train_time:56229ms step_avg:61.12ms
step:921/2330 train_time:56289ms step_avg:61.12ms
step:922/2330 train_time:56352ms step_avg:61.12ms
step:923/2330 train_time:56412ms step_avg:61.12ms
step:924/2330 train_time:56476ms step_avg:61.12ms
step:925/2330 train_time:56535ms step_avg:61.12ms
step:926/2330 train_time:56598ms step_avg:61.12ms
step:927/2330 train_time:56658ms step_avg:61.12ms
step:928/2330 train_time:56721ms step_avg:61.12ms
step:929/2330 train_time:56781ms step_avg:61.12ms
step:930/2330 train_time:56844ms step_avg:61.12ms
step:931/2330 train_time:56903ms step_avg:61.12ms
step:932/2330 train_time:56966ms step_avg:61.12ms
step:933/2330 train_time:57026ms step_avg:61.12ms
step:934/2330 train_time:57088ms step_avg:61.12ms
step:935/2330 train_time:57148ms step_avg:61.12ms
step:936/2330 train_time:57212ms step_avg:61.12ms
step:937/2330 train_time:57272ms step_avg:61.12ms
step:938/2330 train_time:57335ms step_avg:61.12ms
step:939/2330 train_time:57395ms step_avg:61.12ms
step:940/2330 train_time:57458ms step_avg:61.13ms
step:941/2330 train_time:57517ms step_avg:61.12ms
step:942/2330 train_time:57580ms step_avg:61.12ms
step:943/2330 train_time:57640ms step_avg:61.12ms
step:944/2330 train_time:57703ms step_avg:61.13ms
step:945/2330 train_time:57763ms step_avg:61.12ms
step:946/2330 train_time:57826ms step_avg:61.13ms
step:947/2330 train_time:57886ms step_avg:61.13ms
step:948/2330 train_time:57949ms step_avg:61.13ms
step:949/2330 train_time:58009ms step_avg:61.13ms
step:950/2330 train_time:58072ms step_avg:61.13ms
step:951/2330 train_time:58132ms step_avg:61.13ms
step:952/2330 train_time:58195ms step_avg:61.13ms
step:953/2330 train_time:58255ms step_avg:61.13ms
step:954/2330 train_time:58318ms step_avg:61.13ms
step:955/2330 train_time:58378ms step_avg:61.13ms
step:956/2330 train_time:58441ms step_avg:61.13ms
step:957/2330 train_time:58501ms step_avg:61.13ms
step:958/2330 train_time:58563ms step_avg:61.13ms
step:959/2330 train_time:58623ms step_avg:61.13ms
step:960/2330 train_time:58686ms step_avg:61.13ms
step:961/2330 train_time:58746ms step_avg:61.13ms
step:962/2330 train_time:58810ms step_avg:61.13ms
step:963/2330 train_time:58870ms step_avg:61.13ms
step:964/2330 train_time:58933ms step_avg:61.13ms
step:965/2330 train_time:58994ms step_avg:61.13ms
step:966/2330 train_time:59057ms step_avg:61.14ms
step:967/2330 train_time:59116ms step_avg:61.13ms
step:968/2330 train_time:59179ms step_avg:61.14ms
step:969/2330 train_time:59239ms step_avg:61.13ms
step:970/2330 train_time:59302ms step_avg:61.14ms
step:971/2330 train_time:59363ms step_avg:61.14ms
step:972/2330 train_time:59426ms step_avg:61.14ms
step:973/2330 train_time:59486ms step_avg:61.14ms
step:974/2330 train_time:59548ms step_avg:61.14ms
step:975/2330 train_time:59608ms step_avg:61.14ms
step:976/2330 train_time:59671ms step_avg:61.14ms
step:977/2330 train_time:59731ms step_avg:61.14ms
step:978/2330 train_time:59794ms step_avg:61.14ms
step:979/2330 train_time:59854ms step_avg:61.14ms
step:980/2330 train_time:59917ms step_avg:61.14ms
step:981/2330 train_time:59976ms step_avg:61.14ms
step:982/2330 train_time:60040ms step_avg:61.14ms
step:983/2330 train_time:60099ms step_avg:61.14ms
step:984/2330 train_time:60162ms step_avg:61.14ms
step:985/2330 train_time:60222ms step_avg:61.14ms
step:986/2330 train_time:60285ms step_avg:61.14ms
step:987/2330 train_time:60345ms step_avg:61.14ms
step:988/2330 train_time:60408ms step_avg:61.14ms
step:989/2330 train_time:60468ms step_avg:61.14ms
step:990/2330 train_time:60531ms step_avg:61.14ms
step:991/2330 train_time:60591ms step_avg:61.14ms
step:992/2330 train_time:60655ms step_avg:61.14ms
step:993/2330 train_time:60715ms step_avg:61.14ms
step:994/2330 train_time:60778ms step_avg:61.14ms
step:995/2330 train_time:60838ms step_avg:61.14ms
step:996/2330 train_time:60901ms step_avg:61.15ms
step:997/2330 train_time:60961ms step_avg:61.14ms
step:998/2330 train_time:61024ms step_avg:61.15ms
step:999/2330 train_time:61084ms step_avg:61.14ms
step:1000/2330 train_time:61146ms step_avg:61.15ms
step:1000/2330 val_loss:3.5848 train_time:61210ms step_avg:61.21ms
step:1001/2330 train_time:61235ms step_avg:61.17ms
step:1002/2330 train_time:61270ms step_avg:61.15ms
step:1003/2330 train_time:61335ms step_avg:61.15ms
step:1004/2330 train_time:61400ms step_avg:61.16ms
step:1005/2330 train_time:61460ms step_avg:61.15ms
step:1006/2330 train_time:61522ms step_avg:61.16ms
step:1007/2330 train_time:61581ms step_avg:61.15ms
step:1008/2330 train_time:61644ms step_avg:61.15ms
step:1009/2330 train_time:61703ms step_avg:61.15ms
step:1010/2330 train_time:61765ms step_avg:61.15ms
step:1011/2330 train_time:61824ms step_avg:61.15ms
step:1012/2330 train_time:61886ms step_avg:61.15ms
step:1013/2330 train_time:61945ms step_avg:61.15ms
step:1014/2330 train_time:62007ms step_avg:61.15ms
step:1015/2330 train_time:62066ms step_avg:61.15ms
step:1016/2330 train_time:62133ms step_avg:61.15ms
step:1017/2330 train_time:62195ms step_avg:61.16ms
step:1018/2330 train_time:62259ms step_avg:61.16ms
step:1019/2330 train_time:62320ms step_avg:61.16ms
step:1020/2330 train_time:62384ms step_avg:61.16ms
step:1021/2330 train_time:62444ms step_avg:61.16ms
step:1022/2330 train_time:62508ms step_avg:61.16ms
step:1023/2330 train_time:62567ms step_avg:61.16ms
step:1024/2330 train_time:62630ms step_avg:61.16ms
step:1025/2330 train_time:62689ms step_avg:61.16ms
step:1026/2330 train_time:62752ms step_avg:61.16ms
step:1027/2330 train_time:62812ms step_avg:61.16ms
step:1028/2330 train_time:62874ms step_avg:61.16ms
step:1029/2330 train_time:62934ms step_avg:61.16ms
step:1030/2330 train_time:62997ms step_avg:61.16ms
step:1031/2330 train_time:63056ms step_avg:61.16ms
step:1032/2330 train_time:63119ms step_avg:61.16ms
step:1033/2330 train_time:63181ms step_avg:61.16ms
step:1034/2330 train_time:63244ms step_avg:61.16ms
step:1035/2330 train_time:63304ms step_avg:61.16ms
step:1036/2330 train_time:63368ms step_avg:61.17ms
step:1037/2330 train_time:63428ms step_avg:61.16ms
step:1038/2330 train_time:63491ms step_avg:61.17ms
step:1039/2330 train_time:63551ms step_avg:61.17ms
step:1040/2330 train_time:63614ms step_avg:61.17ms
step:1041/2330 train_time:63674ms step_avg:61.17ms
step:1042/2330 train_time:63736ms step_avg:61.17ms
step:1043/2330 train_time:63796ms step_avg:61.17ms
step:1044/2330 train_time:63859ms step_avg:61.17ms
step:1045/2330 train_time:63919ms step_avg:61.17ms
step:1046/2330 train_time:63981ms step_avg:61.17ms
step:1047/2330 train_time:64041ms step_avg:61.17ms
step:1048/2330 train_time:64104ms step_avg:61.17ms
step:1049/2330 train_time:64164ms step_avg:61.17ms
step:1050/2330 train_time:64227ms step_avg:61.17ms
step:1051/2330 train_time:64287ms step_avg:61.17ms
step:1052/2330 train_time:64351ms step_avg:61.17ms
step:1053/2330 train_time:64411ms step_avg:61.17ms
step:1054/2330 train_time:64474ms step_avg:61.17ms
step:1055/2330 train_time:64534ms step_avg:61.17ms
step:1056/2330 train_time:64597ms step_avg:61.17ms
step:1057/2330 train_time:64656ms step_avg:61.17ms
step:1058/2330 train_time:64719ms step_avg:61.17ms
step:1059/2330 train_time:64779ms step_avg:61.17ms
step:1060/2330 train_time:64842ms step_avg:61.17ms
step:1061/2330 train_time:64902ms step_avg:61.17ms
step:1062/2330 train_time:64965ms step_avg:61.17ms
step:1063/2330 train_time:65024ms step_avg:61.17ms
step:1064/2330 train_time:65087ms step_avg:61.17ms
step:1065/2330 train_time:65147ms step_avg:61.17ms
step:1066/2330 train_time:65209ms step_avg:61.17ms
step:1067/2330 train_time:65270ms step_avg:61.17ms
step:1068/2330 train_time:65333ms step_avg:61.17ms
step:1069/2330 train_time:65393ms step_avg:61.17ms
step:1070/2330 train_time:65456ms step_avg:61.17ms
step:1071/2330 train_time:65516ms step_avg:61.17ms
step:1072/2330 train_time:65579ms step_avg:61.17ms
step:1073/2330 train_time:65639ms step_avg:61.17ms
step:1074/2330 train_time:65702ms step_avg:61.17ms
step:1075/2330 train_time:65761ms step_avg:61.17ms
step:1076/2330 train_time:65824ms step_avg:61.17ms
step:1077/2330 train_time:65884ms step_avg:61.17ms
step:1078/2330 train_time:65947ms step_avg:61.18ms
step:1079/2330 train_time:66006ms step_avg:61.17ms
step:1080/2330 train_time:66069ms step_avg:61.17ms
step:1081/2330 train_time:66128ms step_avg:61.17ms
step:1082/2330 train_time:66192ms step_avg:61.18ms
step:1083/2330 train_time:66252ms step_avg:61.17ms
step:1084/2330 train_time:66315ms step_avg:61.18ms
step:1085/2330 train_time:66375ms step_avg:61.17ms
step:1086/2330 train_time:66438ms step_avg:61.18ms
step:1087/2330 train_time:66499ms step_avg:61.18ms
step:1088/2330 train_time:66562ms step_avg:61.18ms
step:1089/2330 train_time:66621ms step_avg:61.18ms
step:1090/2330 train_time:66684ms step_avg:61.18ms
step:1091/2330 train_time:66744ms step_avg:61.18ms
step:1092/2330 train_time:66807ms step_avg:61.18ms
step:1093/2330 train_time:66867ms step_avg:61.18ms
step:1094/2330 train_time:66929ms step_avg:61.18ms
step:1095/2330 train_time:66989ms step_avg:61.18ms
step:1096/2330 train_time:67052ms step_avg:61.18ms
step:1097/2330 train_time:67112ms step_avg:61.18ms
step:1098/2330 train_time:67175ms step_avg:61.18ms
step:1099/2330 train_time:67236ms step_avg:61.18ms
step:1100/2330 train_time:67299ms step_avg:61.18ms
step:1101/2330 train_time:67359ms step_avg:61.18ms
step:1102/2330 train_time:67422ms step_avg:61.18ms
step:1103/2330 train_time:67482ms step_avg:61.18ms
step:1104/2330 train_time:67545ms step_avg:61.18ms
step:1105/2330 train_time:67604ms step_avg:61.18ms
step:1106/2330 train_time:67667ms step_avg:61.18ms
step:1107/2330 train_time:67727ms step_avg:61.18ms
step:1108/2330 train_time:67790ms step_avg:61.18ms
step:1109/2330 train_time:67850ms step_avg:61.18ms
step:1110/2330 train_time:67913ms step_avg:61.18ms
step:1111/2330 train_time:67973ms step_avg:61.18ms
step:1112/2330 train_time:68036ms step_avg:61.18ms
step:1113/2330 train_time:68096ms step_avg:61.18ms
step:1114/2330 train_time:68158ms step_avg:61.18ms
step:1115/2330 train_time:68218ms step_avg:61.18ms
step:1116/2330 train_time:68281ms step_avg:61.18ms
step:1117/2330 train_time:68341ms step_avg:61.18ms
step:1118/2330 train_time:68404ms step_avg:61.18ms
step:1119/2330 train_time:68464ms step_avg:61.18ms
step:1120/2330 train_time:68527ms step_avg:61.18ms
step:1121/2330 train_time:68586ms step_avg:61.18ms
step:1122/2330 train_time:68650ms step_avg:61.18ms
step:1123/2330 train_time:68709ms step_avg:61.18ms
step:1124/2330 train_time:68772ms step_avg:61.18ms
step:1125/2330 train_time:68832ms step_avg:61.18ms
step:1126/2330 train_time:68895ms step_avg:61.19ms
step:1127/2330 train_time:68955ms step_avg:61.18ms
step:1128/2330 train_time:69018ms step_avg:61.19ms
step:1129/2330 train_time:69078ms step_avg:61.18ms
step:1130/2330 train_time:69141ms step_avg:61.19ms
step:1131/2330 train_time:69201ms step_avg:61.19ms
step:1132/2330 train_time:69264ms step_avg:61.19ms
step:1133/2330 train_time:69324ms step_avg:61.19ms
step:1134/2330 train_time:69387ms step_avg:61.19ms
step:1135/2330 train_time:69447ms step_avg:61.19ms
step:1136/2330 train_time:69509ms step_avg:61.19ms
step:1137/2330 train_time:69570ms step_avg:61.19ms
step:1138/2330 train_time:69633ms step_avg:61.19ms
step:1139/2330 train_time:69694ms step_avg:61.19ms
step:1140/2330 train_time:69757ms step_avg:61.19ms
step:1141/2330 train_time:69816ms step_avg:61.19ms
step:1142/2330 train_time:69879ms step_avg:61.19ms
step:1143/2330 train_time:69939ms step_avg:61.19ms
step:1144/2330 train_time:70002ms step_avg:61.19ms
step:1145/2330 train_time:70061ms step_avg:61.19ms
step:1146/2330 train_time:70125ms step_avg:61.19ms
step:1147/2330 train_time:70185ms step_avg:61.19ms
step:1148/2330 train_time:70248ms step_avg:61.19ms
step:1149/2330 train_time:70307ms step_avg:61.19ms
step:1150/2330 train_time:70371ms step_avg:61.19ms
step:1151/2330 train_time:70431ms step_avg:61.19ms
step:1152/2330 train_time:70494ms step_avg:61.19ms
step:1153/2330 train_time:70554ms step_avg:61.19ms
step:1154/2330 train_time:70618ms step_avg:61.19ms
step:1155/2330 train_time:70678ms step_avg:61.19ms
step:1156/2330 train_time:70741ms step_avg:61.19ms
step:1157/2330 train_time:70801ms step_avg:61.19ms
step:1158/2330 train_time:70864ms step_avg:61.20ms
step:1159/2330 train_time:70924ms step_avg:61.19ms
step:1160/2330 train_time:70988ms step_avg:61.20ms
step:1161/2330 train_time:71047ms step_avg:61.19ms
step:1162/2330 train_time:71110ms step_avg:61.20ms
step:1163/2330 train_time:71170ms step_avg:61.20ms
step:1164/2330 train_time:71234ms step_avg:61.20ms
step:1165/2330 train_time:71293ms step_avg:61.20ms
step:1166/2330 train_time:71356ms step_avg:61.20ms
step:1167/2330 train_time:71416ms step_avg:61.20ms
step:1168/2330 train_time:71479ms step_avg:61.20ms
step:1169/2330 train_time:71539ms step_avg:61.20ms
step:1170/2330 train_time:71601ms step_avg:61.20ms
step:1171/2330 train_time:71661ms step_avg:61.20ms
step:1172/2330 train_time:71724ms step_avg:61.20ms
step:1173/2330 train_time:71784ms step_avg:61.20ms
step:1174/2330 train_time:71847ms step_avg:61.20ms
step:1175/2330 train_time:71906ms step_avg:61.20ms
step:1176/2330 train_time:71969ms step_avg:61.20ms
step:1177/2330 train_time:72029ms step_avg:61.20ms
step:1178/2330 train_time:72092ms step_avg:61.20ms
step:1179/2330 train_time:72151ms step_avg:61.20ms
step:1180/2330 train_time:72215ms step_avg:61.20ms
step:1181/2330 train_time:72275ms step_avg:61.20ms
step:1182/2330 train_time:72339ms step_avg:61.20ms
step:1183/2330 train_time:72399ms step_avg:61.20ms
step:1184/2330 train_time:72461ms step_avg:61.20ms
step:1185/2330 train_time:72521ms step_avg:61.20ms
step:1186/2330 train_time:72585ms step_avg:61.20ms
step:1187/2330 train_time:72646ms step_avg:61.20ms
step:1188/2330 train_time:72708ms step_avg:61.20ms
step:1189/2330 train_time:72767ms step_avg:61.20ms
step:1190/2330 train_time:72830ms step_avg:61.20ms
step:1191/2330 train_time:72890ms step_avg:61.20ms
step:1192/2330 train_time:72953ms step_avg:61.20ms
step:1193/2330 train_time:73013ms step_avg:61.20ms
step:1194/2330 train_time:73076ms step_avg:61.20ms
step:1195/2330 train_time:73137ms step_avg:61.20ms
step:1196/2330 train_time:73200ms step_avg:61.20ms
step:1197/2330 train_time:73259ms step_avg:61.20ms
step:1198/2330 train_time:73323ms step_avg:61.20ms
step:1199/2330 train_time:73382ms step_avg:61.20ms
step:1200/2330 train_time:73445ms step_avg:61.20ms
step:1201/2330 train_time:73505ms step_avg:61.20ms
step:1202/2330 train_time:73569ms step_avg:61.21ms
step:1203/2330 train_time:73628ms step_avg:61.20ms
step:1204/2330 train_time:73691ms step_avg:61.21ms
step:1205/2330 train_time:73751ms step_avg:61.20ms
step:1206/2330 train_time:73814ms step_avg:61.21ms
step:1207/2330 train_time:73874ms step_avg:61.20ms
step:1208/2330 train_time:73937ms step_avg:61.21ms
step:1209/2330 train_time:73996ms step_avg:61.20ms
step:1210/2330 train_time:74059ms step_avg:61.21ms
step:1211/2330 train_time:74119ms step_avg:61.20ms
step:1212/2330 train_time:74183ms step_avg:61.21ms
step:1213/2330 train_time:74243ms step_avg:61.21ms
step:1214/2330 train_time:74305ms step_avg:61.21ms
step:1215/2330 train_time:74365ms step_avg:61.21ms
step:1216/2330 train_time:74428ms step_avg:61.21ms
step:1217/2330 train_time:74488ms step_avg:61.21ms
step:1218/2330 train_time:74551ms step_avg:61.21ms
step:1219/2330 train_time:74611ms step_avg:61.21ms
step:1220/2330 train_time:74674ms step_avg:61.21ms
step:1221/2330 train_time:74734ms step_avg:61.21ms
step:1222/2330 train_time:74797ms step_avg:61.21ms
step:1223/2330 train_time:74857ms step_avg:61.21ms
step:1224/2330 train_time:74920ms step_avg:61.21ms
step:1225/2330 train_time:74980ms step_avg:61.21ms
step:1226/2330 train_time:75044ms step_avg:61.21ms
step:1227/2330 train_time:75103ms step_avg:61.21ms
step:1228/2330 train_time:75166ms step_avg:61.21ms
step:1229/2330 train_time:75226ms step_avg:61.21ms
step:1230/2330 train_time:75289ms step_avg:61.21ms
step:1231/2330 train_time:75350ms step_avg:61.21ms
step:1232/2330 train_time:75413ms step_avg:61.21ms
step:1233/2330 train_time:75473ms step_avg:61.21ms
step:1234/2330 train_time:75536ms step_avg:61.21ms
step:1235/2330 train_time:75596ms step_avg:61.21ms
step:1236/2330 train_time:75659ms step_avg:61.21ms
step:1237/2330 train_time:75718ms step_avg:61.21ms
step:1238/2330 train_time:75782ms step_avg:61.21ms
step:1239/2330 train_time:75842ms step_avg:61.21ms
step:1240/2330 train_time:75905ms step_avg:61.21ms
step:1241/2330 train_time:75965ms step_avg:61.21ms
step:1242/2330 train_time:76029ms step_avg:61.21ms
step:1243/2330 train_time:76089ms step_avg:61.21ms
step:1244/2330 train_time:76152ms step_avg:61.22ms
step:1245/2330 train_time:76211ms step_avg:61.21ms
step:1246/2330 train_time:76275ms step_avg:61.22ms
step:1247/2330 train_time:76335ms step_avg:61.22ms
step:1248/2330 train_time:76399ms step_avg:61.22ms
step:1249/2330 train_time:76459ms step_avg:61.22ms
step:1250/2330 train_time:76522ms step_avg:61.22ms
step:1250/2330 val_loss:3.5283 train_time:76586ms step_avg:61.27ms
step:1251/2330 train_time:76610ms step_avg:61.24ms
step:1252/2330 train_time:76647ms step_avg:61.22ms
step:1253/2330 train_time:76711ms step_avg:61.22ms
step:1254/2330 train_time:76778ms step_avg:61.23ms
step:1255/2330 train_time:76838ms step_avg:61.23ms
step:1256/2330 train_time:76901ms step_avg:61.23ms
step:1257/2330 train_time:76960ms step_avg:61.23ms
step:1258/2330 train_time:77023ms step_avg:61.23ms
step:1259/2330 train_time:77082ms step_avg:61.22ms
step:1260/2330 train_time:77144ms step_avg:61.23ms
step:1261/2330 train_time:77203ms step_avg:61.22ms
step:1262/2330 train_time:77266ms step_avg:61.22ms
step:1263/2330 train_time:77325ms step_avg:61.22ms
step:1264/2330 train_time:77388ms step_avg:61.22ms
step:1265/2330 train_time:77447ms step_avg:61.22ms
step:1266/2330 train_time:77510ms step_avg:61.22ms
step:1267/2330 train_time:77570ms step_avg:61.22ms
step:1268/2330 train_time:77634ms step_avg:61.23ms
step:1269/2330 train_time:77701ms step_avg:61.23ms
step:1270/2330 train_time:77759ms step_avg:61.23ms
step:1271/2330 train_time:77820ms step_avg:61.23ms
step:1272/2330 train_time:77883ms step_avg:61.23ms
step:1273/2330 train_time:77943ms step_avg:61.23ms
step:1274/2330 train_time:78007ms step_avg:61.23ms
step:1275/2330 train_time:78066ms step_avg:61.23ms
step:1276/2330 train_time:78129ms step_avg:61.23ms
step:1277/2330 train_time:78189ms step_avg:61.23ms
step:1278/2330 train_time:78251ms step_avg:61.23ms
step:1279/2330 train_time:78310ms step_avg:61.23ms
step:1280/2330 train_time:78373ms step_avg:61.23ms
step:1281/2330 train_time:78433ms step_avg:61.23ms
step:1282/2330 train_time:78496ms step_avg:61.23ms
step:1283/2330 train_time:78556ms step_avg:61.23ms
step:1284/2330 train_time:78619ms step_avg:61.23ms
step:1285/2330 train_time:78680ms step_avg:61.23ms
step:1286/2330 train_time:78743ms step_avg:61.23ms
step:1287/2330 train_time:78804ms step_avg:61.23ms
step:1288/2330 train_time:78868ms step_avg:61.23ms
step:1289/2330 train_time:78928ms step_avg:61.23ms
step:1290/2330 train_time:78991ms step_avg:61.23ms
step:1291/2330 train_time:79051ms step_avg:61.23ms
step:1292/2330 train_time:79114ms step_avg:61.23ms
step:1293/2330 train_time:79173ms step_avg:61.23ms
step:1294/2330 train_time:79236ms step_avg:61.23ms
step:1295/2330 train_time:79296ms step_avg:61.23ms
step:1296/2330 train_time:79359ms step_avg:61.23ms
step:1297/2330 train_time:79418ms step_avg:61.23ms
step:1298/2330 train_time:79481ms step_avg:61.23ms
step:1299/2330 train_time:79540ms step_avg:61.23ms
step:1300/2330 train_time:79603ms step_avg:61.23ms
step:1301/2330 train_time:79664ms step_avg:61.23ms
step:1302/2330 train_time:79728ms step_avg:61.23ms
step:1303/2330 train_time:79789ms step_avg:61.23ms
step:1304/2330 train_time:79852ms step_avg:61.24ms
step:1305/2330 train_time:79912ms step_avg:61.24ms
step:1306/2330 train_time:79975ms step_avg:61.24ms
step:1307/2330 train_time:80035ms step_avg:61.24ms
step:1308/2330 train_time:80098ms step_avg:61.24ms
step:1309/2330 train_time:80158ms step_avg:61.24ms
step:1310/2330 train_time:80221ms step_avg:61.24ms
step:1311/2330 train_time:80280ms step_avg:61.24ms
step:1312/2330 train_time:80343ms step_avg:61.24ms
step:1313/2330 train_time:80402ms step_avg:61.24ms
step:1314/2330 train_time:80465ms step_avg:61.24ms
step:1315/2330 train_time:80524ms step_avg:61.24ms
step:1316/2330 train_time:80588ms step_avg:61.24ms
step:1317/2330 train_time:80649ms step_avg:61.24ms
step:1318/2330 train_time:80712ms step_avg:61.24ms
step:1319/2330 train_time:80772ms step_avg:61.24ms
step:1320/2330 train_time:80835ms step_avg:61.24ms
step:1321/2330 train_time:80894ms step_avg:61.24ms
step:1322/2330 train_time:80958ms step_avg:61.24ms
step:1323/2330 train_time:81017ms step_avg:61.24ms
step:1324/2330 train_time:81081ms step_avg:61.24ms
step:1325/2330 train_time:81140ms step_avg:61.24ms
step:1326/2330 train_time:81204ms step_avg:61.24ms
step:1327/2330 train_time:81263ms step_avg:61.24ms
step:1328/2330 train_time:81326ms step_avg:61.24ms
step:1329/2330 train_time:81385ms step_avg:61.24ms
step:1330/2330 train_time:81448ms step_avg:61.24ms
step:1331/2330 train_time:81509ms step_avg:61.24ms
step:1332/2330 train_time:81572ms step_avg:61.24ms
step:1333/2330 train_time:81632ms step_avg:61.24ms
step:1334/2330 train_time:81695ms step_avg:61.24ms
step:1335/2330 train_time:81755ms step_avg:61.24ms
step:1336/2330 train_time:81819ms step_avg:61.24ms
step:1337/2330 train_time:81879ms step_avg:61.24ms
step:1338/2330 train_time:81942ms step_avg:61.24ms
step:1339/2330 train_time:82003ms step_avg:61.24ms
step:1340/2330 train_time:82067ms step_avg:61.24ms
step:1341/2330 train_time:82128ms step_avg:61.24ms
step:1342/2330 train_time:82190ms step_avg:61.24ms
step:1343/2330 train_time:82250ms step_avg:61.24ms
step:1344/2330 train_time:82313ms step_avg:61.24ms
step:1345/2330 train_time:82373ms step_avg:61.24ms
step:1346/2330 train_time:82437ms step_avg:61.25ms
step:1347/2330 train_time:82496ms step_avg:61.24ms
step:1348/2330 train_time:82560ms step_avg:61.25ms
step:1349/2330 train_time:82620ms step_avg:61.25ms
step:1350/2330 train_time:82682ms step_avg:61.25ms
step:1351/2330 train_time:82742ms step_avg:61.25ms
step:1352/2330 train_time:82806ms step_avg:61.25ms
step:1353/2330 train_time:82866ms step_avg:61.25ms
step:1354/2330 train_time:82930ms step_avg:61.25ms
step:1355/2330 train_time:82991ms step_avg:61.25ms
step:1356/2330 train_time:83054ms step_avg:61.25ms
step:1357/2330 train_time:83113ms step_avg:61.25ms
step:1358/2330 train_time:83176ms step_avg:61.25ms
step:1359/2330 train_time:83236ms step_avg:61.25ms
step:1360/2330 train_time:83299ms step_avg:61.25ms
step:1361/2330 train_time:83358ms step_avg:61.25ms
step:1362/2330 train_time:83421ms step_avg:61.25ms
step:1363/2330 train_time:83481ms step_avg:61.25ms
step:1364/2330 train_time:83544ms step_avg:61.25ms
step:1365/2330 train_time:83603ms step_avg:61.25ms
step:1366/2330 train_time:83667ms step_avg:61.25ms
step:1367/2330 train_time:83727ms step_avg:61.25ms
step:1368/2330 train_time:83791ms step_avg:61.25ms
step:1369/2330 train_time:83851ms step_avg:61.25ms
step:1370/2330 train_time:83914ms step_avg:61.25ms
step:1371/2330 train_time:83974ms step_avg:61.25ms
step:1372/2330 train_time:84037ms step_avg:61.25ms
step:1373/2330 train_time:84097ms step_avg:61.25ms
step:1374/2330 train_time:84160ms step_avg:61.25ms
step:1375/2330 train_time:84220ms step_avg:61.25ms
step:1376/2330 train_time:84282ms step_avg:61.25ms
step:1377/2330 train_time:84342ms step_avg:61.25ms
step:1378/2330 train_time:84405ms step_avg:61.25ms
step:1379/2330 train_time:84466ms step_avg:61.25ms
step:1380/2330 train_time:84528ms step_avg:61.25ms
step:1381/2330 train_time:84588ms step_avg:61.25ms
step:1382/2330 train_time:84651ms step_avg:61.25ms
step:1383/2330 train_time:84711ms step_avg:61.25ms
step:1384/2330 train_time:84774ms step_avg:61.25ms
step:1385/2330 train_time:84834ms step_avg:61.25ms
step:1386/2330 train_time:84897ms step_avg:61.25ms
step:1387/2330 train_time:84957ms step_avg:61.25ms
step:1388/2330 train_time:85021ms step_avg:61.25ms
step:1389/2330 train_time:85080ms step_avg:61.25ms
step:1390/2330 train_time:85143ms step_avg:61.25ms
step:1391/2330 train_time:85203ms step_avg:61.25ms
step:1392/2330 train_time:85267ms step_avg:61.25ms
step:1393/2330 train_time:85327ms step_avg:61.25ms
step:1394/2330 train_time:85390ms step_avg:61.26ms
step:1395/2330 train_time:85450ms step_avg:61.25ms
step:1396/2330 train_time:85513ms step_avg:61.26ms
step:1397/2330 train_time:85573ms step_avg:61.25ms
step:1398/2330 train_time:85636ms step_avg:61.26ms
step:1399/2330 train_time:85696ms step_avg:61.26ms
step:1400/2330 train_time:85759ms step_avg:61.26ms
step:1401/2330 train_time:85819ms step_avg:61.26ms
step:1402/2330 train_time:85882ms step_avg:61.26ms
step:1403/2330 train_time:85942ms step_avg:61.26ms
step:1404/2330 train_time:86005ms step_avg:61.26ms
step:1405/2330 train_time:86066ms step_avg:61.26ms
step:1406/2330 train_time:86129ms step_avg:61.26ms
step:1407/2330 train_time:86189ms step_avg:61.26ms
step:1408/2330 train_time:86252ms step_avg:61.26ms
step:1409/2330 train_time:86311ms step_avg:61.26ms
step:1410/2330 train_time:86375ms step_avg:61.26ms
step:1411/2330 train_time:86435ms step_avg:61.26ms
step:1412/2330 train_time:86498ms step_avg:61.26ms
step:1413/2330 train_time:86557ms step_avg:61.26ms
step:1414/2330 train_time:86620ms step_avg:61.26ms
step:1415/2330 train_time:86680ms step_avg:61.26ms
step:1416/2330 train_time:86743ms step_avg:61.26ms
step:1417/2330 train_time:86802ms step_avg:61.26ms
step:1418/2330 train_time:86865ms step_avg:61.26ms
step:1419/2330 train_time:86926ms step_avg:61.26ms
step:1420/2330 train_time:86989ms step_avg:61.26ms
step:1421/2330 train_time:87050ms step_avg:61.26ms
step:1422/2330 train_time:87113ms step_avg:61.26ms
step:1423/2330 train_time:87173ms step_avg:61.26ms
step:1424/2330 train_time:87236ms step_avg:61.26ms
step:1425/2330 train_time:87295ms step_avg:61.26ms
step:1426/2330 train_time:87358ms step_avg:61.26ms
step:1427/2330 train_time:87418ms step_avg:61.26ms
step:1428/2330 train_time:87482ms step_avg:61.26ms
step:1429/2330 train_time:87542ms step_avg:61.26ms
step:1430/2330 train_time:87605ms step_avg:61.26ms
step:1431/2330 train_time:87665ms step_avg:61.26ms
step:1432/2330 train_time:87728ms step_avg:61.26ms
step:1433/2330 train_time:87787ms step_avg:61.26ms
step:1434/2330 train_time:87850ms step_avg:61.26ms
step:1435/2330 train_time:87910ms step_avg:61.26ms
step:1436/2330 train_time:87974ms step_avg:61.26ms
step:1437/2330 train_time:88033ms step_avg:61.26ms
step:1438/2330 train_time:88096ms step_avg:61.26ms
step:1439/2330 train_time:88157ms step_avg:61.26ms
step:1440/2330 train_time:88220ms step_avg:61.26ms
step:1441/2330 train_time:88280ms step_avg:61.26ms
step:1442/2330 train_time:88342ms step_avg:61.26ms
step:1443/2330 train_time:88402ms step_avg:61.26ms
step:1444/2330 train_time:88465ms step_avg:61.26ms
step:1445/2330 train_time:88525ms step_avg:61.26ms
step:1446/2330 train_time:88588ms step_avg:61.26ms
step:1447/2330 train_time:88648ms step_avg:61.26ms
step:1448/2330 train_time:88711ms step_avg:61.26ms
step:1449/2330 train_time:88771ms step_avg:61.26ms
step:1450/2330 train_time:88834ms step_avg:61.26ms
step:1451/2330 train_time:88894ms step_avg:61.26ms
step:1452/2330 train_time:88956ms step_avg:61.26ms
step:1453/2330 train_time:89016ms step_avg:61.26ms
step:1454/2330 train_time:89079ms step_avg:61.26ms
step:1455/2330 train_time:89138ms step_avg:61.26ms
step:1456/2330 train_time:89201ms step_avg:61.26ms
step:1457/2330 train_time:89260ms step_avg:61.26ms
step:1458/2330 train_time:89323ms step_avg:61.26ms
step:1459/2330 train_time:89384ms step_avg:61.26ms
step:1460/2330 train_time:89447ms step_avg:61.26ms
step:1461/2330 train_time:89506ms step_avg:61.26ms
step:1462/2330 train_time:89570ms step_avg:61.27ms
step:1463/2330 train_time:89630ms step_avg:61.26ms
step:1464/2330 train_time:89693ms step_avg:61.27ms
step:1465/2330 train_time:89753ms step_avg:61.27ms
step:1466/2330 train_time:89817ms step_avg:61.27ms
step:1467/2330 train_time:89876ms step_avg:61.27ms
step:1468/2330 train_time:89939ms step_avg:61.27ms
step:1469/2330 train_time:89999ms step_avg:61.27ms
step:1470/2330 train_time:90061ms step_avg:61.27ms
step:1471/2330 train_time:90121ms step_avg:61.27ms
step:1472/2330 train_time:90185ms step_avg:61.27ms
step:1473/2330 train_time:90245ms step_avg:61.27ms
step:1474/2330 train_time:90308ms step_avg:61.27ms
step:1475/2330 train_time:90368ms step_avg:61.27ms
step:1476/2330 train_time:90431ms step_avg:61.27ms
step:1477/2330 train_time:90491ms step_avg:61.27ms
step:1478/2330 train_time:90554ms step_avg:61.27ms
step:1479/2330 train_time:90614ms step_avg:61.27ms
step:1480/2330 train_time:90677ms step_avg:61.27ms
step:1481/2330 train_time:90737ms step_avg:61.27ms
step:1482/2330 train_time:90799ms step_avg:61.27ms
step:1483/2330 train_time:90859ms step_avg:61.27ms
step:1484/2330 train_time:90922ms step_avg:61.27ms
step:1485/2330 train_time:90982ms step_avg:61.27ms
step:1486/2330 train_time:91045ms step_avg:61.27ms
step:1487/2330 train_time:91104ms step_avg:61.27ms
step:1488/2330 train_time:91168ms step_avg:61.27ms
step:1489/2330 train_time:91228ms step_avg:61.27ms
step:1490/2330 train_time:91291ms step_avg:61.27ms
step:1491/2330 train_time:91351ms step_avg:61.27ms
step:1492/2330 train_time:91414ms step_avg:61.27ms
step:1493/2330 train_time:91473ms step_avg:61.27ms
step:1494/2330 train_time:91537ms step_avg:61.27ms
step:1495/2330 train_time:91596ms step_avg:61.27ms
step:1496/2330 train_time:91659ms step_avg:61.27ms
step:1497/2330 train_time:91719ms step_avg:61.27ms
step:1498/2330 train_time:91782ms step_avg:61.27ms
step:1499/2330 train_time:91842ms step_avg:61.27ms
step:1500/2330 train_time:91904ms step_avg:61.27ms
step:1500/2330 val_loss:3.4834 train_time:91968ms step_avg:61.31ms
step:1501/2330 train_time:91993ms step_avg:61.29ms
step:1502/2330 train_time:92029ms step_avg:61.27ms
step:1503/2330 train_time:92092ms step_avg:61.27ms
step:1504/2330 train_time:92157ms step_avg:61.27ms
step:1505/2330 train_time:92218ms step_avg:61.27ms
step:1506/2330 train_time:92281ms step_avg:61.28ms
step:1507/2330 train_time:92341ms step_avg:61.27ms
step:1508/2330 train_time:92404ms step_avg:61.28ms
step:1509/2330 train_time:92463ms step_avg:61.27ms
step:1510/2330 train_time:92525ms step_avg:61.28ms
step:1511/2330 train_time:92585ms step_avg:61.27ms
step:1512/2330 train_time:92647ms step_avg:61.27ms
step:1513/2330 train_time:92706ms step_avg:61.27ms
step:1514/2330 train_time:92768ms step_avg:61.27ms
step:1515/2330 train_time:92828ms step_avg:61.27ms
step:1516/2330 train_time:92890ms step_avg:61.27ms
step:1517/2330 train_time:92951ms step_avg:61.27ms
step:1518/2330 train_time:93015ms step_avg:61.27ms
step:1519/2330 train_time:93076ms step_avg:61.27ms
step:1520/2330 train_time:93140ms step_avg:61.28ms
step:1521/2330 train_time:93201ms step_avg:61.28ms
step:1522/2330 train_time:93265ms step_avg:61.28ms
step:1523/2330 train_time:93324ms step_avg:61.28ms
step:1524/2330 train_time:93387ms step_avg:61.28ms
step:1525/2330 train_time:93447ms step_avg:61.28ms
step:1526/2330 train_time:93509ms step_avg:61.28ms
step:1527/2330 train_time:93568ms step_avg:61.28ms
step:1528/2330 train_time:93631ms step_avg:61.28ms
step:1529/2330 train_time:93691ms step_avg:61.28ms
step:1530/2330 train_time:93754ms step_avg:61.28ms
step:1531/2330 train_time:93814ms step_avg:61.28ms
step:1532/2330 train_time:93877ms step_avg:61.28ms
step:1533/2330 train_time:93938ms step_avg:61.28ms
step:1534/2330 train_time:94001ms step_avg:61.28ms
step:1535/2330 train_time:94062ms step_avg:61.28ms
step:1536/2330 train_time:94126ms step_avg:61.28ms
step:1537/2330 train_time:94187ms step_avg:61.28ms
step:1538/2330 train_time:94251ms step_avg:61.28ms
step:1539/2330 train_time:94311ms step_avg:61.28ms
step:1540/2330 train_time:94375ms step_avg:61.28ms
step:1541/2330 train_time:94435ms step_avg:61.28ms
step:1542/2330 train_time:94499ms step_avg:61.28ms
step:1543/2330 train_time:94559ms step_avg:61.28ms
step:1544/2330 train_time:94623ms step_avg:61.28ms
step:1545/2330 train_time:94683ms step_avg:61.28ms
step:1546/2330 train_time:94748ms step_avg:61.29ms
step:1547/2330 train_time:94808ms step_avg:61.28ms
step:1548/2330 train_time:94870ms step_avg:61.29ms
step:1549/2330 train_time:94931ms step_avg:61.29ms
step:1550/2330 train_time:94995ms step_avg:61.29ms
step:1551/2330 train_time:95056ms step_avg:61.29ms
step:1552/2330 train_time:95120ms step_avg:61.29ms
step:1553/2330 train_time:95181ms step_avg:61.29ms
step:1554/2330 train_time:95245ms step_avg:61.29ms
step:1555/2330 train_time:95306ms step_avg:61.29ms
step:1556/2330 train_time:95369ms step_avg:61.29ms
step:1557/2330 train_time:95430ms step_avg:61.29ms
step:1558/2330 train_time:95493ms step_avg:61.29ms
step:1559/2330 train_time:95554ms step_avg:61.29ms
step:1560/2330 train_time:95617ms step_avg:61.29ms
step:1561/2330 train_time:95679ms step_avg:61.29ms
step:1562/2330 train_time:95742ms step_avg:61.29ms
step:1563/2330 train_time:95802ms step_avg:61.29ms
step:1564/2330 train_time:95866ms step_avg:61.30ms
step:1565/2330 train_time:95927ms step_avg:61.29ms
step:1566/2330 train_time:95990ms step_avg:61.30ms
step:1567/2330 train_time:96051ms step_avg:61.30ms
step:1568/2330 train_time:96114ms step_avg:61.30ms
step:1569/2330 train_time:96175ms step_avg:61.30ms
step:1570/2330 train_time:96239ms step_avg:61.30ms
step:1571/2330 train_time:96299ms step_avg:61.30ms
step:1572/2330 train_time:96363ms step_avg:61.30ms
step:1573/2330 train_time:96424ms step_avg:61.30ms
step:1574/2330 train_time:96487ms step_avg:61.30ms
step:1575/2330 train_time:96548ms step_avg:61.30ms
step:1576/2330 train_time:96611ms step_avg:61.30ms
step:1577/2330 train_time:96672ms step_avg:61.30ms
step:1578/2330 train_time:96735ms step_avg:61.30ms
step:1579/2330 train_time:96796ms step_avg:61.30ms
step:1580/2330 train_time:96860ms step_avg:61.30ms
step:1581/2330 train_time:96920ms step_avg:61.30ms
step:1582/2330 train_time:96984ms step_avg:61.30ms
step:1583/2330 train_time:97044ms step_avg:61.30ms
step:1584/2330 train_time:97107ms step_avg:61.31ms
step:1585/2330 train_time:97167ms step_avg:61.30ms
step:1586/2330 train_time:97231ms step_avg:61.31ms
step:1587/2330 train_time:97292ms step_avg:61.31ms
step:1588/2330 train_time:97355ms step_avg:61.31ms
step:1589/2330 train_time:97416ms step_avg:61.31ms
step:1590/2330 train_time:97480ms step_avg:61.31ms
step:1591/2330 train_time:97541ms step_avg:61.31ms
step:1592/2330 train_time:97605ms step_avg:61.31ms
step:1593/2330 train_time:97665ms step_avg:61.31ms
step:1594/2330 train_time:97728ms step_avg:61.31ms
step:1595/2330 train_time:97789ms step_avg:61.31ms
step:1596/2330 train_time:97853ms step_avg:61.31ms
step:1597/2330 train_time:97913ms step_avg:61.31ms
step:1598/2330 train_time:97977ms step_avg:61.31ms
step:1599/2330 train_time:98038ms step_avg:61.31ms
step:1600/2330 train_time:98102ms step_avg:61.31ms
step:1601/2330 train_time:98162ms step_avg:61.31ms
step:1602/2330 train_time:98226ms step_avg:61.31ms
step:1603/2330 train_time:98286ms step_avg:61.31ms
step:1604/2330 train_time:98350ms step_avg:61.32ms
step:1605/2330 train_time:98411ms step_avg:61.32ms
step:1606/2330 train_time:98474ms step_avg:61.32ms
step:1607/2330 train_time:98535ms step_avg:61.32ms
step:1608/2330 train_time:98599ms step_avg:61.32ms
step:1609/2330 train_time:98659ms step_avg:61.32ms
step:1610/2330 train_time:98722ms step_avg:61.32ms
step:1611/2330 train_time:98782ms step_avg:61.32ms
step:1612/2330 train_time:98846ms step_avg:61.32ms
step:1613/2330 train_time:98906ms step_avg:61.32ms
step:1614/2330 train_time:98970ms step_avg:61.32ms
step:1615/2330 train_time:99030ms step_avg:61.32ms
step:1616/2330 train_time:99094ms step_avg:61.32ms
step:1617/2330 train_time:99155ms step_avg:61.32ms
step:1618/2330 train_time:99218ms step_avg:61.32ms
step:1619/2330 train_time:99279ms step_avg:61.32ms
step:1620/2330 train_time:99342ms step_avg:61.32ms
step:1621/2330 train_time:99403ms step_avg:61.32ms
step:1622/2330 train_time:99466ms step_avg:61.32ms
step:1623/2330 train_time:99526ms step_avg:61.32ms
step:1624/2330 train_time:99590ms step_avg:61.32ms
step:1625/2330 train_time:99650ms step_avg:61.32ms
step:1626/2330 train_time:99714ms step_avg:61.32ms
step:1627/2330 train_time:99775ms step_avg:61.32ms
step:1628/2330 train_time:99838ms step_avg:61.33ms
step:1629/2330 train_time:99898ms step_avg:61.32ms
step:1630/2330 train_time:99961ms step_avg:61.33ms
step:1631/2330 train_time:100022ms step_avg:61.33ms
step:1632/2330 train_time:100086ms step_avg:61.33ms
step:1633/2330 train_time:100146ms step_avg:61.33ms
step:1634/2330 train_time:100210ms step_avg:61.33ms
step:1635/2330 train_time:100271ms step_avg:61.33ms
step:1636/2330 train_time:100335ms step_avg:61.33ms
step:1637/2330 train_time:100397ms step_avg:61.33ms
step:1638/2330 train_time:100459ms step_avg:61.33ms
step:1639/2330 train_time:100519ms step_avg:61.33ms
step:1640/2330 train_time:100583ms step_avg:61.33ms
step:1641/2330 train_time:100645ms step_avg:61.33ms
step:1642/2330 train_time:100708ms step_avg:61.33ms
step:1643/2330 train_time:100768ms step_avg:61.33ms
step:1644/2330 train_time:100831ms step_avg:61.33ms
step:1645/2330 train_time:100891ms step_avg:61.33ms
step:1646/2330 train_time:100955ms step_avg:61.33ms
step:1647/2330 train_time:101016ms step_avg:61.33ms
step:1648/2330 train_time:101079ms step_avg:61.33ms
step:1649/2330 train_time:101140ms step_avg:61.33ms
step:1650/2330 train_time:101203ms step_avg:61.34ms
step:1651/2330 train_time:101264ms step_avg:61.33ms
step:1652/2330 train_time:101328ms step_avg:61.34ms
step:1653/2330 train_time:101388ms step_avg:61.34ms
step:1654/2330 train_time:101452ms step_avg:61.34ms
step:1655/2330 train_time:101512ms step_avg:61.34ms
step:1656/2330 train_time:101576ms step_avg:61.34ms
step:1657/2330 train_time:101637ms step_avg:61.34ms
step:1658/2330 train_time:101700ms step_avg:61.34ms
step:1659/2330 train_time:101760ms step_avg:61.34ms
step:1660/2330 train_time:101823ms step_avg:61.34ms
step:1661/2330 train_time:101884ms step_avg:61.34ms
step:1662/2330 train_time:101947ms step_avg:61.34ms
step:1663/2330 train_time:102008ms step_avg:61.34ms
step:1664/2330 train_time:102071ms step_avg:61.34ms
step:1665/2330 train_time:102131ms step_avg:61.34ms
step:1666/2330 train_time:102195ms step_avg:61.34ms
step:1667/2330 train_time:102255ms step_avg:61.34ms
step:1668/2330 train_time:102319ms step_avg:61.34ms
step:1669/2330 train_time:102380ms step_avg:61.34ms
step:1670/2330 train_time:102444ms step_avg:61.34ms
step:1671/2330 train_time:102505ms step_avg:61.34ms
step:1672/2330 train_time:102568ms step_avg:61.34ms
step:1673/2330 train_time:102628ms step_avg:61.34ms
step:1674/2330 train_time:102692ms step_avg:61.35ms
step:1675/2330 train_time:102752ms step_avg:61.34ms
step:1676/2330 train_time:102816ms step_avg:61.35ms
step:1677/2330 train_time:102877ms step_avg:61.35ms
step:1678/2330 train_time:102940ms step_avg:61.35ms
step:1679/2330 train_time:103001ms step_avg:61.35ms
step:1680/2330 train_time:103065ms step_avg:61.35ms
step:1681/2330 train_time:103125ms step_avg:61.35ms
step:1682/2330 train_time:103189ms step_avg:61.35ms
step:1683/2330 train_time:103249ms step_avg:61.35ms
step:1684/2330 train_time:103312ms step_avg:61.35ms
step:1685/2330 train_time:103373ms step_avg:61.35ms
step:1686/2330 train_time:103436ms step_avg:61.35ms
step:1687/2330 train_time:103498ms step_avg:61.35ms
step:1688/2330 train_time:103561ms step_avg:61.35ms
step:1689/2330 train_time:103622ms step_avg:61.35ms
step:1690/2330 train_time:103686ms step_avg:61.35ms
step:1691/2330 train_time:103747ms step_avg:61.35ms
step:1692/2330 train_time:103810ms step_avg:61.35ms
step:1693/2330 train_time:103871ms step_avg:61.35ms
step:1694/2330 train_time:103934ms step_avg:61.35ms
step:1695/2330 train_time:103995ms step_avg:61.35ms
step:1696/2330 train_time:104058ms step_avg:61.36ms
step:1697/2330 train_time:104119ms step_avg:61.35ms
step:1698/2330 train_time:104182ms step_avg:61.36ms
step:1699/2330 train_time:104242ms step_avg:61.36ms
step:1700/2330 train_time:104306ms step_avg:61.36ms
step:1701/2330 train_time:104366ms step_avg:61.36ms
step:1702/2330 train_time:104430ms step_avg:61.36ms
step:1703/2330 train_time:104491ms step_avg:61.36ms
step:1704/2330 train_time:104554ms step_avg:61.36ms
step:1705/2330 train_time:104615ms step_avg:61.36ms
step:1706/2330 train_time:104678ms step_avg:61.36ms
step:1707/2330 train_time:104739ms step_avg:61.36ms
step:1708/2330 train_time:104803ms step_avg:61.36ms
step:1709/2330 train_time:104863ms step_avg:61.36ms
step:1710/2330 train_time:104928ms step_avg:61.36ms
step:1711/2330 train_time:104988ms step_avg:61.36ms
step:1712/2330 train_time:105052ms step_avg:61.36ms
step:1713/2330 train_time:105112ms step_avg:61.36ms
step:1714/2330 train_time:105176ms step_avg:61.36ms
step:1715/2330 train_time:105238ms step_avg:61.36ms
step:1716/2330 train_time:105302ms step_avg:61.36ms
step:1717/2330 train_time:105361ms step_avg:61.36ms
step:1718/2330 train_time:105425ms step_avg:61.36ms
step:1719/2330 train_time:105485ms step_avg:61.36ms
step:1720/2330 train_time:105549ms step_avg:61.37ms
step:1721/2330 train_time:105609ms step_avg:61.36ms
step:1722/2330 train_time:105672ms step_avg:61.37ms
step:1723/2330 train_time:105733ms step_avg:61.37ms
step:1724/2330 train_time:105797ms step_avg:61.37ms
step:1725/2330 train_time:105858ms step_avg:61.37ms
step:1726/2330 train_time:105922ms step_avg:61.37ms
step:1727/2330 train_time:105982ms step_avg:61.37ms
step:1728/2330 train_time:106046ms step_avg:61.37ms
step:1729/2330 train_time:106107ms step_avg:61.37ms
step:1730/2330 train_time:106170ms step_avg:61.37ms
step:1731/2330 train_time:106231ms step_avg:61.37ms
step:1732/2330 train_time:106294ms step_avg:61.37ms
step:1733/2330 train_time:106355ms step_avg:61.37ms
step:1734/2330 train_time:106418ms step_avg:61.37ms
step:1735/2330 train_time:106478ms step_avg:61.37ms
step:1736/2330 train_time:106543ms step_avg:61.37ms
step:1737/2330 train_time:106604ms step_avg:61.37ms
step:1738/2330 train_time:106667ms step_avg:61.37ms
step:1739/2330 train_time:106727ms step_avg:61.37ms
step:1740/2330 train_time:106791ms step_avg:61.37ms
step:1741/2330 train_time:106852ms step_avg:61.37ms
step:1742/2330 train_time:106916ms step_avg:61.38ms
step:1743/2330 train_time:106978ms step_avg:61.38ms
step:1744/2330 train_time:107041ms step_avg:61.38ms
step:1745/2330 train_time:107102ms step_avg:61.38ms
step:1746/2330 train_time:107166ms step_avg:61.38ms
step:1747/2330 train_time:107227ms step_avg:61.38ms
step:1748/2330 train_time:107290ms step_avg:61.38ms
step:1749/2330 train_time:107350ms step_avg:61.38ms
step:1750/2330 train_time:107414ms step_avg:61.38ms
step:1750/2330 val_loss:3.4449 train_time:107479ms step_avg:61.42ms
step:1751/2330 train_time:107503ms step_avg:61.40ms
step:1752/2330 train_time:107540ms step_avg:61.38ms
step:1753/2330 train_time:107603ms step_avg:61.38ms
step:1754/2330 train_time:107668ms step_avg:61.38ms
step:1755/2330 train_time:107729ms step_avg:61.38ms
step:1756/2330 train_time:107792ms step_avg:61.38ms
step:1757/2330 train_time:107852ms step_avg:61.38ms
step:1758/2330 train_time:107915ms step_avg:61.39ms
step:1759/2330 train_time:107975ms step_avg:61.38ms
step:1760/2330 train_time:108038ms step_avg:61.38ms
step:1761/2330 train_time:108097ms step_avg:61.38ms
step:1762/2330 train_time:108160ms step_avg:61.38ms
step:1763/2330 train_time:108220ms step_avg:61.38ms
step:1764/2330 train_time:108282ms step_avg:61.38ms
step:1765/2330 train_time:108341ms step_avg:61.38ms
step:1766/2330 train_time:108406ms step_avg:61.39ms
step:1767/2330 train_time:108468ms step_avg:61.39ms
step:1768/2330 train_time:108533ms step_avg:61.39ms
step:1769/2330 train_time:108595ms step_avg:61.39ms
step:1770/2330 train_time:108660ms step_avg:61.39ms
step:1771/2330 train_time:108720ms step_avg:61.39ms
step:1772/2330 train_time:108784ms step_avg:61.39ms
step:1773/2330 train_time:108845ms step_avg:61.39ms
step:1774/2330 train_time:108908ms step_avg:61.39ms
step:1775/2330 train_time:108968ms step_avg:61.39ms
step:1776/2330 train_time:109031ms step_avg:61.39ms
step:1777/2330 train_time:109091ms step_avg:61.39ms
step:1778/2330 train_time:109154ms step_avg:61.39ms
step:1779/2330 train_time:109214ms step_avg:61.39ms
step:1780/2330 train_time:109277ms step_avg:61.39ms
step:1781/2330 train_time:109337ms step_avg:61.39ms
step:1782/2330 train_time:109401ms step_avg:61.39ms
step:1783/2330 train_time:109462ms step_avg:61.39ms
step:1784/2330 train_time:109526ms step_avg:61.39ms
step:1785/2330 train_time:109588ms step_avg:61.39ms
step:1786/2330 train_time:109652ms step_avg:61.40ms
step:1787/2330 train_time:109712ms step_avg:61.39ms
step:1788/2330 train_time:109777ms step_avg:61.40ms
step:1789/2330 train_time:109838ms step_avg:61.40ms
step:1790/2330 train_time:109901ms step_avg:61.40ms
step:1791/2330 train_time:109962ms step_avg:61.40ms
step:1792/2330 train_time:110025ms step_avg:61.40ms
step:1793/2330 train_time:110086ms step_avg:61.40ms
step:1794/2330 train_time:110148ms step_avg:61.40ms
step:1795/2330 train_time:110209ms step_avg:61.40ms
step:1796/2330 train_time:110271ms step_avg:61.40ms
step:1797/2330 train_time:110332ms step_avg:61.40ms
step:1798/2330 train_time:110395ms step_avg:61.40ms
step:1799/2330 train_time:110458ms step_avg:61.40ms
step:1800/2330 train_time:110522ms step_avg:61.40ms
step:1801/2330 train_time:110582ms step_avg:61.40ms
step:1802/2330 train_time:110646ms step_avg:61.40ms
step:1803/2330 train_time:110706ms step_avg:61.40ms
step:1804/2330 train_time:110771ms step_avg:61.40ms
step:1805/2330 train_time:110832ms step_avg:61.40ms
step:1806/2330 train_time:110896ms step_avg:61.40ms
step:1807/2330 train_time:110956ms step_avg:61.40ms
step:1808/2330 train_time:111019ms step_avg:61.40ms
step:1809/2330 train_time:111080ms step_avg:61.40ms
step:1810/2330 train_time:111143ms step_avg:61.40ms
step:1811/2330 train_time:111203ms step_avg:61.40ms
step:1812/2330 train_time:111267ms step_avg:61.41ms
step:1813/2330 train_time:111327ms step_avg:61.40ms
step:1814/2330 train_time:111391ms step_avg:61.41ms
step:1815/2330 train_time:111452ms step_avg:61.41ms
step:1816/2330 train_time:111516ms step_avg:61.41ms
step:1817/2330 train_time:111577ms step_avg:61.41ms
step:1818/2330 train_time:111640ms step_avg:61.41ms
step:1819/2330 train_time:111700ms step_avg:61.41ms
step:1820/2330 train_time:111764ms step_avg:61.41ms
step:1821/2330 train_time:111825ms step_avg:61.41ms
step:1822/2330 train_time:111888ms step_avg:61.41ms
step:1823/2330 train_time:111949ms step_avg:61.41ms
step:1824/2330 train_time:112013ms step_avg:61.41ms
step:1825/2330 train_time:112073ms step_avg:61.41ms
step:1826/2330 train_time:112136ms step_avg:61.41ms
step:1827/2330 train_time:112197ms step_avg:61.41ms
step:1828/2330 train_time:112261ms step_avg:61.41ms
step:1829/2330 train_time:112321ms step_avg:61.41ms
step:1830/2330 train_time:112385ms step_avg:61.41ms
step:1831/2330 train_time:112446ms step_avg:61.41ms
step:1832/2330 train_time:112509ms step_avg:61.41ms
step:1833/2330 train_time:112569ms step_avg:61.41ms
step:1834/2330 train_time:112632ms step_avg:61.41ms
step:1835/2330 train_time:112693ms step_avg:61.41ms
step:1836/2330 train_time:112758ms step_avg:61.42ms
step:1837/2330 train_time:112819ms step_avg:61.41ms
step:1838/2330 train_time:112882ms step_avg:61.42ms
step:1839/2330 train_time:112942ms step_avg:61.42ms
step:1840/2330 train_time:113006ms step_avg:61.42ms
step:1841/2330 train_time:113067ms step_avg:61.42ms
step:1842/2330 train_time:113130ms step_avg:61.42ms
step:1843/2330 train_time:113191ms step_avg:61.42ms
step:1844/2330 train_time:113255ms step_avg:61.42ms
step:1845/2330 train_time:113316ms step_avg:61.42ms
step:1846/2330 train_time:113379ms step_avg:61.42ms
step:1847/2330 train_time:113439ms step_avg:61.42ms
step:1848/2330 train_time:113502ms step_avg:61.42ms
step:1849/2330 train_time:113563ms step_avg:61.42ms
step:1850/2330 train_time:113626ms step_avg:61.42ms
step:1851/2330 train_time:113687ms step_avg:61.42ms
step:1852/2330 train_time:113751ms step_avg:61.42ms
step:1853/2330 train_time:113812ms step_avg:61.42ms
step:1854/2330 train_time:113877ms step_avg:61.42ms
step:1855/2330 train_time:113938ms step_avg:61.42ms
step:1856/2330 train_time:114002ms step_avg:61.42ms
step:1857/2330 train_time:114062ms step_avg:61.42ms
step:1858/2330 train_time:114126ms step_avg:61.42ms
step:1859/2330 train_time:114186ms step_avg:61.42ms
step:1860/2330 train_time:114250ms step_avg:61.42ms
step:1861/2330 train_time:114311ms step_avg:61.42ms
step:1862/2330 train_time:114375ms step_avg:61.43ms
step:1863/2330 train_time:114435ms step_avg:61.42ms
step:1864/2330 train_time:114498ms step_avg:61.43ms
step:1865/2330 train_time:114560ms step_avg:61.43ms
step:1866/2330 train_time:114623ms step_avg:61.43ms
step:1867/2330 train_time:114683ms step_avg:61.43ms
step:1868/2330 train_time:114747ms step_avg:61.43ms
step:1869/2330 train_time:114807ms step_avg:61.43ms
step:1870/2330 train_time:114870ms step_avg:61.43ms
step:1871/2330 train_time:114931ms step_avg:61.43ms
step:1872/2330 train_time:114996ms step_avg:61.43ms
step:1873/2330 train_time:115056ms step_avg:61.43ms
step:1874/2330 train_time:115120ms step_avg:61.43ms
step:1875/2330 train_time:115180ms step_avg:61.43ms
step:1876/2330 train_time:115244ms step_avg:61.43ms
step:1877/2330 train_time:115304ms step_avg:61.43ms
step:1878/2330 train_time:115368ms step_avg:61.43ms
step:1879/2330 train_time:115428ms step_avg:61.43ms
step:1880/2330 train_time:115491ms step_avg:61.43ms
step:1881/2330 train_time:115553ms step_avg:61.43ms
step:1882/2330 train_time:115617ms step_avg:61.43ms
step:1883/2330 train_time:115677ms step_avg:61.43ms
step:1884/2330 train_time:115740ms step_avg:61.43ms
step:1885/2330 train_time:115801ms step_avg:61.43ms
step:1886/2330 train_time:115864ms step_avg:61.43ms
step:1887/2330 train_time:115925ms step_avg:61.43ms
step:1888/2330 train_time:115989ms step_avg:61.43ms
step:1889/2330 train_time:116049ms step_avg:61.43ms
step:1890/2330 train_time:116113ms step_avg:61.44ms
step:1891/2330 train_time:116174ms step_avg:61.44ms
step:1892/2330 train_time:116238ms step_avg:61.44ms
step:1893/2330 train_time:116299ms step_avg:61.44ms
step:1894/2330 train_time:116363ms step_avg:61.44ms
step:1895/2330 train_time:116422ms step_avg:61.44ms
step:1896/2330 train_time:116486ms step_avg:61.44ms
step:1897/2330 train_time:116546ms step_avg:61.44ms
step:1898/2330 train_time:116609ms step_avg:61.44ms
step:1899/2330 train_time:116670ms step_avg:61.44ms
step:1900/2330 train_time:116734ms step_avg:61.44ms
step:1901/2330 train_time:116800ms step_avg:61.44ms
step:1902/2330 train_time:116859ms step_avg:61.44ms
step:1903/2330 train_time:116919ms step_avg:61.44ms
step:1904/2330 train_time:116982ms step_avg:61.44ms
step:1905/2330 train_time:117042ms step_avg:61.44ms
step:1906/2330 train_time:117105ms step_avg:61.44ms
step:1907/2330 train_time:117166ms step_avg:61.44ms
step:1908/2330 train_time:117229ms step_avg:61.44ms
step:1909/2330 train_time:117289ms step_avg:61.44ms
step:1910/2330 train_time:117353ms step_avg:61.44ms
step:1911/2330 train_time:117414ms step_avg:61.44ms
step:1912/2330 train_time:117477ms step_avg:61.44ms
step:1913/2330 train_time:117539ms step_avg:61.44ms
step:1914/2330 train_time:117603ms step_avg:61.44ms
step:1915/2330 train_time:117663ms step_avg:61.44ms
step:1916/2330 train_time:117726ms step_avg:61.44ms
step:1917/2330 train_time:117787ms step_avg:61.44ms
step:1918/2330 train_time:117851ms step_avg:61.44ms
step:1919/2330 train_time:117911ms step_avg:61.44ms
step:1920/2330 train_time:117975ms step_avg:61.45ms
step:1921/2330 train_time:118035ms step_avg:61.44ms
step:1922/2330 train_time:118099ms step_avg:61.45ms
step:1923/2330 train_time:118159ms step_avg:61.45ms
step:1924/2330 train_time:118223ms step_avg:61.45ms
step:1925/2330 train_time:118283ms step_avg:61.45ms
step:1926/2330 train_time:118347ms step_avg:61.45ms
step:1927/2330 train_time:118407ms step_avg:61.45ms
step:1928/2330 train_time:118471ms step_avg:61.45ms
step:1929/2330 train_time:118531ms step_avg:61.45ms
step:1930/2330 train_time:118596ms step_avg:61.45ms
step:1931/2330 train_time:118657ms step_avg:61.45ms
step:1932/2330 train_time:118721ms step_avg:61.45ms
step:1933/2330 train_time:118781ms step_avg:61.45ms
step:1934/2330 train_time:118844ms step_avg:61.45ms
step:1935/2330 train_time:118905ms step_avg:61.45ms
step:1936/2330 train_time:118969ms step_avg:61.45ms
step:1937/2330 train_time:119029ms step_avg:61.45ms
step:1938/2330 train_time:119093ms step_avg:61.45ms
step:1939/2330 train_time:119153ms step_avg:61.45ms
step:1940/2330 train_time:119217ms step_avg:61.45ms
step:1941/2330 train_time:119278ms step_avg:61.45ms
step:1942/2330 train_time:119342ms step_avg:61.45ms
step:1943/2330 train_time:119402ms step_avg:61.45ms
step:1944/2330 train_time:119465ms step_avg:61.45ms
step:1945/2330 train_time:119526ms step_avg:61.45ms
step:1946/2330 train_time:119589ms step_avg:61.45ms
step:1947/2330 train_time:119649ms step_avg:61.45ms
step:1948/2330 train_time:119713ms step_avg:61.45ms
step:1949/2330 train_time:119773ms step_avg:61.45ms
step:1950/2330 train_time:119837ms step_avg:61.46ms
step:1951/2330 train_time:119898ms step_avg:61.45ms
step:1952/2330 train_time:119962ms step_avg:61.46ms
step:1953/2330 train_time:120022ms step_avg:61.46ms
step:1954/2330 train_time:120085ms step_avg:61.46ms
step:1955/2330 train_time:120146ms step_avg:61.46ms
step:1956/2330 train_time:120210ms step_avg:61.46ms
step:1957/2330 train_time:120271ms step_avg:61.46ms
step:1958/2330 train_time:120335ms step_avg:61.46ms
step:1959/2330 train_time:120396ms step_avg:61.46ms
step:1960/2330 train_time:120459ms step_avg:61.46ms
step:1961/2330 train_time:120519ms step_avg:61.46ms
step:1962/2330 train_time:120583ms step_avg:61.46ms
step:1963/2330 train_time:120643ms step_avg:61.46ms
step:1964/2330 train_time:120706ms step_avg:61.46ms
step:1965/2330 train_time:120767ms step_avg:61.46ms
step:1966/2330 train_time:120831ms step_avg:61.46ms
step:1967/2330 train_time:120892ms step_avg:61.46ms
step:1968/2330 train_time:120955ms step_avg:61.46ms
step:1969/2330 train_time:121016ms step_avg:61.46ms
step:1970/2330 train_time:121079ms step_avg:61.46ms
step:1971/2330 train_time:121139ms step_avg:61.46ms
step:1972/2330 train_time:121203ms step_avg:61.46ms
step:1973/2330 train_time:121263ms step_avg:61.46ms
step:1974/2330 train_time:121327ms step_avg:61.46ms
step:1975/2330 train_time:121388ms step_avg:61.46ms
step:1976/2330 train_time:121451ms step_avg:61.46ms
step:1977/2330 train_time:121511ms step_avg:61.46ms
step:1978/2330 train_time:121576ms step_avg:61.46ms
step:1979/2330 train_time:121636ms step_avg:61.46ms
step:1980/2330 train_time:121700ms step_avg:61.46ms
step:1981/2330 train_time:121761ms step_avg:61.46ms
step:1982/2330 train_time:121825ms step_avg:61.47ms
step:1983/2330 train_time:121886ms step_avg:61.47ms
step:1984/2330 train_time:121949ms step_avg:61.47ms
step:1985/2330 train_time:122009ms step_avg:61.47ms
step:1986/2330 train_time:122073ms step_avg:61.47ms
step:1987/2330 train_time:122134ms step_avg:61.47ms
step:1988/2330 train_time:122197ms step_avg:61.47ms
step:1989/2330 train_time:122258ms step_avg:61.47ms
step:1990/2330 train_time:122322ms step_avg:61.47ms
step:1991/2330 train_time:122382ms step_avg:61.47ms
step:1992/2330 train_time:122445ms step_avg:61.47ms
step:1993/2330 train_time:122506ms step_avg:61.47ms
step:1994/2330 train_time:122571ms step_avg:61.47ms
step:1995/2330 train_time:122632ms step_avg:61.47ms
step:1996/2330 train_time:122696ms step_avg:61.47ms
step:1997/2330 train_time:122758ms step_avg:61.47ms
step:1998/2330 train_time:122821ms step_avg:61.47ms
step:1999/2330 train_time:122881ms step_avg:61.47ms
step:2000/2330 train_time:122944ms step_avg:61.47ms
step:2000/2330 val_loss:3.4151 train_time:123010ms step_avg:61.50ms
step:2001/2330 train_time:123032ms step_avg:61.49ms
step:2002/2330 train_time:123071ms step_avg:61.47ms
step:2003/2330 train_time:123134ms step_avg:61.47ms
step:2004/2330 train_time:123199ms step_avg:61.48ms
step:2005/2330 train_time:123259ms step_avg:61.48ms
step:2006/2330 train_time:123322ms step_avg:61.48ms
step:2007/2330 train_time:123382ms step_avg:61.48ms
step:2008/2330 train_time:123445ms step_avg:61.48ms
step:2009/2330 train_time:123504ms step_avg:61.48ms
step:2010/2330 train_time:123567ms step_avg:61.48ms
step:2011/2330 train_time:123627ms step_avg:61.48ms
step:2012/2330 train_time:123690ms step_avg:61.48ms
step:2013/2330 train_time:123750ms step_avg:61.48ms
step:2014/2330 train_time:123813ms step_avg:61.48ms
step:2015/2330 train_time:123873ms step_avg:61.48ms
step:2016/2330 train_time:123937ms step_avg:61.48ms
step:2017/2330 train_time:123999ms step_avg:61.48ms
step:2018/2330 train_time:124063ms step_avg:61.48ms
step:2019/2330 train_time:124124ms step_avg:61.48ms
step:2020/2330 train_time:124189ms step_avg:61.48ms
step:2021/2330 train_time:124250ms step_avg:61.48ms
step:2022/2330 train_time:124313ms step_avg:61.48ms
step:2023/2330 train_time:124374ms step_avg:61.48ms
step:2024/2330 train_time:124438ms step_avg:61.48ms
step:2025/2330 train_time:124497ms step_avg:61.48ms
step:2026/2330 train_time:124560ms step_avg:61.48ms
step:2027/2330 train_time:124620ms step_avg:61.48ms
step:2028/2330 train_time:124684ms step_avg:61.48ms
step:2029/2330 train_time:124744ms step_avg:61.48ms
step:2030/2330 train_time:124807ms step_avg:61.48ms
step:2031/2330 train_time:124868ms step_avg:61.48ms
step:2032/2330 train_time:124932ms step_avg:61.48ms
step:2033/2330 train_time:124993ms step_avg:61.48ms
step:2034/2330 train_time:125057ms step_avg:61.48ms
step:2035/2330 train_time:125118ms step_avg:61.48ms
step:2036/2330 train_time:125183ms step_avg:61.48ms
step:2037/2330 train_time:125243ms step_avg:61.48ms
step:2038/2330 train_time:125307ms step_avg:61.49ms
step:2039/2330 train_time:125367ms step_avg:61.48ms
step:2040/2330 train_time:125431ms step_avg:61.49ms
step:2041/2330 train_time:125492ms step_avg:61.49ms
step:2042/2330 train_time:125556ms step_avg:61.49ms
step:2043/2330 train_time:125616ms step_avg:61.49ms
step:2044/2330 train_time:125679ms step_avg:61.49ms
step:2045/2330 train_time:125740ms step_avg:61.49ms
step:2046/2330 train_time:125803ms step_avg:61.49ms
step:2047/2330 train_time:125863ms step_avg:61.49ms
step:2048/2330 train_time:125928ms step_avg:61.49ms
step:2049/2330 train_time:125988ms step_avg:61.49ms
step:2050/2330 train_time:126052ms step_avg:61.49ms
step:2051/2330 train_time:126114ms step_avg:61.49ms
step:2052/2330 train_time:126179ms step_avg:61.49ms
step:2053/2330 train_time:126239ms step_avg:61.49ms
step:2054/2330 train_time:126302ms step_avg:61.49ms
step:2055/2330 train_time:126363ms step_avg:61.49ms
step:2056/2330 train_time:126427ms step_avg:61.49ms
step:2057/2330 train_time:126488ms step_avg:61.49ms
step:2058/2330 train_time:126552ms step_avg:61.49ms
step:2059/2330 train_time:126612ms step_avg:61.49ms
step:2060/2330 train_time:126677ms step_avg:61.49ms
step:2061/2330 train_time:126737ms step_avg:61.49ms
step:2062/2330 train_time:126800ms step_avg:61.49ms
step:2063/2330 train_time:126860ms step_avg:61.49ms
step:2064/2330 train_time:126923ms step_avg:61.49ms
step:2065/2330 train_time:126984ms step_avg:61.49ms
step:2066/2330 train_time:127049ms step_avg:61.50ms
step:2067/2330 train_time:127110ms step_avg:61.49ms
step:2068/2330 train_time:127174ms step_avg:61.50ms
step:2069/2330 train_time:127235ms step_avg:61.50ms
step:2070/2330 train_time:127298ms step_avg:61.50ms
step:2071/2330 train_time:127359ms step_avg:61.50ms
step:2072/2330 train_time:127422ms step_avg:61.50ms
step:2073/2330 train_time:127482ms step_avg:61.50ms
step:2074/2330 train_time:127545ms step_avg:61.50ms
step:2075/2330 train_time:127606ms step_avg:61.50ms
step:2076/2330 train_time:127669ms step_avg:61.50ms
step:2077/2330 train_time:127730ms step_avg:61.50ms
step:2078/2330 train_time:127793ms step_avg:61.50ms
step:2079/2330 train_time:127854ms step_avg:61.50ms
step:2080/2330 train_time:127917ms step_avg:61.50ms
step:2081/2330 train_time:127978ms step_avg:61.50ms
step:2082/2330 train_time:128041ms step_avg:61.50ms
step:2083/2330 train_time:128101ms step_avg:61.50ms
step:2084/2330 train_time:128165ms step_avg:61.50ms
step:2085/2330 train_time:128226ms step_avg:61.50ms
step:2086/2330 train_time:128290ms step_avg:61.50ms
step:2087/2330 train_time:128350ms step_avg:61.50ms
step:2088/2330 train_time:128414ms step_avg:61.50ms
step:2089/2330 train_time:128475ms step_avg:61.50ms
step:2090/2330 train_time:128538ms step_avg:61.50ms
step:2091/2330 train_time:128599ms step_avg:61.50ms
step:2092/2330 train_time:128662ms step_avg:61.50ms
step:2093/2330 train_time:128723ms step_avg:61.50ms
step:2094/2330 train_time:128786ms step_avg:61.50ms
step:2095/2330 train_time:128847ms step_avg:61.50ms
step:2096/2330 train_time:128910ms step_avg:61.50ms
step:2097/2330 train_time:128971ms step_avg:61.50ms
step:2098/2330 train_time:129035ms step_avg:61.50ms
step:2099/2330 train_time:129095ms step_avg:61.50ms
step:2100/2330 train_time:129158ms step_avg:61.50ms
step:2101/2330 train_time:129218ms step_avg:61.50ms
step:2102/2330 train_time:129282ms step_avg:61.50ms
step:2103/2330 train_time:129342ms step_avg:61.50ms
step:2104/2330 train_time:129407ms step_avg:61.51ms
step:2105/2330 train_time:129467ms step_avg:61.50ms
step:2106/2330 train_time:129532ms step_avg:61.51ms
step:2107/2330 train_time:129594ms step_avg:61.51ms
step:2108/2330 train_time:129658ms step_avg:61.51ms
step:2109/2330 train_time:129718ms step_avg:61.51ms
step:2110/2330 train_time:129782ms step_avg:61.51ms
step:2111/2330 train_time:129842ms step_avg:61.51ms
step:2112/2330 train_time:129905ms step_avg:61.51ms
step:2113/2330 train_time:129967ms step_avg:61.51ms
step:2114/2330 train_time:130030ms step_avg:61.51ms
step:2115/2330 train_time:130090ms step_avg:61.51ms
step:2116/2330 train_time:130155ms step_avg:61.51ms
step:2117/2330 train_time:130216ms step_avg:61.51ms
step:2118/2330 train_time:130280ms step_avg:61.51ms
step:2119/2330 train_time:130340ms step_avg:61.51ms
step:2120/2330 train_time:130404ms step_avg:61.51ms
step:2121/2330 train_time:130464ms step_avg:61.51ms
step:2122/2330 train_time:130529ms step_avg:61.51ms
step:2123/2330 train_time:130589ms step_avg:61.51ms
step:2124/2330 train_time:130652ms step_avg:61.51ms
step:2125/2330 train_time:130713ms step_avg:61.51ms
step:2126/2330 train_time:130778ms step_avg:61.51ms
step:2127/2330 train_time:130838ms step_avg:61.51ms
step:2128/2330 train_time:130901ms step_avg:61.51ms
step:2129/2330 train_time:130962ms step_avg:61.51ms
step:2130/2330 train_time:131025ms step_avg:61.51ms
step:2131/2330 train_time:131086ms step_avg:61.51ms
step:2132/2330 train_time:131150ms step_avg:61.51ms
step:2133/2330 train_time:131210ms step_avg:61.51ms
step:2134/2330 train_time:131275ms step_avg:61.52ms
step:2135/2330 train_time:131335ms step_avg:61.52ms
step:2136/2330 train_time:131398ms step_avg:61.52ms
step:2137/2330 train_time:131459ms step_avg:61.52ms
step:2138/2330 train_time:131522ms step_avg:61.52ms
step:2139/2330 train_time:131583ms step_avg:61.52ms
step:2140/2330 train_time:131647ms step_avg:61.52ms
step:2141/2330 train_time:131707ms step_avg:61.52ms
step:2142/2330 train_time:131772ms step_avg:61.52ms
step:2143/2330 train_time:131832ms step_avg:61.52ms
step:2144/2330 train_time:131896ms step_avg:61.52ms
step:2145/2330 train_time:131957ms step_avg:61.52ms
step:2146/2330 train_time:132021ms step_avg:61.52ms
step:2147/2330 train_time:132081ms step_avg:61.52ms
step:2148/2330 train_time:132144ms step_avg:61.52ms
step:2149/2330 train_time:132205ms step_avg:61.52ms
step:2150/2330 train_time:132269ms step_avg:61.52ms
step:2151/2330 train_time:132330ms step_avg:61.52ms
step:2152/2330 train_time:132394ms step_avg:61.52ms
step:2153/2330 train_time:132455ms step_avg:61.52ms
step:2154/2330 train_time:132519ms step_avg:61.52ms
step:2155/2330 train_time:132579ms step_avg:61.52ms
step:2156/2330 train_time:132642ms step_avg:61.52ms
step:2157/2330 train_time:132703ms step_avg:61.52ms
step:2158/2330 train_time:132766ms step_avg:61.52ms
step:2159/2330 train_time:132828ms step_avg:61.52ms
step:2160/2330 train_time:132892ms step_avg:61.52ms
step:2161/2330 train_time:132953ms step_avg:61.52ms
step:2162/2330 train_time:133017ms step_avg:61.53ms
step:2163/2330 train_time:133078ms step_avg:61.52ms
step:2164/2330 train_time:133141ms step_avg:61.53ms
step:2165/2330 train_time:133200ms step_avg:61.52ms
step:2166/2330 train_time:133264ms step_avg:61.53ms
step:2167/2330 train_time:133324ms step_avg:61.52ms
step:2168/2330 train_time:133388ms step_avg:61.53ms
step:2169/2330 train_time:133449ms step_avg:61.53ms
step:2170/2330 train_time:133513ms step_avg:61.53ms
step:2171/2330 train_time:133574ms step_avg:61.53ms
step:2172/2330 train_time:133638ms step_avg:61.53ms
step:2173/2330 train_time:133698ms step_avg:61.53ms
step:2174/2330 train_time:133761ms step_avg:61.53ms
step:2175/2330 train_time:133822ms step_avg:61.53ms
step:2176/2330 train_time:133885ms step_avg:61.53ms
step:2177/2330 train_time:133945ms step_avg:61.53ms
step:2178/2330 train_time:134009ms step_avg:61.53ms
step:2179/2330 train_time:134070ms step_avg:61.53ms
step:2180/2330 train_time:134135ms step_avg:61.53ms
step:2181/2330 train_time:134196ms step_avg:61.53ms
step:2182/2330 train_time:134259ms step_avg:61.53ms
step:2183/2330 train_time:134319ms step_avg:61.53ms
step:2184/2330 train_time:134384ms step_avg:61.53ms
step:2185/2330 train_time:134444ms step_avg:61.53ms
step:2186/2330 train_time:134507ms step_avg:61.53ms
step:2187/2330 train_time:134568ms step_avg:61.53ms
step:2188/2330 train_time:134632ms step_avg:61.53ms
step:2189/2330 train_time:134693ms step_avg:61.53ms
step:2190/2330 train_time:134757ms step_avg:61.53ms
step:2191/2330 train_time:134818ms step_avg:61.53ms
step:2192/2330 train_time:134881ms step_avg:61.53ms
step:2193/2330 train_time:134941ms step_avg:61.53ms
step:2194/2330 train_time:135005ms step_avg:61.53ms
step:2195/2330 train_time:135065ms step_avg:61.53ms
step:2196/2330 train_time:135130ms step_avg:61.53ms
step:2197/2330 train_time:135190ms step_avg:61.53ms
step:2198/2330 train_time:135254ms step_avg:61.53ms
step:2199/2330 train_time:135315ms step_avg:61.53ms
step:2200/2330 train_time:135380ms step_avg:61.54ms
step:2201/2330 train_time:135440ms step_avg:61.54ms
step:2202/2330 train_time:135503ms step_avg:61.54ms
step:2203/2330 train_time:135563ms step_avg:61.54ms
step:2204/2330 train_time:135628ms step_avg:61.54ms
step:2205/2330 train_time:135688ms step_avg:61.54ms
step:2206/2330 train_time:135753ms step_avg:61.54ms
step:2207/2330 train_time:135813ms step_avg:61.54ms
step:2208/2330 train_time:135877ms step_avg:61.54ms
step:2209/2330 train_time:135938ms step_avg:61.54ms
step:2210/2330 train_time:136001ms step_avg:61.54ms
step:2211/2330 train_time:136062ms step_avg:61.54ms
step:2212/2330 train_time:136126ms step_avg:61.54ms
step:2213/2330 train_time:136187ms step_avg:61.54ms
step:2214/2330 train_time:136250ms step_avg:61.54ms
step:2215/2330 train_time:136310ms step_avg:61.54ms
step:2216/2330 train_time:136375ms step_avg:61.54ms
step:2217/2330 train_time:136435ms step_avg:61.54ms
step:2218/2330 train_time:136498ms step_avg:61.54ms
step:2219/2330 train_time:136558ms step_avg:61.54ms
step:2220/2330 train_time:136622ms step_avg:61.54ms
step:2221/2330 train_time:136682ms step_avg:61.54ms
step:2222/2330 train_time:136746ms step_avg:61.54ms
step:2223/2330 train_time:136806ms step_avg:61.54ms
step:2224/2330 train_time:136871ms step_avg:61.54ms
step:2225/2330 train_time:136932ms step_avg:61.54ms
step:2226/2330 train_time:136996ms step_avg:61.54ms
step:2227/2330 train_time:137057ms step_avg:61.54ms
step:2228/2330 train_time:137120ms step_avg:61.54ms
step:2229/2330 train_time:137180ms step_avg:61.54ms
step:2230/2330 train_time:137244ms step_avg:61.54ms
step:2231/2330 train_time:137304ms step_avg:61.54ms
step:2232/2330 train_time:137368ms step_avg:61.54ms
step:2233/2330 train_time:137429ms step_avg:61.54ms
step:2234/2330 train_time:137493ms step_avg:61.55ms
step:2235/2330 train_time:137554ms step_avg:61.55ms
step:2236/2330 train_time:137618ms step_avg:61.55ms
step:2237/2330 train_time:137679ms step_avg:61.55ms
step:2238/2330 train_time:137742ms step_avg:61.55ms
step:2239/2330 train_time:137803ms step_avg:61.55ms
step:2240/2330 train_time:137867ms step_avg:61.55ms
step:2241/2330 train_time:137928ms step_avg:61.55ms
step:2242/2330 train_time:137992ms step_avg:61.55ms
step:2243/2330 train_time:138052ms step_avg:61.55ms
step:2244/2330 train_time:138117ms step_avg:61.55ms
step:2245/2330 train_time:138177ms step_avg:61.55ms
step:2246/2330 train_time:138241ms step_avg:61.55ms
step:2247/2330 train_time:138301ms step_avg:61.55ms
step:2248/2330 train_time:138364ms step_avg:61.55ms
step:2249/2330 train_time:138425ms step_avg:61.55ms
step:2250/2330 train_time:138489ms step_avg:61.55ms
step:2250/2330 val_loss:3.3909 train_time:138554ms step_avg:61.58ms
step:2251/2330 train_time:138578ms step_avg:61.56ms
step:2252/2330 train_time:138617ms step_avg:61.55ms
step:2253/2330 train_time:138683ms step_avg:61.55ms
step:2254/2330 train_time:138748ms step_avg:61.56ms
step:2255/2330 train_time:138809ms step_avg:61.56ms
step:2256/2330 train_time:138873ms step_avg:61.56ms
step:2257/2330 train_time:138933ms step_avg:61.56ms
step:2258/2330 train_time:138995ms step_avg:61.56ms
step:2259/2330 train_time:139055ms step_avg:61.56ms
step:2260/2330 train_time:139118ms step_avg:61.56ms
step:2261/2330 train_time:139178ms step_avg:61.56ms
step:2262/2330 train_time:139241ms step_avg:61.56ms
step:2263/2330 train_time:139301ms step_avg:61.56ms
step:2264/2330 train_time:139363ms step_avg:61.56ms
step:2265/2330 train_time:139423ms step_avg:61.56ms
step:2266/2330 train_time:139486ms step_avg:61.56ms
step:2267/2330 train_time:139547ms step_avg:61.56ms
step:2268/2330 train_time:139613ms step_avg:61.56ms
step:2269/2330 train_time:139676ms step_avg:61.56ms
step:2270/2330 train_time:139741ms step_avg:61.56ms
step:2271/2330 train_time:139803ms step_avg:61.56ms
step:2272/2330 train_time:139867ms step_avg:61.56ms
step:2273/2330 train_time:139927ms step_avg:61.56ms
step:2274/2330 train_time:139990ms step_avg:61.56ms
step:2275/2330 train_time:140050ms step_avg:61.56ms
step:2276/2330 train_time:140113ms step_avg:61.56ms
step:2277/2330 train_time:140173ms step_avg:61.56ms
step:2278/2330 train_time:140236ms step_avg:61.56ms
step:2279/2330 train_time:140297ms step_avg:61.56ms
step:2280/2330 train_time:140360ms step_avg:61.56ms
step:2281/2330 train_time:140420ms step_avg:61.56ms
step:2282/2330 train_time:140483ms step_avg:61.56ms
step:2283/2330 train_time:140544ms step_avg:61.56ms
step:2284/2330 train_time:140608ms step_avg:61.56ms
step:2285/2330 train_time:140669ms step_avg:61.56ms
step:2286/2330 train_time:140734ms step_avg:61.56ms
step:2287/2330 train_time:140795ms step_avg:61.56ms
step:2288/2330 train_time:140859ms step_avg:61.56ms
step:2289/2330 train_time:140919ms step_avg:61.56ms
step:2290/2330 train_time:140983ms step_avg:61.56ms
step:2291/2330 train_time:141044ms step_avg:61.56ms
step:2292/2330 train_time:141107ms step_avg:61.57ms
step:2293/2330 train_time:141168ms step_avg:61.56ms
step:2294/2330 train_time:141231ms step_avg:61.57ms
step:2295/2330 train_time:141292ms step_avg:61.56ms
step:2296/2330 train_time:141355ms step_avg:61.57ms
step:2297/2330 train_time:141415ms step_avg:61.57ms
step:2298/2330 train_time:141479ms step_avg:61.57ms
step:2299/2330 train_time:141540ms step_avg:61.57ms
step:2300/2330 train_time:141605ms step_avg:61.57ms
step:2301/2330 train_time:141665ms step_avg:61.57ms
step:2302/2330 train_time:141729ms step_avg:61.57ms
step:2303/2330 train_time:141790ms step_avg:61.57ms
step:2304/2330 train_time:141853ms step_avg:61.57ms
step:2305/2330 train_time:141914ms step_avg:61.57ms
step:2306/2330 train_time:141977ms step_avg:61.57ms
step:2307/2330 train_time:142038ms step_avg:61.57ms
step:2308/2330 train_time:142101ms step_avg:61.57ms
step:2309/2330 train_time:142161ms step_avg:61.57ms
step:2310/2330 train_time:142225ms step_avg:61.57ms
step:2311/2330 train_time:142285ms step_avg:61.57ms
step:2312/2330 train_time:142349ms step_avg:61.57ms
step:2313/2330 train_time:142410ms step_avg:61.57ms
step:2314/2330 train_time:142472ms step_avg:61.57ms
step:2315/2330 train_time:142533ms step_avg:61.57ms
step:2316/2330 train_time:142598ms step_avg:61.57ms
step:2317/2330 train_time:142659ms step_avg:61.57ms
step:2318/2330 train_time:142723ms step_avg:61.57ms
step:2319/2330 train_time:142782ms step_avg:61.57ms
step:2320/2330 train_time:142846ms step_avg:61.57ms
step:2321/2330 train_time:142906ms step_avg:61.57ms
step:2322/2330 train_time:142970ms step_avg:61.57ms
step:2323/2330 train_time:143030ms step_avg:61.57ms
step:2324/2330 train_time:143094ms step_avg:61.57ms
step:2325/2330 train_time:143154ms step_avg:61.57ms
step:2326/2330 train_time:143218ms step_avg:61.57ms
step:2327/2330 train_time:143278ms step_avg:61.57ms
step:2328/2330 train_time:143342ms step_avg:61.57ms
step:2329/2330 train_time:143402ms step_avg:61.57ms
step:2330/2330 train_time:143465ms step_avg:61.57ms
step:2330/2330 val_loss:3.3801 train_time:143531ms step_avg:61.60ms
peak memory allocated: 29721 MiB reserved: 44076 MiB
