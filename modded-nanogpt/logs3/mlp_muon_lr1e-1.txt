import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                # v_chunk = divide_by_norm(batched)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                # v_chunk = divide_by_norm(batched_update_grads)
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        # self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "mlp_muon_lr1e-1"
    os.makedirs("logs3", exist_ok=True)
    logfile = f"logs3/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
# embed_params = [p for n, p in model.named_parameters() if "embed" in n]
embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=1e-1, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    return 1.0

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum_min + 0.5 * (momentum_max - momentum_min)

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs3/{run_id}", exist_ok=True)
            torch.save(log, f"logs3/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 07:58:18 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   30C    P0             111W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   28C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   27C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   28C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   27C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   28C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   27C    P0             108W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:78ms step_avg:77.79ms
step:2/2330 train_time:145ms step_avg:72.42ms
step:3/2330 train_time:157ms step_avg:52.45ms
step:4/2330 train_time:171ms step_avg:42.68ms
step:5/2330 train_time:182ms step_avg:36.38ms
step:6/2330 train_time:218ms step_avg:36.27ms
step:7/2330 train_time:251ms step_avg:35.86ms
step:8/2330 train_time:295ms step_avg:36.84ms
step:9/2330 train_time:329ms step_avg:36.56ms
step:10/2330 train_time:373ms step_avg:37.28ms
step:11/2330 train_time:407ms step_avg:37.04ms
step:12/2330 train_time:451ms step_avg:37.60ms
step:13/2330 train_time:486ms step_avg:37.38ms
step:14/2330 train_time:529ms step_avg:37.82ms
step:15/2330 train_time:564ms step_avg:37.59ms
step:16/2330 train_time:607ms step_avg:37.97ms
step:17/2330 train_time:642ms step_avg:37.77ms
step:18/2330 train_time:686ms step_avg:38.09ms
step:19/2330 train_time:720ms step_avg:37.92ms
step:20/2330 train_time:764ms step_avg:38.21ms
step:21/2330 train_time:799ms step_avg:38.04ms
step:22/2330 train_time:843ms step_avg:38.30ms
step:23/2330 train_time:877ms step_avg:38.12ms
step:24/2330 train_time:921ms step_avg:38.36ms
step:25/2330 train_time:956ms step_avg:38.23ms
step:26/2330 train_time:1002ms step_avg:38.54ms
step:27/2330 train_time:1041ms step_avg:38.54ms
step:28/2330 train_time:1088ms step_avg:38.87ms
step:29/2330 train_time:1126ms step_avg:38.84ms
step:30/2330 train_time:1174ms step_avg:39.12ms
step:31/2330 train_time:1210ms step_avg:39.02ms
step:32/2330 train_time:1254ms step_avg:39.20ms
step:33/2330 train_time:1290ms step_avg:39.08ms
step:34/2330 train_time:1333ms step_avg:39.22ms
step:35/2330 train_time:1368ms step_avg:39.09ms
step:36/2330 train_time:1412ms step_avg:39.22ms
step:37/2330 train_time:1447ms step_avg:39.10ms
step:38/2330 train_time:1492ms step_avg:39.27ms
step:39/2330 train_time:1527ms step_avg:39.15ms
step:40/2330 train_time:1571ms step_avg:39.28ms
step:41/2330 train_time:1605ms step_avg:39.16ms
step:42/2330 train_time:1649ms step_avg:39.27ms
step:43/2330 train_time:1683ms step_avg:39.15ms
step:44/2330 train_time:1726ms step_avg:39.23ms
step:45/2330 train_time:1761ms step_avg:39.14ms
step:46/2330 train_time:1805ms step_avg:39.24ms
step:47/2330 train_time:1840ms step_avg:39.16ms
step:48/2330 train_time:1884ms step_avg:39.26ms
step:49/2330 train_time:1920ms step_avg:39.18ms
step:50/2330 train_time:1965ms step_avg:39.30ms
step:51/2330 train_time:2002ms step_avg:39.25ms
step:52/2330 train_time:2048ms step_avg:39.38ms
step:53/2330 train_time:2083ms step_avg:39.31ms
step:54/2330 train_time:2129ms step_avg:39.43ms
step:55/2330 train_time:2166ms step_avg:39.38ms
step:56/2330 train_time:2211ms step_avg:39.48ms
step:57/2330 train_time:2246ms step_avg:39.41ms
step:58/2330 train_time:2292ms step_avg:39.52ms
step:59/2330 train_time:2328ms step_avg:39.46ms
step:60/2330 train_time:2373ms step_avg:39.54ms
step:61/2330 train_time:2407ms step_avg:39.46ms
step:62/2330 train_time:2451ms step_avg:39.53ms
step:63/2330 train_time:2485ms step_avg:39.44ms
step:64/2330 train_time:2529ms step_avg:39.51ms
step:65/2330 train_time:2563ms step_avg:39.43ms
step:66/2330 train_time:2607ms step_avg:39.50ms
step:67/2330 train_time:2642ms step_avg:39.43ms
step:68/2330 train_time:2685ms step_avg:39.49ms
step:69/2330 train_time:2720ms step_avg:39.41ms
step:70/2330 train_time:2763ms step_avg:39.47ms
step:71/2330 train_time:2797ms step_avg:39.39ms
step:72/2330 train_time:2841ms step_avg:39.46ms
step:73/2330 train_time:2876ms step_avg:39.40ms
step:74/2330 train_time:2921ms step_avg:39.47ms
step:75/2330 train_time:2956ms step_avg:39.42ms
step:76/2330 train_time:3002ms step_avg:39.51ms
step:77/2330 train_time:3038ms step_avg:39.46ms
step:78/2330 train_time:3083ms step_avg:39.53ms
step:79/2330 train_time:3119ms step_avg:39.48ms
step:80/2330 train_time:3163ms step_avg:39.54ms
step:81/2330 train_time:3199ms step_avg:39.49ms
step:82/2330 train_time:3244ms step_avg:39.57ms
step:83/2330 train_time:3280ms step_avg:39.52ms
step:84/2330 train_time:3325ms step_avg:39.58ms
step:85/2330 train_time:3361ms step_avg:39.54ms
step:86/2330 train_time:3406ms step_avg:39.60ms
step:87/2330 train_time:3441ms step_avg:39.55ms
step:88/2330 train_time:3485ms step_avg:39.60ms
step:89/2330 train_time:3520ms step_avg:39.55ms
step:90/2330 train_time:3564ms step_avg:39.60ms
step:91/2330 train_time:3599ms step_avg:39.55ms
step:92/2330 train_time:3643ms step_avg:39.60ms
step:93/2330 train_time:3677ms step_avg:39.54ms
step:94/2330 train_time:3721ms step_avg:39.58ms
step:95/2330 train_time:3756ms step_avg:39.53ms
step:96/2330 train_time:3800ms step_avg:39.58ms
step:97/2330 train_time:3835ms step_avg:39.53ms
step:98/2330 train_time:3879ms step_avg:39.58ms
step:99/2330 train_time:3914ms step_avg:39.53ms
step:100/2330 train_time:3959ms step_avg:39.59ms
step:101/2330 train_time:3994ms step_avg:39.54ms
step:102/2330 train_time:4039ms step_avg:39.59ms
step:103/2330 train_time:4074ms step_avg:39.56ms
step:104/2330 train_time:4119ms step_avg:39.60ms
step:105/2330 train_time:4154ms step_avg:39.56ms
step:106/2330 train_time:4199ms step_avg:39.61ms
step:107/2330 train_time:4235ms step_avg:39.58ms
step:108/2330 train_time:4280ms step_avg:39.63ms
step:109/2330 train_time:4315ms step_avg:39.59ms
step:110/2330 train_time:4360ms step_avg:39.63ms
step:111/2330 train_time:4395ms step_avg:39.59ms
step:112/2330 train_time:4440ms step_avg:39.65ms
step:113/2330 train_time:4474ms step_avg:39.60ms
step:114/2330 train_time:4518ms step_avg:39.63ms
step:115/2330 train_time:4553ms step_avg:39.59ms
step:116/2330 train_time:4597ms step_avg:39.63ms
step:117/2330 train_time:4632ms step_avg:39.59ms
step:118/2330 train_time:4676ms step_avg:39.63ms
step:119/2330 train_time:4710ms step_avg:39.58ms
step:120/2330 train_time:4754ms step_avg:39.62ms
step:121/2330 train_time:4789ms step_avg:39.58ms
step:122/2330 train_time:4833ms step_avg:39.61ms
step:123/2330 train_time:4868ms step_avg:39.58ms
step:124/2330 train_time:4912ms step_avg:39.62ms
step:125/2330 train_time:4947ms step_avg:39.58ms
step:126/2330 train_time:4992ms step_avg:39.62ms
step:127/2330 train_time:5028ms step_avg:39.59ms
step:128/2330 train_time:5073ms step_avg:39.63ms
step:129/2330 train_time:5109ms step_avg:39.61ms
step:130/2330 train_time:5154ms step_avg:39.65ms
step:131/2330 train_time:5189ms step_avg:39.61ms
step:132/2330 train_time:5235ms step_avg:39.66ms
step:133/2330 train_time:5270ms step_avg:39.62ms
step:134/2330 train_time:5315ms step_avg:39.67ms
step:135/2330 train_time:5350ms step_avg:39.63ms
step:136/2330 train_time:5394ms step_avg:39.66ms
step:137/2330 train_time:5430ms step_avg:39.63ms
step:138/2330 train_time:5474ms step_avg:39.67ms
step:139/2330 train_time:5509ms step_avg:39.64ms
step:140/2330 train_time:5553ms step_avg:39.67ms
step:141/2330 train_time:5588ms step_avg:39.63ms
step:142/2330 train_time:5633ms step_avg:39.67ms
step:143/2330 train_time:5668ms step_avg:39.63ms
step:144/2330 train_time:5712ms step_avg:39.66ms
step:145/2330 train_time:5746ms step_avg:39.63ms
step:146/2330 train_time:5791ms step_avg:39.66ms
step:147/2330 train_time:5825ms step_avg:39.63ms
step:148/2330 train_time:5870ms step_avg:39.66ms
step:149/2330 train_time:5905ms step_avg:39.63ms
step:150/2330 train_time:5949ms step_avg:39.66ms
step:151/2330 train_time:5984ms step_avg:39.63ms
step:152/2330 train_time:6028ms step_avg:39.66ms
step:153/2330 train_time:6063ms step_avg:39.63ms
step:154/2330 train_time:6108ms step_avg:39.66ms
step:155/2330 train_time:6143ms step_avg:39.63ms
step:156/2330 train_time:6188ms step_avg:39.67ms
step:157/2330 train_time:6224ms step_avg:39.64ms
step:158/2330 train_time:6269ms step_avg:39.68ms
step:159/2330 train_time:6304ms step_avg:39.65ms
step:160/2330 train_time:6349ms step_avg:39.68ms
step:161/2330 train_time:6385ms step_avg:39.66ms
step:162/2330 train_time:6430ms step_avg:39.69ms
step:163/2330 train_time:6465ms step_avg:39.66ms
step:164/2330 train_time:6509ms step_avg:39.69ms
step:165/2330 train_time:6544ms step_avg:39.66ms
step:166/2330 train_time:6588ms step_avg:39.69ms
step:167/2330 train_time:6623ms step_avg:39.66ms
step:168/2330 train_time:6667ms step_avg:39.68ms
step:169/2330 train_time:6702ms step_avg:39.66ms
step:170/2330 train_time:6746ms step_avg:39.68ms
step:171/2330 train_time:6781ms step_avg:39.65ms
step:172/2330 train_time:6826ms step_avg:39.68ms
step:173/2330 train_time:6861ms step_avg:39.66ms
step:174/2330 train_time:6906ms step_avg:39.69ms
step:175/2330 train_time:6941ms step_avg:39.66ms
step:176/2330 train_time:6986ms step_avg:39.70ms
step:177/2330 train_time:7022ms step_avg:39.67ms
step:178/2330 train_time:7067ms step_avg:39.70ms
step:179/2330 train_time:7102ms step_avg:39.68ms
step:180/2330 train_time:7147ms step_avg:39.71ms
step:181/2330 train_time:7183ms step_avg:39.68ms
step:182/2330 train_time:7227ms step_avg:39.71ms
step:183/2330 train_time:7263ms step_avg:39.69ms
step:184/2330 train_time:7308ms step_avg:39.72ms
step:185/2330 train_time:7343ms step_avg:39.69ms
step:186/2330 train_time:7388ms step_avg:39.72ms
step:187/2330 train_time:7423ms step_avg:39.69ms
step:188/2330 train_time:7467ms step_avg:39.72ms
step:189/2330 train_time:7502ms step_avg:39.69ms
step:190/2330 train_time:7547ms step_avg:39.72ms
step:191/2330 train_time:7582ms step_avg:39.70ms
step:192/2330 train_time:7627ms step_avg:39.72ms
step:193/2330 train_time:7662ms step_avg:39.70ms
step:194/2330 train_time:7706ms step_avg:39.72ms
step:195/2330 train_time:7741ms step_avg:39.70ms
step:196/2330 train_time:7786ms step_avg:39.72ms
step:197/2330 train_time:7821ms step_avg:39.70ms
step:198/2330 train_time:7865ms step_avg:39.72ms
step:199/2330 train_time:7900ms step_avg:39.70ms
step:200/2330 train_time:7945ms step_avg:39.72ms
step:201/2330 train_time:7980ms step_avg:39.70ms
step:202/2330 train_time:8024ms step_avg:39.72ms
step:203/2330 train_time:8060ms step_avg:39.70ms
step:204/2330 train_time:8104ms step_avg:39.73ms
step:205/2330 train_time:8140ms step_avg:39.71ms
step:206/2330 train_time:8185ms step_avg:39.73ms
step:207/2330 train_time:8221ms step_avg:39.72ms
step:208/2330 train_time:8267ms step_avg:39.74ms
step:209/2330 train_time:8303ms step_avg:39.73ms
step:210/2330 train_time:8347ms step_avg:39.75ms
step:211/2330 train_time:8382ms step_avg:39.73ms
step:212/2330 train_time:8426ms step_avg:39.75ms
step:213/2330 train_time:8462ms step_avg:39.73ms
step:214/2330 train_time:8506ms step_avg:39.75ms
step:215/2330 train_time:8542ms step_avg:39.73ms
step:216/2330 train_time:8586ms step_avg:39.75ms
step:217/2330 train_time:8622ms step_avg:39.73ms
step:218/2330 train_time:8666ms step_avg:39.75ms
step:219/2330 train_time:8702ms step_avg:39.74ms
step:220/2330 train_time:8747ms step_avg:39.76ms
step:221/2330 train_time:8782ms step_avg:39.74ms
step:222/2330 train_time:8826ms step_avg:39.75ms
step:223/2330 train_time:8861ms step_avg:39.74ms
step:224/2330 train_time:8906ms step_avg:39.76ms
step:225/2330 train_time:8942ms step_avg:39.74ms
step:226/2330 train_time:8986ms step_avg:39.76ms
step:227/2330 train_time:9021ms step_avg:39.74ms
step:228/2330 train_time:9066ms step_avg:39.76ms
step:229/2330 train_time:9101ms step_avg:39.74ms
step:230/2330 train_time:9146ms step_avg:39.76ms
step:231/2330 train_time:9181ms step_avg:39.75ms
step:232/2330 train_time:9227ms step_avg:39.77ms
step:233/2330 train_time:9263ms step_avg:39.76ms
step:234/2330 train_time:9308ms step_avg:39.78ms
step:235/2330 train_time:9343ms step_avg:39.76ms
step:236/2330 train_time:9387ms step_avg:39.78ms
step:237/2330 train_time:9423ms step_avg:39.76ms
step:238/2330 train_time:9468ms step_avg:39.78ms
step:239/2330 train_time:9503ms step_avg:39.76ms
step:240/2330 train_time:9547ms step_avg:39.78ms
step:241/2330 train_time:9582ms step_avg:39.76ms
step:242/2330 train_time:9627ms step_avg:39.78ms
step:243/2330 train_time:9662ms step_avg:39.76ms
step:244/2330 train_time:9706ms step_avg:39.78ms
step:245/2330 train_time:9742ms step_avg:39.76ms
step:246/2330 train_time:9787ms step_avg:39.78ms
step:247/2330 train_time:9823ms step_avg:39.77ms
step:248/2330 train_time:9867ms step_avg:39.79ms
step:249/2330 train_time:9902ms step_avg:39.77ms
step:250/2330 train_time:9947ms step_avg:39.79ms
step:250/2330 val_loss:5.4228 train_time:10034ms step_avg:40.14ms
step:251/2330 train_time:10047ms step_avg:40.03ms
step:252/2330 train_time:10059ms step_avg:39.92ms
step:253/2330 train_time:10069ms step_avg:39.80ms
step:254/2330 train_time:10106ms step_avg:39.79ms
step:255/2330 train_time:10140ms step_avg:39.77ms
step:256/2330 train_time:10184ms step_avg:39.78ms
step:257/2330 train_time:10219ms step_avg:39.76ms
step:258/2330 train_time:10262ms step_avg:39.77ms
step:259/2330 train_time:10296ms step_avg:39.75ms
step:260/2330 train_time:10341ms step_avg:39.77ms
step:261/2330 train_time:10380ms step_avg:39.77ms
step:262/2330 train_time:10427ms step_avg:39.80ms
step:263/2330 train_time:10464ms step_avg:39.79ms
step:264/2330 train_time:10509ms step_avg:39.81ms
step:265/2330 train_time:10545ms step_avg:39.79ms
step:266/2330 train_time:10589ms step_avg:39.81ms
step:267/2330 train_time:10624ms step_avg:39.79ms
step:268/2330 train_time:10668ms step_avg:39.81ms
step:269/2330 train_time:10703ms step_avg:39.79ms
step:270/2330 train_time:10747ms step_avg:39.80ms
step:271/2330 train_time:10782ms step_avg:39.79ms
step:272/2330 train_time:10826ms step_avg:39.80ms
step:273/2330 train_time:10862ms step_avg:39.79ms
step:274/2330 train_time:10906ms step_avg:39.80ms
step:275/2330 train_time:10941ms step_avg:39.79ms
step:276/2330 train_time:10987ms step_avg:39.81ms
step:277/2330 train_time:11022ms step_avg:39.79ms
step:278/2330 train_time:11067ms step_avg:39.81ms
step:279/2330 train_time:11102ms step_avg:39.79ms
step:280/2330 train_time:11146ms step_avg:39.81ms
step:281/2330 train_time:11181ms step_avg:39.79ms
step:282/2330 train_time:11225ms step_avg:39.80ms
step:283/2330 train_time:11260ms step_avg:39.79ms
step:284/2330 train_time:11306ms step_avg:39.81ms
step:285/2330 train_time:11342ms step_avg:39.80ms
step:286/2330 train_time:11388ms step_avg:39.82ms
step:287/2330 train_time:11425ms step_avg:39.81ms
step:288/2330 train_time:11470ms step_avg:39.83ms
step:289/2330 train_time:11506ms step_avg:39.81ms
step:290/2330 train_time:11551ms step_avg:39.83ms
step:291/2330 train_time:11586ms step_avg:39.82ms
step:292/2330 train_time:11631ms step_avg:39.83ms
step:293/2330 train_time:11666ms step_avg:39.82ms
step:294/2330 train_time:11711ms step_avg:39.83ms
step:295/2330 train_time:11746ms step_avg:39.82ms
step:296/2330 train_time:11790ms step_avg:39.83ms
step:297/2330 train_time:11825ms step_avg:39.82ms
step:298/2330 train_time:11870ms step_avg:39.83ms
step:299/2330 train_time:11906ms step_avg:39.82ms
step:300/2330 train_time:11950ms step_avg:39.83ms
step:301/2330 train_time:11986ms step_avg:39.82ms
step:302/2330 train_time:12030ms step_avg:39.84ms
step:303/2330 train_time:12066ms step_avg:39.82ms
step:304/2330 train_time:12111ms step_avg:39.84ms
step:305/2330 train_time:12146ms step_avg:39.82ms
step:306/2330 train_time:12191ms step_avg:39.84ms
step:307/2330 train_time:12227ms step_avg:39.83ms
step:308/2330 train_time:12272ms step_avg:39.84ms
step:309/2330 train_time:12308ms step_avg:39.83ms
step:310/2330 train_time:12352ms step_avg:39.85ms
step:311/2330 train_time:12388ms step_avg:39.83ms
step:312/2330 train_time:12433ms step_avg:39.85ms
step:313/2330 train_time:12468ms step_avg:39.83ms
step:314/2330 train_time:12512ms step_avg:39.85ms
step:315/2330 train_time:12547ms step_avg:39.83ms
step:316/2330 train_time:12593ms step_avg:39.85ms
step:317/2330 train_time:12628ms step_avg:39.84ms
step:318/2330 train_time:12673ms step_avg:39.85ms
step:319/2330 train_time:12707ms step_avg:39.83ms
step:320/2330 train_time:12752ms step_avg:39.85ms
step:321/2330 train_time:12786ms step_avg:39.83ms
step:322/2330 train_time:12831ms step_avg:39.85ms
step:323/2330 train_time:12866ms step_avg:39.83ms
step:324/2330 train_time:12910ms step_avg:39.85ms
step:325/2330 train_time:12945ms step_avg:39.83ms
step:326/2330 train_time:12990ms step_avg:39.85ms
step:327/2330 train_time:13025ms step_avg:39.83ms
step:328/2330 train_time:13070ms step_avg:39.85ms
step:329/2330 train_time:13105ms step_avg:39.83ms
step:330/2330 train_time:13150ms step_avg:39.85ms
step:331/2330 train_time:13185ms step_avg:39.83ms
step:332/2330 train_time:13231ms step_avg:39.85ms
step:333/2330 train_time:13266ms step_avg:39.84ms
step:334/2330 train_time:13310ms step_avg:39.85ms
step:335/2330 train_time:13344ms step_avg:39.83ms
step:336/2330 train_time:13389ms step_avg:39.85ms
step:337/2330 train_time:13425ms step_avg:39.84ms
step:338/2330 train_time:13470ms step_avg:39.85ms
step:339/2330 train_time:13505ms step_avg:39.84ms
step:340/2330 train_time:13550ms step_avg:39.85ms
step:341/2330 train_time:13585ms step_avg:39.84ms
step:342/2330 train_time:13630ms step_avg:39.85ms
step:343/2330 train_time:13665ms step_avg:39.84ms
step:344/2330 train_time:13709ms step_avg:39.85ms
step:345/2330 train_time:13744ms step_avg:39.84ms
step:346/2330 train_time:13789ms step_avg:39.85ms
step:347/2330 train_time:13824ms step_avg:39.84ms
step:348/2330 train_time:13868ms step_avg:39.85ms
step:349/2330 train_time:13903ms step_avg:39.84ms
step:350/2330 train_time:13948ms step_avg:39.85ms
step:351/2330 train_time:13983ms step_avg:39.84ms
step:352/2330 train_time:14027ms step_avg:39.85ms
step:353/2330 train_time:14062ms step_avg:39.84ms
step:354/2330 train_time:14107ms step_avg:39.85ms
step:355/2330 train_time:14142ms step_avg:39.84ms
step:356/2330 train_time:14186ms step_avg:39.85ms
step:357/2330 train_time:14222ms step_avg:39.84ms
step:358/2330 train_time:14266ms step_avg:39.85ms
step:359/2330 train_time:14301ms step_avg:39.84ms
step:360/2330 train_time:14347ms step_avg:39.85ms
step:361/2330 train_time:14382ms step_avg:39.84ms
step:362/2330 train_time:14427ms step_avg:39.85ms
step:363/2330 train_time:14463ms step_avg:39.84ms
step:364/2330 train_time:14508ms step_avg:39.86ms
step:365/2330 train_time:14544ms step_avg:39.85ms
step:366/2330 train_time:14588ms step_avg:39.86ms
step:367/2330 train_time:14624ms step_avg:39.85ms
step:368/2330 train_time:14668ms step_avg:39.86ms
step:369/2330 train_time:14703ms step_avg:39.85ms
step:370/2330 train_time:14748ms step_avg:39.86ms
step:371/2330 train_time:14783ms step_avg:39.85ms
step:372/2330 train_time:14828ms step_avg:39.86ms
step:373/2330 train_time:14863ms step_avg:39.85ms
step:374/2330 train_time:14907ms step_avg:39.86ms
step:375/2330 train_time:14942ms step_avg:39.85ms
step:376/2330 train_time:14987ms step_avg:39.86ms
step:377/2330 train_time:15022ms step_avg:39.85ms
step:378/2330 train_time:15066ms step_avg:39.86ms
step:379/2330 train_time:15102ms step_avg:39.85ms
step:380/2330 train_time:15146ms step_avg:39.86ms
step:381/2330 train_time:15182ms step_avg:39.85ms
step:382/2330 train_time:15226ms step_avg:39.86ms
step:383/2330 train_time:15262ms step_avg:39.85ms
step:384/2330 train_time:15307ms step_avg:39.86ms
step:385/2330 train_time:15343ms step_avg:39.85ms
step:386/2330 train_time:15387ms step_avg:39.86ms
step:387/2330 train_time:15423ms step_avg:39.85ms
step:388/2330 train_time:15468ms step_avg:39.87ms
step:389/2330 train_time:15503ms step_avg:39.85ms
step:390/2330 train_time:15548ms step_avg:39.87ms
step:391/2330 train_time:15583ms step_avg:39.86ms
step:392/2330 train_time:15628ms step_avg:39.87ms
step:393/2330 train_time:15663ms step_avg:39.85ms
step:394/2330 train_time:15707ms step_avg:39.87ms
step:395/2330 train_time:15743ms step_avg:39.85ms
step:396/2330 train_time:15787ms step_avg:39.87ms
step:397/2330 train_time:15823ms step_avg:39.86ms
step:398/2330 train_time:15868ms step_avg:39.87ms
step:399/2330 train_time:15903ms step_avg:39.86ms
step:400/2330 train_time:15948ms step_avg:39.87ms
step:401/2330 train_time:15983ms step_avg:39.86ms
step:402/2330 train_time:16027ms step_avg:39.87ms
step:403/2330 train_time:16062ms step_avg:39.86ms
step:404/2330 train_time:16106ms step_avg:39.87ms
step:405/2330 train_time:16142ms step_avg:39.86ms
step:406/2330 train_time:16186ms step_avg:39.87ms
step:407/2330 train_time:16221ms step_avg:39.86ms
step:408/2330 train_time:16266ms step_avg:39.87ms
step:409/2330 train_time:16301ms step_avg:39.86ms
step:410/2330 train_time:16347ms step_avg:39.87ms
step:411/2330 train_time:16382ms step_avg:39.86ms
step:412/2330 train_time:16427ms step_avg:39.87ms
step:413/2330 train_time:16462ms step_avg:39.86ms
step:414/2330 train_time:16507ms step_avg:39.87ms
step:415/2330 train_time:16542ms step_avg:39.86ms
step:416/2330 train_time:16586ms step_avg:39.87ms
step:417/2330 train_time:16622ms step_avg:39.86ms
step:418/2330 train_time:16666ms step_avg:39.87ms
step:419/2330 train_time:16702ms step_avg:39.86ms
step:420/2330 train_time:16746ms step_avg:39.87ms
step:421/2330 train_time:16782ms step_avg:39.86ms
step:422/2330 train_time:16827ms step_avg:39.87ms
step:423/2330 train_time:16863ms step_avg:39.86ms
step:424/2330 train_time:16907ms step_avg:39.88ms
step:425/2330 train_time:16942ms step_avg:39.86ms
step:426/2330 train_time:16987ms step_avg:39.87ms
step:427/2330 train_time:17022ms step_avg:39.86ms
step:428/2330 train_time:17066ms step_avg:39.87ms
step:429/2330 train_time:17101ms step_avg:39.86ms
step:430/2330 train_time:17146ms step_avg:39.87ms
step:431/2330 train_time:17182ms step_avg:39.86ms
step:432/2330 train_time:17227ms step_avg:39.88ms
step:433/2330 train_time:17262ms step_avg:39.87ms
step:434/2330 train_time:17307ms step_avg:39.88ms
step:435/2330 train_time:17342ms step_avg:39.87ms
step:436/2330 train_time:17386ms step_avg:39.88ms
step:437/2330 train_time:17422ms step_avg:39.87ms
step:438/2330 train_time:17467ms step_avg:39.88ms
step:439/2330 train_time:17502ms step_avg:39.87ms
step:440/2330 train_time:17547ms step_avg:39.88ms
step:441/2330 train_time:17583ms step_avg:39.87ms
step:442/2330 train_time:17627ms step_avg:39.88ms
step:443/2330 train_time:17663ms step_avg:39.87ms
step:444/2330 train_time:17708ms step_avg:39.88ms
step:445/2330 train_time:17743ms step_avg:39.87ms
step:446/2330 train_time:17788ms step_avg:39.88ms
step:447/2330 train_time:17823ms step_avg:39.87ms
step:448/2330 train_time:17868ms step_avg:39.88ms
step:449/2330 train_time:17904ms step_avg:39.87ms
step:450/2330 train_time:17948ms step_avg:39.88ms
step:451/2330 train_time:17983ms step_avg:39.87ms
step:452/2330 train_time:18028ms step_avg:39.88ms
step:453/2330 train_time:18064ms step_avg:39.88ms
step:454/2330 train_time:18109ms step_avg:39.89ms
step:455/2330 train_time:18144ms step_avg:39.88ms
step:456/2330 train_time:18189ms step_avg:39.89ms
step:457/2330 train_time:18224ms step_avg:39.88ms
step:458/2330 train_time:18269ms step_avg:39.89ms
step:459/2330 train_time:18305ms step_avg:39.88ms
step:460/2330 train_time:18350ms step_avg:39.89ms
step:461/2330 train_time:18385ms step_avg:39.88ms
step:462/2330 train_time:18432ms step_avg:39.90ms
step:463/2330 train_time:18466ms step_avg:39.88ms
step:464/2330 train_time:18512ms step_avg:39.90ms
step:465/2330 train_time:18546ms step_avg:39.88ms
step:466/2330 train_time:18592ms step_avg:39.90ms
step:467/2330 train_time:18627ms step_avg:39.89ms
step:468/2330 train_time:18672ms step_avg:39.90ms
step:469/2330 train_time:18706ms step_avg:39.89ms
step:470/2330 train_time:18752ms step_avg:39.90ms
step:471/2330 train_time:18787ms step_avg:39.89ms
step:472/2330 train_time:18832ms step_avg:39.90ms
step:473/2330 train_time:18867ms step_avg:39.89ms
step:474/2330 train_time:18911ms step_avg:39.90ms
step:475/2330 train_time:18946ms step_avg:39.89ms
step:476/2330 train_time:18991ms step_avg:39.90ms
step:477/2330 train_time:19027ms step_avg:39.89ms
step:478/2330 train_time:19071ms step_avg:39.90ms
step:479/2330 train_time:19107ms step_avg:39.89ms
step:480/2330 train_time:19152ms step_avg:39.90ms
step:481/2330 train_time:19186ms step_avg:39.89ms
step:482/2330 train_time:19231ms step_avg:39.90ms
step:483/2330 train_time:19267ms step_avg:39.89ms
step:484/2330 train_time:19312ms step_avg:39.90ms
step:485/2330 train_time:19346ms step_avg:39.89ms
step:486/2330 train_time:19391ms step_avg:39.90ms
step:487/2330 train_time:19426ms step_avg:39.89ms
step:488/2330 train_time:19472ms step_avg:39.90ms
step:489/2330 train_time:19507ms step_avg:39.89ms
step:490/2330 train_time:19552ms step_avg:39.90ms
step:491/2330 train_time:19589ms step_avg:39.90ms
step:492/2330 train_time:19633ms step_avg:39.91ms
step:493/2330 train_time:19668ms step_avg:39.89ms
step:494/2330 train_time:19713ms step_avg:39.91ms
step:495/2330 train_time:19748ms step_avg:39.89ms
step:496/2330 train_time:19792ms step_avg:39.90ms
step:497/2330 train_time:19828ms step_avg:39.89ms
step:498/2330 train_time:19872ms step_avg:39.90ms
step:499/2330 train_time:19907ms step_avg:39.89ms
step:500/2330 train_time:19952ms step_avg:39.90ms
step:500/2330 val_loss:5.3156 train_time:20039ms step_avg:40.08ms
step:501/2330 train_time:20052ms step_avg:40.02ms
step:502/2330 train_time:20065ms step_avg:39.97ms
step:503/2330 train_time:20076ms step_avg:39.91ms
step:504/2330 train_time:20112ms step_avg:39.90ms
step:505/2330 train_time:20146ms step_avg:39.89ms
step:506/2330 train_time:20190ms step_avg:39.90ms
step:507/2330 train_time:20224ms step_avg:39.89ms
step:508/2330 train_time:20267ms step_avg:39.90ms
step:509/2330 train_time:20302ms step_avg:39.89ms
step:510/2330 train_time:20347ms step_avg:39.90ms
step:511/2330 train_time:20385ms step_avg:39.89ms
step:512/2330 train_time:20433ms step_avg:39.91ms
step:513/2330 train_time:20470ms step_avg:39.90ms
step:514/2330 train_time:20515ms step_avg:39.91ms
step:515/2330 train_time:20551ms step_avg:39.91ms
step:516/2330 train_time:20596ms step_avg:39.91ms
step:517/2330 train_time:20631ms step_avg:39.91ms
step:518/2330 train_time:20676ms step_avg:39.92ms
step:519/2330 train_time:20711ms step_avg:39.91ms
step:520/2330 train_time:20756ms step_avg:39.92ms
step:521/2330 train_time:20791ms step_avg:39.91ms
step:522/2330 train_time:20835ms step_avg:39.91ms
step:523/2330 train_time:20869ms step_avg:39.90ms
step:524/2330 train_time:20914ms step_avg:39.91ms
step:525/2330 train_time:20950ms step_avg:39.90ms
step:526/2330 train_time:20995ms step_avg:39.91ms
step:527/2330 train_time:21031ms step_avg:39.91ms
step:528/2330 train_time:21075ms step_avg:39.92ms
step:529/2330 train_time:21111ms step_avg:39.91ms
step:530/2330 train_time:21156ms step_avg:39.92ms
step:531/2330 train_time:21191ms step_avg:39.91ms
step:532/2330 train_time:21235ms step_avg:39.92ms
step:533/2330 train_time:21271ms step_avg:39.91ms
step:534/2330 train_time:21316ms step_avg:39.92ms
step:535/2330 train_time:21353ms step_avg:39.91ms
step:536/2330 train_time:21399ms step_avg:39.92ms
step:537/2330 train_time:21435ms step_avg:39.92ms
step:538/2330 train_time:21481ms step_avg:39.93ms
step:539/2330 train_time:21516ms step_avg:39.92ms
step:540/2330 train_time:21561ms step_avg:39.93ms
step:541/2330 train_time:21597ms step_avg:39.92ms
step:542/2330 train_time:21641ms step_avg:39.93ms
step:543/2330 train_time:21677ms step_avg:39.92ms
step:544/2330 train_time:21722ms step_avg:39.93ms
step:545/2330 train_time:21757ms step_avg:39.92ms
step:546/2330 train_time:21802ms step_avg:39.93ms
step:547/2330 train_time:21837ms step_avg:39.92ms
step:548/2330 train_time:21882ms step_avg:39.93ms
step:549/2330 train_time:21917ms step_avg:39.92ms
step:550/2330 train_time:21962ms step_avg:39.93ms
step:551/2330 train_time:21998ms step_avg:39.92ms
step:552/2330 train_time:22043ms step_avg:39.93ms
step:553/2330 train_time:22078ms step_avg:39.92ms
step:554/2330 train_time:22123ms step_avg:39.93ms
step:555/2330 train_time:22158ms step_avg:39.92ms
step:556/2330 train_time:22203ms step_avg:39.93ms
step:557/2330 train_time:22239ms step_avg:39.93ms
step:558/2330 train_time:22284ms step_avg:39.94ms
step:559/2330 train_time:22320ms step_avg:39.93ms
step:560/2330 train_time:22366ms step_avg:39.94ms
step:561/2330 train_time:22401ms step_avg:39.93ms
step:562/2330 train_time:22446ms step_avg:39.94ms
step:563/2330 train_time:22481ms step_avg:39.93ms
step:564/2330 train_time:22526ms step_avg:39.94ms
step:565/2330 train_time:22562ms step_avg:39.93ms
step:566/2330 train_time:22607ms step_avg:39.94ms
step:567/2330 train_time:22642ms step_avg:39.93ms
step:568/2330 train_time:22687ms step_avg:39.94ms
step:569/2330 train_time:22723ms step_avg:39.94ms
step:570/2330 train_time:22768ms step_avg:39.94ms
step:571/2330 train_time:22803ms step_avg:39.93ms
step:572/2330 train_time:22848ms step_avg:39.94ms
step:573/2330 train_time:22884ms step_avg:39.94ms
step:574/2330 train_time:22929ms step_avg:39.95ms
step:575/2330 train_time:22965ms step_avg:39.94ms
step:576/2330 train_time:23009ms step_avg:39.95ms
step:577/2330 train_time:23045ms step_avg:39.94ms
step:578/2330 train_time:23089ms step_avg:39.95ms
step:579/2330 train_time:23124ms step_avg:39.94ms
step:580/2330 train_time:23168ms step_avg:39.95ms
step:581/2330 train_time:23203ms step_avg:39.94ms
step:582/2330 train_time:23248ms step_avg:39.94ms
step:583/2330 train_time:23283ms step_avg:39.94ms
step:584/2330 train_time:23328ms step_avg:39.95ms
step:585/2330 train_time:23363ms step_avg:39.94ms
step:586/2330 train_time:23408ms step_avg:39.95ms
step:587/2330 train_time:23443ms step_avg:39.94ms
step:588/2330 train_time:23488ms step_avg:39.95ms
step:589/2330 train_time:23524ms step_avg:39.94ms
step:590/2330 train_time:23568ms step_avg:39.95ms
step:591/2330 train_time:23604ms step_avg:39.94ms
step:592/2330 train_time:23649ms step_avg:39.95ms
step:593/2330 train_time:23684ms step_avg:39.94ms
step:594/2330 train_time:23729ms step_avg:39.95ms
step:595/2330 train_time:23764ms step_avg:39.94ms
step:596/2330 train_time:23809ms step_avg:39.95ms
step:597/2330 train_time:23845ms step_avg:39.94ms
step:598/2330 train_time:23890ms step_avg:39.95ms
step:599/2330 train_time:23925ms step_avg:39.94ms
step:600/2330 train_time:23970ms step_avg:39.95ms
step:601/2330 train_time:24005ms step_avg:39.94ms
step:602/2330 train_time:24049ms step_avg:39.95ms
step:603/2330 train_time:24084ms step_avg:39.94ms
step:604/2330 train_time:24129ms step_avg:39.95ms
step:605/2330 train_time:24164ms step_avg:39.94ms
step:606/2330 train_time:24209ms step_avg:39.95ms
step:607/2330 train_time:24244ms step_avg:39.94ms
step:608/2330 train_time:24289ms step_avg:39.95ms
step:609/2330 train_time:24324ms step_avg:39.94ms
step:610/2330 train_time:24369ms step_avg:39.95ms
step:611/2330 train_time:24405ms step_avg:39.94ms
step:612/2330 train_time:24450ms step_avg:39.95ms
step:613/2330 train_time:24486ms step_avg:39.94ms
step:614/2330 train_time:24531ms step_avg:39.95ms
step:615/2330 train_time:24567ms step_avg:39.95ms
step:616/2330 train_time:24611ms step_avg:39.95ms
step:617/2330 train_time:24647ms step_avg:39.95ms
step:618/2330 train_time:24692ms step_avg:39.95ms
step:619/2330 train_time:24727ms step_avg:39.95ms
step:620/2330 train_time:24772ms step_avg:39.95ms
step:621/2330 train_time:24806ms step_avg:39.95ms
step:622/2330 train_time:24851ms step_avg:39.95ms
step:623/2330 train_time:24886ms step_avg:39.95ms
step:624/2330 train_time:24931ms step_avg:39.95ms
step:625/2330 train_time:24966ms step_avg:39.95ms
step:626/2330 train_time:25010ms step_avg:39.95ms
step:627/2330 train_time:25045ms step_avg:39.94ms
step:628/2330 train_time:25090ms step_avg:39.95ms
step:629/2330 train_time:25125ms step_avg:39.94ms
step:630/2330 train_time:25170ms step_avg:39.95ms
step:631/2330 train_time:25205ms step_avg:39.94ms
step:632/2330 train_time:25250ms step_avg:39.95ms
step:633/2330 train_time:25285ms step_avg:39.94ms
step:634/2330 train_time:25330ms step_avg:39.95ms
step:635/2330 train_time:25365ms step_avg:39.95ms
step:636/2330 train_time:25410ms step_avg:39.95ms
step:637/2330 train_time:25445ms step_avg:39.94ms
step:638/2330 train_time:25490ms step_avg:39.95ms
step:639/2330 train_time:25524ms step_avg:39.94ms
step:640/2330 train_time:25569ms step_avg:39.95ms
step:641/2330 train_time:25604ms step_avg:39.94ms
step:642/2330 train_time:25648ms step_avg:39.95ms
step:643/2330 train_time:25684ms step_avg:39.94ms
step:644/2330 train_time:25728ms step_avg:39.95ms
step:645/2330 train_time:25763ms step_avg:39.94ms
step:646/2330 train_time:25808ms step_avg:39.95ms
step:647/2330 train_time:25843ms step_avg:39.94ms
step:648/2330 train_time:25888ms step_avg:39.95ms
step:649/2330 train_time:25923ms step_avg:39.94ms
step:650/2330 train_time:25967ms step_avg:39.95ms
step:651/2330 train_time:26002ms step_avg:39.94ms
step:652/2330 train_time:26046ms step_avg:39.95ms
step:653/2330 train_time:26082ms step_avg:39.94ms
step:654/2330 train_time:26126ms step_avg:39.95ms
step:655/2330 train_time:26161ms step_avg:39.94ms
step:656/2330 train_time:26206ms step_avg:39.95ms
step:657/2330 train_time:26241ms step_avg:39.94ms
step:658/2330 train_time:26285ms step_avg:39.95ms
step:659/2330 train_time:26322ms step_avg:39.94ms
step:660/2330 train_time:26366ms step_avg:39.95ms
step:661/2330 train_time:26402ms step_avg:39.94ms
step:662/2330 train_time:26446ms step_avg:39.95ms
step:663/2330 train_time:26482ms step_avg:39.94ms
step:664/2330 train_time:26526ms step_avg:39.95ms
step:665/2330 train_time:26562ms step_avg:39.94ms
step:666/2330 train_time:26606ms step_avg:39.95ms
step:667/2330 train_time:26642ms step_avg:39.94ms
step:668/2330 train_time:26686ms step_avg:39.95ms
step:669/2330 train_time:26721ms step_avg:39.94ms
step:670/2330 train_time:26766ms step_avg:39.95ms
step:671/2330 train_time:26801ms step_avg:39.94ms
step:672/2330 train_time:26846ms step_avg:39.95ms
step:673/2330 train_time:26882ms step_avg:39.94ms
step:674/2330 train_time:26926ms step_avg:39.95ms
step:675/2330 train_time:26962ms step_avg:39.94ms
step:676/2330 train_time:27007ms step_avg:39.95ms
step:677/2330 train_time:27042ms step_avg:39.94ms
step:678/2330 train_time:27087ms step_avg:39.95ms
step:679/2330 train_time:27123ms step_avg:39.94ms
step:680/2330 train_time:27167ms step_avg:39.95ms
step:681/2330 train_time:27202ms step_avg:39.94ms
step:682/2330 train_time:27247ms step_avg:39.95ms
step:683/2330 train_time:27283ms step_avg:39.95ms
step:684/2330 train_time:27327ms step_avg:39.95ms
step:685/2330 train_time:27362ms step_avg:39.94ms
step:686/2330 train_time:27407ms step_avg:39.95ms
step:687/2330 train_time:27442ms step_avg:39.94ms
step:688/2330 train_time:27486ms step_avg:39.95ms
step:689/2330 train_time:27522ms step_avg:39.95ms
step:690/2330 train_time:27567ms step_avg:39.95ms
step:691/2330 train_time:27602ms step_avg:39.95ms
step:692/2330 train_time:27647ms step_avg:39.95ms
step:693/2330 train_time:27682ms step_avg:39.95ms
step:694/2330 train_time:27726ms step_avg:39.95ms
step:695/2330 train_time:27762ms step_avg:39.94ms
step:696/2330 train_time:27806ms step_avg:39.95ms
step:697/2330 train_time:27842ms step_avg:39.94ms
step:698/2330 train_time:27886ms step_avg:39.95ms
step:699/2330 train_time:27922ms step_avg:39.95ms
step:700/2330 train_time:27966ms step_avg:39.95ms
step:701/2330 train_time:28002ms step_avg:39.95ms
step:702/2330 train_time:28047ms step_avg:39.95ms
step:703/2330 train_time:28082ms step_avg:39.95ms
step:704/2330 train_time:28127ms step_avg:39.95ms
step:705/2330 train_time:28162ms step_avg:39.95ms
step:706/2330 train_time:28207ms step_avg:39.95ms
step:707/2330 train_time:28243ms step_avg:39.95ms
step:708/2330 train_time:28288ms step_avg:39.95ms
step:709/2330 train_time:28323ms step_avg:39.95ms
step:710/2330 train_time:28367ms step_avg:39.95ms
step:711/2330 train_time:28402ms step_avg:39.95ms
step:712/2330 train_time:28446ms step_avg:39.95ms
step:713/2330 train_time:28482ms step_avg:39.95ms
step:714/2330 train_time:28527ms step_avg:39.95ms
step:715/2330 train_time:28562ms step_avg:39.95ms
step:716/2330 train_time:28606ms step_avg:39.95ms
step:717/2330 train_time:28642ms step_avg:39.95ms
step:718/2330 train_time:28687ms step_avg:39.95ms
step:719/2330 train_time:28722ms step_avg:39.95ms
step:720/2330 train_time:28767ms step_avg:39.95ms
step:721/2330 train_time:28802ms step_avg:39.95ms
step:722/2330 train_time:28846ms step_avg:39.95ms
step:723/2330 train_time:28881ms step_avg:39.95ms
step:724/2330 train_time:28926ms step_avg:39.95ms
step:725/2330 train_time:28961ms step_avg:39.95ms
step:726/2330 train_time:29006ms step_avg:39.95ms
step:727/2330 train_time:29041ms step_avg:39.95ms
step:728/2330 train_time:29086ms step_avg:39.95ms
step:729/2330 train_time:29121ms step_avg:39.95ms
step:730/2330 train_time:29166ms step_avg:39.95ms
step:731/2330 train_time:29201ms step_avg:39.95ms
step:732/2330 train_time:29247ms step_avg:39.95ms
step:733/2330 train_time:29282ms step_avg:39.95ms
step:734/2330 train_time:29327ms step_avg:39.95ms
step:735/2330 train_time:29362ms step_avg:39.95ms
step:736/2330 train_time:29407ms step_avg:39.95ms
step:737/2330 train_time:29442ms step_avg:39.95ms
step:738/2330 train_time:29486ms step_avg:39.95ms
step:739/2330 train_time:29522ms step_avg:39.95ms
step:740/2330 train_time:29567ms step_avg:39.95ms
step:741/2330 train_time:29602ms step_avg:39.95ms
step:742/2330 train_time:29646ms step_avg:39.95ms
step:743/2330 train_time:29682ms step_avg:39.95ms
step:744/2330 train_time:29726ms step_avg:39.95ms
step:745/2330 train_time:29762ms step_avg:39.95ms
step:746/2330 train_time:29806ms step_avg:39.96ms
step:747/2330 train_time:29842ms step_avg:39.95ms
step:748/2330 train_time:29886ms step_avg:39.95ms
step:749/2330 train_time:29921ms step_avg:39.95ms
step:750/2330 train_time:29966ms step_avg:39.95ms
step:750/2330 val_loss:5.2479 train_time:30055ms step_avg:40.07ms
step:751/2330 train_time:30068ms step_avg:40.04ms
step:752/2330 train_time:30080ms step_avg:40.00ms
step:753/2330 train_time:30091ms step_avg:39.96ms
step:754/2330 train_time:30128ms step_avg:39.96ms
step:755/2330 train_time:30163ms step_avg:39.95ms
step:756/2330 train_time:30206ms step_avg:39.96ms
step:757/2330 train_time:30241ms step_avg:39.95ms
step:758/2330 train_time:30285ms step_avg:39.95ms
step:759/2330 train_time:30321ms step_avg:39.95ms
step:760/2330 train_time:30370ms step_avg:39.96ms
step:761/2330 train_time:30407ms step_avg:39.96ms
step:762/2330 train_time:30454ms step_avg:39.97ms
step:763/2330 train_time:30490ms step_avg:39.96ms
step:764/2330 train_time:30535ms step_avg:39.97ms
step:765/2330 train_time:30572ms step_avg:39.96ms
step:766/2330 train_time:30615ms step_avg:39.97ms
step:767/2330 train_time:30650ms step_avg:39.96ms
step:768/2330 train_time:30694ms step_avg:39.97ms
step:769/2330 train_time:30729ms step_avg:39.96ms
step:770/2330 train_time:30774ms step_avg:39.97ms
step:771/2330 train_time:30809ms step_avg:39.96ms
step:772/2330 train_time:30853ms step_avg:39.97ms
step:773/2330 train_time:30888ms step_avg:39.96ms
step:774/2330 train_time:30932ms step_avg:39.96ms
step:775/2330 train_time:30967ms step_avg:39.96ms
step:776/2330 train_time:31012ms step_avg:39.96ms
step:777/2330 train_time:31048ms step_avg:39.96ms
step:778/2330 train_time:31094ms step_avg:39.97ms
step:779/2330 train_time:31129ms step_avg:39.96ms
step:780/2330 train_time:31174ms step_avg:39.97ms
step:781/2330 train_time:31208ms step_avg:39.96ms
step:782/2330 train_time:31253ms step_avg:39.97ms
step:783/2330 train_time:31289ms step_avg:39.96ms
step:784/2330 train_time:31334ms step_avg:39.97ms
step:785/2330 train_time:31371ms step_avg:39.96ms
step:786/2330 train_time:31415ms step_avg:39.97ms
step:787/2330 train_time:31452ms step_avg:39.96ms
step:788/2330 train_time:31497ms step_avg:39.97ms
step:789/2330 train_time:31533ms step_avg:39.97ms
step:790/2330 train_time:31578ms step_avg:39.97ms
step:791/2330 train_time:31613ms step_avg:39.97ms
step:792/2330 train_time:31658ms step_avg:39.97ms
step:793/2330 train_time:31694ms step_avg:39.97ms
step:794/2330 train_time:31738ms step_avg:39.97ms
step:795/2330 train_time:31773ms step_avg:39.97ms
step:796/2330 train_time:31817ms step_avg:39.97ms
step:797/2330 train_time:31852ms step_avg:39.96ms
step:798/2330 train_time:31896ms step_avg:39.97ms
step:799/2330 train_time:31932ms step_avg:39.96ms
step:800/2330 train_time:31977ms step_avg:39.97ms
step:801/2330 train_time:32012ms step_avg:39.97ms
step:802/2330 train_time:32056ms step_avg:39.97ms
step:803/2330 train_time:32092ms step_avg:39.96ms
step:804/2330 train_time:32136ms step_avg:39.97ms
step:805/2330 train_time:32172ms step_avg:39.96ms
step:806/2330 train_time:32216ms step_avg:39.97ms
step:807/2330 train_time:32252ms step_avg:39.97ms
step:808/2330 train_time:32298ms step_avg:39.97ms
step:809/2330 train_time:32333ms step_avg:39.97ms
step:810/2330 train_time:32378ms step_avg:39.97ms
step:811/2330 train_time:32413ms step_avg:39.97ms
step:812/2330 train_time:32458ms step_avg:39.97ms
step:813/2330 train_time:32494ms step_avg:39.97ms
step:814/2330 train_time:32539ms step_avg:39.97ms
step:815/2330 train_time:32574ms step_avg:39.97ms
step:816/2330 train_time:32619ms step_avg:39.97ms
step:817/2330 train_time:32654ms step_avg:39.97ms
step:818/2330 train_time:32699ms step_avg:39.97ms
step:819/2330 train_time:32734ms step_avg:39.97ms
step:820/2330 train_time:32778ms step_avg:39.97ms
step:821/2330 train_time:32814ms step_avg:39.97ms
step:822/2330 train_time:32858ms step_avg:39.97ms
step:823/2330 train_time:32893ms step_avg:39.97ms
step:824/2330 train_time:32937ms step_avg:39.97ms
step:825/2330 train_time:32973ms step_avg:39.97ms
step:826/2330 train_time:33017ms step_avg:39.97ms
step:827/2330 train_time:33053ms step_avg:39.97ms
step:828/2330 train_time:33097ms step_avg:39.97ms
step:829/2330 train_time:33132ms step_avg:39.97ms
step:830/2330 train_time:33177ms step_avg:39.97ms
step:831/2330 train_time:33214ms step_avg:39.97ms
step:832/2330 train_time:33259ms step_avg:39.97ms
step:833/2330 train_time:33294ms step_avg:39.97ms
step:834/2330 train_time:33338ms step_avg:39.97ms
step:835/2330 train_time:33374ms step_avg:39.97ms
step:836/2330 train_time:33419ms step_avg:39.98ms
step:837/2330 train_time:33455ms step_avg:39.97ms
step:838/2330 train_time:33501ms step_avg:39.98ms
step:839/2330 train_time:33537ms step_avg:39.97ms
step:840/2330 train_time:33581ms step_avg:39.98ms
step:841/2330 train_time:33616ms step_avg:39.97ms
step:842/2330 train_time:33661ms step_avg:39.98ms
step:843/2330 train_time:33697ms step_avg:39.97ms
step:844/2330 train_time:33742ms step_avg:39.98ms
step:845/2330 train_time:33777ms step_avg:39.97ms
step:846/2330 train_time:33822ms step_avg:39.98ms
step:847/2330 train_time:33857ms step_avg:39.97ms
step:848/2330 train_time:33902ms step_avg:39.98ms
step:849/2330 train_time:33937ms step_avg:39.97ms
step:850/2330 train_time:33981ms step_avg:39.98ms
step:851/2330 train_time:34016ms step_avg:39.97ms
step:852/2330 train_time:34061ms step_avg:39.98ms
step:853/2330 train_time:34096ms step_avg:39.97ms
step:854/2330 train_time:34142ms step_avg:39.98ms
step:855/2330 train_time:34177ms step_avg:39.97ms
step:856/2330 train_time:34223ms step_avg:39.98ms
step:857/2330 train_time:34258ms step_avg:39.97ms
step:858/2330 train_time:34302ms step_avg:39.98ms
step:859/2330 train_time:34337ms step_avg:39.97ms
step:860/2330 train_time:34382ms step_avg:39.98ms
step:861/2330 train_time:34417ms step_avg:39.97ms
step:862/2330 train_time:34462ms step_avg:39.98ms
step:863/2330 train_time:34497ms step_avg:39.97ms
step:864/2330 train_time:34541ms step_avg:39.98ms
step:865/2330 train_time:34575ms step_avg:39.97ms
step:866/2330 train_time:34620ms step_avg:39.98ms
step:867/2330 train_time:34655ms step_avg:39.97ms
step:868/2330 train_time:34700ms step_avg:39.98ms
step:869/2330 train_time:34735ms step_avg:39.97ms
step:870/2330 train_time:34780ms step_avg:39.98ms
step:871/2330 train_time:34815ms step_avg:39.97ms
step:872/2330 train_time:34860ms step_avg:39.98ms
step:873/2330 train_time:34895ms step_avg:39.97ms
step:874/2330 train_time:34939ms step_avg:39.98ms
step:875/2330 train_time:34974ms step_avg:39.97ms
step:876/2330 train_time:35018ms step_avg:39.98ms
step:877/2330 train_time:35054ms step_avg:39.97ms
step:878/2330 train_time:35098ms step_avg:39.98ms
step:879/2330 train_time:35133ms step_avg:39.97ms
step:880/2330 train_time:35178ms step_avg:39.98ms
step:881/2330 train_time:35214ms step_avg:39.97ms
step:882/2330 train_time:35259ms step_avg:39.98ms
step:883/2330 train_time:35294ms step_avg:39.97ms
step:884/2330 train_time:35338ms step_avg:39.98ms
step:885/2330 train_time:35374ms step_avg:39.97ms
step:886/2330 train_time:35418ms step_avg:39.98ms
step:887/2330 train_time:35453ms step_avg:39.97ms
step:888/2330 train_time:35498ms step_avg:39.97ms
step:889/2330 train_time:35533ms step_avg:39.97ms
step:890/2330 train_time:35578ms step_avg:39.98ms
step:891/2330 train_time:35614ms step_avg:39.97ms
step:892/2330 train_time:35659ms step_avg:39.98ms
step:893/2330 train_time:35694ms step_avg:39.97ms
step:894/2330 train_time:35739ms step_avg:39.98ms
step:895/2330 train_time:35774ms step_avg:39.97ms
step:896/2330 train_time:35819ms step_avg:39.98ms
step:897/2330 train_time:35854ms step_avg:39.97ms
step:898/2330 train_time:35898ms step_avg:39.98ms
step:899/2330 train_time:35934ms step_avg:39.97ms
step:900/2330 train_time:35978ms step_avg:39.98ms
step:901/2330 train_time:36014ms step_avg:39.97ms
step:902/2330 train_time:36058ms step_avg:39.98ms
step:903/2330 train_time:36093ms step_avg:39.97ms
step:904/2330 train_time:36137ms step_avg:39.97ms
step:905/2330 train_time:36172ms step_avg:39.97ms
step:906/2330 train_time:36217ms step_avg:39.97ms
step:907/2330 train_time:36253ms step_avg:39.97ms
step:908/2330 train_time:36298ms step_avg:39.98ms
step:909/2330 train_time:36333ms step_avg:39.97ms
step:910/2330 train_time:36378ms step_avg:39.98ms
step:911/2330 train_time:36413ms step_avg:39.97ms
step:912/2330 train_time:36458ms step_avg:39.98ms
step:913/2330 train_time:36494ms step_avg:39.97ms
step:914/2330 train_time:36539ms step_avg:39.98ms
step:915/2330 train_time:36574ms step_avg:39.97ms
step:916/2330 train_time:36619ms step_avg:39.98ms
step:917/2330 train_time:36655ms step_avg:39.97ms
step:918/2330 train_time:36699ms step_avg:39.98ms
step:919/2330 train_time:36734ms step_avg:39.97ms
step:920/2330 train_time:36779ms step_avg:39.98ms
step:921/2330 train_time:36815ms step_avg:39.97ms
step:922/2330 train_time:36859ms step_avg:39.98ms
step:923/2330 train_time:36894ms step_avg:39.97ms
step:924/2330 train_time:36939ms step_avg:39.98ms
step:925/2330 train_time:36974ms step_avg:39.97ms
step:926/2330 train_time:37018ms step_avg:39.98ms
step:927/2330 train_time:37053ms step_avg:39.97ms
step:928/2330 train_time:37098ms step_avg:39.98ms
step:929/2330 train_time:37133ms step_avg:39.97ms
step:930/2330 train_time:37177ms step_avg:39.98ms
step:931/2330 train_time:37213ms step_avg:39.97ms
step:932/2330 train_time:37258ms step_avg:39.98ms
step:933/2330 train_time:37293ms step_avg:39.97ms
step:934/2330 train_time:37337ms step_avg:39.98ms
step:935/2330 train_time:37373ms step_avg:39.97ms
step:936/2330 train_time:37418ms step_avg:39.98ms
step:937/2330 train_time:37453ms step_avg:39.97ms
step:938/2330 train_time:37498ms step_avg:39.98ms
step:939/2330 train_time:37533ms step_avg:39.97ms
step:940/2330 train_time:37578ms step_avg:39.98ms
step:941/2330 train_time:37614ms step_avg:39.97ms
step:942/2330 train_time:37658ms step_avg:39.98ms
step:943/2330 train_time:37694ms step_avg:39.97ms
step:944/2330 train_time:37739ms step_avg:39.98ms
step:945/2330 train_time:37774ms step_avg:39.97ms
step:946/2330 train_time:37819ms step_avg:39.98ms
step:947/2330 train_time:37854ms step_avg:39.97ms
step:948/2330 train_time:37899ms step_avg:39.98ms
step:949/2330 train_time:37935ms step_avg:39.97ms
step:950/2330 train_time:37979ms step_avg:39.98ms
step:951/2330 train_time:38014ms step_avg:39.97ms
step:952/2330 train_time:38059ms step_avg:39.98ms
step:953/2330 train_time:38094ms step_avg:39.97ms
step:954/2330 train_time:38139ms step_avg:39.98ms
step:955/2330 train_time:38174ms step_avg:39.97ms
step:956/2330 train_time:38218ms step_avg:39.98ms
step:957/2330 train_time:38253ms step_avg:39.97ms
step:958/2330 train_time:38298ms step_avg:39.98ms
step:959/2330 train_time:38333ms step_avg:39.97ms
step:960/2330 train_time:38377ms step_avg:39.98ms
step:961/2330 train_time:38413ms step_avg:39.97ms
step:962/2330 train_time:38457ms step_avg:39.98ms
step:963/2330 train_time:38493ms step_avg:39.97ms
step:964/2330 train_time:38538ms step_avg:39.98ms
step:965/2330 train_time:38575ms step_avg:39.97ms
step:966/2330 train_time:38619ms step_avg:39.98ms
step:967/2330 train_time:38655ms step_avg:39.97ms
step:968/2330 train_time:38700ms step_avg:39.98ms
step:969/2330 train_time:38735ms step_avg:39.97ms
step:970/2330 train_time:38780ms step_avg:39.98ms
step:971/2330 train_time:38815ms step_avg:39.97ms
step:972/2330 train_time:38860ms step_avg:39.98ms
step:973/2330 train_time:38895ms step_avg:39.97ms
step:974/2330 train_time:38940ms step_avg:39.98ms
step:975/2330 train_time:38975ms step_avg:39.97ms
step:976/2330 train_time:39019ms step_avg:39.98ms
step:977/2330 train_time:39055ms step_avg:39.97ms
step:978/2330 train_time:39099ms step_avg:39.98ms
step:979/2330 train_time:39135ms step_avg:39.97ms
step:980/2330 train_time:39179ms step_avg:39.98ms
step:981/2330 train_time:39215ms step_avg:39.97ms
step:982/2330 train_time:39260ms step_avg:39.98ms
step:983/2330 train_time:39296ms step_avg:39.98ms
step:984/2330 train_time:39341ms step_avg:39.98ms
step:985/2330 train_time:39376ms step_avg:39.98ms
step:986/2330 train_time:39421ms step_avg:39.98ms
step:987/2330 train_time:39456ms step_avg:39.98ms
step:988/2330 train_time:39501ms step_avg:39.98ms
step:989/2330 train_time:39536ms step_avg:39.98ms
step:990/2330 train_time:39581ms step_avg:39.98ms
step:991/2330 train_time:39616ms step_avg:39.98ms
step:992/2330 train_time:39660ms step_avg:39.98ms
step:993/2330 train_time:39696ms step_avg:39.98ms
step:994/2330 train_time:39740ms step_avg:39.98ms
step:995/2330 train_time:39776ms step_avg:39.98ms
step:996/2330 train_time:39820ms step_avg:39.98ms
step:997/2330 train_time:39855ms step_avg:39.98ms
step:998/2330 train_time:39900ms step_avg:39.98ms
step:999/2330 train_time:39936ms step_avg:39.98ms
step:1000/2330 train_time:39980ms step_avg:39.98ms
step:1000/2330 val_loss:5.2115 train_time:40068ms step_avg:40.07ms
step:1001/2330 train_time:40081ms step_avg:40.04ms
step:1002/2330 train_time:40094ms step_avg:40.01ms
step:1003/2330 train_time:40105ms step_avg:39.98ms
step:1004/2330 train_time:40141ms step_avg:39.98ms
step:1005/2330 train_time:40175ms step_avg:39.97ms
step:1006/2330 train_time:40219ms step_avg:39.98ms
step:1007/2330 train_time:40253ms step_avg:39.97ms
step:1008/2330 train_time:40297ms step_avg:39.98ms
step:1009/2330 train_time:40332ms step_avg:39.97ms
step:1010/2330 train_time:40377ms step_avg:39.98ms
step:1011/2330 train_time:40417ms step_avg:39.98ms
step:1012/2330 train_time:40465ms step_avg:39.99ms
step:1013/2330 train_time:40504ms step_avg:39.98ms
step:1014/2330 train_time:40549ms step_avg:39.99ms
step:1015/2330 train_time:40585ms step_avg:39.99ms
step:1016/2330 train_time:40629ms step_avg:39.99ms
step:1017/2330 train_time:40663ms step_avg:39.98ms
step:1018/2330 train_time:40707ms step_avg:39.99ms
step:1019/2330 train_time:40742ms step_avg:39.98ms
step:1020/2330 train_time:40787ms step_avg:39.99ms
step:1021/2330 train_time:40821ms step_avg:39.98ms
step:1022/2330 train_time:40866ms step_avg:39.99ms
step:1023/2330 train_time:40900ms step_avg:39.98ms
step:1024/2330 train_time:40944ms step_avg:39.98ms
step:1025/2330 train_time:40981ms step_avg:39.98ms
step:1026/2330 train_time:41027ms step_avg:39.99ms
step:1027/2330 train_time:41063ms step_avg:39.98ms
step:1028/2330 train_time:41108ms step_avg:39.99ms
step:1029/2330 train_time:41144ms step_avg:39.98ms
step:1030/2330 train_time:41187ms step_avg:39.99ms
step:1031/2330 train_time:41222ms step_avg:39.98ms
step:1032/2330 train_time:41267ms step_avg:39.99ms
step:1033/2330 train_time:41303ms step_avg:39.98ms
step:1034/2330 train_time:41348ms step_avg:39.99ms
step:1035/2330 train_time:41383ms step_avg:39.98ms
step:1036/2330 train_time:41428ms step_avg:39.99ms
step:1037/2330 train_time:41464ms step_avg:39.98ms
step:1038/2330 train_time:41509ms step_avg:39.99ms
step:1039/2330 train_time:41545ms step_avg:39.99ms
step:1040/2330 train_time:41589ms step_avg:39.99ms
step:1041/2330 train_time:41624ms step_avg:39.99ms
step:1042/2330 train_time:41669ms step_avg:39.99ms
step:1043/2330 train_time:41704ms step_avg:39.98ms
step:1044/2330 train_time:41749ms step_avg:39.99ms
step:1045/2330 train_time:41784ms step_avg:39.98ms
step:1046/2330 train_time:41828ms step_avg:39.99ms
step:1047/2330 train_time:41863ms step_avg:39.98ms
step:1048/2330 train_time:41908ms step_avg:39.99ms
step:1049/2330 train_time:41945ms step_avg:39.99ms
step:1050/2330 train_time:41990ms step_avg:39.99ms
step:1051/2330 train_time:42025ms step_avg:39.99ms
step:1052/2330 train_time:42071ms step_avg:39.99ms
step:1053/2330 train_time:42107ms step_avg:39.99ms
step:1054/2330 train_time:42151ms step_avg:39.99ms
step:1055/2330 train_time:42186ms step_avg:39.99ms
step:1056/2330 train_time:42231ms step_avg:39.99ms
step:1057/2330 train_time:42268ms step_avg:39.99ms
step:1058/2330 train_time:42312ms step_avg:39.99ms
step:1059/2330 train_time:42347ms step_avg:39.99ms
step:1060/2330 train_time:42392ms step_avg:39.99ms
step:1061/2330 train_time:42427ms step_avg:39.99ms
step:1062/2330 train_time:42471ms step_avg:39.99ms
step:1063/2330 train_time:42506ms step_avg:39.99ms
step:1064/2330 train_time:42551ms step_avg:39.99ms
step:1065/2330 train_time:42586ms step_avg:39.99ms
step:1066/2330 train_time:42630ms step_avg:39.99ms
step:1067/2330 train_time:42665ms step_avg:39.99ms
step:1068/2330 train_time:42710ms step_avg:39.99ms
step:1069/2330 train_time:42745ms step_avg:39.99ms
step:1070/2330 train_time:42790ms step_avg:39.99ms
step:1071/2330 train_time:42825ms step_avg:39.99ms
step:1072/2330 train_time:42871ms step_avg:39.99ms
step:1073/2330 train_time:42906ms step_avg:39.99ms
step:1074/2330 train_time:42950ms step_avg:39.99ms
step:1075/2330 train_time:42986ms step_avg:39.99ms
step:1076/2330 train_time:43032ms step_avg:39.99ms
step:1077/2330 train_time:43068ms step_avg:39.99ms
step:1078/2330 train_time:43113ms step_avg:39.99ms
step:1079/2330 train_time:43148ms step_avg:39.99ms
step:1080/2330 train_time:43193ms step_avg:39.99ms
step:1081/2330 train_time:43228ms step_avg:39.99ms
step:1082/2330 train_time:43273ms step_avg:39.99ms
step:1083/2330 train_time:43308ms step_avg:39.99ms
step:1084/2330 train_time:43352ms step_avg:39.99ms
step:1085/2330 train_time:43388ms step_avg:39.99ms
step:1086/2330 train_time:43432ms step_avg:39.99ms
step:1087/2330 train_time:43467ms step_avg:39.99ms
step:1088/2330 train_time:43512ms step_avg:39.99ms
step:1089/2330 train_time:43547ms step_avg:39.99ms
step:1090/2330 train_time:43591ms step_avg:39.99ms
step:1091/2330 train_time:43626ms step_avg:39.99ms
step:1092/2330 train_time:43670ms step_avg:39.99ms
step:1093/2330 train_time:43705ms step_avg:39.99ms
step:1094/2330 train_time:43750ms step_avg:39.99ms
step:1095/2330 train_time:43786ms step_avg:39.99ms
step:1096/2330 train_time:43830ms step_avg:39.99ms
step:1097/2330 train_time:43865ms step_avg:39.99ms
step:1098/2330 train_time:43909ms step_avg:39.99ms
step:1099/2330 train_time:43946ms step_avg:39.99ms
step:1100/2330 train_time:43991ms step_avg:39.99ms
step:1101/2330 train_time:44026ms step_avg:39.99ms
step:1102/2330 train_time:44071ms step_avg:39.99ms
step:1103/2330 train_time:44106ms step_avg:39.99ms
step:1104/2330 train_time:44151ms step_avg:39.99ms
step:1105/2330 train_time:44186ms step_avg:39.99ms
step:1106/2330 train_time:44230ms step_avg:39.99ms
step:1107/2330 train_time:44266ms step_avg:39.99ms
step:1108/2330 train_time:44312ms step_avg:39.99ms
step:1109/2330 train_time:44348ms step_avg:39.99ms
step:1110/2330 train_time:44392ms step_avg:39.99ms
step:1111/2330 train_time:44427ms step_avg:39.99ms
step:1112/2330 train_time:44472ms step_avg:39.99ms
step:1113/2330 train_time:44507ms step_avg:39.99ms
step:1114/2330 train_time:44551ms step_avg:39.99ms
step:1115/2330 train_time:44586ms step_avg:39.99ms
step:1116/2330 train_time:44630ms step_avg:39.99ms
step:1117/2330 train_time:44664ms step_avg:39.99ms
step:1118/2330 train_time:44708ms step_avg:39.99ms
step:1119/2330 train_time:44743ms step_avg:39.98ms
step:1120/2330 train_time:44787ms step_avg:39.99ms
step:1121/2330 train_time:44822ms step_avg:39.98ms
step:1122/2330 train_time:44867ms step_avg:39.99ms
step:1123/2330 train_time:44902ms step_avg:39.98ms
step:1124/2330 train_time:44947ms step_avg:39.99ms
step:1125/2330 train_time:44982ms step_avg:39.98ms
step:1126/2330 train_time:45027ms step_avg:39.99ms
step:1127/2330 train_time:45062ms step_avg:39.98ms
step:1128/2330 train_time:45107ms step_avg:39.99ms
step:1129/2330 train_time:45142ms step_avg:39.98ms
step:1130/2330 train_time:45186ms step_avg:39.99ms
step:1131/2330 train_time:45221ms step_avg:39.98ms
step:1132/2330 train_time:45266ms step_avg:39.99ms
step:1133/2330 train_time:45303ms step_avg:39.98ms
step:1134/2330 train_time:45348ms step_avg:39.99ms
step:1135/2330 train_time:45382ms step_avg:39.98ms
step:1136/2330 train_time:45426ms step_avg:39.99ms
step:1137/2330 train_time:45462ms step_avg:39.98ms
step:1138/2330 train_time:45507ms step_avg:39.99ms
step:1139/2330 train_time:45541ms step_avg:39.98ms
step:1140/2330 train_time:45585ms step_avg:39.99ms
step:1141/2330 train_time:45620ms step_avg:39.98ms
step:1142/2330 train_time:45665ms step_avg:39.99ms
step:1143/2330 train_time:45700ms step_avg:39.98ms
step:1144/2330 train_time:45744ms step_avg:39.99ms
step:1145/2330 train_time:45780ms step_avg:39.98ms
step:1146/2330 train_time:45824ms step_avg:39.99ms
step:1147/2330 train_time:45859ms step_avg:39.98ms
step:1148/2330 train_time:45903ms step_avg:39.99ms
step:1149/2330 train_time:45938ms step_avg:39.98ms
step:1150/2330 train_time:45982ms step_avg:39.98ms
step:1151/2330 train_time:46018ms step_avg:39.98ms
step:1152/2330 train_time:46063ms step_avg:39.98ms
step:1153/2330 train_time:46098ms step_avg:39.98ms
step:1154/2330 train_time:46143ms step_avg:39.99ms
step:1155/2330 train_time:46178ms step_avg:39.98ms
step:1156/2330 train_time:46223ms step_avg:39.99ms
step:1157/2330 train_time:46258ms step_avg:39.98ms
step:1158/2330 train_time:46303ms step_avg:39.99ms
step:1159/2330 train_time:46338ms step_avg:39.98ms
step:1160/2330 train_time:46383ms step_avg:39.99ms
step:1161/2330 train_time:46419ms step_avg:39.98ms
step:1162/2330 train_time:46463ms step_avg:39.99ms
step:1163/2330 train_time:46499ms step_avg:39.98ms
step:1164/2330 train_time:46544ms step_avg:39.99ms
step:1165/2330 train_time:46579ms step_avg:39.98ms
step:1166/2330 train_time:46623ms step_avg:39.99ms
step:1167/2330 train_time:46658ms step_avg:39.98ms
step:1168/2330 train_time:46702ms step_avg:39.98ms
step:1169/2330 train_time:46737ms step_avg:39.98ms
step:1170/2330 train_time:46782ms step_avg:39.98ms
step:1171/2330 train_time:46817ms step_avg:39.98ms
step:1172/2330 train_time:46862ms step_avg:39.98ms
step:1173/2330 train_time:46897ms step_avg:39.98ms
step:1174/2330 train_time:46942ms step_avg:39.98ms
step:1175/2330 train_time:46977ms step_avg:39.98ms
step:1176/2330 train_time:47022ms step_avg:39.98ms
step:1177/2330 train_time:47057ms step_avg:39.98ms
step:1178/2330 train_time:47101ms step_avg:39.98ms
step:1179/2330 train_time:47137ms step_avg:39.98ms
step:1180/2330 train_time:47182ms step_avg:39.98ms
step:1181/2330 train_time:47217ms step_avg:39.98ms
step:1182/2330 train_time:47262ms step_avg:39.98ms
step:1183/2330 train_time:47298ms step_avg:39.98ms
step:1184/2330 train_time:47343ms step_avg:39.99ms
step:1185/2330 train_time:47378ms step_avg:39.98ms
step:1186/2330 train_time:47423ms step_avg:39.99ms
step:1187/2330 train_time:47458ms step_avg:39.98ms
step:1188/2330 train_time:47503ms step_avg:39.99ms
step:1189/2330 train_time:47538ms step_avg:39.98ms
step:1190/2330 train_time:47583ms step_avg:39.99ms
step:1191/2330 train_time:47618ms step_avg:39.98ms
step:1192/2330 train_time:47663ms step_avg:39.99ms
step:1193/2330 train_time:47698ms step_avg:39.98ms
step:1194/2330 train_time:47742ms step_avg:39.98ms
step:1195/2330 train_time:47777ms step_avg:39.98ms
step:1196/2330 train_time:47822ms step_avg:39.98ms
step:1197/2330 train_time:47857ms step_avg:39.98ms
step:1198/2330 train_time:47901ms step_avg:39.98ms
step:1199/2330 train_time:47936ms step_avg:39.98ms
step:1200/2330 train_time:47980ms step_avg:39.98ms
step:1201/2330 train_time:48016ms step_avg:39.98ms
step:1202/2330 train_time:48061ms step_avg:39.98ms
step:1203/2330 train_time:48097ms step_avg:39.98ms
step:1204/2330 train_time:48141ms step_avg:39.98ms
step:1205/2330 train_time:48176ms step_avg:39.98ms
step:1206/2330 train_time:48221ms step_avg:39.98ms
step:1207/2330 train_time:48257ms step_avg:39.98ms
step:1208/2330 train_time:48301ms step_avg:39.98ms
step:1209/2330 train_time:48338ms step_avg:39.98ms
step:1210/2330 train_time:48382ms step_avg:39.99ms
step:1211/2330 train_time:48418ms step_avg:39.98ms
step:1212/2330 train_time:48463ms step_avg:39.99ms
step:1213/2330 train_time:48498ms step_avg:39.98ms
step:1214/2330 train_time:48543ms step_avg:39.99ms
step:1215/2330 train_time:48578ms step_avg:39.98ms
step:1216/2330 train_time:48623ms step_avg:39.99ms
step:1217/2330 train_time:48658ms step_avg:39.98ms
step:1218/2330 train_time:48703ms step_avg:39.99ms
step:1219/2330 train_time:48739ms step_avg:39.98ms
step:1220/2330 train_time:48783ms step_avg:39.99ms
step:1221/2330 train_time:48818ms step_avg:39.98ms
step:1222/2330 train_time:48863ms step_avg:39.99ms
step:1223/2330 train_time:48898ms step_avg:39.98ms
step:1224/2330 train_time:48943ms step_avg:39.99ms
step:1225/2330 train_time:48978ms step_avg:39.98ms
step:1226/2330 train_time:49023ms step_avg:39.99ms
step:1227/2330 train_time:49058ms step_avg:39.98ms
step:1228/2330 train_time:49102ms step_avg:39.99ms
step:1229/2330 train_time:49138ms step_avg:39.98ms
step:1230/2330 train_time:49182ms step_avg:39.99ms
step:1231/2330 train_time:49218ms step_avg:39.98ms
step:1232/2330 train_time:49262ms step_avg:39.99ms
step:1233/2330 train_time:49298ms step_avg:39.98ms
step:1234/2330 train_time:49343ms step_avg:39.99ms
step:1235/2330 train_time:49379ms step_avg:39.98ms
step:1236/2330 train_time:49423ms step_avg:39.99ms
step:1237/2330 train_time:49459ms step_avg:39.98ms
step:1238/2330 train_time:49503ms step_avg:39.99ms
step:1239/2330 train_time:49539ms step_avg:39.98ms
step:1240/2330 train_time:49583ms step_avg:39.99ms
step:1241/2330 train_time:49618ms step_avg:39.98ms
step:1242/2330 train_time:49663ms step_avg:39.99ms
step:1243/2330 train_time:49698ms step_avg:39.98ms
step:1244/2330 train_time:49742ms step_avg:39.99ms
step:1245/2330 train_time:49778ms step_avg:39.98ms
step:1246/2330 train_time:49823ms step_avg:39.99ms
step:1247/2330 train_time:49858ms step_avg:39.98ms
step:1248/2330 train_time:49902ms step_avg:39.99ms
step:1249/2330 train_time:49937ms step_avg:39.98ms
step:1250/2330 train_time:49982ms step_avg:39.99ms
step:1250/2330 val_loss:5.1852 train_time:50070ms step_avg:40.06ms
step:1251/2330 train_time:50083ms step_avg:40.03ms
step:1252/2330 train_time:50095ms step_avg:40.01ms
step:1253/2330 train_time:50105ms step_avg:39.99ms
step:1254/2330 train_time:50143ms step_avg:39.99ms
step:1255/2330 train_time:50177ms step_avg:39.98ms
step:1256/2330 train_time:50221ms step_avg:39.98ms
step:1257/2330 train_time:50255ms step_avg:39.98ms
step:1258/2330 train_time:50299ms step_avg:39.98ms
step:1259/2330 train_time:50333ms step_avg:39.98ms
step:1260/2330 train_time:50379ms step_avg:39.98ms
step:1261/2330 train_time:50418ms step_avg:39.98ms
step:1262/2330 train_time:50466ms step_avg:39.99ms
step:1263/2330 train_time:50504ms step_avg:39.99ms
step:1264/2330 train_time:50550ms step_avg:39.99ms
step:1265/2330 train_time:50585ms step_avg:39.99ms
step:1266/2330 train_time:50629ms step_avg:39.99ms
step:1267/2330 train_time:50664ms step_avg:39.99ms
step:1268/2330 train_time:50708ms step_avg:39.99ms
step:1269/2330 train_time:50965ms step_avg:40.16ms
step:1270/2330 train_time:51007ms step_avg:40.16ms
step:1271/2330 train_time:51042ms step_avg:40.16ms
step:1272/2330 train_time:51085ms step_avg:40.16ms
step:1273/2330 train_time:51119ms step_avg:40.16ms
step:1274/2330 train_time:51163ms step_avg:40.16ms
step:1275/2330 train_time:51198ms step_avg:40.16ms
step:1276/2330 train_time:51242ms step_avg:40.16ms
step:1277/2330 train_time:51276ms step_avg:40.15ms
step:1278/2330 train_time:51320ms step_avg:40.16ms
step:1279/2330 train_time:51354ms step_avg:40.15ms
step:1280/2330 train_time:51398ms step_avg:40.15ms
step:1281/2330 train_time:51432ms step_avg:40.15ms
step:1282/2330 train_time:51476ms step_avg:40.15ms
step:1283/2330 train_time:51510ms step_avg:40.15ms
step:1284/2330 train_time:51554ms step_avg:40.15ms
step:1285/2330 train_time:51588ms step_avg:40.15ms
step:1286/2330 train_time:51632ms step_avg:40.15ms
step:1287/2330 train_time:51666ms step_avg:40.14ms
step:1288/2330 train_time:51710ms step_avg:40.15ms
step:1289/2330 train_time:51744ms step_avg:40.14ms
step:1290/2330 train_time:51788ms step_avg:40.15ms
step:1291/2330 train_time:51825ms step_avg:40.14ms
step:1292/2330 train_time:51879ms step_avg:40.15ms
step:1293/2330 train_time:51918ms step_avg:40.15ms
step:1294/2330 train_time:51967ms step_avg:40.16ms
step:1295/2330 train_time:52002ms step_avg:40.16ms
step:1296/2330 train_time:52047ms step_avg:40.16ms
step:1297/2330 train_time:52082ms step_avg:40.16ms
step:1298/2330 train_time:52126ms step_avg:40.16ms
step:1299/2330 train_time:52161ms step_avg:40.16ms
step:1300/2330 train_time:52206ms step_avg:40.16ms
step:1301/2330 train_time:52240ms step_avg:40.15ms
step:1302/2330 train_time:52285ms step_avg:40.16ms
step:1303/2330 train_time:52320ms step_avg:40.15ms
step:1304/2330 train_time:52364ms step_avg:40.16ms
step:1305/2330 train_time:52399ms step_avg:40.15ms
step:1306/2330 train_time:52443ms step_avg:40.16ms
step:1307/2330 train_time:52478ms step_avg:40.15ms
step:1308/2330 train_time:52522ms step_avg:40.15ms
step:1309/2330 train_time:52557ms step_avg:40.15ms
step:1310/2330 train_time:52601ms step_avg:40.15ms
step:1311/2330 train_time:52636ms step_avg:40.15ms
step:1312/2330 train_time:52680ms step_avg:40.15ms
step:1313/2330 train_time:52714ms step_avg:40.15ms
step:1314/2330 train_time:52760ms step_avg:40.15ms
step:1315/2330 train_time:52799ms step_avg:40.15ms
step:1316/2330 train_time:52845ms step_avg:40.16ms
step:1317/2330 train_time:52882ms step_avg:40.15ms
step:1318/2330 train_time:52928ms step_avg:40.16ms
step:1319/2330 train_time:52965ms step_avg:40.16ms
step:1320/2330 train_time:53010ms step_avg:40.16ms
step:1321/2330 train_time:53045ms step_avg:40.16ms
step:1322/2330 train_time:53090ms step_avg:40.16ms
step:1323/2330 train_time:53125ms step_avg:40.15ms
step:1324/2330 train_time:53169ms step_avg:40.16ms
step:1325/2330 train_time:53204ms step_avg:40.15ms
step:1326/2330 train_time:53248ms step_avg:40.16ms
step:1327/2330 train_time:53283ms step_avg:40.15ms
step:1328/2330 train_time:53328ms step_avg:40.16ms
step:1329/2330 train_time:53362ms step_avg:40.15ms
step:1330/2330 train_time:53407ms step_avg:40.16ms
step:1331/2330 train_time:53442ms step_avg:40.15ms
step:1332/2330 train_time:53486ms step_avg:40.15ms
step:1333/2330 train_time:53521ms step_avg:40.15ms
step:1334/2330 train_time:53566ms step_avg:40.15ms
step:1335/2330 train_time:53601ms step_avg:40.15ms
step:1336/2330 train_time:53646ms step_avg:40.15ms
step:1337/2330 train_time:53681ms step_avg:40.15ms
step:1338/2330 train_time:53727ms step_avg:40.15ms
step:1339/2330 train_time:53762ms step_avg:40.15ms
step:1340/2330 train_time:53809ms step_avg:40.16ms
step:1341/2330 train_time:53845ms step_avg:40.15ms
step:1342/2330 train_time:53891ms step_avg:40.16ms
step:1343/2330 train_time:53927ms step_avg:40.15ms
step:1344/2330 train_time:53972ms step_avg:40.16ms
step:1345/2330 train_time:54008ms step_avg:40.15ms
step:1346/2330 train_time:54052ms step_avg:40.16ms
step:1347/2330 train_time:54087ms step_avg:40.15ms
step:1348/2330 train_time:54132ms step_avg:40.16ms
step:1349/2330 train_time:54167ms step_avg:40.15ms
step:1350/2330 train_time:54212ms step_avg:40.16ms
step:1351/2330 train_time:54247ms step_avg:40.15ms
step:1352/2330 train_time:54292ms step_avg:40.16ms
step:1353/2330 train_time:54327ms step_avg:40.15ms
step:1354/2330 train_time:54372ms step_avg:40.16ms
step:1355/2330 train_time:54408ms step_avg:40.15ms
step:1356/2330 train_time:54453ms step_avg:40.16ms
step:1357/2330 train_time:54488ms step_avg:40.15ms
step:1358/2330 train_time:54532ms step_avg:40.16ms
step:1359/2330 train_time:54569ms step_avg:40.15ms
step:1360/2330 train_time:54613ms step_avg:40.16ms
step:1361/2330 train_time:54649ms step_avg:40.15ms
step:1362/2330 train_time:54693ms step_avg:40.16ms
step:1363/2330 train_time:54730ms step_avg:40.15ms
step:1364/2330 train_time:54776ms step_avg:40.16ms
step:1365/2330 train_time:54812ms step_avg:40.16ms
step:1366/2330 train_time:54856ms step_avg:40.16ms
step:1367/2330 train_time:54892ms step_avg:40.15ms
step:1368/2330 train_time:54936ms step_avg:40.16ms
step:1369/2330 train_time:54971ms step_avg:40.15ms
step:1370/2330 train_time:55015ms step_avg:40.16ms
step:1371/2330 train_time:55050ms step_avg:40.15ms
step:1372/2330 train_time:55096ms step_avg:40.16ms
step:1373/2330 train_time:55131ms step_avg:40.15ms
step:1374/2330 train_time:55176ms step_avg:40.16ms
step:1375/2330 train_time:55211ms step_avg:40.15ms
step:1376/2330 train_time:55256ms step_avg:40.16ms
step:1377/2330 train_time:55292ms step_avg:40.15ms
step:1378/2330 train_time:55336ms step_avg:40.16ms
step:1379/2330 train_time:55371ms step_avg:40.15ms
step:1380/2330 train_time:55416ms step_avg:40.16ms
step:1381/2330 train_time:55451ms step_avg:40.15ms
step:1382/2330 train_time:55496ms step_avg:40.16ms
step:1383/2330 train_time:55531ms step_avg:40.15ms
step:1384/2330 train_time:55576ms step_avg:40.16ms
step:1385/2330 train_time:55611ms step_avg:40.15ms
step:1386/2330 train_time:55656ms step_avg:40.16ms
step:1387/2330 train_time:55692ms step_avg:40.15ms
step:1388/2330 train_time:55736ms step_avg:40.16ms
step:1389/2330 train_time:55772ms step_avg:40.15ms
step:1390/2330 train_time:55816ms step_avg:40.16ms
step:1391/2330 train_time:55851ms step_avg:40.15ms
step:1392/2330 train_time:55896ms step_avg:40.16ms
step:1393/2330 train_time:55931ms step_avg:40.15ms
step:1394/2330 train_time:55976ms step_avg:40.16ms
step:1395/2330 train_time:56012ms step_avg:40.15ms
step:1396/2330 train_time:56057ms step_avg:40.16ms
step:1397/2330 train_time:56092ms step_avg:40.15ms
step:1398/2330 train_time:56138ms step_avg:40.16ms
step:1399/2330 train_time:56173ms step_avg:40.15ms
step:1400/2330 train_time:56217ms step_avg:40.16ms
step:1401/2330 train_time:56253ms step_avg:40.15ms
step:1402/2330 train_time:56297ms step_avg:40.15ms
step:1403/2330 train_time:56332ms step_avg:40.15ms
step:1404/2330 train_time:56377ms step_avg:40.15ms
step:1405/2330 train_time:56412ms step_avg:40.15ms
step:1406/2330 train_time:56457ms step_avg:40.15ms
step:1407/2330 train_time:56493ms step_avg:40.15ms
step:1408/2330 train_time:56537ms step_avg:40.15ms
step:1409/2330 train_time:56573ms step_avg:40.15ms
step:1410/2330 train_time:56617ms step_avg:40.15ms
step:1411/2330 train_time:56653ms step_avg:40.15ms
step:1412/2330 train_time:56698ms step_avg:40.15ms
step:1413/2330 train_time:56733ms step_avg:40.15ms
step:1414/2330 train_time:56778ms step_avg:40.15ms
step:1415/2330 train_time:56813ms step_avg:40.15ms
step:1416/2330 train_time:56858ms step_avg:40.15ms
step:1417/2330 train_time:56894ms step_avg:40.15ms
step:1418/2330 train_time:56939ms step_avg:40.15ms
step:1419/2330 train_time:56974ms step_avg:40.15ms
step:1420/2330 train_time:57018ms step_avg:40.15ms
step:1421/2330 train_time:57053ms step_avg:40.15ms
step:1422/2330 train_time:57098ms step_avg:40.15ms
step:1423/2330 train_time:57133ms step_avg:40.15ms
step:1424/2330 train_time:57178ms step_avg:40.15ms
step:1425/2330 train_time:57213ms step_avg:40.15ms
step:1426/2330 train_time:57257ms step_avg:40.15ms
step:1427/2330 train_time:57292ms step_avg:40.15ms
step:1428/2330 train_time:57337ms step_avg:40.15ms
step:1429/2330 train_time:57372ms step_avg:40.15ms
step:1430/2330 train_time:57417ms step_avg:40.15ms
step:1431/2330 train_time:57452ms step_avg:40.15ms
step:1432/2330 train_time:57497ms step_avg:40.15ms
step:1433/2330 train_time:57532ms step_avg:40.15ms
step:1434/2330 train_time:57576ms step_avg:40.15ms
step:1435/2330 train_time:57612ms step_avg:40.15ms
step:1436/2330 train_time:57656ms step_avg:40.15ms
step:1437/2330 train_time:57692ms step_avg:40.15ms
step:1438/2330 train_time:57737ms step_avg:40.15ms
step:1439/2330 train_time:57772ms step_avg:40.15ms
step:1440/2330 train_time:57816ms step_avg:40.15ms
step:1441/2330 train_time:57851ms step_avg:40.15ms
step:1442/2330 train_time:57896ms step_avg:40.15ms
step:1443/2330 train_time:57932ms step_avg:40.15ms
step:1444/2330 train_time:57977ms step_avg:40.15ms
step:1445/2330 train_time:58012ms step_avg:40.15ms
step:1446/2330 train_time:58057ms step_avg:40.15ms
step:1447/2330 train_time:58093ms step_avg:40.15ms
step:1448/2330 train_time:58137ms step_avg:40.15ms
step:1449/2330 train_time:58172ms step_avg:40.15ms
step:1450/2330 train_time:58217ms step_avg:40.15ms
step:1451/2330 train_time:58251ms step_avg:40.15ms
step:1452/2330 train_time:58296ms step_avg:40.15ms
step:1453/2330 train_time:58331ms step_avg:40.15ms
step:1454/2330 train_time:58376ms step_avg:40.15ms
step:1455/2330 train_time:58411ms step_avg:40.15ms
step:1456/2330 train_time:58456ms step_avg:40.15ms
step:1457/2330 train_time:58493ms step_avg:40.15ms
step:1458/2330 train_time:58537ms step_avg:40.15ms
step:1459/2330 train_time:58573ms step_avg:40.15ms
step:1460/2330 train_time:58617ms step_avg:40.15ms
step:1461/2330 train_time:58653ms step_avg:40.15ms
step:1462/2330 train_time:58698ms step_avg:40.15ms
step:1463/2330 train_time:58733ms step_avg:40.15ms
step:1464/2330 train_time:58777ms step_avg:40.15ms
step:1465/2330 train_time:58813ms step_avg:40.15ms
step:1466/2330 train_time:58858ms step_avg:40.15ms
step:1467/2330 train_time:58894ms step_avg:40.15ms
step:1468/2330 train_time:58939ms step_avg:40.15ms
step:1469/2330 train_time:58974ms step_avg:40.15ms
step:1470/2330 train_time:59019ms step_avg:40.15ms
step:1471/2330 train_time:59054ms step_avg:40.15ms
step:1472/2330 train_time:59099ms step_avg:40.15ms
step:1473/2330 train_time:59134ms step_avg:40.15ms
step:1474/2330 train_time:59179ms step_avg:40.15ms
step:1475/2330 train_time:59214ms step_avg:40.14ms
step:1476/2330 train_time:59259ms step_avg:40.15ms
step:1477/2330 train_time:59295ms step_avg:40.15ms
step:1478/2330 train_time:59340ms step_avg:40.15ms
step:1479/2330 train_time:59375ms step_avg:40.15ms
step:1480/2330 train_time:59421ms step_avg:40.15ms
step:1481/2330 train_time:59456ms step_avg:40.15ms
step:1482/2330 train_time:59501ms step_avg:40.15ms
step:1483/2330 train_time:59535ms step_avg:40.15ms
step:1484/2330 train_time:59580ms step_avg:40.15ms
step:1485/2330 train_time:59615ms step_avg:40.14ms
step:1486/2330 train_time:59660ms step_avg:40.15ms
step:1487/2330 train_time:59695ms step_avg:40.14ms
step:1488/2330 train_time:59740ms step_avg:40.15ms
step:1489/2330 train_time:59775ms step_avg:40.14ms
step:1490/2330 train_time:59820ms step_avg:40.15ms
step:1491/2330 train_time:59855ms step_avg:40.14ms
step:1492/2330 train_time:59902ms step_avg:40.15ms
step:1493/2330 train_time:59937ms step_avg:40.15ms
step:1494/2330 train_time:59982ms step_avg:40.15ms
step:1495/2330 train_time:60017ms step_avg:40.15ms
step:1496/2330 train_time:60062ms step_avg:40.15ms
step:1497/2330 train_time:60097ms step_avg:40.15ms
step:1498/2330 train_time:60142ms step_avg:40.15ms
step:1499/2330 train_time:60177ms step_avg:40.14ms
step:1500/2330 train_time:60222ms step_avg:40.15ms
step:1500/2330 val_loss:5.1669 train_time:60309ms step_avg:40.21ms
step:1501/2330 train_time:60322ms step_avg:40.19ms
step:1502/2330 train_time:60334ms step_avg:40.17ms
step:1503/2330 train_time:60345ms step_avg:40.15ms
step:1504/2330 train_time:60385ms step_avg:40.15ms
step:1505/2330 train_time:60419ms step_avg:40.15ms
step:1506/2330 train_time:60463ms step_avg:40.15ms
step:1507/2330 train_time:60497ms step_avg:40.14ms
step:1508/2330 train_time:60541ms step_avg:40.15ms
step:1509/2330 train_time:60575ms step_avg:40.14ms
step:1510/2330 train_time:60621ms step_avg:40.15ms
step:1511/2330 train_time:60660ms step_avg:40.15ms
step:1512/2330 train_time:60876ms step_avg:40.26ms
step:1513/2330 train_time:60888ms step_avg:40.24ms
step:1514/2330 train_time:60900ms step_avg:40.22ms
step:1515/2330 train_time:60925ms step_avg:40.21ms
step:1516/2330 train_time:60968ms step_avg:40.22ms
step:1517/2330 train_time:61002ms step_avg:40.21ms
step:1518/2330 train_time:61046ms step_avg:40.21ms
step:1519/2330 train_time:61080ms step_avg:40.21ms
step:1520/2330 train_time:61124ms step_avg:40.21ms
step:1521/2330 train_time:61158ms step_avg:40.21ms
step:1522/2330 train_time:61202ms step_avg:40.21ms
step:1523/2330 train_time:61237ms step_avg:40.21ms
step:1524/2330 train_time:61281ms step_avg:40.21ms
step:1525/2330 train_time:61315ms step_avg:40.21ms
step:1526/2330 train_time:61359ms step_avg:40.21ms
step:1527/2330 train_time:61394ms step_avg:40.21ms
step:1528/2330 train_time:61437ms step_avg:40.21ms
step:1529/2330 train_time:61473ms step_avg:40.20ms
step:1530/2330 train_time:61516ms step_avg:40.21ms
step:1531/2330 train_time:61550ms step_avg:40.20ms
step:1532/2330 train_time:61594ms step_avg:40.21ms
step:1533/2330 train_time:61629ms step_avg:40.20ms
step:1534/2330 train_time:61672ms step_avg:40.20ms
step:1535/2330 train_time:61707ms step_avg:40.20ms
step:1536/2330 train_time:61752ms step_avg:40.20ms
step:1537/2330 train_time:61791ms step_avg:40.20ms
step:1538/2330 train_time:61839ms step_avg:40.21ms
step:1539/2330 train_time:61878ms step_avg:40.21ms
step:1540/2330 train_time:61926ms step_avg:40.21ms
step:1541/2330 train_time:61962ms step_avg:40.21ms
step:1542/2330 train_time:62007ms step_avg:40.21ms
step:1543/2330 train_time:62042ms step_avg:40.21ms
step:1544/2330 train_time:62086ms step_avg:40.21ms
step:1545/2330 train_time:62121ms step_avg:40.21ms
step:1546/2330 train_time:62166ms step_avg:40.21ms
step:1547/2330 train_time:62201ms step_avg:40.21ms
step:1548/2330 train_time:62245ms step_avg:40.21ms
step:1549/2330 train_time:62279ms step_avg:40.21ms
step:1550/2330 train_time:62323ms step_avg:40.21ms
step:1551/2330 train_time:62358ms step_avg:40.20ms
step:1552/2330 train_time:62402ms step_avg:40.21ms
step:1553/2330 train_time:62437ms step_avg:40.20ms
step:1554/2330 train_time:62482ms step_avg:40.21ms
step:1555/2330 train_time:62517ms step_avg:40.20ms
step:1556/2330 train_time:62562ms step_avg:40.21ms
step:1557/2330 train_time:62597ms step_avg:40.20ms
step:1558/2330 train_time:62642ms step_avg:40.21ms
step:1559/2330 train_time:62678ms step_avg:40.20ms
step:1560/2330 train_time:62723ms step_avg:40.21ms
step:1561/2330 train_time:62759ms step_avg:40.20ms
step:1562/2330 train_time:62805ms step_avg:40.21ms
step:1563/2330 train_time:62843ms step_avg:40.21ms
step:1564/2330 train_time:62890ms step_avg:40.21ms
step:1565/2330 train_time:62926ms step_avg:40.21ms
step:1566/2330 train_time:62971ms step_avg:40.21ms
step:1567/2330 train_time:63006ms step_avg:40.21ms
step:1568/2330 train_time:63051ms step_avg:40.21ms
step:1569/2330 train_time:63086ms step_avg:40.21ms
step:1570/2330 train_time:63131ms step_avg:40.21ms
step:1571/2330 train_time:63166ms step_avg:40.21ms
step:1572/2330 train_time:63210ms step_avg:40.21ms
step:1573/2330 train_time:63246ms step_avg:40.21ms
step:1574/2330 train_time:63290ms step_avg:40.21ms
step:1575/2330 train_time:63326ms step_avg:40.21ms
step:1576/2330 train_time:63370ms step_avg:40.21ms
step:1577/2330 train_time:63405ms step_avg:40.21ms
step:1578/2330 train_time:63450ms step_avg:40.21ms
step:1579/2330 train_time:63485ms step_avg:40.21ms
step:1580/2330 train_time:63530ms step_avg:40.21ms
step:1581/2330 train_time:63565ms step_avg:40.21ms
step:1582/2330 train_time:63610ms step_avg:40.21ms
step:1583/2330 train_time:63645ms step_avg:40.21ms
step:1584/2330 train_time:63690ms step_avg:40.21ms
step:1585/2330 train_time:63726ms step_avg:40.21ms
step:1586/2330 train_time:63771ms step_avg:40.21ms
step:1587/2330 train_time:63807ms step_avg:40.21ms
step:1588/2330 train_time:63852ms step_avg:40.21ms
step:1589/2330 train_time:63887ms step_avg:40.21ms
step:1590/2330 train_time:63932ms step_avg:40.21ms
step:1591/2330 train_time:63967ms step_avg:40.21ms
step:1592/2330 train_time:64011ms step_avg:40.21ms
step:1593/2330 train_time:64047ms step_avg:40.21ms
step:1594/2330 train_time:64091ms step_avg:40.21ms
step:1595/2330 train_time:64127ms step_avg:40.21ms
step:1596/2330 train_time:64172ms step_avg:40.21ms
step:1597/2330 train_time:64207ms step_avg:40.20ms
step:1598/2330 train_time:64251ms step_avg:40.21ms
step:1599/2330 train_time:64287ms step_avg:40.20ms
step:1600/2330 train_time:64331ms step_avg:40.21ms
step:1601/2330 train_time:64367ms step_avg:40.20ms
step:1602/2330 train_time:64411ms step_avg:40.21ms
step:1603/2330 train_time:64446ms step_avg:40.20ms
step:1604/2330 train_time:64491ms step_avg:40.21ms
step:1605/2330 train_time:64526ms step_avg:40.20ms
step:1606/2330 train_time:64571ms step_avg:40.21ms
step:1607/2330 train_time:64606ms step_avg:40.20ms
step:1608/2330 train_time:64652ms step_avg:40.21ms
step:1609/2330 train_time:64687ms step_avg:40.20ms
step:1610/2330 train_time:64732ms step_avg:40.21ms
step:1611/2330 train_time:64768ms step_avg:40.20ms
step:1612/2330 train_time:64812ms step_avg:40.21ms
step:1613/2330 train_time:64848ms step_avg:40.20ms
step:1614/2330 train_time:64892ms step_avg:40.21ms
step:1615/2330 train_time:64928ms step_avg:40.20ms
step:1616/2330 train_time:64973ms step_avg:40.21ms
step:1617/2330 train_time:65009ms step_avg:40.20ms
step:1618/2330 train_time:65054ms step_avg:40.21ms
step:1619/2330 train_time:65089ms step_avg:40.20ms
step:1620/2330 train_time:65134ms step_avg:40.21ms
step:1621/2330 train_time:65169ms step_avg:40.20ms
step:1622/2330 train_time:65214ms step_avg:40.21ms
step:1623/2330 train_time:65250ms step_avg:40.20ms
step:1624/2330 train_time:65294ms step_avg:40.21ms
step:1625/2330 train_time:65330ms step_avg:40.20ms
step:1626/2330 train_time:65374ms step_avg:40.21ms
step:1627/2330 train_time:65410ms step_avg:40.20ms
step:1628/2330 train_time:65454ms step_avg:40.21ms
step:1629/2330 train_time:65490ms step_avg:40.20ms
step:1630/2330 train_time:65534ms step_avg:40.20ms
step:1631/2330 train_time:65569ms step_avg:40.20ms
step:1632/2330 train_time:65614ms step_avg:40.20ms
step:1633/2330 train_time:65649ms step_avg:40.20ms
step:1634/2330 train_time:65693ms step_avg:40.20ms
step:1635/2330 train_time:65729ms step_avg:40.20ms
step:1636/2330 train_time:65773ms step_avg:40.20ms
step:1637/2330 train_time:65809ms step_avg:40.20ms
step:1638/2330 train_time:65854ms step_avg:40.20ms
step:1639/2330 train_time:65890ms step_avg:40.20ms
step:1640/2330 train_time:65935ms step_avg:40.20ms
step:1641/2330 train_time:65970ms step_avg:40.20ms
step:1642/2330 train_time:66015ms step_avg:40.20ms
step:1643/2330 train_time:66050ms step_avg:40.20ms
step:1644/2330 train_time:66095ms step_avg:40.20ms
step:1645/2330 train_time:66131ms step_avg:40.20ms
step:1646/2330 train_time:66176ms step_avg:40.20ms
step:1647/2330 train_time:66211ms step_avg:40.20ms
step:1648/2330 train_time:66256ms step_avg:40.20ms
step:1649/2330 train_time:66292ms step_avg:40.20ms
step:1650/2330 train_time:66337ms step_avg:40.20ms
step:1651/2330 train_time:66372ms step_avg:40.20ms
step:1652/2330 train_time:66417ms step_avg:40.20ms
step:1653/2330 train_time:66452ms step_avg:40.20ms
step:1654/2330 train_time:66496ms step_avg:40.20ms
step:1655/2330 train_time:66532ms step_avg:40.20ms
step:1656/2330 train_time:66577ms step_avg:40.20ms
step:1657/2330 train_time:66612ms step_avg:40.20ms
step:1658/2330 train_time:66657ms step_avg:40.20ms
step:1659/2330 train_time:66692ms step_avg:40.20ms
step:1660/2330 train_time:66737ms step_avg:40.20ms
step:1661/2330 train_time:66772ms step_avg:40.20ms
step:1662/2330 train_time:66818ms step_avg:40.20ms
step:1663/2330 train_time:66853ms step_avg:40.20ms
step:1664/2330 train_time:66899ms step_avg:40.20ms
step:1665/2330 train_time:66934ms step_avg:40.20ms
step:1666/2330 train_time:66978ms step_avg:40.20ms
step:1667/2330 train_time:67014ms step_avg:40.20ms
step:1668/2330 train_time:67060ms step_avg:40.20ms
step:1669/2330 train_time:67095ms step_avg:40.20ms
step:1670/2330 train_time:67139ms step_avg:40.20ms
step:1671/2330 train_time:67175ms step_avg:40.20ms
step:1672/2330 train_time:67220ms step_avg:40.20ms
step:1673/2330 train_time:67255ms step_avg:40.20ms
step:1674/2330 train_time:67300ms step_avg:40.20ms
step:1675/2330 train_time:67335ms step_avg:40.20ms
step:1676/2330 train_time:67380ms step_avg:40.20ms
step:1677/2330 train_time:67415ms step_avg:40.20ms
step:1678/2330 train_time:67459ms step_avg:40.20ms
step:1679/2330 train_time:67495ms step_avg:40.20ms
step:1680/2330 train_time:67539ms step_avg:40.20ms
step:1681/2330 train_time:67575ms step_avg:40.20ms
step:1682/2330 train_time:67619ms step_avg:40.20ms
step:1683/2330 train_time:67654ms step_avg:40.20ms
step:1684/2330 train_time:67699ms step_avg:40.20ms
step:1685/2330 train_time:67735ms step_avg:40.20ms
step:1686/2330 train_time:67779ms step_avg:40.20ms
step:1687/2330 train_time:67815ms step_avg:40.20ms
step:1688/2330 train_time:67859ms step_avg:40.20ms
step:1689/2330 train_time:67895ms step_avg:40.20ms
step:1690/2330 train_time:67939ms step_avg:40.20ms
step:1691/2330 train_time:67975ms step_avg:40.20ms
step:1692/2330 train_time:68019ms step_avg:40.20ms
step:1693/2330 train_time:68054ms step_avg:40.20ms
step:1694/2330 train_time:68099ms step_avg:40.20ms
step:1695/2330 train_time:68133ms step_avg:40.20ms
step:1696/2330 train_time:68179ms step_avg:40.20ms
step:1697/2330 train_time:68213ms step_avg:40.20ms
step:1698/2330 train_time:68258ms step_avg:40.20ms
step:1699/2330 train_time:68293ms step_avg:40.20ms
step:1700/2330 train_time:68337ms step_avg:40.20ms
step:1701/2330 train_time:68372ms step_avg:40.20ms
step:1702/2330 train_time:68417ms step_avg:40.20ms
step:1703/2330 train_time:68453ms step_avg:40.20ms
step:1704/2330 train_time:68498ms step_avg:40.20ms
step:1705/2330 train_time:68533ms step_avg:40.20ms
step:1706/2330 train_time:68578ms step_avg:40.20ms
step:1707/2330 train_time:68613ms step_avg:40.20ms
step:1708/2330 train_time:68658ms step_avg:40.20ms
step:1709/2330 train_time:68693ms step_avg:40.19ms
step:1710/2330 train_time:68738ms step_avg:40.20ms
step:1711/2330 train_time:68773ms step_avg:40.19ms
step:1712/2330 train_time:68818ms step_avg:40.20ms
step:1713/2330 train_time:68854ms step_avg:40.20ms
step:1714/2330 train_time:68899ms step_avg:40.20ms
step:1715/2330 train_time:68934ms step_avg:40.19ms
step:1716/2330 train_time:68979ms step_avg:40.20ms
step:1717/2330 train_time:69015ms step_avg:40.19ms
step:1718/2330 train_time:69059ms step_avg:40.20ms
step:1719/2330 train_time:69094ms step_avg:40.19ms
step:1720/2330 train_time:69139ms step_avg:40.20ms
step:1721/2330 train_time:69174ms step_avg:40.19ms
step:1722/2330 train_time:69219ms step_avg:40.20ms
step:1723/2330 train_time:69255ms step_avg:40.19ms
step:1724/2330 train_time:69300ms step_avg:40.20ms
step:1725/2330 train_time:69335ms step_avg:40.19ms
step:1726/2330 train_time:69379ms step_avg:40.20ms
step:1727/2330 train_time:69416ms step_avg:40.19ms
step:1728/2330 train_time:69461ms step_avg:40.20ms
step:1729/2330 train_time:69496ms step_avg:40.19ms
step:1730/2330 train_time:69542ms step_avg:40.20ms
step:1731/2330 train_time:69577ms step_avg:40.19ms
step:1732/2330 train_time:69622ms step_avg:40.20ms
step:1733/2330 train_time:69659ms step_avg:40.20ms
step:1734/2330 train_time:69704ms step_avg:40.20ms
step:1735/2330 train_time:69740ms step_avg:40.20ms
step:1736/2330 train_time:69785ms step_avg:40.20ms
step:1737/2330 train_time:69820ms step_avg:40.20ms
step:1738/2330 train_time:69865ms step_avg:40.20ms
step:1739/2330 train_time:69901ms step_avg:40.20ms
step:1740/2330 train_time:69946ms step_avg:40.20ms
step:1741/2330 train_time:69981ms step_avg:40.20ms
step:1742/2330 train_time:70026ms step_avg:40.20ms
step:1743/2330 train_time:70061ms step_avg:40.20ms
step:1744/2330 train_time:70106ms step_avg:40.20ms
step:1745/2330 train_time:70141ms step_avg:40.20ms
step:1746/2330 train_time:70186ms step_avg:40.20ms
step:1747/2330 train_time:70221ms step_avg:40.20ms
step:1748/2330 train_time:70266ms step_avg:40.20ms
step:1749/2330 train_time:70300ms step_avg:40.19ms
step:1750/2330 train_time:70345ms step_avg:40.20ms
step:1750/2330 val_loss:5.1526 train_time:70432ms step_avg:40.25ms
step:1751/2330 train_time:70446ms step_avg:40.23ms
step:1752/2330 train_time:70458ms step_avg:40.22ms
step:1753/2330 train_time:70469ms step_avg:40.20ms
step:1754/2330 train_time:70504ms step_avg:40.20ms
step:1755/2330 train_time:70539ms step_avg:40.19ms
step:1756/2330 train_time:70583ms step_avg:40.20ms
step:1757/2330 train_time:70617ms step_avg:40.19ms
step:1758/2330 train_time:70660ms step_avg:40.19ms
step:1759/2330 train_time:70696ms step_avg:40.19ms
step:1760/2330 train_time:70739ms step_avg:40.19ms
step:1761/2330 train_time:70779ms step_avg:40.19ms
step:1762/2330 train_time:70827ms step_avg:40.20ms
step:1763/2330 train_time:70865ms step_avg:40.20ms
step:1764/2330 train_time:70910ms step_avg:40.20ms
step:1765/2330 train_time:70945ms step_avg:40.20ms
step:1766/2330 train_time:70989ms step_avg:40.20ms
step:1767/2330 train_time:71024ms step_avg:40.19ms
step:1768/2330 train_time:71068ms step_avg:40.20ms
step:1769/2330 train_time:71103ms step_avg:40.19ms
step:1770/2330 train_time:71147ms step_avg:40.20ms
step:1771/2330 train_time:71181ms step_avg:40.19ms
step:1772/2330 train_time:71225ms step_avg:40.19ms
step:1773/2330 train_time:71260ms step_avg:40.19ms
step:1774/2330 train_time:71306ms step_avg:40.19ms
step:1775/2330 train_time:71345ms step_avg:40.19ms
step:1776/2330 train_time:71393ms step_avg:40.20ms
step:1777/2330 train_time:71429ms step_avg:40.20ms
step:1778/2330 train_time:71474ms step_avg:40.20ms
step:1779/2330 train_time:71509ms step_avg:40.20ms
step:1780/2330 train_time:71553ms step_avg:40.20ms
step:1781/2330 train_time:71588ms step_avg:40.20ms
step:1782/2330 train_time:71632ms step_avg:40.20ms
step:1783/2330 train_time:71667ms step_avg:40.19ms
step:1784/2330 train_time:71713ms step_avg:40.20ms
step:1785/2330 train_time:71749ms step_avg:40.20ms
step:1786/2330 train_time:71795ms step_avg:40.20ms
step:1787/2330 train_time:71832ms step_avg:40.20ms
step:1788/2330 train_time:71877ms step_avg:40.20ms
step:1789/2330 train_time:71913ms step_avg:40.20ms
step:1790/2330 train_time:71959ms step_avg:40.20ms
step:1791/2330 train_time:71994ms step_avg:40.20ms
step:1792/2330 train_time:72038ms step_avg:40.20ms
step:1793/2330 train_time:72073ms step_avg:40.20ms
step:1794/2330 train_time:72118ms step_avg:40.20ms
step:1795/2330 train_time:72152ms step_avg:40.20ms
step:1796/2330 train_time:72196ms step_avg:40.20ms
step:1797/2330 train_time:72232ms step_avg:40.20ms
step:1798/2330 train_time:72277ms step_avg:40.20ms
step:1799/2330 train_time:72313ms step_avg:40.20ms
step:1800/2330 train_time:72360ms step_avg:40.20ms
step:1801/2330 train_time:72398ms step_avg:40.20ms
step:1802/2330 train_time:72444ms step_avg:40.20ms
step:1803/2330 train_time:72480ms step_avg:40.20ms
step:1804/2330 train_time:72525ms step_avg:40.20ms
step:1805/2330 train_time:72561ms step_avg:40.20ms
step:1806/2330 train_time:72606ms step_avg:40.20ms
step:1807/2330 train_time:72642ms step_avg:40.20ms
step:1808/2330 train_time:72688ms step_avg:40.20ms
step:1809/2330 train_time:72722ms step_avg:40.20ms
step:1810/2330 train_time:72766ms step_avg:40.20ms
step:1811/2330 train_time:72801ms step_avg:40.20ms
step:1812/2330 train_time:72846ms step_avg:40.20ms
step:1813/2330 train_time:72882ms step_avg:40.20ms
step:1814/2330 train_time:72926ms step_avg:40.20ms
step:1815/2330 train_time:72961ms step_avg:40.20ms
step:1816/2330 train_time:73006ms step_avg:40.20ms
step:1817/2330 train_time:73041ms step_avg:40.20ms
step:1818/2330 train_time:73085ms step_avg:40.20ms
step:1819/2330 train_time:73120ms step_avg:40.20ms
step:1820/2330 train_time:73165ms step_avg:40.20ms
step:1821/2330 train_time:73200ms step_avg:40.20ms
step:1822/2330 train_time:73244ms step_avg:40.20ms
step:1823/2330 train_time:73280ms step_avg:40.20ms
step:1824/2330 train_time:73325ms step_avg:40.20ms
step:1825/2330 train_time:73361ms step_avg:40.20ms
step:1826/2330 train_time:73406ms step_avg:40.20ms
step:1827/2330 train_time:73442ms step_avg:40.20ms
step:1828/2330 train_time:73487ms step_avg:40.20ms
step:1829/2330 train_time:73523ms step_avg:40.20ms
step:1830/2330 train_time:73568ms step_avg:40.20ms
step:1831/2330 train_time:73604ms step_avg:40.20ms
step:1832/2330 train_time:73650ms step_avg:40.20ms
step:1833/2330 train_time:73685ms step_avg:40.20ms
step:1834/2330 train_time:73730ms step_avg:40.20ms
step:1835/2330 train_time:73765ms step_avg:40.20ms
step:1836/2330 train_time:73809ms step_avg:40.20ms
step:1837/2330 train_time:73845ms step_avg:40.20ms
step:1838/2330 train_time:73889ms step_avg:40.20ms
step:1839/2330 train_time:73925ms step_avg:40.20ms
step:1840/2330 train_time:73970ms step_avg:40.20ms
step:1841/2330 train_time:74005ms step_avg:40.20ms
step:1842/2330 train_time:74050ms step_avg:40.20ms
step:1843/2330 train_time:74085ms step_avg:40.20ms
step:1844/2330 train_time:74130ms step_avg:40.20ms
step:1845/2330 train_time:74166ms step_avg:40.20ms
step:1846/2330 train_time:74210ms step_avg:40.20ms
step:1847/2330 train_time:74246ms step_avg:40.20ms
step:1848/2330 train_time:74291ms step_avg:40.20ms
step:1849/2330 train_time:74326ms step_avg:40.20ms
step:1850/2330 train_time:74371ms step_avg:40.20ms
step:1851/2330 train_time:74407ms step_avg:40.20ms
step:1852/2330 train_time:74451ms step_avg:40.20ms
step:1853/2330 train_time:74487ms step_avg:40.20ms
step:1854/2330 train_time:74532ms step_avg:40.20ms
step:1855/2330 train_time:74568ms step_avg:40.20ms
step:1856/2330 train_time:74612ms step_avg:40.20ms
step:1857/2330 train_time:74647ms step_avg:40.20ms
step:1858/2330 train_time:74691ms step_avg:40.20ms
step:1859/2330 train_time:74726ms step_avg:40.20ms
step:1860/2330 train_time:74771ms step_avg:40.20ms
step:1861/2330 train_time:74807ms step_avg:40.20ms
step:1862/2330 train_time:74851ms step_avg:40.20ms
step:1863/2330 train_time:74887ms step_avg:40.20ms
step:1864/2330 train_time:74931ms step_avg:40.20ms
step:1865/2330 train_time:74967ms step_avg:40.20ms
step:1866/2330 train_time:75011ms step_avg:40.20ms
step:1867/2330 train_time:75046ms step_avg:40.20ms
step:1868/2330 train_time:75092ms step_avg:40.20ms
step:1869/2330 train_time:75127ms step_avg:40.20ms
step:1870/2330 train_time:75172ms step_avg:40.20ms
step:1871/2330 train_time:75208ms step_avg:40.20ms
step:1872/2330 train_time:75253ms step_avg:40.20ms
step:1873/2330 train_time:75289ms step_avg:40.20ms
step:1874/2330 train_time:75334ms step_avg:40.20ms
step:1875/2330 train_time:75369ms step_avg:40.20ms
step:1876/2330 train_time:75414ms step_avg:40.20ms
step:1877/2330 train_time:75450ms step_avg:40.20ms
step:1878/2330 train_time:75494ms step_avg:40.20ms
step:1879/2330 train_time:75530ms step_avg:40.20ms
step:1880/2330 train_time:75575ms step_avg:40.20ms
step:1881/2330 train_time:75610ms step_avg:40.20ms
step:1882/2330 train_time:75655ms step_avg:40.20ms
step:1883/2330 train_time:75691ms step_avg:40.20ms
step:1884/2330 train_time:75735ms step_avg:40.20ms
step:1885/2330 train_time:75770ms step_avg:40.20ms
step:1886/2330 train_time:75815ms step_avg:40.20ms
step:1887/2330 train_time:75850ms step_avg:40.20ms
step:1888/2330 train_time:75894ms step_avg:40.20ms
step:1889/2330 train_time:75930ms step_avg:40.20ms
step:1890/2330 train_time:75974ms step_avg:40.20ms
step:1891/2330 train_time:76009ms step_avg:40.20ms
step:1892/2330 train_time:76053ms step_avg:40.20ms
step:1893/2330 train_time:76089ms step_avg:40.19ms
step:1894/2330 train_time:76134ms step_avg:40.20ms
step:1895/2330 train_time:76169ms step_avg:40.19ms
step:1896/2330 train_time:76214ms step_avg:40.20ms
step:1897/2330 train_time:76250ms step_avg:40.19ms
step:1898/2330 train_time:76294ms step_avg:40.20ms
step:1899/2330 train_time:76330ms step_avg:40.19ms
step:1900/2330 train_time:76375ms step_avg:40.20ms
step:1901/2330 train_time:76411ms step_avg:40.19ms
step:1902/2330 train_time:76456ms step_avg:40.20ms
step:1903/2330 train_time:76491ms step_avg:40.19ms
step:1904/2330 train_time:76535ms step_avg:40.20ms
step:1905/2330 train_time:76571ms step_avg:40.19ms
step:1906/2330 train_time:76615ms step_avg:40.20ms
step:1907/2330 train_time:76651ms step_avg:40.19ms
step:1908/2330 train_time:76695ms step_avg:40.20ms
step:1909/2330 train_time:76730ms step_avg:40.19ms
step:1910/2330 train_time:76775ms step_avg:40.20ms
step:1911/2330 train_time:76810ms step_avg:40.19ms
step:1912/2330 train_time:76854ms step_avg:40.20ms
step:1913/2330 train_time:76890ms step_avg:40.19ms
step:1914/2330 train_time:76934ms step_avg:40.20ms
step:1915/2330 train_time:76969ms step_avg:40.19ms
step:1916/2330 train_time:77014ms step_avg:40.19ms
step:1917/2330 train_time:77049ms step_avg:40.19ms
step:1918/2330 train_time:77093ms step_avg:40.19ms
step:1919/2330 train_time:77129ms step_avg:40.19ms
step:1920/2330 train_time:77174ms step_avg:40.19ms
step:1921/2330 train_time:77209ms step_avg:40.19ms
step:1922/2330 train_time:77254ms step_avg:40.19ms
step:1923/2330 train_time:77290ms step_avg:40.19ms
step:1924/2330 train_time:77335ms step_avg:40.19ms
step:1925/2330 train_time:77371ms step_avg:40.19ms
step:1926/2330 train_time:77416ms step_avg:40.20ms
step:1927/2330 train_time:77451ms step_avg:40.19ms
step:1928/2330 train_time:77495ms step_avg:40.19ms
step:1929/2330 train_time:77530ms step_avg:40.19ms
step:1930/2330 train_time:77575ms step_avg:40.19ms
step:1931/2330 train_time:77610ms step_avg:40.19ms
step:1932/2330 train_time:77655ms step_avg:40.19ms
step:1933/2330 train_time:77690ms step_avg:40.19ms
step:1934/2330 train_time:77735ms step_avg:40.19ms
step:1935/2330 train_time:77770ms step_avg:40.19ms
step:1936/2330 train_time:77814ms step_avg:40.19ms
step:1937/2330 train_time:77850ms step_avg:40.19ms
step:1938/2330 train_time:77895ms step_avg:40.19ms
step:1939/2330 train_time:77930ms step_avg:40.19ms
step:1940/2330 train_time:77974ms step_avg:40.19ms
step:1941/2330 train_time:78009ms step_avg:40.19ms
step:1942/2330 train_time:78054ms step_avg:40.19ms
step:1943/2330 train_time:78089ms step_avg:40.19ms
step:1944/2330 train_time:78133ms step_avg:40.19ms
step:1945/2330 train_time:78169ms step_avg:40.19ms
step:1946/2330 train_time:78213ms step_avg:40.19ms
step:1947/2330 train_time:78249ms step_avg:40.19ms
step:1948/2330 train_time:78294ms step_avg:40.19ms
step:1949/2330 train_time:78330ms step_avg:40.19ms
step:1950/2330 train_time:78375ms step_avg:40.19ms
step:1951/2330 train_time:78410ms step_avg:40.19ms
step:1952/2330 train_time:78454ms step_avg:40.19ms
step:1953/2330 train_time:78490ms step_avg:40.19ms
step:1954/2330 train_time:78535ms step_avg:40.19ms
step:1955/2330 train_time:78571ms step_avg:40.19ms
step:1956/2330 train_time:78615ms step_avg:40.19ms
step:1957/2330 train_time:78651ms step_avg:40.19ms
step:1958/2330 train_time:78695ms step_avg:40.19ms
step:1959/2330 train_time:78730ms step_avg:40.19ms
step:1960/2330 train_time:78775ms step_avg:40.19ms
step:1961/2330 train_time:78810ms step_avg:40.19ms
step:1962/2330 train_time:78854ms step_avg:40.19ms
step:1963/2330 train_time:78890ms step_avg:40.19ms
step:1964/2330 train_time:78934ms step_avg:40.19ms
step:1965/2330 train_time:78970ms step_avg:40.19ms
step:1966/2330 train_time:79015ms step_avg:40.19ms
step:1967/2330 train_time:79050ms step_avg:40.19ms
step:1968/2330 train_time:79095ms step_avg:40.19ms
step:1969/2330 train_time:79130ms step_avg:40.19ms
step:1970/2330 train_time:79175ms step_avg:40.19ms
step:1971/2330 train_time:79210ms step_avg:40.19ms
step:1972/2330 train_time:79255ms step_avg:40.19ms
step:1973/2330 train_time:79290ms step_avg:40.19ms
step:1974/2330 train_time:79335ms step_avg:40.19ms
step:1975/2330 train_time:79370ms step_avg:40.19ms
step:1976/2330 train_time:79415ms step_avg:40.19ms
step:1977/2330 train_time:79450ms step_avg:40.19ms
step:1978/2330 train_time:79495ms step_avg:40.19ms
step:1979/2330 train_time:79531ms step_avg:40.19ms
step:1980/2330 train_time:79575ms step_avg:40.19ms
step:1981/2330 train_time:79610ms step_avg:40.19ms
step:1982/2330 train_time:79654ms step_avg:40.19ms
step:1983/2330 train_time:79689ms step_avg:40.19ms
step:1984/2330 train_time:79733ms step_avg:40.19ms
step:1985/2330 train_time:79769ms step_avg:40.19ms
step:1986/2330 train_time:79814ms step_avg:40.19ms
step:1987/2330 train_time:79849ms step_avg:40.19ms
step:1988/2330 train_time:79894ms step_avg:40.19ms
step:1989/2330 train_time:79930ms step_avg:40.19ms
step:1990/2330 train_time:79974ms step_avg:40.19ms
step:1991/2330 train_time:80009ms step_avg:40.19ms
step:1992/2330 train_time:80054ms step_avg:40.19ms
step:1993/2330 train_time:80089ms step_avg:40.19ms
step:1994/2330 train_time:80133ms step_avg:40.19ms
step:1995/2330 train_time:80169ms step_avg:40.18ms
step:1996/2330 train_time:80213ms step_avg:40.19ms
step:1997/2330 train_time:80249ms step_avg:40.18ms
step:1998/2330 train_time:80293ms step_avg:40.19ms
step:1999/2330 train_time:80329ms step_avg:40.18ms
step:2000/2330 train_time:80373ms step_avg:40.19ms
step:2000/2330 val_loss:5.1395 train_time:80463ms step_avg:40.23ms
step:2001/2330 train_time:80476ms step_avg:40.22ms
step:2002/2330 train_time:80489ms step_avg:40.20ms
step:2003/2330 train_time:80500ms step_avg:40.19ms
step:2004/2330 train_time:80536ms step_avg:40.19ms
step:2005/2330 train_time:80570ms step_avg:40.18ms
step:2006/2330 train_time:80614ms step_avg:40.19ms
step:2007/2330 train_time:80648ms step_avg:40.18ms
step:2008/2330 train_time:80692ms step_avg:40.19ms
step:2009/2330 train_time:80728ms step_avg:40.18ms
step:2010/2330 train_time:80775ms step_avg:40.19ms
step:2011/2330 train_time:80815ms step_avg:40.19ms
step:2012/2330 train_time:80862ms step_avg:40.19ms
step:2013/2330 train_time:80898ms step_avg:40.19ms
step:2014/2330 train_time:80943ms step_avg:40.19ms
step:2015/2330 train_time:80978ms step_avg:40.19ms
step:2016/2330 train_time:81022ms step_avg:40.19ms
step:2017/2330 train_time:81057ms step_avg:40.19ms
step:2018/2330 train_time:81101ms step_avg:40.19ms
step:2019/2330 train_time:81136ms step_avg:40.19ms
step:2020/2330 train_time:81181ms step_avg:40.19ms
step:2021/2330 train_time:81215ms step_avg:40.19ms
step:2022/2330 train_time:81259ms step_avg:40.19ms
step:2023/2330 train_time:81293ms step_avg:40.18ms
step:2024/2330 train_time:81337ms step_avg:40.19ms
step:2025/2330 train_time:81372ms step_avg:40.18ms
step:2026/2330 train_time:81417ms step_avg:40.19ms
step:2027/2330 train_time:81452ms step_avg:40.18ms
step:2028/2330 train_time:81496ms step_avg:40.19ms
step:2029/2330 train_time:81531ms step_avg:40.18ms
step:2030/2330 train_time:81575ms step_avg:40.18ms
step:2031/2330 train_time:81610ms step_avg:40.18ms
step:2032/2330 train_time:81654ms step_avg:40.18ms
step:2033/2330 train_time:81690ms step_avg:40.18ms
step:2034/2330 train_time:81737ms step_avg:40.19ms
step:2035/2330 train_time:81772ms step_avg:40.18ms
step:2036/2330 train_time:81818ms step_avg:40.19ms
step:2037/2330 train_time:81855ms step_avg:40.18ms
step:2038/2330 train_time:81901ms step_avg:40.19ms
step:2039/2330 train_time:81937ms step_avg:40.18ms
step:2040/2330 train_time:81982ms step_avg:40.19ms
step:2041/2330 train_time:82017ms step_avg:40.18ms
step:2042/2330 train_time:82062ms step_avg:40.19ms
step:2043/2330 train_time:82096ms step_avg:40.18ms
step:2044/2330 train_time:82141ms step_avg:40.19ms
step:2045/2330 train_time:82176ms step_avg:40.18ms
step:2046/2330 train_time:82220ms step_avg:40.19ms
step:2047/2330 train_time:82255ms step_avg:40.18ms
step:2048/2330 train_time:82299ms step_avg:40.19ms
step:2049/2330 train_time:82335ms step_avg:40.18ms
step:2050/2330 train_time:82379ms step_avg:40.18ms
step:2051/2330 train_time:82414ms step_avg:40.18ms
step:2052/2330 train_time:82458ms step_avg:40.18ms
step:2053/2330 train_time:82494ms step_avg:40.18ms
step:2054/2330 train_time:82539ms step_avg:40.18ms
step:2055/2330 train_time:82574ms step_avg:40.18ms
step:2056/2330 train_time:82618ms step_avg:40.18ms
step:2057/2330 train_time:82653ms step_avg:40.18ms
step:2058/2330 train_time:82697ms step_avg:40.18ms
step:2059/2330 train_time:82733ms step_avg:40.18ms
step:2060/2330 train_time:82777ms step_avg:40.18ms
step:2061/2330 train_time:82814ms step_avg:40.18ms
step:2062/2330 train_time:82859ms step_avg:40.18ms
step:2063/2330 train_time:82895ms step_avg:40.18ms
step:2064/2330 train_time:82940ms step_avg:40.18ms
step:2065/2330 train_time:82976ms step_avg:40.18ms
step:2066/2330 train_time:83021ms step_avg:40.18ms
step:2067/2330 train_time:83056ms step_avg:40.18ms
step:2068/2330 train_time:83100ms step_avg:40.18ms
step:2069/2330 train_time:83135ms step_avg:40.18ms
step:2070/2330 train_time:83180ms step_avg:40.18ms
step:2071/2330 train_time:83215ms step_avg:40.18ms
step:2072/2330 train_time:83258ms step_avg:40.18ms
step:2073/2330 train_time:83293ms step_avg:40.18ms
step:2074/2330 train_time:83338ms step_avg:40.18ms
step:2075/2330 train_time:83373ms step_avg:40.18ms
step:2076/2330 train_time:83417ms step_avg:40.18ms
step:2077/2330 train_time:83452ms step_avg:40.18ms
step:2078/2330 train_time:83497ms step_avg:40.18ms
step:2079/2330 train_time:83532ms step_avg:40.18ms
step:2080/2330 train_time:83576ms step_avg:40.18ms
step:2081/2330 train_time:83611ms step_avg:40.18ms
step:2082/2330 train_time:83655ms step_avg:40.18ms
step:2083/2330 train_time:83691ms step_avg:40.18ms
step:2084/2330 train_time:83736ms step_avg:40.18ms
step:2085/2330 train_time:83773ms step_avg:40.18ms
step:2086/2330 train_time:83817ms step_avg:40.18ms
step:2087/2330 train_time:83853ms step_avg:40.18ms
step:2088/2330 train_time:83898ms step_avg:40.18ms
step:2089/2330 train_time:83934ms step_avg:40.18ms
step:2090/2330 train_time:83979ms step_avg:40.18ms
step:2091/2330 train_time:84015ms step_avg:40.18ms
step:2092/2330 train_time:84060ms step_avg:40.18ms
step:2093/2330 train_time:84096ms step_avg:40.18ms
step:2094/2330 train_time:84140ms step_avg:40.18ms
step:2095/2330 train_time:84176ms step_avg:40.18ms
step:2096/2330 train_time:84220ms step_avg:40.18ms
step:2097/2330 train_time:84254ms step_avg:40.18ms
step:2098/2330 train_time:84299ms step_avg:40.18ms
step:2099/2330 train_time:84333ms step_avg:40.18ms
step:2100/2330 train_time:84377ms step_avg:40.18ms
step:2101/2330 train_time:84412ms step_avg:40.18ms
step:2102/2330 train_time:84456ms step_avg:40.18ms
step:2103/2330 train_time:84492ms step_avg:40.18ms
step:2104/2330 train_time:84536ms step_avg:40.18ms
step:2105/2330 train_time:84571ms step_avg:40.18ms
step:2106/2330 train_time:84615ms step_avg:40.18ms
step:2107/2330 train_time:84651ms step_avg:40.18ms
step:2108/2330 train_time:84696ms step_avg:40.18ms
step:2109/2330 train_time:84731ms step_avg:40.18ms
step:2110/2330 train_time:84776ms step_avg:40.18ms
step:2111/2330 train_time:84812ms step_avg:40.18ms
step:2112/2330 train_time:84857ms step_avg:40.18ms
step:2113/2330 train_time:84892ms step_avg:40.18ms
step:2114/2330 train_time:84937ms step_avg:40.18ms
step:2115/2330 train_time:84973ms step_avg:40.18ms
step:2116/2330 train_time:85017ms step_avg:40.18ms
step:2117/2330 train_time:85053ms step_avg:40.18ms
step:2118/2330 train_time:85098ms step_avg:40.18ms
step:2119/2330 train_time:85134ms step_avg:40.18ms
step:2120/2330 train_time:85178ms step_avg:40.18ms
step:2121/2330 train_time:85213ms step_avg:40.18ms
step:2122/2330 train_time:85257ms step_avg:40.18ms
step:2123/2330 train_time:85293ms step_avg:40.18ms
step:2124/2330 train_time:85337ms step_avg:40.18ms
step:2125/2330 train_time:85371ms step_avg:40.17ms
step:2126/2330 train_time:85416ms step_avg:40.18ms
step:2127/2330 train_time:85451ms step_avg:40.17ms
step:2128/2330 train_time:85495ms step_avg:40.18ms
step:2129/2330 train_time:85530ms step_avg:40.17ms
step:2130/2330 train_time:85574ms step_avg:40.18ms
step:2131/2330 train_time:85610ms step_avg:40.17ms
step:2132/2330 train_time:85655ms step_avg:40.18ms
step:2133/2330 train_time:85690ms step_avg:40.17ms
step:2134/2330 train_time:85735ms step_avg:40.18ms
step:2135/2330 train_time:85770ms step_avg:40.17ms
step:2136/2330 train_time:85815ms step_avg:40.18ms
step:2137/2330 train_time:85850ms step_avg:40.17ms
step:2138/2330 train_time:85895ms step_avg:40.18ms
step:2139/2330 train_time:85931ms step_avg:40.17ms
step:2140/2330 train_time:85976ms step_avg:40.18ms
step:2141/2330 train_time:86012ms step_avg:40.17ms
step:2142/2330 train_time:86057ms step_avg:40.18ms
step:2143/2330 train_time:86093ms step_avg:40.17ms
step:2144/2330 train_time:86137ms step_avg:40.18ms
step:2145/2330 train_time:86172ms step_avg:40.17ms
step:2146/2330 train_time:86217ms step_avg:40.18ms
step:2147/2330 train_time:86252ms step_avg:40.17ms
step:2148/2330 train_time:86296ms step_avg:40.17ms
step:2149/2330 train_time:86330ms step_avg:40.17ms
step:2150/2330 train_time:86375ms step_avg:40.17ms
step:2151/2330 train_time:86410ms step_avg:40.17ms
step:2152/2330 train_time:86454ms step_avg:40.17ms
step:2153/2330 train_time:86489ms step_avg:40.17ms
step:2154/2330 train_time:86534ms step_avg:40.17ms
step:2155/2330 train_time:86570ms step_avg:40.17ms
step:2156/2330 train_time:86614ms step_avg:40.17ms
step:2157/2330 train_time:86649ms step_avg:40.17ms
step:2158/2330 train_time:86694ms step_avg:40.17ms
step:2159/2330 train_time:86729ms step_avg:40.17ms
step:2160/2330 train_time:86773ms step_avg:40.17ms
step:2161/2330 train_time:86809ms step_avg:40.17ms
step:2162/2330 train_time:86854ms step_avg:40.17ms
step:2163/2330 train_time:86889ms step_avg:40.17ms
step:2164/2330 train_time:86935ms step_avg:40.17ms
step:2165/2330 train_time:86971ms step_avg:40.17ms
step:2166/2330 train_time:87016ms step_avg:40.17ms
step:2167/2330 train_time:87052ms step_avg:40.17ms
step:2168/2330 train_time:87097ms step_avg:40.17ms
step:2169/2330 train_time:87132ms step_avg:40.17ms
step:2170/2330 train_time:87177ms step_avg:40.17ms
step:2171/2330 train_time:87212ms step_avg:40.17ms
step:2172/2330 train_time:87256ms step_avg:40.17ms
step:2173/2330 train_time:87292ms step_avg:40.17ms
step:2174/2330 train_time:87337ms step_avg:40.17ms
step:2175/2330 train_time:87372ms step_avg:40.17ms
step:2176/2330 train_time:87416ms step_avg:40.17ms
step:2177/2330 train_time:87451ms step_avg:40.17ms
step:2178/2330 train_time:87496ms step_avg:40.17ms
step:2179/2330 train_time:87531ms step_avg:40.17ms
step:2180/2330 train_time:87575ms step_avg:40.17ms
step:2181/2330 train_time:87610ms step_avg:40.17ms
step:2182/2330 train_time:87655ms step_avg:40.17ms
step:2183/2330 train_time:87690ms step_avg:40.17ms
step:2184/2330 train_time:87735ms step_avg:40.17ms
step:2185/2330 train_time:87770ms step_avg:40.17ms
step:2186/2330 train_time:87814ms step_avg:40.17ms
step:2187/2330 train_time:87850ms step_avg:40.17ms
step:2188/2330 train_time:87895ms step_avg:40.17ms
step:2189/2330 train_time:87930ms step_avg:40.17ms
step:2190/2330 train_time:87975ms step_avg:40.17ms
step:2191/2330 train_time:88011ms step_avg:40.17ms
step:2192/2330 train_time:88056ms step_avg:40.17ms
step:2193/2330 train_time:88091ms step_avg:40.17ms
step:2194/2330 train_time:88136ms step_avg:40.17ms
step:2195/2330 train_time:88172ms step_avg:40.17ms
step:2196/2330 train_time:88216ms step_avg:40.17ms
step:2197/2330 train_time:88252ms step_avg:40.17ms
step:2198/2330 train_time:88296ms step_avg:40.17ms
step:2199/2330 train_time:88331ms step_avg:40.17ms
step:2200/2330 train_time:88376ms step_avg:40.17ms
step:2201/2330 train_time:88411ms step_avg:40.17ms
step:2202/2330 train_time:88456ms step_avg:40.17ms
step:2203/2330 train_time:88491ms step_avg:40.17ms
step:2204/2330 train_time:88536ms step_avg:40.17ms
step:2205/2330 train_time:88572ms step_avg:40.17ms
step:2206/2330 train_time:88617ms step_avg:40.17ms
step:2207/2330 train_time:88652ms step_avg:40.17ms
step:2208/2330 train_time:88697ms step_avg:40.17ms
step:2209/2330 train_time:88732ms step_avg:40.17ms
step:2210/2330 train_time:88777ms step_avg:40.17ms
step:2211/2330 train_time:88812ms step_avg:40.17ms
step:2212/2330 train_time:88857ms step_avg:40.17ms
step:2213/2330 train_time:88892ms step_avg:40.17ms
step:2214/2330 train_time:88937ms step_avg:40.17ms
step:2215/2330 train_time:88973ms step_avg:40.17ms
step:2216/2330 train_time:89017ms step_avg:40.17ms
step:2217/2330 train_time:89052ms step_avg:40.17ms
step:2218/2330 train_time:89097ms step_avg:40.17ms
step:2219/2330 train_time:89133ms step_avg:40.17ms
step:2220/2330 train_time:89177ms step_avg:40.17ms
step:2221/2330 train_time:89212ms step_avg:40.17ms
step:2222/2330 train_time:89257ms step_avg:40.17ms
step:2223/2330 train_time:89292ms step_avg:40.17ms
step:2224/2330 train_time:89337ms step_avg:40.17ms
step:2225/2330 train_time:89372ms step_avg:40.17ms
step:2226/2330 train_time:89417ms step_avg:40.17ms
step:2227/2330 train_time:89453ms step_avg:40.17ms
step:2228/2330 train_time:89497ms step_avg:40.17ms
step:2229/2330 train_time:89533ms step_avg:40.17ms
step:2230/2330 train_time:89577ms step_avg:40.17ms
step:2231/2330 train_time:89612ms step_avg:40.17ms
step:2232/2330 train_time:89657ms step_avg:40.17ms
step:2233/2330 train_time:89692ms step_avg:40.17ms
step:2234/2330 train_time:89737ms step_avg:40.17ms
step:2235/2330 train_time:89772ms step_avg:40.17ms
step:2236/2330 train_time:89817ms step_avg:40.17ms
step:2237/2330 train_time:89853ms step_avg:40.17ms
step:2238/2330 train_time:89897ms step_avg:40.17ms
step:2239/2330 train_time:89932ms step_avg:40.17ms
step:2240/2330 train_time:89977ms step_avg:40.17ms
step:2241/2330 train_time:90013ms step_avg:40.17ms
step:2242/2330 train_time:90057ms step_avg:40.17ms
step:2243/2330 train_time:90093ms step_avg:40.17ms
step:2244/2330 train_time:90138ms step_avg:40.17ms
step:2245/2330 train_time:90173ms step_avg:40.17ms
step:2246/2330 train_time:90218ms step_avg:40.17ms
step:2247/2330 train_time:90253ms step_avg:40.17ms
step:2248/2330 train_time:90297ms step_avg:40.17ms
step:2249/2330 train_time:90333ms step_avg:40.17ms
step:2250/2330 train_time:90377ms step_avg:40.17ms
step:2250/2330 val_loss:5.1320 train_time:90465ms step_avg:40.21ms
step:2251/2330 train_time:90478ms step_avg:40.19ms
step:2252/2330 train_time:90490ms step_avg:40.18ms
step:2253/2330 train_time:90501ms step_avg:40.17ms
step:2254/2330 train_time:90538ms step_avg:40.17ms
step:2255/2330 train_time:90572ms step_avg:40.16ms
step:2256/2330 train_time:90616ms step_avg:40.17ms
step:2257/2330 train_time:90650ms step_avg:40.16ms
step:2258/2330 train_time:90694ms step_avg:40.17ms
step:2259/2330 train_time:90729ms step_avg:40.16ms
step:2260/2330 train_time:90777ms step_avg:40.17ms
step:2261/2330 train_time:90815ms step_avg:40.17ms
step:2262/2330 train_time:90861ms step_avg:40.17ms
step:2263/2330 train_time:90897ms step_avg:40.17ms
step:2264/2330 train_time:90942ms step_avg:40.17ms
step:2265/2330 train_time:90978ms step_avg:40.17ms
step:2266/2330 train_time:91022ms step_avg:40.17ms
step:2267/2330 train_time:91058ms step_avg:40.17ms
step:2268/2330 train_time:91103ms step_avg:40.17ms
step:2269/2330 train_time:91138ms step_avg:40.17ms
step:2270/2330 train_time:91182ms step_avg:40.17ms
step:2271/2330 train_time:91218ms step_avg:40.17ms
step:2272/2330 train_time:91263ms step_avg:40.17ms
step:2273/2330 train_time:91298ms step_avg:40.17ms
step:2274/2330 train_time:91342ms step_avg:40.17ms
step:2275/2330 train_time:91377ms step_avg:40.17ms
step:2276/2330 train_time:91423ms step_avg:40.17ms
step:2277/2330 train_time:91458ms step_avg:40.17ms
step:2278/2330 train_time:91503ms step_avg:40.17ms
step:2279/2330 train_time:91538ms step_avg:40.17ms
step:2280/2330 train_time:91583ms step_avg:40.17ms
step:2281/2330 train_time:91618ms step_avg:40.17ms
step:2282/2330 train_time:91663ms step_avg:40.17ms
step:2283/2330 train_time:91698ms step_avg:40.17ms
step:2284/2330 train_time:91743ms step_avg:40.17ms
step:2285/2330 train_time:91779ms step_avg:40.17ms
step:2286/2330 train_time:91824ms step_avg:40.17ms
step:2287/2330 train_time:91860ms step_avg:40.17ms
step:2288/2330 train_time:91905ms step_avg:40.17ms
step:2289/2330 train_time:91941ms step_avg:40.17ms
step:2290/2330 train_time:91986ms step_avg:40.17ms
step:2291/2330 train_time:92021ms step_avg:40.17ms
step:2292/2330 train_time:92066ms step_avg:40.17ms
step:2293/2330 train_time:92101ms step_avg:40.17ms
step:2294/2330 train_time:92146ms step_avg:40.17ms
step:2295/2330 train_time:92182ms step_avg:40.17ms
step:2296/2330 train_time:92226ms step_avg:40.17ms
step:2297/2330 train_time:92262ms step_avg:40.17ms
step:2298/2330 train_time:92306ms step_avg:40.17ms
step:2299/2330 train_time:92342ms step_avg:40.17ms
step:2300/2330 train_time:92386ms step_avg:40.17ms
step:2301/2330 train_time:92421ms step_avg:40.17ms
step:2302/2330 train_time:92467ms step_avg:40.17ms
step:2303/2330 train_time:92502ms step_avg:40.17ms
step:2304/2330 train_time:92547ms step_avg:40.17ms
step:2305/2330 train_time:92582ms step_avg:40.17ms
step:2306/2330 train_time:92627ms step_avg:40.17ms
step:2307/2330 train_time:92663ms step_avg:40.17ms
step:2308/2330 train_time:92708ms step_avg:40.17ms
step:2309/2330 train_time:92743ms step_avg:40.17ms
step:2310/2330 train_time:92789ms step_avg:40.17ms
step:2311/2330 train_time:92824ms step_avg:40.17ms
step:2312/2330 train_time:92868ms step_avg:40.17ms
step:2313/2330 train_time:92903ms step_avg:40.17ms
step:2314/2330 train_time:92948ms step_avg:40.17ms
step:2315/2330 train_time:92983ms step_avg:40.17ms
step:2316/2330 train_time:93028ms step_avg:40.17ms
step:2317/2330 train_time:93063ms step_avg:40.17ms
step:2318/2330 train_time:93107ms step_avg:40.17ms
step:2319/2330 train_time:93142ms step_avg:40.16ms
step:2320/2330 train_time:93187ms step_avg:40.17ms
step:2321/2330 train_time:93222ms step_avg:40.16ms
step:2322/2330 train_time:93267ms step_avg:40.17ms
step:2323/2330 train_time:93302ms step_avg:40.16ms
step:2324/2330 train_time:93346ms step_avg:40.17ms
step:2325/2330 train_time:93382ms step_avg:40.16ms
step:2326/2330 train_time:93427ms step_avg:40.17ms
step:2327/2330 train_time:93462ms step_avg:40.16ms
step:2328/2330 train_time:93506ms step_avg:40.17ms
step:2329/2330 train_time:93541ms step_avg:40.16ms
step:2330/2330 train_time:93585ms step_avg:40.17ms
step:2330/2330 val_loss:5.1290 train_time:93673ms step_avg:40.20ms
peak memory allocated: 29226 MiB reserved: 38888 MiB
