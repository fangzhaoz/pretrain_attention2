import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                # v_chunk = divide_by_norm(batched)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                # v_chunk = divide_by_norm(batched_update_grads)
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        # self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "mlp_muon_lr7e-2"
    os.makedirs("logs3", exist_ok=True)
    logfile = f"logs3/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
# embed_params = [p for n, p in model.named_parameters() if "embed" in n]
embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=7e-2, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    return 1.0

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum_min + 0.5 * (momentum_max - momentum_min)

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs3/{run_id}", exist_ok=True)
            torch.save(log, f"logs3/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 08:00:55 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   34C    P0             112W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   32C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   32C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             116W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.05ms
step:1/2330 train_time:74ms step_avg:73.75ms
step:2/2330 train_time:144ms step_avg:72.11ms
step:3/2330 train_time:159ms step_avg:52.95ms
step:4/2330 train_time:173ms step_avg:43.36ms
step:5/2330 train_time:185ms step_avg:37.09ms
step:6/2330 train_time:215ms step_avg:35.79ms
step:7/2330 train_time:248ms step_avg:35.46ms
step:8/2330 train_time:292ms step_avg:36.49ms
step:9/2330 train_time:326ms step_avg:36.25ms
step:10/2330 train_time:371ms step_avg:37.12ms
step:11/2330 train_time:406ms step_avg:36.90ms
step:12/2330 train_time:450ms step_avg:37.46ms
step:13/2330 train_time:484ms step_avg:37.23ms
step:14/2330 train_time:528ms step_avg:37.69ms
step:15/2330 train_time:562ms step_avg:37.47ms
step:16/2330 train_time:606ms step_avg:37.87ms
step:17/2330 train_time:641ms step_avg:37.69ms
step:18/2330 train_time:685ms step_avg:38.05ms
step:19/2330 train_time:720ms step_avg:37.89ms
step:20/2330 train_time:764ms step_avg:38.18ms
step:21/2330 train_time:798ms step_avg:38.02ms
step:22/2330 train_time:843ms step_avg:38.30ms
step:23/2330 train_time:877ms step_avg:38.14ms
step:24/2330 train_time:921ms step_avg:38.38ms
step:25/2330 train_time:956ms step_avg:38.26ms
step:26/2330 train_time:1003ms step_avg:38.58ms
step:27/2330 train_time:1042ms step_avg:38.58ms
step:28/2330 train_time:1090ms step_avg:38.91ms
step:29/2330 train_time:1128ms step_avg:38.91ms
step:30/2330 train_time:1176ms step_avg:39.19ms
step:31/2330 train_time:1211ms step_avg:39.07ms
step:32/2330 train_time:1256ms step_avg:39.25ms
step:33/2330 train_time:1291ms step_avg:39.12ms
step:34/2330 train_time:1335ms step_avg:39.27ms
step:35/2330 train_time:1370ms step_avg:39.14ms
step:36/2330 train_time:1415ms step_avg:39.30ms
step:37/2330 train_time:1450ms step_avg:39.18ms
step:38/2330 train_time:1494ms step_avg:39.33ms
step:39/2330 train_time:1530ms step_avg:39.23ms
step:40/2330 train_time:1574ms step_avg:39.36ms
step:41/2330 train_time:1609ms step_avg:39.26ms
step:42/2330 train_time:1654ms step_avg:39.39ms
step:43/2330 train_time:1689ms step_avg:39.28ms
step:44/2330 train_time:1733ms step_avg:39.39ms
step:45/2330 train_time:1768ms step_avg:39.29ms
step:46/2330 train_time:1812ms step_avg:39.40ms
step:47/2330 train_time:1847ms step_avg:39.31ms
step:48/2330 train_time:1892ms step_avg:39.41ms
step:49/2330 train_time:1927ms step_avg:39.33ms
step:50/2330 train_time:1972ms step_avg:39.44ms
step:51/2330 train_time:2008ms step_avg:39.38ms
step:52/2330 train_time:2053ms step_avg:39.48ms
step:53/2330 train_time:2088ms step_avg:39.39ms
step:54/2330 train_time:2133ms step_avg:39.49ms
step:55/2330 train_time:2169ms step_avg:39.43ms
step:56/2330 train_time:2214ms step_avg:39.53ms
step:57/2330 train_time:2249ms step_avg:39.45ms
step:58/2330 train_time:2293ms step_avg:39.53ms
step:59/2330 train_time:2328ms step_avg:39.45ms
step:60/2330 train_time:2372ms step_avg:39.54ms
step:61/2330 train_time:2406ms step_avg:39.45ms
step:62/2330 train_time:2451ms step_avg:39.54ms
step:63/2330 train_time:2486ms step_avg:39.46ms
step:64/2330 train_time:2530ms step_avg:39.53ms
step:65/2330 train_time:2565ms step_avg:39.46ms
step:66/2330 train_time:2609ms step_avg:39.53ms
step:67/2330 train_time:2643ms step_avg:39.45ms
step:68/2330 train_time:2687ms step_avg:39.52ms
step:69/2330 train_time:2721ms step_avg:39.44ms
step:70/2330 train_time:2765ms step_avg:39.50ms
step:71/2330 train_time:2800ms step_avg:39.44ms
step:72/2330 train_time:2844ms step_avg:39.50ms
step:73/2330 train_time:2879ms step_avg:39.43ms
step:74/2330 train_time:2922ms step_avg:39.49ms
step:75/2330 train_time:2958ms step_avg:39.44ms
step:76/2330 train_time:3003ms step_avg:39.51ms
step:77/2330 train_time:3038ms step_avg:39.45ms
step:78/2330 train_time:3083ms step_avg:39.52ms
step:79/2330 train_time:3118ms step_avg:39.47ms
step:80/2330 train_time:3163ms step_avg:39.53ms
step:81/2330 train_time:3198ms step_avg:39.49ms
step:82/2330 train_time:3243ms step_avg:39.55ms
step:83/2330 train_time:3279ms step_avg:39.50ms
step:84/2330 train_time:3323ms step_avg:39.56ms
step:85/2330 train_time:3358ms step_avg:39.50ms
step:86/2330 train_time:3403ms step_avg:39.57ms
step:87/2330 train_time:3438ms step_avg:39.52ms
step:88/2330 train_time:3482ms step_avg:39.57ms
step:89/2330 train_time:3517ms step_avg:39.52ms
step:90/2330 train_time:3562ms step_avg:39.58ms
step:91/2330 train_time:3598ms step_avg:39.53ms
step:92/2330 train_time:3641ms step_avg:39.58ms
step:93/2330 train_time:3676ms step_avg:39.53ms
step:94/2330 train_time:3721ms step_avg:39.58ms
step:95/2330 train_time:3756ms step_avg:39.53ms
step:96/2330 train_time:3800ms step_avg:39.58ms
step:97/2330 train_time:3835ms step_avg:39.53ms
step:98/2330 train_time:3879ms step_avg:39.58ms
step:99/2330 train_time:3914ms step_avg:39.53ms
step:100/2330 train_time:3958ms step_avg:39.58ms
step:101/2330 train_time:3993ms step_avg:39.53ms
step:102/2330 train_time:4038ms step_avg:39.59ms
step:103/2330 train_time:4073ms step_avg:39.55ms
step:104/2330 train_time:4118ms step_avg:39.60ms
step:105/2330 train_time:4153ms step_avg:39.55ms
step:106/2330 train_time:4198ms step_avg:39.60ms
step:107/2330 train_time:4233ms step_avg:39.56ms
step:108/2330 train_time:4278ms step_avg:39.61ms
step:109/2330 train_time:4314ms step_avg:39.57ms
step:110/2330 train_time:4358ms step_avg:39.62ms
step:111/2330 train_time:4393ms step_avg:39.58ms
step:112/2330 train_time:4437ms step_avg:39.62ms
step:113/2330 train_time:4473ms step_avg:39.58ms
step:114/2330 train_time:4516ms step_avg:39.62ms
step:115/2330 train_time:4551ms step_avg:39.58ms
step:116/2330 train_time:4596ms step_avg:39.62ms
step:117/2330 train_time:4631ms step_avg:39.58ms
step:118/2330 train_time:4676ms step_avg:39.63ms
step:119/2330 train_time:4711ms step_avg:39.59ms
step:120/2330 train_time:4755ms step_avg:39.63ms
step:121/2330 train_time:4790ms step_avg:39.59ms
step:122/2330 train_time:4835ms step_avg:39.63ms
step:123/2330 train_time:4870ms step_avg:39.60ms
step:124/2330 train_time:4914ms step_avg:39.63ms
step:125/2330 train_time:4950ms step_avg:39.60ms
step:126/2330 train_time:4994ms step_avg:39.64ms
step:127/2330 train_time:5030ms step_avg:39.61ms
step:128/2330 train_time:5075ms step_avg:39.65ms
step:129/2330 train_time:5111ms step_avg:39.62ms
step:130/2330 train_time:5155ms step_avg:39.66ms
step:131/2330 train_time:5191ms step_avg:39.62ms
step:132/2330 train_time:5236ms step_avg:39.67ms
step:133/2330 train_time:5272ms step_avg:39.64ms
step:134/2330 train_time:5317ms step_avg:39.68ms
step:135/2330 train_time:5351ms step_avg:39.64ms
step:136/2330 train_time:5396ms step_avg:39.67ms
step:137/2330 train_time:5431ms step_avg:39.64ms
step:138/2330 train_time:5475ms step_avg:39.68ms
step:139/2330 train_time:5510ms step_avg:39.64ms
step:140/2330 train_time:5555ms step_avg:39.68ms
step:141/2330 train_time:5590ms step_avg:39.65ms
step:142/2330 train_time:5635ms step_avg:39.68ms
step:143/2330 train_time:5670ms step_avg:39.65ms
step:144/2330 train_time:5714ms step_avg:39.68ms
step:145/2330 train_time:5750ms step_avg:39.65ms
step:146/2330 train_time:5794ms step_avg:39.69ms
step:147/2330 train_time:5830ms step_avg:39.66ms
step:148/2330 train_time:5874ms step_avg:39.69ms
step:149/2330 train_time:5910ms step_avg:39.67ms
step:150/2330 train_time:5955ms step_avg:39.70ms
step:151/2330 train_time:5991ms step_avg:39.68ms
step:152/2330 train_time:6036ms step_avg:39.71ms
step:153/2330 train_time:6071ms step_avg:39.68ms
step:154/2330 train_time:6115ms step_avg:39.71ms
step:155/2330 train_time:6150ms step_avg:39.68ms
step:156/2330 train_time:6196ms step_avg:39.72ms
step:157/2330 train_time:6232ms step_avg:39.69ms
step:158/2330 train_time:6276ms step_avg:39.72ms
step:159/2330 train_time:6310ms step_avg:39.69ms
step:160/2330 train_time:6355ms step_avg:39.72ms
step:161/2330 train_time:6390ms step_avg:39.69ms
step:162/2330 train_time:6435ms step_avg:39.72ms
step:163/2330 train_time:6470ms step_avg:39.69ms
step:164/2330 train_time:6515ms step_avg:39.72ms
step:165/2330 train_time:6550ms step_avg:39.70ms
step:166/2330 train_time:6595ms step_avg:39.73ms
step:167/2330 train_time:6630ms step_avg:39.70ms
step:168/2330 train_time:6673ms step_avg:39.72ms
step:169/2330 train_time:6708ms step_avg:39.69ms
step:170/2330 train_time:6753ms step_avg:39.72ms
step:171/2330 train_time:6788ms step_avg:39.70ms
step:172/2330 train_time:6833ms step_avg:39.73ms
step:173/2330 train_time:6868ms step_avg:39.70ms
step:174/2330 train_time:6912ms step_avg:39.73ms
step:175/2330 train_time:6948ms step_avg:39.70ms
step:176/2330 train_time:6992ms step_avg:39.73ms
step:177/2330 train_time:7027ms step_avg:39.70ms
step:178/2330 train_time:7071ms step_avg:39.73ms
step:179/2330 train_time:7106ms step_avg:39.70ms
step:180/2330 train_time:7151ms step_avg:39.73ms
step:181/2330 train_time:7186ms step_avg:39.70ms
step:182/2330 train_time:7231ms step_avg:39.73ms
step:183/2330 train_time:7266ms step_avg:39.70ms
step:184/2330 train_time:7310ms step_avg:39.73ms
step:185/2330 train_time:7345ms step_avg:39.70ms
step:186/2330 train_time:7389ms step_avg:39.73ms
step:187/2330 train_time:7424ms step_avg:39.70ms
step:188/2330 train_time:7469ms step_avg:39.73ms
step:189/2330 train_time:7504ms step_avg:39.70ms
step:190/2330 train_time:7548ms step_avg:39.73ms
step:191/2330 train_time:7583ms step_avg:39.70ms
step:192/2330 train_time:7627ms step_avg:39.73ms
step:193/2330 train_time:7662ms step_avg:39.70ms
step:194/2330 train_time:7707ms step_avg:39.73ms
step:195/2330 train_time:7742ms step_avg:39.70ms
step:196/2330 train_time:7786ms step_avg:39.73ms
step:197/2330 train_time:7822ms step_avg:39.71ms
step:198/2330 train_time:7866ms step_avg:39.73ms
step:199/2330 train_time:7901ms step_avg:39.70ms
step:200/2330 train_time:7945ms step_avg:39.73ms
step:201/2330 train_time:7981ms step_avg:39.70ms
step:202/2330 train_time:8024ms step_avg:39.72ms
step:203/2330 train_time:8060ms step_avg:39.71ms
step:204/2330 train_time:8105ms step_avg:39.73ms
step:205/2330 train_time:8140ms step_avg:39.71ms
step:206/2330 train_time:8185ms step_avg:39.73ms
step:207/2330 train_time:8220ms step_avg:39.71ms
step:208/2330 train_time:8265ms step_avg:39.74ms
step:209/2330 train_time:8301ms step_avg:39.72ms
step:210/2330 train_time:8345ms step_avg:39.74ms
step:211/2330 train_time:8381ms step_avg:39.72ms
step:212/2330 train_time:8426ms step_avg:39.74ms
step:213/2330 train_time:8462ms step_avg:39.73ms
step:214/2330 train_time:8507ms step_avg:39.75ms
step:215/2330 train_time:8542ms step_avg:39.73ms
step:216/2330 train_time:8585ms step_avg:39.75ms
step:217/2330 train_time:8621ms step_avg:39.73ms
step:218/2330 train_time:8665ms step_avg:39.75ms
step:219/2330 train_time:8701ms step_avg:39.73ms
step:220/2330 train_time:8746ms step_avg:39.75ms
step:221/2330 train_time:8781ms step_avg:39.73ms
step:222/2330 train_time:8825ms step_avg:39.75ms
step:223/2330 train_time:8860ms step_avg:39.73ms
step:224/2330 train_time:8905ms step_avg:39.75ms
step:225/2330 train_time:8940ms step_avg:39.73ms
step:226/2330 train_time:8985ms step_avg:39.76ms
step:227/2330 train_time:9021ms step_avg:39.74ms
step:228/2330 train_time:9065ms step_avg:39.76ms
step:229/2330 train_time:9100ms step_avg:39.74ms
step:230/2330 train_time:9145ms step_avg:39.76ms
step:231/2330 train_time:9181ms step_avg:39.74ms
step:232/2330 train_time:9226ms step_avg:39.77ms
step:233/2330 train_time:9261ms step_avg:39.75ms
step:234/2330 train_time:9305ms step_avg:39.77ms
step:235/2330 train_time:9341ms step_avg:39.75ms
step:236/2330 train_time:9386ms step_avg:39.77ms
step:237/2330 train_time:9422ms step_avg:39.75ms
step:238/2330 train_time:9466ms step_avg:39.77ms
step:239/2330 train_time:9502ms step_avg:39.76ms
step:240/2330 train_time:9546ms step_avg:39.77ms
step:241/2330 train_time:9581ms step_avg:39.76ms
step:242/2330 train_time:9625ms step_avg:39.77ms
step:243/2330 train_time:9661ms step_avg:39.76ms
step:244/2330 train_time:9705ms step_avg:39.77ms
step:245/2330 train_time:9740ms step_avg:39.76ms
step:246/2330 train_time:9785ms step_avg:39.78ms
step:247/2330 train_time:9820ms step_avg:39.76ms
step:248/2330 train_time:9864ms step_avg:39.77ms
step:249/2330 train_time:9899ms step_avg:39.76ms
step:250/2330 train_time:9944ms step_avg:39.78ms
step:250/2330 val_loss:5.4191 train_time:10031ms step_avg:40.12ms
step:251/2330 train_time:10045ms step_avg:40.02ms
step:252/2330 train_time:10058ms step_avg:39.91ms
step:253/2330 train_time:10069ms step_avg:39.80ms
step:254/2330 train_time:10104ms step_avg:39.78ms
step:255/2330 train_time:10138ms step_avg:39.76ms
step:256/2330 train_time:10181ms step_avg:39.77ms
step:257/2330 train_time:10216ms step_avg:39.75ms
step:258/2330 train_time:10259ms step_avg:39.76ms
step:259/2330 train_time:10293ms step_avg:39.74ms
step:260/2330 train_time:10337ms step_avg:39.76ms
step:261/2330 train_time:10376ms step_avg:39.75ms
step:262/2330 train_time:10426ms step_avg:39.79ms
step:263/2330 train_time:10463ms step_avg:39.78ms
step:264/2330 train_time:10508ms step_avg:39.80ms
step:265/2330 train_time:10543ms step_avg:39.79ms
step:266/2330 train_time:10588ms step_avg:39.80ms
step:267/2330 train_time:10623ms step_avg:39.79ms
step:268/2330 train_time:10667ms step_avg:39.80ms
step:269/2330 train_time:10702ms step_avg:39.78ms
step:270/2330 train_time:10747ms step_avg:39.80ms
step:271/2330 train_time:10781ms step_avg:39.78ms
step:272/2330 train_time:10825ms step_avg:39.80ms
step:273/2330 train_time:10860ms step_avg:39.78ms
step:274/2330 train_time:10905ms step_avg:39.80ms
step:275/2330 train_time:10941ms step_avg:39.79ms
step:276/2330 train_time:10988ms step_avg:39.81ms
step:277/2330 train_time:11023ms step_avg:39.80ms
step:278/2330 train_time:11068ms step_avg:39.81ms
step:279/2330 train_time:11103ms step_avg:39.80ms
step:280/2330 train_time:11147ms step_avg:39.81ms
step:281/2330 train_time:11182ms step_avg:39.79ms
step:282/2330 train_time:11226ms step_avg:39.81ms
step:283/2330 train_time:11262ms step_avg:39.79ms
step:284/2330 train_time:11307ms step_avg:39.81ms
step:285/2330 train_time:11342ms step_avg:39.80ms
step:286/2330 train_time:11387ms step_avg:39.82ms
step:287/2330 train_time:11424ms step_avg:39.80ms
step:288/2330 train_time:11469ms step_avg:39.82ms
step:289/2330 train_time:11506ms step_avg:39.81ms
step:290/2330 train_time:11551ms step_avg:39.83ms
step:291/2330 train_time:11586ms step_avg:39.82ms
step:292/2330 train_time:11631ms step_avg:39.83ms
step:293/2330 train_time:11667ms step_avg:39.82ms
step:294/2330 train_time:11712ms step_avg:39.84ms
step:295/2330 train_time:11747ms step_avg:39.82ms
step:296/2330 train_time:11791ms step_avg:39.84ms
step:297/2330 train_time:11826ms step_avg:39.82ms
step:298/2330 train_time:11871ms step_avg:39.84ms
step:299/2330 train_time:11906ms step_avg:39.82ms
step:300/2330 train_time:11951ms step_avg:39.84ms
step:301/2330 train_time:11985ms step_avg:39.82ms
step:302/2330 train_time:12030ms step_avg:39.83ms
step:303/2330 train_time:12065ms step_avg:39.82ms
step:304/2330 train_time:12110ms step_avg:39.83ms
step:305/2330 train_time:12144ms step_avg:39.82ms
step:306/2330 train_time:12188ms step_avg:39.83ms
step:307/2330 train_time:12224ms step_avg:39.82ms
step:308/2330 train_time:12269ms step_avg:39.83ms
step:309/2330 train_time:12303ms step_avg:39.82ms
step:310/2330 train_time:12349ms step_avg:39.84ms
step:311/2330 train_time:12384ms step_avg:39.82ms
step:312/2330 train_time:12430ms step_avg:39.84ms
step:313/2330 train_time:12466ms step_avg:39.83ms
step:314/2330 train_time:12510ms step_avg:39.84ms
step:315/2330 train_time:12546ms step_avg:39.83ms
step:316/2330 train_time:12591ms step_avg:39.84ms
step:317/2330 train_time:12626ms step_avg:39.83ms
step:318/2330 train_time:12671ms step_avg:39.85ms
step:319/2330 train_time:12706ms step_avg:39.83ms
step:320/2330 train_time:12750ms step_avg:39.84ms
step:321/2330 train_time:12786ms step_avg:39.83ms
step:322/2330 train_time:12830ms step_avg:39.84ms
step:323/2330 train_time:12865ms step_avg:39.83ms
step:324/2330 train_time:12910ms step_avg:39.84ms
step:325/2330 train_time:12945ms step_avg:39.83ms
step:326/2330 train_time:12988ms step_avg:39.84ms
step:327/2330 train_time:13023ms step_avg:39.83ms
step:328/2330 train_time:13068ms step_avg:39.84ms
step:329/2330 train_time:13103ms step_avg:39.83ms
step:330/2330 train_time:13147ms step_avg:39.84ms
step:331/2330 train_time:13182ms step_avg:39.82ms
step:332/2330 train_time:13227ms step_avg:39.84ms
step:333/2330 train_time:13262ms step_avg:39.83ms
step:334/2330 train_time:13307ms step_avg:39.84ms
step:335/2330 train_time:13343ms step_avg:39.83ms
step:336/2330 train_time:13388ms step_avg:39.84ms
step:337/2330 train_time:13423ms step_avg:39.83ms
step:338/2330 train_time:13468ms step_avg:39.85ms
step:339/2330 train_time:13503ms step_avg:39.83ms
step:340/2330 train_time:13549ms step_avg:39.85ms
step:341/2330 train_time:13584ms step_avg:39.84ms
step:342/2330 train_time:13628ms step_avg:39.85ms
step:343/2330 train_time:13664ms step_avg:39.84ms
step:344/2330 train_time:13709ms step_avg:39.85ms
step:345/2330 train_time:13744ms step_avg:39.84ms
step:346/2330 train_time:13788ms step_avg:39.85ms
step:347/2330 train_time:13823ms step_avg:39.84ms
step:348/2330 train_time:13868ms step_avg:39.85ms
step:349/2330 train_time:13903ms step_avg:39.84ms
step:350/2330 train_time:13948ms step_avg:39.85ms
step:351/2330 train_time:13982ms step_avg:39.83ms
step:352/2330 train_time:14026ms step_avg:39.85ms
step:353/2330 train_time:14061ms step_avg:39.83ms
step:354/2330 train_time:14106ms step_avg:39.85ms
step:355/2330 train_time:14141ms step_avg:39.83ms
step:356/2330 train_time:14186ms step_avg:39.85ms
step:357/2330 train_time:14222ms step_avg:39.84ms
step:358/2330 train_time:14266ms step_avg:39.85ms
step:359/2330 train_time:14301ms step_avg:39.84ms
step:360/2330 train_time:14346ms step_avg:39.85ms
step:361/2330 train_time:14382ms step_avg:39.84ms
step:362/2330 train_time:14427ms step_avg:39.85ms
step:363/2330 train_time:14463ms step_avg:39.84ms
step:364/2330 train_time:14508ms step_avg:39.86ms
step:365/2330 train_time:14543ms step_avg:39.84ms
step:366/2330 train_time:14588ms step_avg:39.86ms
step:367/2330 train_time:14624ms step_avg:39.85ms
step:368/2330 train_time:14668ms step_avg:39.86ms
step:369/2330 train_time:14704ms step_avg:39.85ms
step:370/2330 train_time:14748ms step_avg:39.86ms
step:371/2330 train_time:14783ms step_avg:39.85ms
step:372/2330 train_time:14827ms step_avg:39.86ms
step:373/2330 train_time:14863ms step_avg:39.85ms
step:374/2330 train_time:14907ms step_avg:39.86ms
step:375/2330 train_time:14943ms step_avg:39.85ms
step:376/2330 train_time:14987ms step_avg:39.86ms
step:377/2330 train_time:15021ms step_avg:39.84ms
step:378/2330 train_time:15066ms step_avg:39.86ms
step:379/2330 train_time:15101ms step_avg:39.84ms
step:380/2330 train_time:15146ms step_avg:39.86ms
step:381/2330 train_time:15181ms step_avg:39.85ms
step:382/2330 train_time:15226ms step_avg:39.86ms
step:383/2330 train_time:15261ms step_avg:39.85ms
step:384/2330 train_time:15305ms step_avg:39.86ms
step:385/2330 train_time:15341ms step_avg:39.85ms
step:386/2330 train_time:15385ms step_avg:39.86ms
step:387/2330 train_time:15421ms step_avg:39.85ms
step:388/2330 train_time:15466ms step_avg:39.86ms
step:389/2330 train_time:15502ms step_avg:39.85ms
step:390/2330 train_time:15548ms step_avg:39.87ms
step:391/2330 train_time:15583ms step_avg:39.85ms
step:392/2330 train_time:15629ms step_avg:39.87ms
step:393/2330 train_time:15664ms step_avg:39.86ms
step:394/2330 train_time:15708ms step_avg:39.87ms
step:395/2330 train_time:15744ms step_avg:39.86ms
step:396/2330 train_time:15788ms step_avg:39.87ms
step:397/2330 train_time:15823ms step_avg:39.86ms
step:398/2330 train_time:15868ms step_avg:39.87ms
step:399/2330 train_time:15904ms step_avg:39.86ms
step:400/2330 train_time:15949ms step_avg:39.87ms
step:401/2330 train_time:15983ms step_avg:39.86ms
step:402/2330 train_time:16027ms step_avg:39.87ms
step:403/2330 train_time:16063ms step_avg:39.86ms
step:404/2330 train_time:16107ms step_avg:39.87ms
step:405/2330 train_time:16142ms step_avg:39.86ms
step:406/2330 train_time:16186ms step_avg:39.87ms
step:407/2330 train_time:16221ms step_avg:39.85ms
step:408/2330 train_time:16266ms step_avg:39.87ms
step:409/2330 train_time:16300ms step_avg:39.85ms
step:410/2330 train_time:16346ms step_avg:39.87ms
step:411/2330 train_time:16381ms step_avg:39.86ms
step:412/2330 train_time:16426ms step_avg:39.87ms
step:413/2330 train_time:16463ms step_avg:39.86ms
step:414/2330 train_time:16508ms step_avg:39.87ms
step:415/2330 train_time:16544ms step_avg:39.86ms
step:416/2330 train_time:16589ms step_avg:39.88ms
step:417/2330 train_time:16624ms step_avg:39.87ms
step:418/2330 train_time:16668ms step_avg:39.88ms
step:419/2330 train_time:16703ms step_avg:39.86ms
step:420/2330 train_time:16748ms step_avg:39.88ms
step:421/2330 train_time:16783ms step_avg:39.86ms
step:422/2330 train_time:16827ms step_avg:39.88ms
step:423/2330 train_time:16862ms step_avg:39.86ms
step:424/2330 train_time:16907ms step_avg:39.88ms
step:425/2330 train_time:16942ms step_avg:39.86ms
step:426/2330 train_time:16987ms step_avg:39.87ms
step:427/2330 train_time:17021ms step_avg:39.86ms
step:428/2330 train_time:17066ms step_avg:39.87ms
step:429/2330 train_time:17101ms step_avg:39.86ms
step:430/2330 train_time:17145ms step_avg:39.87ms
step:431/2330 train_time:17181ms step_avg:39.86ms
step:432/2330 train_time:17225ms step_avg:39.87ms
step:433/2330 train_time:17260ms step_avg:39.86ms
step:434/2330 train_time:17305ms step_avg:39.87ms
step:435/2330 train_time:17340ms step_avg:39.86ms
step:436/2330 train_time:17386ms step_avg:39.88ms
step:437/2330 train_time:17422ms step_avg:39.87ms
step:438/2330 train_time:17467ms step_avg:39.88ms
step:439/2330 train_time:17503ms step_avg:39.87ms
step:440/2330 train_time:17548ms step_avg:39.88ms
step:441/2330 train_time:17584ms step_avg:39.87ms
step:442/2330 train_time:17629ms step_avg:39.88ms
step:443/2330 train_time:17664ms step_avg:39.87ms
step:444/2330 train_time:17708ms step_avg:39.88ms
step:445/2330 train_time:17744ms step_avg:39.87ms
step:446/2330 train_time:17788ms step_avg:39.88ms
step:447/2330 train_time:17823ms step_avg:39.87ms
step:448/2330 train_time:17868ms step_avg:39.88ms
step:449/2330 train_time:17903ms step_avg:39.87ms
step:450/2330 train_time:17947ms step_avg:39.88ms
step:451/2330 train_time:17982ms step_avg:39.87ms
step:452/2330 train_time:18026ms step_avg:39.88ms
step:453/2330 train_time:18062ms step_avg:39.87ms
step:454/2330 train_time:18106ms step_avg:39.88ms
step:455/2330 train_time:18141ms step_avg:39.87ms
step:456/2330 train_time:18186ms step_avg:39.88ms
step:457/2330 train_time:18220ms step_avg:39.87ms
step:458/2330 train_time:18265ms step_avg:39.88ms
step:459/2330 train_time:18300ms step_avg:39.87ms
step:460/2330 train_time:18345ms step_avg:39.88ms
step:461/2330 train_time:18380ms step_avg:39.87ms
step:462/2330 train_time:18425ms step_avg:39.88ms
step:463/2330 train_time:18461ms step_avg:39.87ms
step:464/2330 train_time:18507ms step_avg:39.88ms
step:465/2330 train_time:18542ms step_avg:39.88ms
step:466/2330 train_time:18587ms step_avg:39.89ms
step:467/2330 train_time:18623ms step_avg:39.88ms
step:468/2330 train_time:18668ms step_avg:39.89ms
step:469/2330 train_time:18703ms step_avg:39.88ms
step:470/2330 train_time:18747ms step_avg:39.89ms
step:471/2330 train_time:18782ms step_avg:39.88ms
step:472/2330 train_time:18827ms step_avg:39.89ms
step:473/2330 train_time:18862ms step_avg:39.88ms
step:474/2330 train_time:18906ms step_avg:39.89ms
step:475/2330 train_time:18942ms step_avg:39.88ms
step:476/2330 train_time:18986ms step_avg:39.89ms
step:477/2330 train_time:19021ms step_avg:39.88ms
step:478/2330 train_time:19066ms step_avg:39.89ms
step:479/2330 train_time:19101ms step_avg:39.88ms
step:480/2330 train_time:19146ms step_avg:39.89ms
step:481/2330 train_time:19181ms step_avg:39.88ms
step:482/2330 train_time:19226ms step_avg:39.89ms
step:483/2330 train_time:19261ms step_avg:39.88ms
step:484/2330 train_time:19306ms step_avg:39.89ms
step:485/2330 train_time:19342ms step_avg:39.88ms
step:486/2330 train_time:19387ms step_avg:39.89ms
step:487/2330 train_time:19422ms step_avg:39.88ms
step:488/2330 train_time:19467ms step_avg:39.89ms
step:489/2330 train_time:19502ms step_avg:39.88ms
step:490/2330 train_time:19547ms step_avg:39.89ms
step:491/2330 train_time:19582ms step_avg:39.88ms
step:492/2330 train_time:19627ms step_avg:39.89ms
step:493/2330 train_time:19663ms step_avg:39.88ms
step:494/2330 train_time:19707ms step_avg:39.89ms
step:495/2330 train_time:19743ms step_avg:39.88ms
step:496/2330 train_time:19788ms step_avg:39.89ms
step:497/2330 train_time:19823ms step_avg:39.89ms
step:498/2330 train_time:19868ms step_avg:39.89ms
step:499/2330 train_time:19902ms step_avg:39.88ms
step:500/2330 train_time:19947ms step_avg:39.89ms
step:500/2330 val_loss:5.3010 train_time:20033ms step_avg:40.07ms
step:501/2330 train_time:20046ms step_avg:40.01ms
step:502/2330 train_time:20058ms step_avg:39.96ms
step:503/2330 train_time:20068ms step_avg:39.90ms
step:504/2330 train_time:20106ms step_avg:39.89ms
step:505/2330 train_time:20140ms step_avg:39.88ms
step:506/2330 train_time:20183ms step_avg:39.89ms
step:507/2330 train_time:20217ms step_avg:39.88ms
step:508/2330 train_time:20261ms step_avg:39.88ms
step:509/2330 train_time:20295ms step_avg:39.87ms
step:510/2330 train_time:20339ms step_avg:39.88ms
step:511/2330 train_time:20378ms step_avg:39.88ms
step:512/2330 train_time:20427ms step_avg:39.90ms
step:513/2330 train_time:20464ms step_avg:39.89ms
step:514/2330 train_time:20509ms step_avg:39.90ms
step:515/2330 train_time:20544ms step_avg:39.89ms
step:516/2330 train_time:20588ms step_avg:39.90ms
step:517/2330 train_time:20624ms step_avg:39.89ms
step:518/2330 train_time:20668ms step_avg:39.90ms
step:519/2330 train_time:20702ms step_avg:39.89ms
step:520/2330 train_time:20747ms step_avg:39.90ms
step:521/2330 train_time:20781ms step_avg:39.89ms
step:522/2330 train_time:20825ms step_avg:39.89ms
step:523/2330 train_time:20861ms step_avg:39.89ms
step:524/2330 train_time:20905ms step_avg:39.89ms
step:525/2330 train_time:20941ms step_avg:39.89ms
step:526/2330 train_time:20987ms step_avg:39.90ms
step:527/2330 train_time:21023ms step_avg:39.89ms
step:528/2330 train_time:21068ms step_avg:39.90ms
step:529/2330 train_time:21103ms step_avg:39.89ms
step:530/2330 train_time:21148ms step_avg:39.90ms
step:531/2330 train_time:21183ms step_avg:39.89ms
step:532/2330 train_time:21227ms step_avg:39.90ms
step:533/2330 train_time:21262ms step_avg:39.89ms
step:534/2330 train_time:21308ms step_avg:39.90ms
step:535/2330 train_time:21344ms step_avg:39.89ms
step:536/2330 train_time:21390ms step_avg:39.91ms
step:537/2330 train_time:21425ms step_avg:39.90ms
step:538/2330 train_time:21470ms step_avg:39.91ms
step:539/2330 train_time:21505ms step_avg:39.90ms
step:540/2330 train_time:21549ms step_avg:39.91ms
step:541/2330 train_time:21584ms step_avg:39.90ms
step:542/2330 train_time:21629ms step_avg:39.91ms
step:543/2330 train_time:21664ms step_avg:39.90ms
step:544/2330 train_time:21708ms step_avg:39.90ms
step:545/2330 train_time:21743ms step_avg:39.90ms
step:546/2330 train_time:21786ms step_avg:39.90ms
step:547/2330 train_time:21822ms step_avg:39.89ms
step:548/2330 train_time:21866ms step_avg:39.90ms
step:549/2330 train_time:21901ms step_avg:39.89ms
step:550/2330 train_time:21946ms step_avg:39.90ms
step:551/2330 train_time:21982ms step_avg:39.89ms
step:552/2330 train_time:22027ms step_avg:39.90ms
step:553/2330 train_time:22062ms step_avg:39.90ms
step:554/2330 train_time:22107ms step_avg:39.90ms
step:555/2330 train_time:22142ms step_avg:39.90ms
step:556/2330 train_time:22187ms step_avg:39.90ms
step:557/2330 train_time:22223ms step_avg:39.90ms
step:558/2330 train_time:22268ms step_avg:39.91ms
step:559/2330 train_time:22303ms step_avg:39.90ms
step:560/2330 train_time:22348ms step_avg:39.91ms
step:561/2330 train_time:22384ms step_avg:39.90ms
step:562/2330 train_time:22429ms step_avg:39.91ms
step:563/2330 train_time:22464ms step_avg:39.90ms
step:564/2330 train_time:22508ms step_avg:39.91ms
step:565/2330 train_time:22544ms step_avg:39.90ms
step:566/2330 train_time:22588ms step_avg:39.91ms
step:567/2330 train_time:22624ms step_avg:39.90ms
step:568/2330 train_time:22668ms step_avg:39.91ms
step:569/2330 train_time:22703ms step_avg:39.90ms
step:570/2330 train_time:22746ms step_avg:39.91ms
step:571/2330 train_time:22781ms step_avg:39.90ms
step:572/2330 train_time:22826ms step_avg:39.90ms
step:573/2330 train_time:22860ms step_avg:39.90ms
step:574/2330 train_time:22905ms step_avg:39.90ms
step:575/2330 train_time:22940ms step_avg:39.90ms
step:576/2330 train_time:22986ms step_avg:39.91ms
step:577/2330 train_time:23021ms step_avg:39.90ms
step:578/2330 train_time:23065ms step_avg:39.90ms
step:579/2330 train_time:23100ms step_avg:39.90ms
step:580/2330 train_time:23145ms step_avg:39.91ms
step:581/2330 train_time:23181ms step_avg:39.90ms
step:582/2330 train_time:23226ms step_avg:39.91ms
step:583/2330 train_time:23260ms step_avg:39.90ms
step:584/2330 train_time:23306ms step_avg:39.91ms
step:585/2330 train_time:23342ms step_avg:39.90ms
step:586/2330 train_time:23387ms step_avg:39.91ms
step:587/2330 train_time:23422ms step_avg:39.90ms
step:588/2330 train_time:23467ms step_avg:39.91ms
step:589/2330 train_time:23503ms step_avg:39.90ms
step:590/2330 train_time:23548ms step_avg:39.91ms
step:591/2330 train_time:23583ms step_avg:39.90ms
step:592/2330 train_time:23627ms step_avg:39.91ms
step:593/2330 train_time:23663ms step_avg:39.90ms
step:594/2330 train_time:23707ms step_avg:39.91ms
step:595/2330 train_time:23743ms step_avg:39.90ms
step:596/2330 train_time:23787ms step_avg:39.91ms
step:597/2330 train_time:23823ms step_avg:39.90ms
step:598/2330 train_time:23867ms step_avg:39.91ms
step:599/2330 train_time:23902ms step_avg:39.90ms
step:600/2330 train_time:23947ms step_avg:39.91ms
step:601/2330 train_time:23982ms step_avg:39.90ms
step:602/2330 train_time:24027ms step_avg:39.91ms
step:603/2330 train_time:24062ms step_avg:39.90ms
step:604/2330 train_time:24107ms step_avg:39.91ms
step:605/2330 train_time:24142ms step_avg:39.90ms
step:606/2330 train_time:24187ms step_avg:39.91ms
step:607/2330 train_time:24222ms step_avg:39.91ms
step:608/2330 train_time:24268ms step_avg:39.91ms
step:609/2330 train_time:24303ms step_avg:39.91ms
step:610/2330 train_time:24348ms step_avg:39.92ms
step:611/2330 train_time:24384ms step_avg:39.91ms
step:612/2330 train_time:24428ms step_avg:39.92ms
step:613/2330 train_time:24464ms step_avg:39.91ms
step:614/2330 train_time:24508ms step_avg:39.92ms
step:615/2330 train_time:24543ms step_avg:39.91ms
step:616/2330 train_time:24588ms step_avg:39.92ms
step:617/2330 train_time:24623ms step_avg:39.91ms
step:618/2330 train_time:24668ms step_avg:39.92ms
step:619/2330 train_time:24703ms step_avg:39.91ms
step:620/2330 train_time:24747ms step_avg:39.91ms
step:621/2330 train_time:24782ms step_avg:39.91ms
step:622/2330 train_time:24827ms step_avg:39.91ms
step:623/2330 train_time:24862ms step_avg:39.91ms
step:624/2330 train_time:24906ms step_avg:39.91ms
step:625/2330 train_time:24941ms step_avg:39.91ms
step:626/2330 train_time:24987ms step_avg:39.91ms
step:627/2330 train_time:25022ms step_avg:39.91ms
step:628/2330 train_time:25067ms step_avg:39.92ms
step:629/2330 train_time:25102ms step_avg:39.91ms
step:630/2330 train_time:25147ms step_avg:39.92ms
step:631/2330 train_time:25182ms step_avg:39.91ms
step:632/2330 train_time:25227ms step_avg:39.92ms
step:633/2330 train_time:25262ms step_avg:39.91ms
step:634/2330 train_time:25307ms step_avg:39.92ms
step:635/2330 train_time:25343ms step_avg:39.91ms
step:636/2330 train_time:25388ms step_avg:39.92ms
step:637/2330 train_time:25423ms step_avg:39.91ms
step:638/2330 train_time:25469ms step_avg:39.92ms
step:639/2330 train_time:25504ms step_avg:39.91ms
step:640/2330 train_time:25548ms step_avg:39.92ms
step:641/2330 train_time:25583ms step_avg:39.91ms
step:642/2330 train_time:25627ms step_avg:39.92ms
step:643/2330 train_time:25663ms step_avg:39.91ms
step:644/2330 train_time:25708ms step_avg:39.92ms
step:645/2330 train_time:25743ms step_avg:39.91ms
step:646/2330 train_time:25787ms step_avg:39.92ms
step:647/2330 train_time:25822ms step_avg:39.91ms
step:648/2330 train_time:25867ms step_avg:39.92ms
step:649/2330 train_time:25902ms step_avg:39.91ms
step:650/2330 train_time:25946ms step_avg:39.92ms
step:651/2330 train_time:25982ms step_avg:39.91ms
step:652/2330 train_time:26027ms step_avg:39.92ms
step:653/2330 train_time:26062ms step_avg:39.91ms
step:654/2330 train_time:26107ms step_avg:39.92ms
step:655/2330 train_time:26142ms step_avg:39.91ms
step:656/2330 train_time:26187ms step_avg:39.92ms
step:657/2330 train_time:26223ms step_avg:39.91ms
step:658/2330 train_time:26268ms step_avg:39.92ms
step:659/2330 train_time:26303ms step_avg:39.91ms
step:660/2330 train_time:26347ms step_avg:39.92ms
step:661/2330 train_time:26383ms step_avg:39.91ms
step:662/2330 train_time:26428ms step_avg:39.92ms
step:663/2330 train_time:26463ms step_avg:39.91ms
step:664/2330 train_time:26508ms step_avg:39.92ms
step:665/2330 train_time:26543ms step_avg:39.91ms
step:666/2330 train_time:26587ms step_avg:39.92ms
step:667/2330 train_time:26623ms step_avg:39.91ms
step:668/2330 train_time:26668ms step_avg:39.92ms
step:669/2330 train_time:26703ms step_avg:39.92ms
step:670/2330 train_time:26748ms step_avg:39.92ms
step:671/2330 train_time:26783ms step_avg:39.91ms
step:672/2330 train_time:26827ms step_avg:39.92ms
step:673/2330 train_time:26862ms step_avg:39.91ms
step:674/2330 train_time:26907ms step_avg:39.92ms
step:675/2330 train_time:26942ms step_avg:39.91ms
step:676/2330 train_time:26987ms step_avg:39.92ms
step:677/2330 train_time:27021ms step_avg:39.91ms
step:678/2330 train_time:27067ms step_avg:39.92ms
step:679/2330 train_time:27102ms step_avg:39.91ms
step:680/2330 train_time:27147ms step_avg:39.92ms
step:681/2330 train_time:27183ms step_avg:39.92ms
step:682/2330 train_time:27227ms step_avg:39.92ms
step:683/2330 train_time:27263ms step_avg:39.92ms
step:684/2330 train_time:27308ms step_avg:39.92ms
step:685/2330 train_time:27343ms step_avg:39.92ms
step:686/2330 train_time:27387ms step_avg:39.92ms
step:687/2330 train_time:27423ms step_avg:39.92ms
step:688/2330 train_time:27467ms step_avg:39.92ms
step:689/2330 train_time:27502ms step_avg:39.92ms
step:690/2330 train_time:27548ms step_avg:39.92ms
step:691/2330 train_time:27583ms step_avg:39.92ms
step:692/2330 train_time:27628ms step_avg:39.92ms
step:693/2330 train_time:27663ms step_avg:39.92ms
step:694/2330 train_time:27708ms step_avg:39.92ms
step:695/2330 train_time:27743ms step_avg:39.92ms
step:696/2330 train_time:27788ms step_avg:39.92ms
step:697/2330 train_time:27822ms step_avg:39.92ms
step:698/2330 train_time:27867ms step_avg:39.92ms
step:699/2330 train_time:27903ms step_avg:39.92ms
step:700/2330 train_time:27948ms step_avg:39.93ms
step:701/2330 train_time:27982ms step_avg:39.92ms
step:702/2330 train_time:28027ms step_avg:39.92ms
step:703/2330 train_time:28062ms step_avg:39.92ms
step:704/2330 train_time:28107ms step_avg:39.93ms
step:705/2330 train_time:28143ms step_avg:39.92ms
step:706/2330 train_time:28187ms step_avg:39.92ms
step:707/2330 train_time:28223ms step_avg:39.92ms
step:708/2330 train_time:28267ms step_avg:39.93ms
step:709/2330 train_time:28303ms step_avg:39.92ms
step:710/2330 train_time:28348ms step_avg:39.93ms
step:711/2330 train_time:28383ms step_avg:39.92ms
step:712/2330 train_time:28428ms step_avg:39.93ms
step:713/2330 train_time:28463ms step_avg:39.92ms
step:714/2330 train_time:28508ms step_avg:39.93ms
step:715/2330 train_time:28543ms step_avg:39.92ms
step:716/2330 train_time:28588ms step_avg:39.93ms
step:717/2330 train_time:28623ms step_avg:39.92ms
step:718/2330 train_time:28667ms step_avg:39.93ms
step:719/2330 train_time:28702ms step_avg:39.92ms
step:720/2330 train_time:28747ms step_avg:39.93ms
step:721/2330 train_time:28781ms step_avg:39.92ms
step:722/2330 train_time:28826ms step_avg:39.92ms
step:723/2330 train_time:28861ms step_avg:39.92ms
step:724/2330 train_time:28906ms step_avg:39.92ms
step:725/2330 train_time:28940ms step_avg:39.92ms
step:726/2330 train_time:28985ms step_avg:39.92ms
step:727/2330 train_time:29021ms step_avg:39.92ms
step:728/2330 train_time:29066ms step_avg:39.93ms
step:729/2330 train_time:29100ms step_avg:39.92ms
step:730/2330 train_time:29145ms step_avg:39.93ms
step:731/2330 train_time:29181ms step_avg:39.92ms
step:732/2330 train_time:29226ms step_avg:39.93ms
step:733/2330 train_time:29262ms step_avg:39.92ms
step:734/2330 train_time:29307ms step_avg:39.93ms
step:735/2330 train_time:29342ms step_avg:39.92ms
step:736/2330 train_time:29387ms step_avg:39.93ms
step:737/2330 train_time:29423ms step_avg:39.92ms
step:738/2330 train_time:29468ms step_avg:39.93ms
step:739/2330 train_time:29503ms step_avg:39.92ms
step:740/2330 train_time:29548ms step_avg:39.93ms
step:741/2330 train_time:29583ms step_avg:39.92ms
step:742/2330 train_time:29628ms step_avg:39.93ms
step:743/2330 train_time:29663ms step_avg:39.92ms
step:744/2330 train_time:29708ms step_avg:39.93ms
step:745/2330 train_time:29743ms step_avg:39.92ms
step:746/2330 train_time:29788ms step_avg:39.93ms
step:747/2330 train_time:29823ms step_avg:39.92ms
step:748/2330 train_time:29867ms step_avg:39.93ms
step:749/2330 train_time:29902ms step_avg:39.92ms
step:750/2330 train_time:29947ms step_avg:39.93ms
step:750/2330 val_loss:5.2398 train_time:30034ms step_avg:40.05ms
step:751/2330 train_time:30047ms step_avg:40.01ms
step:752/2330 train_time:30059ms step_avg:39.97ms
step:753/2330 train_time:30069ms step_avg:39.93ms
step:754/2330 train_time:30107ms step_avg:39.93ms
step:755/2330 train_time:30140ms step_avg:39.92ms
step:756/2330 train_time:30184ms step_avg:39.93ms
step:757/2330 train_time:30218ms step_avg:39.92ms
step:758/2330 train_time:30262ms step_avg:39.92ms
step:759/2330 train_time:30297ms step_avg:39.92ms
step:760/2330 train_time:30341ms step_avg:39.92ms
step:761/2330 train_time:30381ms step_avg:39.92ms
step:762/2330 train_time:30428ms step_avg:39.93ms
step:763/2330 train_time:30464ms step_avg:39.93ms
step:764/2330 train_time:30509ms step_avg:39.93ms
step:765/2330 train_time:30546ms step_avg:39.93ms
step:766/2330 train_time:30590ms step_avg:39.93ms
step:767/2330 train_time:30625ms step_avg:39.93ms
step:768/2330 train_time:30669ms step_avg:39.93ms
step:769/2330 train_time:30704ms step_avg:39.93ms
step:770/2330 train_time:30747ms step_avg:39.93ms
step:771/2330 train_time:30782ms step_avg:39.93ms
step:772/2330 train_time:30826ms step_avg:39.93ms
step:773/2330 train_time:30860ms step_avg:39.92ms
step:774/2330 train_time:30904ms step_avg:39.93ms
step:775/2330 train_time:30940ms step_avg:39.92ms
step:776/2330 train_time:30987ms step_avg:39.93ms
step:777/2330 train_time:31023ms step_avg:39.93ms
step:778/2330 train_time:31067ms step_avg:39.93ms
step:779/2330 train_time:31103ms step_avg:39.93ms
step:780/2330 train_time:31147ms step_avg:39.93ms
step:781/2330 train_time:31182ms step_avg:39.93ms
step:782/2330 train_time:31227ms step_avg:39.93ms
step:783/2330 train_time:31262ms step_avg:39.93ms
step:784/2330 train_time:31307ms step_avg:39.93ms
step:785/2330 train_time:31343ms step_avg:39.93ms
step:786/2330 train_time:31388ms step_avg:39.93ms
step:787/2330 train_time:31424ms step_avg:39.93ms
step:788/2330 train_time:31469ms step_avg:39.94ms
step:789/2330 train_time:31505ms step_avg:39.93ms
step:790/2330 train_time:31549ms step_avg:39.94ms
step:791/2330 train_time:31585ms step_avg:39.93ms
step:792/2330 train_time:31629ms step_avg:39.94ms
step:793/2330 train_time:31664ms step_avg:39.93ms
step:794/2330 train_time:31708ms step_avg:39.93ms
step:795/2330 train_time:31743ms step_avg:39.93ms
step:796/2330 train_time:31787ms step_avg:39.93ms
step:797/2330 train_time:31822ms step_avg:39.93ms
step:798/2330 train_time:31866ms step_avg:39.93ms
step:799/2330 train_time:31902ms step_avg:39.93ms
step:800/2330 train_time:31946ms step_avg:39.93ms
step:801/2330 train_time:31982ms step_avg:39.93ms
step:802/2330 train_time:32027ms step_avg:39.93ms
step:803/2330 train_time:32062ms step_avg:39.93ms
step:804/2330 train_time:32106ms step_avg:39.93ms
step:805/2330 train_time:32142ms step_avg:39.93ms
step:806/2330 train_time:32187ms step_avg:39.93ms
step:807/2330 train_time:32223ms step_avg:39.93ms
step:808/2330 train_time:32267ms step_avg:39.93ms
step:809/2330 train_time:32302ms step_avg:39.93ms
step:810/2330 train_time:32348ms step_avg:39.94ms
step:811/2330 train_time:32383ms step_avg:39.93ms
step:812/2330 train_time:32429ms step_avg:39.94ms
step:813/2330 train_time:32464ms step_avg:39.93ms
step:814/2330 train_time:32509ms step_avg:39.94ms
step:815/2330 train_time:32545ms step_avg:39.93ms
step:816/2330 train_time:32589ms step_avg:39.94ms
step:817/2330 train_time:32624ms step_avg:39.93ms
step:818/2330 train_time:32668ms step_avg:39.94ms
step:819/2330 train_time:32703ms step_avg:39.93ms
step:820/2330 train_time:32748ms step_avg:39.94ms
step:821/2330 train_time:32783ms step_avg:39.93ms
step:822/2330 train_time:32827ms step_avg:39.94ms
step:823/2330 train_time:32863ms step_avg:39.93ms
step:824/2330 train_time:32907ms step_avg:39.94ms
step:825/2330 train_time:32943ms step_avg:39.93ms
step:826/2330 train_time:32987ms step_avg:39.94ms
step:827/2330 train_time:33022ms step_avg:39.93ms
step:828/2330 train_time:33067ms step_avg:39.94ms
step:829/2330 train_time:33102ms step_avg:39.93ms
step:830/2330 train_time:33147ms step_avg:39.94ms
step:831/2330 train_time:33182ms step_avg:39.93ms
step:832/2330 train_time:33227ms step_avg:39.94ms
step:833/2330 train_time:33263ms step_avg:39.93ms
step:834/2330 train_time:33308ms step_avg:39.94ms
step:835/2330 train_time:33344ms step_avg:39.93ms
step:836/2330 train_time:33389ms step_avg:39.94ms
step:837/2330 train_time:33424ms step_avg:39.93ms
step:838/2330 train_time:33469ms step_avg:39.94ms
step:839/2330 train_time:33505ms step_avg:39.93ms
step:840/2330 train_time:33549ms step_avg:39.94ms
step:841/2330 train_time:33584ms step_avg:39.93ms
step:842/2330 train_time:33628ms step_avg:39.94ms
step:843/2330 train_time:33664ms step_avg:39.93ms
step:844/2330 train_time:33708ms step_avg:39.94ms
step:845/2330 train_time:33743ms step_avg:39.93ms
step:846/2330 train_time:33787ms step_avg:39.94ms
step:847/2330 train_time:33822ms step_avg:39.93ms
step:848/2330 train_time:33867ms step_avg:39.94ms
step:849/2330 train_time:33902ms step_avg:39.93ms
step:850/2330 train_time:33947ms step_avg:39.94ms
step:851/2330 train_time:33982ms step_avg:39.93ms
step:852/2330 train_time:34026ms step_avg:39.94ms
step:853/2330 train_time:34061ms step_avg:39.93ms
step:854/2330 train_time:34106ms step_avg:39.94ms
step:855/2330 train_time:34142ms step_avg:39.93ms
step:856/2330 train_time:34187ms step_avg:39.94ms
step:857/2330 train_time:34222ms step_avg:39.93ms
step:858/2330 train_time:34267ms step_avg:39.94ms
step:859/2330 train_time:34303ms step_avg:39.93ms
step:860/2330 train_time:34348ms step_avg:39.94ms
step:861/2330 train_time:34384ms step_avg:39.93ms
step:862/2330 train_time:34428ms step_avg:39.94ms
step:863/2330 train_time:34464ms step_avg:39.94ms
step:864/2330 train_time:34509ms step_avg:39.94ms
step:865/2330 train_time:34544ms step_avg:39.94ms
step:866/2330 train_time:34588ms step_avg:39.94ms
step:867/2330 train_time:34623ms step_avg:39.93ms
step:868/2330 train_time:34668ms step_avg:39.94ms
step:869/2330 train_time:34704ms step_avg:39.94ms
step:870/2330 train_time:34748ms step_avg:39.94ms
step:871/2330 train_time:34784ms step_avg:39.94ms
step:872/2330 train_time:34828ms step_avg:39.94ms
step:873/2330 train_time:34863ms step_avg:39.93ms
step:874/2330 train_time:34907ms step_avg:39.94ms
step:875/2330 train_time:34942ms step_avg:39.93ms
step:876/2330 train_time:34987ms step_avg:39.94ms
step:877/2330 train_time:35022ms step_avg:39.93ms
step:878/2330 train_time:35066ms step_avg:39.94ms
step:879/2330 train_time:35102ms step_avg:39.93ms
step:880/2330 train_time:35146ms step_avg:39.94ms
step:881/2330 train_time:35182ms step_avg:39.93ms
step:882/2330 train_time:35226ms step_avg:39.94ms
step:883/2330 train_time:35262ms step_avg:39.93ms
step:884/2330 train_time:35306ms step_avg:39.94ms
step:885/2330 train_time:35342ms step_avg:39.93ms
step:886/2330 train_time:35388ms step_avg:39.94ms
step:887/2330 train_time:35424ms step_avg:39.94ms
step:888/2330 train_time:35469ms step_avg:39.94ms
step:889/2330 train_time:35504ms step_avg:39.94ms
step:890/2330 train_time:35549ms step_avg:39.94ms
step:891/2330 train_time:35584ms step_avg:39.94ms
step:892/2330 train_time:35629ms step_avg:39.94ms
step:893/2330 train_time:35664ms step_avg:39.94ms
step:894/2330 train_time:35708ms step_avg:39.94ms
step:895/2330 train_time:35743ms step_avg:39.94ms
step:896/2330 train_time:35788ms step_avg:39.94ms
step:897/2330 train_time:35823ms step_avg:39.94ms
step:898/2330 train_time:35867ms step_avg:39.94ms
step:899/2330 train_time:35902ms step_avg:39.94ms
step:900/2330 train_time:35947ms step_avg:39.94ms
step:901/2330 train_time:35982ms step_avg:39.94ms
step:902/2330 train_time:36026ms step_avg:39.94ms
step:903/2330 train_time:36061ms step_avg:39.94ms
step:904/2330 train_time:36106ms step_avg:39.94ms
step:905/2330 train_time:36142ms step_avg:39.94ms
step:906/2330 train_time:36186ms step_avg:39.94ms
step:907/2330 train_time:36222ms step_avg:39.94ms
step:908/2330 train_time:36267ms step_avg:39.94ms
step:909/2330 train_time:36302ms step_avg:39.94ms
step:910/2330 train_time:36347ms step_avg:39.94ms
step:911/2330 train_time:36383ms step_avg:39.94ms
step:912/2330 train_time:36427ms step_avg:39.94ms
step:913/2330 train_time:36463ms step_avg:39.94ms
step:914/2330 train_time:36508ms step_avg:39.94ms
step:915/2330 train_time:36544ms step_avg:39.94ms
step:916/2330 train_time:36589ms step_avg:39.94ms
step:917/2330 train_time:36624ms step_avg:39.94ms
step:918/2330 train_time:36669ms step_avg:39.94ms
step:919/2330 train_time:36705ms step_avg:39.94ms
step:920/2330 train_time:36749ms step_avg:39.94ms
step:921/2330 train_time:36784ms step_avg:39.94ms
step:922/2330 train_time:36829ms step_avg:39.94ms
step:923/2330 train_time:36864ms step_avg:39.94ms
step:924/2330 train_time:36908ms step_avg:39.94ms
step:925/2330 train_time:36944ms step_avg:39.94ms
step:926/2330 train_time:36987ms step_avg:39.94ms
step:927/2330 train_time:37023ms step_avg:39.94ms
step:928/2330 train_time:37066ms step_avg:39.94ms
step:929/2330 train_time:37102ms step_avg:39.94ms
step:930/2330 train_time:37146ms step_avg:39.94ms
step:931/2330 train_time:37181ms step_avg:39.94ms
step:932/2330 train_time:37226ms step_avg:39.94ms
step:933/2330 train_time:37261ms step_avg:39.94ms
step:934/2330 train_time:37306ms step_avg:39.94ms
step:935/2330 train_time:37341ms step_avg:39.94ms
step:936/2330 train_time:37386ms step_avg:39.94ms
step:937/2330 train_time:37422ms step_avg:39.94ms
step:938/2330 train_time:37467ms step_avg:39.94ms
step:939/2330 train_time:37503ms step_avg:39.94ms
step:940/2330 train_time:37548ms step_avg:39.94ms
step:941/2330 train_time:37583ms step_avg:39.94ms
step:942/2330 train_time:37627ms step_avg:39.94ms
step:943/2330 train_time:37663ms step_avg:39.94ms
step:944/2330 train_time:37708ms step_avg:39.94ms
step:945/2330 train_time:37743ms step_avg:39.94ms
step:946/2330 train_time:37787ms step_avg:39.94ms
step:947/2330 train_time:37823ms step_avg:39.94ms
step:948/2330 train_time:37867ms step_avg:39.94ms
step:949/2330 train_time:37902ms step_avg:39.94ms
step:950/2330 train_time:37947ms step_avg:39.94ms
step:951/2330 train_time:37982ms step_avg:39.94ms
step:952/2330 train_time:38026ms step_avg:39.94ms
step:953/2330 train_time:38060ms step_avg:39.94ms
step:954/2330 train_time:38105ms step_avg:39.94ms
step:955/2330 train_time:38140ms step_avg:39.94ms
step:956/2330 train_time:38185ms step_avg:39.94ms
step:957/2330 train_time:38220ms step_avg:39.94ms
step:958/2330 train_time:38263ms step_avg:39.94ms
step:959/2330 train_time:38299ms step_avg:39.94ms
step:960/2330 train_time:38343ms step_avg:39.94ms
step:961/2330 train_time:38379ms step_avg:39.94ms
step:962/2330 train_time:38424ms step_avg:39.94ms
step:963/2330 train_time:38460ms step_avg:39.94ms
step:964/2330 train_time:38505ms step_avg:39.94ms
step:965/2330 train_time:38540ms step_avg:39.94ms
step:966/2330 train_time:38585ms step_avg:39.94ms
step:967/2330 train_time:38621ms step_avg:39.94ms
step:968/2330 train_time:38665ms step_avg:39.94ms
step:969/2330 train_time:38700ms step_avg:39.94ms
step:970/2330 train_time:38745ms step_avg:39.94ms
step:971/2330 train_time:38780ms step_avg:39.94ms
step:972/2330 train_time:38825ms step_avg:39.94ms
step:973/2330 train_time:38860ms step_avg:39.94ms
step:974/2330 train_time:38905ms step_avg:39.94ms
step:975/2330 train_time:38940ms step_avg:39.94ms
step:976/2330 train_time:38985ms step_avg:39.94ms
step:977/2330 train_time:39019ms step_avg:39.94ms
step:978/2330 train_time:39064ms step_avg:39.94ms
step:979/2330 train_time:39099ms step_avg:39.94ms
step:980/2330 train_time:39142ms step_avg:39.94ms
step:981/2330 train_time:39178ms step_avg:39.94ms
step:982/2330 train_time:39223ms step_avg:39.94ms
step:983/2330 train_time:39258ms step_avg:39.94ms
step:984/2330 train_time:39303ms step_avg:39.94ms
step:985/2330 train_time:39337ms step_avg:39.94ms
step:986/2330 train_time:39382ms step_avg:39.94ms
step:987/2330 train_time:39418ms step_avg:39.94ms
step:988/2330 train_time:39462ms step_avg:39.94ms
step:989/2330 train_time:39498ms step_avg:39.94ms
step:990/2330 train_time:39543ms step_avg:39.94ms
step:991/2330 train_time:39579ms step_avg:39.94ms
step:992/2330 train_time:39622ms step_avg:39.94ms
step:993/2330 train_time:39658ms step_avg:39.94ms
step:994/2330 train_time:39702ms step_avg:39.94ms
step:995/2330 train_time:39737ms step_avg:39.94ms
step:996/2330 train_time:39782ms step_avg:39.94ms
step:997/2330 train_time:39818ms step_avg:39.94ms
step:998/2330 train_time:39862ms step_avg:39.94ms
step:999/2330 train_time:39897ms step_avg:39.94ms
step:1000/2330 train_time:39942ms step_avg:39.94ms
step:1000/2330 val_loss:5.2042 train_time:40029ms step_avg:40.03ms
step:1001/2330 train_time:40042ms step_avg:40.00ms
step:1002/2330 train_time:40054ms step_avg:39.97ms
step:1003/2330 train_time:40065ms step_avg:39.95ms
step:1004/2330 train_time:40101ms step_avg:39.94ms
step:1005/2330 train_time:40135ms step_avg:39.94ms
step:1006/2330 train_time:40178ms step_avg:39.94ms
step:1007/2330 train_time:40213ms step_avg:39.93ms
step:1008/2330 train_time:40257ms step_avg:39.94ms
step:1009/2330 train_time:40291ms step_avg:39.93ms
step:1010/2330 train_time:40335ms step_avg:39.94ms
step:1011/2330 train_time:40374ms step_avg:39.93ms
step:1012/2330 train_time:40419ms step_avg:39.94ms
step:1013/2330 train_time:40455ms step_avg:39.94ms
step:1014/2330 train_time:40500ms step_avg:39.94ms
step:1015/2330 train_time:40536ms step_avg:39.94ms
step:1016/2330 train_time:40580ms step_avg:39.94ms
step:1017/2330 train_time:40615ms step_avg:39.94ms
step:1018/2330 train_time:40659ms step_avg:39.94ms
step:1019/2330 train_time:40694ms step_avg:39.94ms
step:1020/2330 train_time:40738ms step_avg:39.94ms
step:1021/2330 train_time:40773ms step_avg:39.93ms
step:1022/2330 train_time:40817ms step_avg:39.94ms
step:1023/2330 train_time:40852ms step_avg:39.93ms
step:1024/2330 train_time:40895ms step_avg:39.94ms
step:1025/2330 train_time:40930ms step_avg:39.93ms
step:1026/2330 train_time:40977ms step_avg:39.94ms
step:1027/2330 train_time:41013ms step_avg:39.94ms
step:1028/2330 train_time:41058ms step_avg:39.94ms
step:1029/2330 train_time:41094ms step_avg:39.94ms
step:1030/2330 train_time:41138ms step_avg:39.94ms
step:1031/2330 train_time:41174ms step_avg:39.94ms
step:1032/2330 train_time:41218ms step_avg:39.94ms
step:1033/2330 train_time:41253ms step_avg:39.94ms
step:1034/2330 train_time:41298ms step_avg:39.94ms
step:1035/2330 train_time:41333ms step_avg:39.94ms
step:1036/2330 train_time:41379ms step_avg:39.94ms
step:1037/2330 train_time:41414ms step_avg:39.94ms
step:1038/2330 train_time:41459ms step_avg:39.94ms
step:1039/2330 train_time:41495ms step_avg:39.94ms
step:1040/2330 train_time:41539ms step_avg:39.94ms
step:1041/2330 train_time:41574ms step_avg:39.94ms
step:1042/2330 train_time:41619ms step_avg:39.94ms
step:1043/2330 train_time:41654ms step_avg:39.94ms
step:1044/2330 train_time:41698ms step_avg:39.94ms
step:1045/2330 train_time:41733ms step_avg:39.94ms
step:1046/2330 train_time:41778ms step_avg:39.94ms
step:1047/2330 train_time:41813ms step_avg:39.94ms
step:1048/2330 train_time:41857ms step_avg:39.94ms
step:1049/2330 train_time:41892ms step_avg:39.94ms
step:1050/2330 train_time:41937ms step_avg:39.94ms
step:1051/2330 train_time:41973ms step_avg:39.94ms
step:1052/2330 train_time:42018ms step_avg:39.94ms
step:1053/2330 train_time:42054ms step_avg:39.94ms
step:1054/2330 train_time:42098ms step_avg:39.94ms
step:1055/2330 train_time:42133ms step_avg:39.94ms
step:1056/2330 train_time:42177ms step_avg:39.94ms
step:1057/2330 train_time:42213ms step_avg:39.94ms
step:1058/2330 train_time:42258ms step_avg:39.94ms
step:1059/2330 train_time:42294ms step_avg:39.94ms
step:1060/2330 train_time:42338ms step_avg:39.94ms
step:1061/2330 train_time:42374ms step_avg:39.94ms
step:1062/2330 train_time:42419ms step_avg:39.94ms
step:1063/2330 train_time:42454ms step_avg:39.94ms
step:1064/2330 train_time:42498ms step_avg:39.94ms
step:1065/2330 train_time:42534ms step_avg:39.94ms
step:1066/2330 train_time:42578ms step_avg:39.94ms
step:1067/2330 train_time:42614ms step_avg:39.94ms
step:1068/2330 train_time:42658ms step_avg:39.94ms
step:1069/2330 train_time:42693ms step_avg:39.94ms
step:1070/2330 train_time:42737ms step_avg:39.94ms
step:1071/2330 train_time:42772ms step_avg:39.94ms
step:1072/2330 train_time:42817ms step_avg:39.94ms
step:1073/2330 train_time:42852ms step_avg:39.94ms
step:1074/2330 train_time:42896ms step_avg:39.94ms
step:1075/2330 train_time:42931ms step_avg:39.94ms
step:1076/2330 train_time:42975ms step_avg:39.94ms
step:1077/2330 train_time:43011ms step_avg:39.94ms
step:1078/2330 train_time:43056ms step_avg:39.94ms
step:1079/2330 train_time:43091ms step_avg:39.94ms
step:1080/2330 train_time:43136ms step_avg:39.94ms
step:1081/2330 train_time:43171ms step_avg:39.94ms
step:1082/2330 train_time:43215ms step_avg:39.94ms
step:1083/2330 train_time:43251ms step_avg:39.94ms
step:1084/2330 train_time:43295ms step_avg:39.94ms
step:1085/2330 train_time:43330ms step_avg:39.94ms
step:1086/2330 train_time:43375ms step_avg:39.94ms
step:1087/2330 train_time:43411ms step_avg:39.94ms
step:1088/2330 train_time:43455ms step_avg:39.94ms
step:1089/2330 train_time:43491ms step_avg:39.94ms
step:1090/2330 train_time:43536ms step_avg:39.94ms
step:1091/2330 train_time:43571ms step_avg:39.94ms
step:1092/2330 train_time:43615ms step_avg:39.94ms
step:1093/2330 train_time:43651ms step_avg:39.94ms
step:1094/2330 train_time:43695ms step_avg:39.94ms
step:1095/2330 train_time:43730ms step_avg:39.94ms
step:1096/2330 train_time:43775ms step_avg:39.94ms
step:1097/2330 train_time:43810ms step_avg:39.94ms
step:1098/2330 train_time:43855ms step_avg:39.94ms
step:1099/2330 train_time:43889ms step_avg:39.94ms
step:1100/2330 train_time:43934ms step_avg:39.94ms
step:1101/2330 train_time:43969ms step_avg:39.94ms
step:1102/2330 train_time:44013ms step_avg:39.94ms
step:1103/2330 train_time:44048ms step_avg:39.94ms
step:1104/2330 train_time:44093ms step_avg:39.94ms
step:1105/2330 train_time:44128ms step_avg:39.94ms
step:1106/2330 train_time:44173ms step_avg:39.94ms
step:1107/2330 train_time:44208ms step_avg:39.94ms
step:1108/2330 train_time:44253ms step_avg:39.94ms
step:1109/2330 train_time:44289ms step_avg:39.94ms
step:1110/2330 train_time:44333ms step_avg:39.94ms
step:1111/2330 train_time:44369ms step_avg:39.94ms
step:1112/2330 train_time:44413ms step_avg:39.94ms
step:1113/2330 train_time:44449ms step_avg:39.94ms
step:1114/2330 train_time:44493ms step_avg:39.94ms
step:1115/2330 train_time:44529ms step_avg:39.94ms
step:1116/2330 train_time:44574ms step_avg:39.94ms
step:1117/2330 train_time:44609ms step_avg:39.94ms
step:1118/2330 train_time:44653ms step_avg:39.94ms
step:1119/2330 train_time:44688ms step_avg:39.94ms
step:1120/2330 train_time:44733ms step_avg:39.94ms
step:1121/2330 train_time:44768ms step_avg:39.94ms
step:1122/2330 train_time:44813ms step_avg:39.94ms
step:1123/2330 train_time:44847ms step_avg:39.94ms
step:1124/2330 train_time:44892ms step_avg:39.94ms
step:1125/2330 train_time:44927ms step_avg:39.94ms
step:1126/2330 train_time:44972ms step_avg:39.94ms
step:1127/2330 train_time:45007ms step_avg:39.94ms
step:1128/2330 train_time:45051ms step_avg:39.94ms
step:1129/2330 train_time:45087ms step_avg:39.94ms
step:1130/2330 train_time:45132ms step_avg:39.94ms
step:1131/2330 train_time:45167ms step_avg:39.94ms
step:1132/2330 train_time:45211ms step_avg:39.94ms
step:1133/2330 train_time:45245ms step_avg:39.93ms
step:1134/2330 train_time:45291ms step_avg:39.94ms
step:1135/2330 train_time:45326ms step_avg:39.93ms
step:1136/2330 train_time:45371ms step_avg:39.94ms
step:1137/2330 train_time:45406ms step_avg:39.93ms
step:1138/2330 train_time:45451ms step_avg:39.94ms
step:1139/2330 train_time:45487ms step_avg:39.94ms
step:1140/2330 train_time:45532ms step_avg:39.94ms
step:1141/2330 train_time:45567ms step_avg:39.94ms
step:1142/2330 train_time:45610ms step_avg:39.94ms
step:1143/2330 train_time:45646ms step_avg:39.93ms
step:1144/2330 train_time:45690ms step_avg:39.94ms
step:1145/2330 train_time:45726ms step_avg:39.94ms
step:1146/2330 train_time:45771ms step_avg:39.94ms
step:1147/2330 train_time:45806ms step_avg:39.94ms
step:1148/2330 train_time:45850ms step_avg:39.94ms
step:1149/2330 train_time:45884ms step_avg:39.93ms
step:1150/2330 train_time:45929ms step_avg:39.94ms
step:1151/2330 train_time:45964ms step_avg:39.93ms
step:1152/2330 train_time:46009ms step_avg:39.94ms
step:1153/2330 train_time:46045ms step_avg:39.93ms
step:1154/2330 train_time:46089ms step_avg:39.94ms
step:1155/2330 train_time:46125ms step_avg:39.93ms
step:1156/2330 train_time:46169ms step_avg:39.94ms
step:1157/2330 train_time:46204ms step_avg:39.93ms
step:1158/2330 train_time:46249ms step_avg:39.94ms
step:1159/2330 train_time:46284ms step_avg:39.93ms
step:1160/2330 train_time:46329ms step_avg:39.94ms
step:1161/2330 train_time:46365ms step_avg:39.94ms
step:1162/2330 train_time:46410ms step_avg:39.94ms
step:1163/2330 train_time:46444ms step_avg:39.94ms
step:1164/2330 train_time:46489ms step_avg:39.94ms
step:1165/2330 train_time:46525ms step_avg:39.94ms
step:1166/2330 train_time:46569ms step_avg:39.94ms
step:1167/2330 train_time:46604ms step_avg:39.94ms
step:1168/2330 train_time:46649ms step_avg:39.94ms
step:1169/2330 train_time:46684ms step_avg:39.94ms
step:1170/2330 train_time:46729ms step_avg:39.94ms
step:1171/2330 train_time:46764ms step_avg:39.93ms
step:1172/2330 train_time:46809ms step_avg:39.94ms
step:1173/2330 train_time:46844ms step_avg:39.94ms
step:1174/2330 train_time:46889ms step_avg:39.94ms
step:1175/2330 train_time:46924ms step_avg:39.94ms
step:1176/2330 train_time:46968ms step_avg:39.94ms
step:1177/2330 train_time:47004ms step_avg:39.94ms
step:1178/2330 train_time:47049ms step_avg:39.94ms
step:1179/2330 train_time:47084ms step_avg:39.94ms
step:1180/2330 train_time:47130ms step_avg:39.94ms
step:1181/2330 train_time:47165ms step_avg:39.94ms
step:1182/2330 train_time:47209ms step_avg:39.94ms
step:1183/2330 train_time:47244ms step_avg:39.94ms
step:1184/2330 train_time:47289ms step_avg:39.94ms
step:1185/2330 train_time:47325ms step_avg:39.94ms
step:1186/2330 train_time:47370ms step_avg:39.94ms
step:1187/2330 train_time:47405ms step_avg:39.94ms
step:1188/2330 train_time:47450ms step_avg:39.94ms
step:1189/2330 train_time:47486ms step_avg:39.94ms
step:1190/2330 train_time:47531ms step_avg:39.94ms
step:1191/2330 train_time:47566ms step_avg:39.94ms
step:1192/2330 train_time:47611ms step_avg:39.94ms
step:1193/2330 train_time:47646ms step_avg:39.94ms
step:1194/2330 train_time:47691ms step_avg:39.94ms
step:1195/2330 train_time:47726ms step_avg:39.94ms
step:1196/2330 train_time:47771ms step_avg:39.94ms
step:1197/2330 train_time:47806ms step_avg:39.94ms
step:1198/2330 train_time:47851ms step_avg:39.94ms
step:1199/2330 train_time:47886ms step_avg:39.94ms
step:1200/2330 train_time:47930ms step_avg:39.94ms
step:1201/2330 train_time:47966ms step_avg:39.94ms
step:1202/2330 train_time:48011ms step_avg:39.94ms
step:1203/2330 train_time:48046ms step_avg:39.94ms
step:1204/2330 train_time:48091ms step_avg:39.94ms
step:1205/2330 train_time:48126ms step_avg:39.94ms
step:1206/2330 train_time:48170ms step_avg:39.94ms
step:1207/2330 train_time:48205ms step_avg:39.94ms
step:1208/2330 train_time:48250ms step_avg:39.94ms
step:1209/2330 train_time:48285ms step_avg:39.94ms
step:1210/2330 train_time:48330ms step_avg:39.94ms
step:1211/2330 train_time:48366ms step_avg:39.94ms
step:1212/2330 train_time:48410ms step_avg:39.94ms
step:1213/2330 train_time:48445ms step_avg:39.94ms
step:1214/2330 train_time:48489ms step_avg:39.94ms
step:1215/2330 train_time:48525ms step_avg:39.94ms
step:1216/2330 train_time:48570ms step_avg:39.94ms
step:1217/2330 train_time:48606ms step_avg:39.94ms
step:1218/2330 train_time:48650ms step_avg:39.94ms
step:1219/2330 train_time:48686ms step_avg:39.94ms
step:1220/2330 train_time:48730ms step_avg:39.94ms
step:1221/2330 train_time:48766ms step_avg:39.94ms
step:1222/2330 train_time:48810ms step_avg:39.94ms
step:1223/2330 train_time:48845ms step_avg:39.94ms
step:1224/2330 train_time:48890ms step_avg:39.94ms
step:1225/2330 train_time:48925ms step_avg:39.94ms
step:1226/2330 train_time:48969ms step_avg:39.94ms
step:1227/2330 train_time:49004ms step_avg:39.94ms
step:1228/2330 train_time:49049ms step_avg:39.94ms
step:1229/2330 train_time:49085ms step_avg:39.94ms
step:1230/2330 train_time:49130ms step_avg:39.94ms
step:1231/2330 train_time:49164ms step_avg:39.94ms
step:1232/2330 train_time:49209ms step_avg:39.94ms
step:1233/2330 train_time:49245ms step_avg:39.94ms
step:1234/2330 train_time:49289ms step_avg:39.94ms
step:1235/2330 train_time:49324ms step_avg:39.94ms
step:1236/2330 train_time:49369ms step_avg:39.94ms
step:1237/2330 train_time:49405ms step_avg:39.94ms
step:1238/2330 train_time:49449ms step_avg:39.94ms
step:1239/2330 train_time:49484ms step_avg:39.94ms
step:1240/2330 train_time:49530ms step_avg:39.94ms
step:1241/2330 train_time:49565ms step_avg:39.94ms
step:1242/2330 train_time:49610ms step_avg:39.94ms
step:1243/2330 train_time:49645ms step_avg:39.94ms
step:1244/2330 train_time:49689ms step_avg:39.94ms
step:1245/2330 train_time:49725ms step_avg:39.94ms
step:1246/2330 train_time:49770ms step_avg:39.94ms
step:1247/2330 train_time:49805ms step_avg:39.94ms
step:1248/2330 train_time:49849ms step_avg:39.94ms
step:1249/2330 train_time:49884ms step_avg:39.94ms
step:1250/2330 train_time:49929ms step_avg:39.94ms
step:1250/2330 val_loss:5.1773 train_time:50017ms step_avg:40.01ms
step:1251/2330 train_time:50029ms step_avg:39.99ms
step:1252/2330 train_time:50042ms step_avg:39.97ms
step:1253/2330 train_time:50052ms step_avg:39.95ms
step:1254/2330 train_time:50091ms step_avg:39.95ms
step:1255/2330 train_time:50125ms step_avg:39.94ms
step:1256/2330 train_time:50169ms step_avg:39.94ms
step:1257/2330 train_time:50203ms step_avg:39.94ms
step:1258/2330 train_time:50247ms step_avg:39.94ms
step:1259/2330 train_time:50282ms step_avg:39.94ms
step:1260/2330 train_time:50326ms step_avg:39.94ms
step:1261/2330 train_time:50363ms step_avg:39.94ms
step:1262/2330 train_time:50409ms step_avg:39.94ms
step:1263/2330 train_time:50447ms step_avg:39.94ms
step:1264/2330 train_time:50493ms step_avg:39.95ms
step:1265/2330 train_time:50529ms step_avg:39.94ms
step:1266/2330 train_time:50573ms step_avg:39.95ms
step:1267/2330 train_time:50608ms step_avg:39.94ms
step:1268/2330 train_time:50769ms step_avg:40.04ms
step:1269/2330 train_time:50803ms step_avg:40.03ms
step:1270/2330 train_time:50846ms step_avg:40.04ms
step:1271/2330 train_time:50880ms step_avg:40.03ms
step:1272/2330 train_time:51075ms step_avg:40.15ms
step:1273/2330 train_time:51108ms step_avg:40.15ms
step:1274/2330 train_time:51151ms step_avg:40.15ms
step:1275/2330 train_time:51186ms step_avg:40.15ms
step:1276/2330 train_time:51229ms step_avg:40.15ms
step:1277/2330 train_time:51263ms step_avg:40.14ms
step:1278/2330 train_time:51307ms step_avg:40.15ms
step:1279/2330 train_time:51341ms step_avg:40.14ms
step:1280/2330 train_time:51384ms step_avg:40.14ms
step:1281/2330 train_time:51419ms step_avg:40.14ms
step:1282/2330 train_time:51463ms step_avg:40.14ms
step:1283/2330 train_time:51498ms step_avg:40.14ms
step:1284/2330 train_time:51542ms step_avg:40.14ms
step:1285/2330 train_time:51576ms step_avg:40.14ms
step:1286/2330 train_time:51620ms step_avg:40.14ms
step:1287/2330 train_time:51654ms step_avg:40.14ms
step:1288/2330 train_time:51697ms step_avg:40.14ms
step:1289/2330 train_time:51732ms step_avg:40.13ms
step:1290/2330 train_time:51776ms step_avg:40.14ms
step:1291/2330 train_time:51811ms step_avg:40.13ms
step:1292/2330 train_time:51854ms step_avg:40.13ms
step:1293/2330 train_time:51890ms step_avg:40.13ms
step:1294/2330 train_time:51938ms step_avg:40.14ms
step:1295/2330 train_time:51977ms step_avg:40.14ms
step:1296/2330 train_time:52025ms step_avg:40.14ms
step:1297/2330 train_time:52062ms step_avg:40.14ms
step:1298/2330 train_time:52107ms step_avg:40.14ms
step:1299/2330 train_time:52142ms step_avg:40.14ms
step:1300/2330 train_time:52187ms step_avg:40.14ms
step:1301/2330 train_time:52222ms step_avg:40.14ms
step:1302/2330 train_time:52266ms step_avg:40.14ms
step:1303/2330 train_time:52301ms step_avg:40.14ms
step:1304/2330 train_time:52344ms step_avg:40.14ms
step:1305/2330 train_time:52379ms step_avg:40.14ms
step:1306/2330 train_time:52422ms step_avg:40.14ms
step:1307/2330 train_time:52457ms step_avg:40.14ms
step:1308/2330 train_time:52500ms step_avg:40.14ms
step:1309/2330 train_time:52535ms step_avg:40.13ms
step:1310/2330 train_time:52579ms step_avg:40.14ms
step:1311/2330 train_time:52614ms step_avg:40.13ms
step:1312/2330 train_time:52657ms step_avg:40.14ms
step:1313/2330 train_time:52692ms step_avg:40.13ms
step:1314/2330 train_time:52736ms step_avg:40.13ms
step:1315/2330 train_time:52770ms step_avg:40.13ms
step:1316/2330 train_time:52814ms step_avg:40.13ms
step:1317/2330 train_time:52849ms step_avg:40.13ms
step:1318/2330 train_time:52895ms step_avg:40.13ms
step:1319/2330 train_time:52931ms step_avg:40.13ms
step:1320/2330 train_time:52979ms step_avg:40.14ms
step:1321/2330 train_time:53016ms step_avg:40.13ms
step:1322/2330 train_time:53062ms step_avg:40.14ms
step:1323/2330 train_time:53099ms step_avg:40.13ms
step:1324/2330 train_time:53144ms step_avg:40.14ms
step:1325/2330 train_time:53179ms step_avg:40.14ms
step:1326/2330 train_time:53223ms step_avg:40.14ms
step:1327/2330 train_time:53259ms step_avg:40.13ms
step:1328/2330 train_time:53303ms step_avg:40.14ms
step:1329/2330 train_time:53337ms step_avg:40.13ms
step:1330/2330 train_time:53382ms step_avg:40.14ms
step:1331/2330 train_time:53416ms step_avg:40.13ms
step:1332/2330 train_time:53460ms step_avg:40.13ms
step:1333/2330 train_time:53495ms step_avg:40.13ms
step:1334/2330 train_time:53539ms step_avg:40.13ms
step:1335/2330 train_time:53574ms step_avg:40.13ms
step:1336/2330 train_time:53617ms step_avg:40.13ms
step:1337/2330 train_time:53651ms step_avg:40.13ms
step:1338/2330 train_time:53696ms step_avg:40.13ms
step:1339/2330 train_time:53730ms step_avg:40.13ms
step:1340/2330 train_time:53774ms step_avg:40.13ms
step:1341/2330 train_time:53809ms step_avg:40.13ms
step:1342/2330 train_time:53854ms step_avg:40.13ms
step:1343/2330 train_time:53890ms step_avg:40.13ms
step:1344/2330 train_time:53936ms step_avg:40.13ms
step:1345/2330 train_time:53973ms step_avg:40.13ms
step:1346/2330 train_time:54020ms step_avg:40.13ms
step:1347/2330 train_time:54056ms step_avg:40.13ms
step:1348/2330 train_time:54102ms step_avg:40.13ms
step:1349/2330 train_time:54138ms step_avg:40.13ms
step:1350/2330 train_time:54182ms step_avg:40.13ms
step:1351/2330 train_time:54218ms step_avg:40.13ms
step:1352/2330 train_time:54263ms step_avg:40.14ms
step:1353/2330 train_time:54298ms step_avg:40.13ms
step:1354/2330 train_time:54342ms step_avg:40.13ms
step:1355/2330 train_time:54377ms step_avg:40.13ms
step:1356/2330 train_time:54421ms step_avg:40.13ms
step:1357/2330 train_time:54455ms step_avg:40.13ms
step:1358/2330 train_time:54499ms step_avg:40.13ms
step:1359/2330 train_time:54534ms step_avg:40.13ms
step:1360/2330 train_time:54577ms step_avg:40.13ms
step:1361/2330 train_time:54612ms step_avg:40.13ms
step:1362/2330 train_time:54657ms step_avg:40.13ms
step:1363/2330 train_time:54692ms step_avg:40.13ms
step:1364/2330 train_time:54737ms step_avg:40.13ms
step:1365/2330 train_time:54771ms step_avg:40.13ms
step:1366/2330 train_time:54815ms step_avg:40.13ms
step:1367/2330 train_time:54850ms step_avg:40.12ms
step:1368/2330 train_time:54896ms step_avg:40.13ms
step:1369/2330 train_time:54932ms step_avg:40.13ms
step:1370/2330 train_time:54978ms step_avg:40.13ms
step:1371/2330 train_time:55015ms step_avg:40.13ms
step:1372/2330 train_time:55061ms step_avg:40.13ms
step:1373/2330 train_time:55097ms step_avg:40.13ms
step:1374/2330 train_time:55142ms step_avg:40.13ms
step:1375/2330 train_time:55177ms step_avg:40.13ms
step:1376/2330 train_time:55221ms step_avg:40.13ms
step:1377/2330 train_time:55256ms step_avg:40.13ms
step:1378/2330 train_time:55301ms step_avg:40.13ms
step:1379/2330 train_time:55336ms step_avg:40.13ms
step:1380/2330 train_time:55381ms step_avg:40.13ms
step:1381/2330 train_time:55416ms step_avg:40.13ms
step:1382/2330 train_time:55460ms step_avg:40.13ms
step:1383/2330 train_time:55495ms step_avg:40.13ms
step:1384/2330 train_time:55539ms step_avg:40.13ms
step:1385/2330 train_time:55574ms step_avg:40.13ms
step:1386/2330 train_time:55618ms step_avg:40.13ms
step:1387/2330 train_time:55652ms step_avg:40.12ms
step:1388/2330 train_time:55696ms step_avg:40.13ms
step:1389/2330 train_time:55732ms step_avg:40.12ms
step:1390/2330 train_time:55776ms step_avg:40.13ms
step:1391/2330 train_time:55812ms step_avg:40.12ms
step:1392/2330 train_time:55857ms step_avg:40.13ms
step:1393/2330 train_time:55893ms step_avg:40.12ms
step:1394/2330 train_time:55939ms step_avg:40.13ms
step:1395/2330 train_time:55975ms step_avg:40.13ms
step:1396/2330 train_time:56020ms step_avg:40.13ms
step:1397/2330 train_time:56056ms step_avg:40.13ms
step:1398/2330 train_time:56102ms step_avg:40.13ms
step:1399/2330 train_time:56137ms step_avg:40.13ms
step:1400/2330 train_time:56182ms step_avg:40.13ms
step:1401/2330 train_time:56217ms step_avg:40.13ms
step:1402/2330 train_time:56262ms step_avg:40.13ms
step:1403/2330 train_time:56297ms step_avg:40.13ms
step:1404/2330 train_time:56342ms step_avg:40.13ms
step:1405/2330 train_time:56377ms step_avg:40.13ms
step:1406/2330 train_time:56421ms step_avg:40.13ms
step:1407/2330 train_time:56456ms step_avg:40.13ms
step:1408/2330 train_time:56500ms step_avg:40.13ms
step:1409/2330 train_time:56534ms step_avg:40.12ms
step:1410/2330 train_time:56579ms step_avg:40.13ms
step:1411/2330 train_time:56615ms step_avg:40.12ms
step:1412/2330 train_time:56659ms step_avg:40.13ms
step:1413/2330 train_time:56694ms step_avg:40.12ms
step:1414/2330 train_time:56738ms step_avg:40.13ms
step:1415/2330 train_time:56773ms step_avg:40.12ms
step:1416/2330 train_time:56818ms step_avg:40.13ms
step:1417/2330 train_time:56853ms step_avg:40.12ms
step:1418/2330 train_time:56898ms step_avg:40.13ms
step:1419/2330 train_time:56935ms step_avg:40.12ms
step:1420/2330 train_time:56980ms step_avg:40.13ms
step:1421/2330 train_time:57015ms step_avg:40.12ms
step:1422/2330 train_time:57060ms step_avg:40.13ms
step:1423/2330 train_time:57096ms step_avg:40.12ms
step:1424/2330 train_time:57141ms step_avg:40.13ms
step:1425/2330 train_time:57177ms step_avg:40.12ms
step:1426/2330 train_time:57222ms step_avg:40.13ms
step:1427/2330 train_time:57257ms step_avg:40.12ms
step:1428/2330 train_time:57302ms step_avg:40.13ms
step:1429/2330 train_time:57337ms step_avg:40.12ms
step:1430/2330 train_time:57382ms step_avg:40.13ms
step:1431/2330 train_time:57417ms step_avg:40.12ms
step:1432/2330 train_time:57461ms step_avg:40.13ms
step:1433/2330 train_time:57496ms step_avg:40.12ms
step:1434/2330 train_time:57540ms step_avg:40.13ms
step:1435/2330 train_time:57576ms step_avg:40.12ms
step:1436/2330 train_time:57619ms step_avg:40.12ms
step:1437/2330 train_time:57654ms step_avg:40.12ms
step:1438/2330 train_time:57699ms step_avg:40.12ms
step:1439/2330 train_time:57734ms step_avg:40.12ms
step:1440/2330 train_time:57779ms step_avg:40.12ms
step:1441/2330 train_time:57814ms step_avg:40.12ms
step:1442/2330 train_time:57859ms step_avg:40.12ms
step:1443/2330 train_time:57894ms step_avg:40.12ms
step:1444/2330 train_time:57939ms step_avg:40.12ms
step:1445/2330 train_time:57975ms step_avg:40.12ms
step:1446/2330 train_time:58021ms step_avg:40.12ms
step:1447/2330 train_time:58056ms step_avg:40.12ms
step:1448/2330 train_time:58101ms step_avg:40.13ms
step:1449/2330 train_time:58137ms step_avg:40.12ms
step:1450/2330 train_time:58182ms step_avg:40.13ms
step:1451/2330 train_time:58217ms step_avg:40.12ms
step:1452/2330 train_time:58262ms step_avg:40.13ms
step:1453/2330 train_time:58298ms step_avg:40.12ms
step:1454/2330 train_time:58343ms step_avg:40.13ms
step:1455/2330 train_time:58378ms step_avg:40.12ms
step:1456/2330 train_time:58422ms step_avg:40.12ms
step:1457/2330 train_time:58457ms step_avg:40.12ms
step:1458/2330 train_time:58501ms step_avg:40.12ms
step:1459/2330 train_time:58535ms step_avg:40.12ms
step:1460/2330 train_time:58580ms step_avg:40.12ms
step:1461/2330 train_time:58615ms step_avg:40.12ms
step:1462/2330 train_time:58659ms step_avg:40.12ms
step:1463/2330 train_time:58695ms step_avg:40.12ms
step:1464/2330 train_time:58739ms step_avg:40.12ms
step:1465/2330 train_time:58774ms step_avg:40.12ms
step:1466/2330 train_time:58818ms step_avg:40.12ms
step:1467/2330 train_time:58854ms step_avg:40.12ms
step:1468/2330 train_time:58899ms step_avg:40.12ms
step:1469/2330 train_time:58934ms step_avg:40.12ms
step:1470/2330 train_time:58979ms step_avg:40.12ms
step:1471/2330 train_time:59014ms step_avg:40.12ms
step:1472/2330 train_time:59059ms step_avg:40.12ms
step:1473/2330 train_time:59096ms step_avg:40.12ms
step:1474/2330 train_time:59140ms step_avg:40.12ms
step:1475/2330 train_time:59175ms step_avg:40.12ms
step:1476/2330 train_time:59220ms step_avg:40.12ms
step:1477/2330 train_time:59255ms step_avg:40.12ms
step:1478/2330 train_time:59300ms step_avg:40.12ms
step:1479/2330 train_time:59336ms step_avg:40.12ms
step:1480/2330 train_time:59381ms step_avg:40.12ms
step:1481/2330 train_time:59416ms step_avg:40.12ms
step:1482/2330 train_time:59461ms step_avg:40.12ms
step:1483/2330 train_time:59495ms step_avg:40.12ms
step:1484/2330 train_time:59540ms step_avg:40.12ms
step:1485/2330 train_time:59575ms step_avg:40.12ms
step:1486/2330 train_time:59619ms step_avg:40.12ms
step:1487/2330 train_time:59655ms step_avg:40.12ms
step:1488/2330 train_time:59701ms step_avg:40.12ms
step:1489/2330 train_time:59735ms step_avg:40.12ms
step:1490/2330 train_time:59779ms step_avg:40.12ms
step:1491/2330 train_time:59814ms step_avg:40.12ms
step:1492/2330 train_time:59858ms step_avg:40.12ms
step:1493/2330 train_time:59894ms step_avg:40.12ms
step:1494/2330 train_time:59939ms step_avg:40.12ms
step:1495/2330 train_time:59975ms step_avg:40.12ms
step:1496/2330 train_time:60019ms step_avg:40.12ms
step:1497/2330 train_time:60055ms step_avg:40.12ms
step:1498/2330 train_time:60099ms step_avg:40.12ms
step:1499/2330 train_time:60135ms step_avg:40.12ms
step:1500/2330 train_time:60180ms step_avg:40.12ms
step:1500/2330 val_loss:5.1590 train_time:60267ms step_avg:40.18ms
step:1501/2330 train_time:60281ms step_avg:40.16ms
step:1502/2330 train_time:60294ms step_avg:40.14ms
step:1503/2330 train_time:60304ms step_avg:40.12ms
step:1504/2330 train_time:60340ms step_avg:40.12ms
step:1505/2330 train_time:60374ms step_avg:40.12ms
step:1506/2330 train_time:60418ms step_avg:40.12ms
step:1507/2330 train_time:60452ms step_avg:40.11ms
step:1508/2330 train_time:60495ms step_avg:40.12ms
step:1509/2330 train_time:60529ms step_avg:40.11ms
step:1510/2330 train_time:60574ms step_avg:40.12ms
step:1511/2330 train_time:60613ms step_avg:40.11ms
step:1512/2330 train_time:60661ms step_avg:40.12ms
step:1513/2330 train_time:60697ms step_avg:40.12ms
step:1514/2330 train_time:60742ms step_avg:40.12ms
step:1515/2330 train_time:60777ms step_avg:40.12ms
step:1516/2330 train_time:60993ms step_avg:40.23ms
step:1517/2330 train_time:61005ms step_avg:40.21ms
step:1518/2330 train_time:61017ms step_avg:40.20ms
step:1519/2330 train_time:61042ms step_avg:40.19ms
step:1520/2330 train_time:61085ms step_avg:40.19ms
step:1521/2330 train_time:61120ms step_avg:40.18ms
step:1522/2330 train_time:61306ms step_avg:40.28ms
step:1523/2330 train_time:61339ms step_avg:40.28ms
step:1524/2330 train_time:61382ms step_avg:40.28ms
step:1525/2330 train_time:61416ms step_avg:40.27ms
step:1526/2330 train_time:61459ms step_avg:40.27ms
step:1527/2330 train_time:61494ms step_avg:40.27ms
step:1528/2330 train_time:61538ms step_avg:40.27ms
step:1529/2330 train_time:61574ms step_avg:40.27ms
step:1530/2330 train_time:61616ms step_avg:40.27ms
step:1531/2330 train_time:61650ms step_avg:40.27ms
step:1532/2330 train_time:61693ms step_avg:40.27ms
step:1533/2330 train_time:61727ms step_avg:40.27ms
step:1534/2330 train_time:61771ms step_avg:40.27ms
step:1535/2330 train_time:61805ms step_avg:40.26ms
step:1536/2330 train_time:61849ms step_avg:40.27ms
step:1537/2330 train_time:61883ms step_avg:40.26ms
step:1538/2330 train_time:61927ms step_avg:40.26ms
step:1539/2330 train_time:61961ms step_avg:40.26ms
step:1540/2330 train_time:62004ms step_avg:40.26ms
step:1541/2330 train_time:62039ms step_avg:40.26ms
step:1542/2330 train_time:62082ms step_avg:40.26ms
step:1543/2330 train_time:62117ms step_avg:40.26ms
step:1544/2330 train_time:62163ms step_avg:40.26ms
step:1545/2330 train_time:62204ms step_avg:40.26ms
step:1546/2330 train_time:62252ms step_avg:40.27ms
step:1547/2330 train_time:62289ms step_avg:40.26ms
step:1548/2330 train_time:62334ms step_avg:40.27ms
step:1549/2330 train_time:62370ms step_avg:40.26ms
step:1550/2330 train_time:62414ms step_avg:40.27ms
step:1551/2330 train_time:62449ms step_avg:40.26ms
step:1552/2330 train_time:62494ms step_avg:40.27ms
step:1553/2330 train_time:62529ms step_avg:40.26ms
step:1554/2330 train_time:62573ms step_avg:40.27ms
step:1555/2330 train_time:62607ms step_avg:40.26ms
step:1556/2330 train_time:62652ms step_avg:40.26ms
step:1557/2330 train_time:62687ms step_avg:40.26ms
step:1558/2330 train_time:62731ms step_avg:40.26ms
step:1559/2330 train_time:62765ms step_avg:40.26ms
step:1560/2330 train_time:62809ms step_avg:40.26ms
step:1561/2330 train_time:62843ms step_avg:40.26ms
step:1562/2330 train_time:62887ms step_avg:40.26ms
step:1563/2330 train_time:62921ms step_avg:40.26ms
step:1564/2330 train_time:62965ms step_avg:40.26ms
step:1565/2330 train_time:63000ms step_avg:40.26ms
step:1566/2330 train_time:63043ms step_avg:40.26ms
step:1567/2330 train_time:63078ms step_avg:40.25ms
step:1568/2330 train_time:63122ms step_avg:40.26ms
step:1569/2330 train_time:63159ms step_avg:40.25ms
step:1570/2330 train_time:63205ms step_avg:40.26ms
step:1571/2330 train_time:63242ms step_avg:40.26ms
step:1572/2330 train_time:63287ms step_avg:40.26ms
step:1573/2330 train_time:63324ms step_avg:40.26ms
step:1574/2330 train_time:63371ms step_avg:40.26ms
step:1575/2330 train_time:63406ms step_avg:40.26ms
step:1576/2330 train_time:63450ms step_avg:40.26ms
step:1577/2330 train_time:63486ms step_avg:40.26ms
step:1578/2330 train_time:63530ms step_avg:40.26ms
step:1579/2330 train_time:63566ms step_avg:40.26ms
step:1580/2330 train_time:63610ms step_avg:40.26ms
step:1581/2330 train_time:63645ms step_avg:40.26ms
step:1582/2330 train_time:63689ms step_avg:40.26ms
step:1583/2330 train_time:63724ms step_avg:40.25ms
step:1584/2330 train_time:63767ms step_avg:40.26ms
step:1585/2330 train_time:63803ms step_avg:40.25ms
step:1586/2330 train_time:63847ms step_avg:40.26ms
step:1587/2330 train_time:63882ms step_avg:40.25ms
step:1588/2330 train_time:63925ms step_avg:40.26ms
step:1589/2330 train_time:63960ms step_avg:40.25ms
step:1590/2330 train_time:64004ms step_avg:40.25ms
step:1591/2330 train_time:64038ms step_avg:40.25ms
step:1592/2330 train_time:64082ms step_avg:40.25ms
step:1593/2330 train_time:64118ms step_avg:40.25ms
step:1594/2330 train_time:64164ms step_avg:40.25ms
step:1595/2330 train_time:64200ms step_avg:40.25ms
step:1596/2330 train_time:64245ms step_avg:40.25ms
step:1597/2330 train_time:64280ms step_avg:40.25ms
step:1598/2330 train_time:64325ms step_avg:40.25ms
step:1599/2330 train_time:64362ms step_avg:40.25ms
step:1600/2330 train_time:64407ms step_avg:40.25ms
step:1601/2330 train_time:64443ms step_avg:40.25ms
step:1602/2330 train_time:64487ms step_avg:40.25ms
step:1603/2330 train_time:64522ms step_avg:40.25ms
step:1604/2330 train_time:64566ms step_avg:40.25ms
step:1605/2330 train_time:64601ms step_avg:40.25ms
step:1606/2330 train_time:64645ms step_avg:40.25ms
step:1607/2330 train_time:64680ms step_avg:40.25ms
step:1608/2330 train_time:64725ms step_avg:40.25ms
step:1609/2330 train_time:64759ms step_avg:40.25ms
step:1610/2330 train_time:64804ms step_avg:40.25ms
step:1611/2330 train_time:64839ms step_avg:40.25ms
step:1612/2330 train_time:64883ms step_avg:40.25ms
step:1613/2330 train_time:64917ms step_avg:40.25ms
step:1614/2330 train_time:64961ms step_avg:40.25ms
step:1615/2330 train_time:64996ms step_avg:40.25ms
step:1616/2330 train_time:65040ms step_avg:40.25ms
step:1617/2330 train_time:65075ms step_avg:40.24ms
step:1618/2330 train_time:65120ms step_avg:40.25ms
step:1619/2330 train_time:65156ms step_avg:40.24ms
step:1620/2330 train_time:65200ms step_avg:40.25ms
step:1621/2330 train_time:65235ms step_avg:40.24ms
step:1622/2330 train_time:65280ms step_avg:40.25ms
step:1623/2330 train_time:65315ms step_avg:40.24ms
step:1624/2330 train_time:65362ms step_avg:40.25ms
step:1625/2330 train_time:65399ms step_avg:40.25ms
step:1626/2330 train_time:65444ms step_avg:40.25ms
step:1627/2330 train_time:65479ms step_avg:40.25ms
step:1628/2330 train_time:65524ms step_avg:40.25ms
step:1629/2330 train_time:65559ms step_avg:40.25ms
step:1630/2330 train_time:65604ms step_avg:40.25ms
step:1631/2330 train_time:65639ms step_avg:40.24ms
step:1632/2330 train_time:65684ms step_avg:40.25ms
step:1633/2330 train_time:65720ms step_avg:40.24ms
step:1634/2330 train_time:65764ms step_avg:40.25ms
step:1635/2330 train_time:65799ms step_avg:40.24ms
step:1636/2330 train_time:65842ms step_avg:40.25ms
step:1637/2330 train_time:65877ms step_avg:40.24ms
step:1638/2330 train_time:65921ms step_avg:40.24ms
step:1639/2330 train_time:65956ms step_avg:40.24ms
step:1640/2330 train_time:66001ms step_avg:40.24ms
step:1641/2330 train_time:66035ms step_avg:40.24ms
step:1642/2330 train_time:66080ms step_avg:40.24ms
step:1643/2330 train_time:66114ms step_avg:40.24ms
step:1644/2330 train_time:66161ms step_avg:40.24ms
step:1645/2330 train_time:66195ms step_avg:40.24ms
step:1646/2330 train_time:66240ms step_avg:40.24ms
step:1647/2330 train_time:66274ms step_avg:40.24ms
step:1648/2330 train_time:66319ms step_avg:40.24ms
step:1649/2330 train_time:66355ms step_avg:40.24ms
step:1650/2330 train_time:66400ms step_avg:40.24ms
step:1651/2330 train_time:66435ms step_avg:40.24ms
step:1652/2330 train_time:66480ms step_avg:40.24ms
step:1653/2330 train_time:66516ms step_avg:40.24ms
step:1654/2330 train_time:66561ms step_avg:40.24ms
step:1655/2330 train_time:66596ms step_avg:40.24ms
step:1656/2330 train_time:66641ms step_avg:40.24ms
step:1657/2330 train_time:66676ms step_avg:40.24ms
step:1658/2330 train_time:66721ms step_avg:40.24ms
step:1659/2330 train_time:66755ms step_avg:40.24ms
step:1660/2330 train_time:66799ms step_avg:40.24ms
step:1661/2330 train_time:66834ms step_avg:40.24ms
step:1662/2330 train_time:66878ms step_avg:40.24ms
step:1663/2330 train_time:66914ms step_avg:40.24ms
step:1664/2330 train_time:66958ms step_avg:40.24ms
step:1665/2330 train_time:66993ms step_avg:40.24ms
step:1666/2330 train_time:67037ms step_avg:40.24ms
step:1667/2330 train_time:67071ms step_avg:40.23ms
step:1668/2330 train_time:67115ms step_avg:40.24ms
step:1669/2330 train_time:67150ms step_avg:40.23ms
step:1670/2330 train_time:67194ms step_avg:40.24ms
step:1671/2330 train_time:67229ms step_avg:40.23ms
step:1672/2330 train_time:67273ms step_avg:40.24ms
step:1673/2330 train_time:67309ms step_avg:40.23ms
step:1674/2330 train_time:67354ms step_avg:40.24ms
step:1675/2330 train_time:67390ms step_avg:40.23ms
step:1676/2330 train_time:67434ms step_avg:40.24ms
step:1677/2330 train_time:67470ms step_avg:40.23ms
step:1678/2330 train_time:67515ms step_avg:40.24ms
step:1679/2330 train_time:67551ms step_avg:40.23ms
step:1680/2330 train_time:67596ms step_avg:40.24ms
step:1681/2330 train_time:67632ms step_avg:40.23ms
step:1682/2330 train_time:67676ms step_avg:40.24ms
step:1683/2330 train_time:67711ms step_avg:40.23ms
step:1684/2330 train_time:67756ms step_avg:40.24ms
step:1685/2330 train_time:67791ms step_avg:40.23ms
step:1686/2330 train_time:67836ms step_avg:40.23ms
step:1687/2330 train_time:67871ms step_avg:40.23ms
step:1688/2330 train_time:67915ms step_avg:40.23ms
step:1689/2330 train_time:67949ms step_avg:40.23ms
step:1690/2330 train_time:67994ms step_avg:40.23ms
step:1691/2330 train_time:68029ms step_avg:40.23ms
step:1692/2330 train_time:68073ms step_avg:40.23ms
step:1693/2330 train_time:68108ms step_avg:40.23ms
step:1694/2330 train_time:68152ms step_avg:40.23ms
step:1695/2330 train_time:68188ms step_avg:40.23ms
step:1696/2330 train_time:68232ms step_avg:40.23ms
step:1697/2330 train_time:68268ms step_avg:40.23ms
step:1698/2330 train_time:68312ms step_avg:40.23ms
step:1699/2330 train_time:68347ms step_avg:40.23ms
step:1700/2330 train_time:68392ms step_avg:40.23ms
step:1701/2330 train_time:68427ms step_avg:40.23ms
step:1702/2330 train_time:68472ms step_avg:40.23ms
step:1703/2330 train_time:68508ms step_avg:40.23ms
step:1704/2330 train_time:68553ms step_avg:40.23ms
step:1705/2330 train_time:68589ms step_avg:40.23ms
step:1706/2330 train_time:68633ms step_avg:40.23ms
step:1707/2330 train_time:68669ms step_avg:40.23ms
step:1708/2330 train_time:68713ms step_avg:40.23ms
step:1709/2330 train_time:68748ms step_avg:40.23ms
step:1710/2330 train_time:68792ms step_avg:40.23ms
step:1711/2330 train_time:68828ms step_avg:40.23ms
step:1712/2330 train_time:68872ms step_avg:40.23ms
step:1713/2330 train_time:68907ms step_avg:40.23ms
step:1714/2330 train_time:68952ms step_avg:40.23ms
step:1715/2330 train_time:68987ms step_avg:40.23ms
step:1716/2330 train_time:69032ms step_avg:40.23ms
step:1717/2330 train_time:69067ms step_avg:40.23ms
step:1718/2330 train_time:69111ms step_avg:40.23ms
step:1719/2330 train_time:69146ms step_avg:40.22ms
step:1720/2330 train_time:69190ms step_avg:40.23ms
step:1721/2330 train_time:69225ms step_avg:40.22ms
step:1722/2330 train_time:69269ms step_avg:40.23ms
step:1723/2330 train_time:69304ms step_avg:40.22ms
step:1724/2330 train_time:69349ms step_avg:40.23ms
step:1725/2330 train_time:69384ms step_avg:40.22ms
step:1726/2330 train_time:69429ms step_avg:40.23ms
step:1727/2330 train_time:69465ms step_avg:40.22ms
step:1728/2330 train_time:69509ms step_avg:40.22ms
step:1729/2330 train_time:69544ms step_avg:40.22ms
step:1730/2330 train_time:69589ms step_avg:40.23ms
step:1731/2330 train_time:69625ms step_avg:40.22ms
step:1732/2330 train_time:69670ms step_avg:40.23ms
step:1733/2330 train_time:69705ms step_avg:40.22ms
step:1734/2330 train_time:69749ms step_avg:40.22ms
step:1735/2330 train_time:69785ms step_avg:40.22ms
step:1736/2330 train_time:69830ms step_avg:40.22ms
step:1737/2330 train_time:69865ms step_avg:40.22ms
step:1738/2330 train_time:69909ms step_avg:40.22ms
step:1739/2330 train_time:69945ms step_avg:40.22ms
step:1740/2330 train_time:69989ms step_avg:40.22ms
step:1741/2330 train_time:70024ms step_avg:40.22ms
step:1742/2330 train_time:70069ms step_avg:40.22ms
step:1743/2330 train_time:70104ms step_avg:40.22ms
step:1744/2330 train_time:70148ms step_avg:40.22ms
step:1745/2330 train_time:70183ms step_avg:40.22ms
step:1746/2330 train_time:70227ms step_avg:40.22ms
step:1747/2330 train_time:70262ms step_avg:40.22ms
step:1748/2330 train_time:70307ms step_avg:40.22ms
step:1749/2330 train_time:70342ms step_avg:40.22ms
step:1750/2330 train_time:70387ms step_avg:40.22ms
step:1750/2330 val_loss:5.1445 train_time:70474ms step_avg:40.27ms
step:1751/2330 train_time:70488ms step_avg:40.26ms
step:1752/2330 train_time:70500ms step_avg:40.24ms
step:1753/2330 train_time:70511ms step_avg:40.22ms
step:1754/2330 train_time:70546ms step_avg:40.22ms
step:1755/2330 train_time:70580ms step_avg:40.22ms
step:1756/2330 train_time:70623ms step_avg:40.22ms
step:1757/2330 train_time:70657ms step_avg:40.21ms
step:1758/2330 train_time:70701ms step_avg:40.22ms
step:1759/2330 train_time:70735ms step_avg:40.21ms
step:1760/2330 train_time:70779ms step_avg:40.22ms
step:1761/2330 train_time:70815ms step_avg:40.21ms
step:1762/2330 train_time:70864ms step_avg:40.22ms
step:1763/2330 train_time:70902ms step_avg:40.22ms
step:1764/2330 train_time:70947ms step_avg:40.22ms
step:1765/2330 train_time:70983ms step_avg:40.22ms
step:1766/2330 train_time:71028ms step_avg:40.22ms
step:1767/2330 train_time:71063ms step_avg:40.22ms
step:1768/2330 train_time:71107ms step_avg:40.22ms
step:1769/2330 train_time:71141ms step_avg:40.22ms
step:1770/2330 train_time:71185ms step_avg:40.22ms
step:1771/2330 train_time:71219ms step_avg:40.21ms
step:1772/2330 train_time:71263ms step_avg:40.22ms
step:1773/2330 train_time:71298ms step_avg:40.21ms
step:1774/2330 train_time:71342ms step_avg:40.22ms
step:1775/2330 train_time:71377ms step_avg:40.21ms
step:1776/2330 train_time:71426ms step_avg:40.22ms
step:1777/2330 train_time:71464ms step_avg:40.22ms
step:1778/2330 train_time:71512ms step_avg:40.22ms
step:1779/2330 train_time:71549ms step_avg:40.22ms
step:1780/2330 train_time:71593ms step_avg:40.22ms
step:1781/2330 train_time:71628ms step_avg:40.22ms
step:1782/2330 train_time:71672ms step_avg:40.22ms
step:1783/2330 train_time:71707ms step_avg:40.22ms
step:1784/2330 train_time:71752ms step_avg:40.22ms
step:1785/2330 train_time:71787ms step_avg:40.22ms
step:1786/2330 train_time:71831ms step_avg:40.22ms
step:1787/2330 train_time:71867ms step_avg:40.22ms
step:1788/2330 train_time:71911ms step_avg:40.22ms
step:1789/2330 train_time:71946ms step_avg:40.22ms
step:1790/2330 train_time:71991ms step_avg:40.22ms
step:1791/2330 train_time:72027ms step_avg:40.22ms
step:1792/2330 train_time:72071ms step_avg:40.22ms
step:1793/2330 train_time:72105ms step_avg:40.21ms
step:1794/2330 train_time:72148ms step_avg:40.22ms
step:1795/2330 train_time:72184ms step_avg:40.21ms
step:1796/2330 train_time:72228ms step_avg:40.22ms
step:1797/2330 train_time:72262ms step_avg:40.21ms
step:1798/2330 train_time:72306ms step_avg:40.21ms
step:1799/2330 train_time:72340ms step_avg:40.21ms
step:1800/2330 train_time:72385ms step_avg:40.21ms
step:1801/2330 train_time:72420ms step_avg:40.21ms
step:1802/2330 train_time:72466ms step_avg:40.21ms
step:1803/2330 train_time:72502ms step_avg:40.21ms
step:1804/2330 train_time:72547ms step_avg:40.21ms
step:1805/2330 train_time:72583ms step_avg:40.21ms
step:1806/2330 train_time:72628ms step_avg:40.21ms
step:1807/2330 train_time:72664ms step_avg:40.21ms
step:1808/2330 train_time:72708ms step_avg:40.21ms
step:1809/2330 train_time:72743ms step_avg:40.21ms
step:1810/2330 train_time:72788ms step_avg:40.21ms
step:1811/2330 train_time:72822ms step_avg:40.21ms
step:1812/2330 train_time:72866ms step_avg:40.21ms
step:1813/2330 train_time:72901ms step_avg:40.21ms
step:1814/2330 train_time:72946ms step_avg:40.21ms
step:1815/2330 train_time:72982ms step_avg:40.21ms
step:1816/2330 train_time:73026ms step_avg:40.21ms
step:1817/2330 train_time:73061ms step_avg:40.21ms
step:1818/2330 train_time:73105ms step_avg:40.21ms
step:1819/2330 train_time:73140ms step_avg:40.21ms
step:1820/2330 train_time:73184ms step_avg:40.21ms
step:1821/2330 train_time:73220ms step_avg:40.21ms
step:1822/2330 train_time:73264ms step_avg:40.21ms
step:1823/2330 train_time:73299ms step_avg:40.21ms
step:1824/2330 train_time:73343ms step_avg:40.21ms
step:1825/2330 train_time:73378ms step_avg:40.21ms
step:1826/2330 train_time:73422ms step_avg:40.21ms
step:1827/2330 train_time:73458ms step_avg:40.21ms
step:1828/2330 train_time:73503ms step_avg:40.21ms
step:1829/2330 train_time:73538ms step_avg:40.21ms
step:1830/2330 train_time:73582ms step_avg:40.21ms
step:1831/2330 train_time:73617ms step_avg:40.21ms
step:1832/2330 train_time:73663ms step_avg:40.21ms
step:1833/2330 train_time:73698ms step_avg:40.21ms
step:1834/2330 train_time:73743ms step_avg:40.21ms
step:1835/2330 train_time:73778ms step_avg:40.21ms
step:1836/2330 train_time:73823ms step_avg:40.21ms
step:1837/2330 train_time:73858ms step_avg:40.21ms
step:1838/2330 train_time:73903ms step_avg:40.21ms
step:1839/2330 train_time:73939ms step_avg:40.21ms
step:1840/2330 train_time:73983ms step_avg:40.21ms
step:1841/2330 train_time:74019ms step_avg:40.21ms
step:1842/2330 train_time:74063ms step_avg:40.21ms
step:1843/2330 train_time:74098ms step_avg:40.20ms
step:1844/2330 train_time:74142ms step_avg:40.21ms
step:1845/2330 train_time:74178ms step_avg:40.20ms
step:1846/2330 train_time:74222ms step_avg:40.21ms
step:1847/2330 train_time:74256ms step_avg:40.20ms
step:1848/2330 train_time:74301ms step_avg:40.21ms
step:1849/2330 train_time:74336ms step_avg:40.20ms
step:1850/2330 train_time:74381ms step_avg:40.21ms
step:1851/2330 train_time:74416ms step_avg:40.20ms
step:1852/2330 train_time:74460ms step_avg:40.21ms
step:1853/2330 train_time:74495ms step_avg:40.20ms
step:1854/2330 train_time:74540ms step_avg:40.20ms
step:1855/2330 train_time:74575ms step_avg:40.20ms
step:1856/2330 train_time:74620ms step_avg:40.20ms
step:1857/2330 train_time:74655ms step_avg:40.20ms
step:1858/2330 train_time:74699ms step_avg:40.20ms
step:1859/2330 train_time:74734ms step_avg:40.20ms
step:1860/2330 train_time:74779ms step_avg:40.20ms
step:1861/2330 train_time:74814ms step_avg:40.20ms
step:1862/2330 train_time:74859ms step_avg:40.20ms
step:1863/2330 train_time:74894ms step_avg:40.20ms
step:1864/2330 train_time:74938ms step_avg:40.20ms
step:1865/2330 train_time:74974ms step_avg:40.20ms
step:1866/2330 train_time:75018ms step_avg:40.20ms
step:1867/2330 train_time:75053ms step_avg:40.20ms
step:1868/2330 train_time:75097ms step_avg:40.20ms
step:1869/2330 train_time:75132ms step_avg:40.20ms
step:1870/2330 train_time:75176ms step_avg:40.20ms
step:1871/2330 train_time:75211ms step_avg:40.20ms
step:1872/2330 train_time:75255ms step_avg:40.20ms
step:1873/2330 train_time:75290ms step_avg:40.20ms
step:1874/2330 train_time:75334ms step_avg:40.20ms
step:1875/2330 train_time:75369ms step_avg:40.20ms
step:1876/2330 train_time:75414ms step_avg:40.20ms
step:1877/2330 train_time:75449ms step_avg:40.20ms
step:1878/2330 train_time:75493ms step_avg:40.20ms
step:1879/2330 train_time:75529ms step_avg:40.20ms
step:1880/2330 train_time:75574ms step_avg:40.20ms
step:1881/2330 train_time:75608ms step_avg:40.20ms
step:1882/2330 train_time:75653ms step_avg:40.20ms
step:1883/2330 train_time:75689ms step_avg:40.20ms
step:1884/2330 train_time:75734ms step_avg:40.20ms
step:1885/2330 train_time:75769ms step_avg:40.20ms
step:1886/2330 train_time:75815ms step_avg:40.20ms
step:1887/2330 train_time:75850ms step_avg:40.20ms
step:1888/2330 train_time:75894ms step_avg:40.20ms
step:1889/2330 train_time:75930ms step_avg:40.20ms
step:1890/2330 train_time:75974ms step_avg:40.20ms
step:1891/2330 train_time:76009ms step_avg:40.20ms
step:1892/2330 train_time:76054ms step_avg:40.20ms
step:1893/2330 train_time:76089ms step_avg:40.20ms
step:1894/2330 train_time:76134ms step_avg:40.20ms
step:1895/2330 train_time:76169ms step_avg:40.19ms
step:1896/2330 train_time:76214ms step_avg:40.20ms
step:1897/2330 train_time:76248ms step_avg:40.19ms
step:1898/2330 train_time:76293ms step_avg:40.20ms
step:1899/2330 train_time:76329ms step_avg:40.19ms
step:1900/2330 train_time:76373ms step_avg:40.20ms
step:1901/2330 train_time:76408ms step_avg:40.19ms
step:1902/2330 train_time:76452ms step_avg:40.20ms
step:1903/2330 train_time:76487ms step_avg:40.19ms
step:1904/2330 train_time:76532ms step_avg:40.20ms
step:1905/2330 train_time:76566ms step_avg:40.19ms
step:1906/2330 train_time:76611ms step_avg:40.19ms
step:1907/2330 train_time:76645ms step_avg:40.19ms
step:1908/2330 train_time:76690ms step_avg:40.19ms
step:1909/2330 train_time:76726ms step_avg:40.19ms
step:1910/2330 train_time:76770ms step_avg:40.19ms
step:1911/2330 train_time:76805ms step_avg:40.19ms
step:1912/2330 train_time:76850ms step_avg:40.19ms
step:1913/2330 train_time:76886ms step_avg:40.19ms
step:1914/2330 train_time:76930ms step_avg:40.19ms
step:1915/2330 train_time:76965ms step_avg:40.19ms
step:1916/2330 train_time:77010ms step_avg:40.19ms
step:1917/2330 train_time:77044ms step_avg:40.19ms
step:1918/2330 train_time:77088ms step_avg:40.19ms
step:1919/2330 train_time:77122ms step_avg:40.19ms
step:1920/2330 train_time:77167ms step_avg:40.19ms
step:1921/2330 train_time:77202ms step_avg:40.19ms
step:1922/2330 train_time:77246ms step_avg:40.19ms
step:1923/2330 train_time:77282ms step_avg:40.19ms
step:1924/2330 train_time:77326ms step_avg:40.19ms
step:1925/2330 train_time:77361ms step_avg:40.19ms
step:1926/2330 train_time:77405ms step_avg:40.19ms
step:1927/2330 train_time:77440ms step_avg:40.19ms
step:1928/2330 train_time:77485ms step_avg:40.19ms
step:1929/2330 train_time:77520ms step_avg:40.19ms
step:1930/2330 train_time:77564ms step_avg:40.19ms
step:1931/2330 train_time:77599ms step_avg:40.19ms
step:1932/2330 train_time:77644ms step_avg:40.19ms
step:1933/2330 train_time:77679ms step_avg:40.19ms
step:1934/2330 train_time:77724ms step_avg:40.19ms
step:1935/2330 train_time:77760ms step_avg:40.19ms
step:1936/2330 train_time:77804ms step_avg:40.19ms
step:1937/2330 train_time:77839ms step_avg:40.19ms
step:1938/2330 train_time:77884ms step_avg:40.19ms
step:1939/2330 train_time:77920ms step_avg:40.19ms
step:1940/2330 train_time:77964ms step_avg:40.19ms
step:1941/2330 train_time:78000ms step_avg:40.19ms
step:1942/2330 train_time:78044ms step_avg:40.19ms
step:1943/2330 train_time:78080ms step_avg:40.19ms
step:1944/2330 train_time:78124ms step_avg:40.19ms
step:1945/2330 train_time:78159ms step_avg:40.18ms
step:1946/2330 train_time:78204ms step_avg:40.19ms
step:1947/2330 train_time:78240ms step_avg:40.18ms
step:1948/2330 train_time:78285ms step_avg:40.19ms
step:1949/2330 train_time:78320ms step_avg:40.18ms
step:1950/2330 train_time:78364ms step_avg:40.19ms
step:1951/2330 train_time:78399ms step_avg:40.18ms
step:1952/2330 train_time:78444ms step_avg:40.19ms
step:1953/2330 train_time:78479ms step_avg:40.18ms
step:1954/2330 train_time:78524ms step_avg:40.19ms
step:1955/2330 train_time:78559ms step_avg:40.18ms
step:1956/2330 train_time:78603ms step_avg:40.19ms
step:1957/2330 train_time:78639ms step_avg:40.18ms
step:1958/2330 train_time:78683ms step_avg:40.19ms
step:1959/2330 train_time:78718ms step_avg:40.18ms
step:1960/2330 train_time:78763ms step_avg:40.19ms
step:1961/2330 train_time:78799ms step_avg:40.18ms
step:1962/2330 train_time:78843ms step_avg:40.19ms
step:1963/2330 train_time:78878ms step_avg:40.18ms
step:1964/2330 train_time:78923ms step_avg:40.18ms
step:1965/2330 train_time:78958ms step_avg:40.18ms
step:1966/2330 train_time:79003ms step_avg:40.18ms
step:1967/2330 train_time:79038ms step_avg:40.18ms
step:1968/2330 train_time:79082ms step_avg:40.18ms
step:1969/2330 train_time:79118ms step_avg:40.18ms
step:1970/2330 train_time:79163ms step_avg:40.18ms
step:1971/2330 train_time:79198ms step_avg:40.18ms
step:1972/2330 train_time:79243ms step_avg:40.18ms
step:1973/2330 train_time:79278ms step_avg:40.18ms
step:1974/2330 train_time:79323ms step_avg:40.18ms
step:1975/2330 train_time:79358ms step_avg:40.18ms
step:1976/2330 train_time:79403ms step_avg:40.18ms
step:1977/2330 train_time:79438ms step_avg:40.18ms
step:1978/2330 train_time:79482ms step_avg:40.18ms
step:1979/2330 train_time:79517ms step_avg:40.18ms
step:1980/2330 train_time:79561ms step_avg:40.18ms
step:1981/2330 train_time:79596ms step_avg:40.18ms
step:1982/2330 train_time:79641ms step_avg:40.18ms
step:1983/2330 train_time:79675ms step_avg:40.18ms
step:1984/2330 train_time:79720ms step_avg:40.18ms
step:1985/2330 train_time:79755ms step_avg:40.18ms
step:1986/2330 train_time:79800ms step_avg:40.18ms
step:1987/2330 train_time:79836ms step_avg:40.18ms
step:1988/2330 train_time:79881ms step_avg:40.18ms
step:1989/2330 train_time:79916ms step_avg:40.18ms
step:1990/2330 train_time:79961ms step_avg:40.18ms
step:1991/2330 train_time:79996ms step_avg:40.18ms
step:1992/2330 train_time:80041ms step_avg:40.18ms
step:1993/2330 train_time:80076ms step_avg:40.18ms
step:1994/2330 train_time:80120ms step_avg:40.18ms
step:1995/2330 train_time:80156ms step_avg:40.18ms
step:1996/2330 train_time:80200ms step_avg:40.18ms
step:1997/2330 train_time:80236ms step_avg:40.18ms
step:1998/2330 train_time:80280ms step_avg:40.18ms
step:1999/2330 train_time:80316ms step_avg:40.18ms
step:2000/2330 train_time:80360ms step_avg:40.18ms
step:2000/2330 val_loss:5.1374 train_time:80447ms step_avg:40.22ms
step:2001/2330 train_time:80460ms step_avg:40.21ms
step:2002/2330 train_time:80472ms step_avg:40.20ms
step:2003/2330 train_time:80482ms step_avg:40.18ms
step:2004/2330 train_time:80519ms step_avg:40.18ms
step:2005/2330 train_time:80554ms step_avg:40.18ms
step:2006/2330 train_time:80597ms step_avg:40.18ms
step:2007/2330 train_time:80631ms step_avg:40.17ms
step:2008/2330 train_time:80674ms step_avg:40.18ms
step:2009/2330 train_time:80709ms step_avg:40.17ms
step:2010/2330 train_time:80753ms step_avg:40.18ms
step:2011/2330 train_time:80792ms step_avg:40.17ms
step:2012/2330 train_time:80840ms step_avg:40.18ms
step:2013/2330 train_time:80877ms step_avg:40.18ms
step:2014/2330 train_time:80923ms step_avg:40.18ms
step:2015/2330 train_time:80959ms step_avg:40.18ms
step:2016/2330 train_time:81003ms step_avg:40.18ms
step:2017/2330 train_time:81039ms step_avg:40.18ms
step:2018/2330 train_time:81083ms step_avg:40.18ms
step:2019/2330 train_time:81118ms step_avg:40.18ms
step:2020/2330 train_time:81162ms step_avg:40.18ms
step:2021/2330 train_time:81197ms step_avg:40.18ms
step:2022/2330 train_time:81241ms step_avg:40.18ms
step:2023/2330 train_time:81276ms step_avg:40.18ms
step:2024/2330 train_time:81319ms step_avg:40.18ms
step:2025/2330 train_time:81354ms step_avg:40.17ms
step:2026/2330 train_time:81398ms step_avg:40.18ms
step:2027/2330 train_time:81433ms step_avg:40.17ms
step:2028/2330 train_time:81477ms step_avg:40.18ms
step:2029/2330 train_time:81512ms step_avg:40.17ms
step:2030/2330 train_time:81555ms step_avg:40.18ms
step:2031/2330 train_time:81589ms step_avg:40.17ms
step:2032/2330 train_time:81633ms step_avg:40.17ms
step:2033/2330 train_time:81667ms step_avg:40.17ms
step:2034/2330 train_time:81712ms step_avg:40.17ms
step:2035/2330 train_time:81748ms step_avg:40.17ms
step:2036/2330 train_time:81792ms step_avg:40.17ms
step:2037/2330 train_time:81828ms step_avg:40.17ms
step:2038/2330 train_time:81874ms step_avg:40.17ms
step:2039/2330 train_time:81909ms step_avg:40.17ms
step:2040/2330 train_time:81954ms step_avg:40.17ms
step:2041/2330 train_time:81989ms step_avg:40.17ms
step:2042/2330 train_time:82034ms step_avg:40.17ms
step:2043/2330 train_time:82070ms step_avg:40.17ms
step:2044/2330 train_time:82114ms step_avg:40.17ms
step:2045/2330 train_time:82149ms step_avg:40.17ms
step:2046/2330 train_time:82193ms step_avg:40.17ms
step:2047/2330 train_time:82228ms step_avg:40.17ms
step:2048/2330 train_time:82272ms step_avg:40.17ms
step:2049/2330 train_time:82307ms step_avg:40.17ms
step:2050/2330 train_time:82352ms step_avg:40.17ms
step:2051/2330 train_time:82387ms step_avg:40.17ms
step:2052/2330 train_time:82430ms step_avg:40.17ms
step:2053/2330 train_time:82465ms step_avg:40.17ms
step:2054/2330 train_time:82509ms step_avg:40.17ms
step:2055/2330 train_time:82544ms step_avg:40.17ms
step:2056/2330 train_time:82589ms step_avg:40.17ms
step:2057/2330 train_time:82623ms step_avg:40.17ms
step:2058/2330 train_time:82667ms step_avg:40.17ms
step:2059/2330 train_time:82702ms step_avg:40.17ms
step:2060/2330 train_time:82747ms step_avg:40.17ms
step:2061/2330 train_time:82783ms step_avg:40.17ms
step:2062/2330 train_time:82828ms step_avg:40.17ms
step:2063/2330 train_time:82863ms step_avg:40.17ms
step:2064/2330 train_time:82908ms step_avg:40.17ms
step:2065/2330 train_time:82943ms step_avg:40.17ms
step:2066/2330 train_time:82988ms step_avg:40.17ms
step:2067/2330 train_time:83023ms step_avg:40.17ms
step:2068/2330 train_time:83067ms step_avg:40.17ms
step:2069/2330 train_time:83103ms step_avg:40.17ms
step:2070/2330 train_time:83147ms step_avg:40.17ms
step:2071/2330 train_time:83182ms step_avg:40.17ms
step:2072/2330 train_time:83226ms step_avg:40.17ms
step:2073/2330 train_time:83261ms step_avg:40.16ms
step:2074/2330 train_time:83306ms step_avg:40.17ms
step:2075/2330 train_time:83340ms step_avg:40.16ms
step:2076/2330 train_time:83385ms step_avg:40.17ms
step:2077/2330 train_time:83419ms step_avg:40.16ms
step:2078/2330 train_time:83463ms step_avg:40.17ms
step:2079/2330 train_time:83499ms step_avg:40.16ms
step:2080/2330 train_time:83543ms step_avg:40.16ms
step:2081/2330 train_time:83577ms step_avg:40.16ms
step:2082/2330 train_time:83622ms step_avg:40.16ms
step:2083/2330 train_time:83657ms step_avg:40.16ms
step:2084/2330 train_time:83702ms step_avg:40.16ms
step:2085/2330 train_time:83737ms step_avg:40.16ms
step:2086/2330 train_time:83782ms step_avg:40.16ms
step:2087/2330 train_time:83817ms step_avg:40.16ms
step:2088/2330 train_time:83862ms step_avg:40.16ms
step:2089/2330 train_time:83899ms step_avg:40.16ms
step:2090/2330 train_time:83944ms step_avg:40.16ms
step:2091/2330 train_time:83979ms step_avg:40.16ms
step:2092/2330 train_time:84025ms step_avg:40.16ms
step:2093/2330 train_time:84060ms step_avg:40.16ms
step:2094/2330 train_time:84105ms step_avg:40.16ms
step:2095/2330 train_time:84141ms step_avg:40.16ms
step:2096/2330 train_time:84185ms step_avg:40.16ms
step:2097/2330 train_time:84220ms step_avg:40.16ms
step:2098/2330 train_time:84265ms step_avg:40.16ms
step:2099/2330 train_time:84299ms step_avg:40.16ms
step:2100/2330 train_time:84344ms step_avg:40.16ms
step:2101/2330 train_time:84379ms step_avg:40.16ms
step:2102/2330 train_time:84423ms step_avg:40.16ms
step:2103/2330 train_time:84457ms step_avg:40.16ms
step:2104/2330 train_time:84502ms step_avg:40.16ms
step:2105/2330 train_time:84537ms step_avg:40.16ms
step:2106/2330 train_time:84581ms step_avg:40.16ms
step:2107/2330 train_time:84616ms step_avg:40.16ms
step:2108/2330 train_time:84660ms step_avg:40.16ms
step:2109/2330 train_time:84696ms step_avg:40.16ms
step:2110/2330 train_time:84741ms step_avg:40.16ms
step:2111/2330 train_time:84775ms step_avg:40.16ms
step:2112/2330 train_time:84819ms step_avg:40.16ms
step:2113/2330 train_time:84854ms step_avg:40.16ms
step:2114/2330 train_time:84899ms step_avg:40.16ms
step:2115/2330 train_time:84934ms step_avg:40.16ms
step:2116/2330 train_time:84979ms step_avg:40.16ms
step:2117/2330 train_time:85013ms step_avg:40.16ms
step:2118/2330 train_time:85058ms step_avg:40.16ms
step:2119/2330 train_time:85094ms step_avg:40.16ms
step:2120/2330 train_time:85138ms step_avg:40.16ms
step:2121/2330 train_time:85173ms step_avg:40.16ms
step:2122/2330 train_time:85217ms step_avg:40.16ms
step:2123/2330 train_time:85252ms step_avg:40.16ms
step:2124/2330 train_time:85296ms step_avg:40.16ms
step:2125/2330 train_time:85331ms step_avg:40.16ms
step:2126/2330 train_time:85375ms step_avg:40.16ms
step:2127/2330 train_time:85411ms step_avg:40.16ms
step:2128/2330 train_time:85455ms step_avg:40.16ms
step:2129/2330 train_time:85490ms step_avg:40.15ms
step:2130/2330 train_time:85534ms step_avg:40.16ms
step:2131/2330 train_time:85569ms step_avg:40.15ms
step:2132/2330 train_time:85613ms step_avg:40.16ms
step:2133/2330 train_time:85648ms step_avg:40.15ms
step:2134/2330 train_time:85692ms step_avg:40.16ms
step:2135/2330 train_time:85728ms step_avg:40.15ms
step:2136/2330 train_time:85772ms step_avg:40.16ms
step:2137/2330 train_time:85808ms step_avg:40.15ms
step:2138/2330 train_time:85852ms step_avg:40.16ms
step:2139/2330 train_time:85887ms step_avg:40.15ms
step:2140/2330 train_time:85932ms step_avg:40.16ms
step:2141/2330 train_time:85968ms step_avg:40.15ms
step:2142/2330 train_time:86012ms step_avg:40.16ms
step:2143/2330 train_time:86047ms step_avg:40.15ms
step:2144/2330 train_time:86091ms step_avg:40.15ms
step:2145/2330 train_time:86127ms step_avg:40.15ms
step:2146/2330 train_time:86172ms step_avg:40.15ms
step:2147/2330 train_time:86207ms step_avg:40.15ms
step:2148/2330 train_time:86251ms step_avg:40.15ms
step:2149/2330 train_time:86286ms step_avg:40.15ms
step:2150/2330 train_time:86330ms step_avg:40.15ms
step:2151/2330 train_time:86365ms step_avg:40.15ms
step:2152/2330 train_time:86410ms step_avg:40.15ms
step:2153/2330 train_time:86445ms step_avg:40.15ms
step:2154/2330 train_time:86489ms step_avg:40.15ms
step:2155/2330 train_time:86524ms step_avg:40.15ms
step:2156/2330 train_time:86569ms step_avg:40.15ms
step:2157/2330 train_time:86604ms step_avg:40.15ms
step:2158/2330 train_time:86648ms step_avg:40.15ms
step:2159/2330 train_time:86683ms step_avg:40.15ms
step:2160/2330 train_time:86728ms step_avg:40.15ms
step:2161/2330 train_time:86764ms step_avg:40.15ms
step:2162/2330 train_time:86809ms step_avg:40.15ms
step:2163/2330 train_time:86843ms step_avg:40.15ms
step:2164/2330 train_time:86887ms step_avg:40.15ms
step:2165/2330 train_time:86923ms step_avg:40.15ms
step:2166/2330 train_time:86968ms step_avg:40.15ms
step:2167/2330 train_time:87003ms step_avg:40.15ms
step:2168/2330 train_time:87047ms step_avg:40.15ms
step:2169/2330 train_time:87083ms step_avg:40.15ms
step:2170/2330 train_time:87127ms step_avg:40.15ms
step:2171/2330 train_time:87163ms step_avg:40.15ms
step:2172/2330 train_time:87207ms step_avg:40.15ms
step:2173/2330 train_time:87242ms step_avg:40.15ms
step:2174/2330 train_time:87286ms step_avg:40.15ms
step:2175/2330 train_time:87321ms step_avg:40.15ms
step:2176/2330 train_time:87365ms step_avg:40.15ms
step:2177/2330 train_time:87399ms step_avg:40.15ms
step:2178/2330 train_time:87444ms step_avg:40.15ms
step:2179/2330 train_time:87480ms step_avg:40.15ms
step:2180/2330 train_time:87524ms step_avg:40.15ms
step:2181/2330 train_time:87560ms step_avg:40.15ms
step:2182/2330 train_time:87604ms step_avg:40.15ms
step:2183/2330 train_time:87638ms step_avg:40.15ms
step:2184/2330 train_time:87684ms step_avg:40.15ms
step:2185/2330 train_time:87718ms step_avg:40.15ms
step:2186/2330 train_time:87763ms step_avg:40.15ms
step:2187/2330 train_time:87799ms step_avg:40.15ms
step:2188/2330 train_time:87843ms step_avg:40.15ms
step:2189/2330 train_time:87878ms step_avg:40.15ms
step:2190/2330 train_time:87923ms step_avg:40.15ms
step:2191/2330 train_time:87958ms step_avg:40.15ms
step:2192/2330 train_time:88003ms step_avg:40.15ms
step:2193/2330 train_time:88038ms step_avg:40.15ms
step:2194/2330 train_time:88083ms step_avg:40.15ms
step:2195/2330 train_time:88118ms step_avg:40.15ms
step:2196/2330 train_time:88163ms step_avg:40.15ms
step:2197/2330 train_time:88199ms step_avg:40.15ms
step:2198/2330 train_time:88243ms step_avg:40.15ms
step:2199/2330 train_time:88278ms step_avg:40.14ms
step:2200/2330 train_time:88322ms step_avg:40.15ms
step:2201/2330 train_time:88357ms step_avg:40.14ms
step:2202/2330 train_time:88402ms step_avg:40.15ms
step:2203/2330 train_time:88436ms step_avg:40.14ms
step:2204/2330 train_time:88480ms step_avg:40.15ms
step:2205/2330 train_time:88515ms step_avg:40.14ms
step:2206/2330 train_time:88559ms step_avg:40.14ms
step:2207/2330 train_time:88594ms step_avg:40.14ms
step:2208/2330 train_time:88639ms step_avg:40.14ms
step:2209/2330 train_time:88674ms step_avg:40.14ms
step:2210/2330 train_time:88718ms step_avg:40.14ms
step:2211/2330 train_time:88752ms step_avg:40.14ms
step:2212/2330 train_time:88797ms step_avg:40.14ms
step:2213/2330 train_time:88833ms step_avg:40.14ms
step:2214/2330 train_time:88877ms step_avg:40.14ms
step:2215/2330 train_time:88911ms step_avg:40.14ms
step:2216/2330 train_time:88956ms step_avg:40.14ms
step:2217/2330 train_time:88991ms step_avg:40.14ms
step:2218/2330 train_time:89035ms step_avg:40.14ms
step:2219/2330 train_time:89071ms step_avg:40.14ms
step:2220/2330 train_time:89115ms step_avg:40.14ms
step:2221/2330 train_time:89150ms step_avg:40.14ms
step:2222/2330 train_time:89195ms step_avg:40.14ms
step:2223/2330 train_time:89229ms step_avg:40.14ms
step:2224/2330 train_time:89274ms step_avg:40.14ms
step:2225/2330 train_time:89309ms step_avg:40.14ms
step:2226/2330 train_time:89353ms step_avg:40.14ms
step:2227/2330 train_time:89389ms step_avg:40.14ms
step:2228/2330 train_time:89433ms step_avg:40.14ms
step:2229/2330 train_time:89468ms step_avg:40.14ms
step:2230/2330 train_time:89512ms step_avg:40.14ms
step:2231/2330 train_time:89547ms step_avg:40.14ms
step:2232/2330 train_time:89591ms step_avg:40.14ms
step:2233/2330 train_time:89626ms step_avg:40.14ms
step:2234/2330 train_time:89671ms step_avg:40.14ms
step:2235/2330 train_time:89707ms step_avg:40.14ms
step:2236/2330 train_time:89751ms step_avg:40.14ms
step:2237/2330 train_time:89786ms step_avg:40.14ms
step:2238/2330 train_time:89830ms step_avg:40.14ms
step:2239/2330 train_time:89865ms step_avg:40.14ms
step:2240/2330 train_time:89910ms step_avg:40.14ms
step:2241/2330 train_time:89946ms step_avg:40.14ms
step:2242/2330 train_time:89990ms step_avg:40.14ms
step:2243/2330 train_time:90025ms step_avg:40.14ms
step:2244/2330 train_time:90069ms step_avg:40.14ms
step:2245/2330 train_time:90105ms step_avg:40.14ms
step:2246/2330 train_time:90149ms step_avg:40.14ms
step:2247/2330 train_time:90185ms step_avg:40.14ms
step:2248/2330 train_time:90228ms step_avg:40.14ms
step:2249/2330 train_time:90263ms step_avg:40.13ms
step:2250/2330 train_time:90308ms step_avg:40.14ms
step:2250/2330 val_loss:5.1265 train_time:90395ms step_avg:40.18ms
step:2251/2330 train_time:90409ms step_avg:40.16ms
step:2252/2330 train_time:90422ms step_avg:40.15ms
step:2253/2330 train_time:90433ms step_avg:40.14ms
step:2254/2330 train_time:90469ms step_avg:40.14ms
step:2255/2330 train_time:90503ms step_avg:40.13ms
step:2256/2330 train_time:90546ms step_avg:40.14ms
step:2257/2330 train_time:90580ms step_avg:40.13ms
step:2258/2330 train_time:90624ms step_avg:40.13ms
step:2259/2330 train_time:90658ms step_avg:40.13ms
step:2260/2330 train_time:90703ms step_avg:40.13ms
step:2261/2330 train_time:90742ms step_avg:40.13ms
step:2262/2330 train_time:90789ms step_avg:40.14ms
step:2263/2330 train_time:90826ms step_avg:40.14ms
step:2264/2330 train_time:90871ms step_avg:40.14ms
step:2265/2330 train_time:90906ms step_avg:40.14ms
step:2266/2330 train_time:90950ms step_avg:40.14ms
step:2267/2330 train_time:90985ms step_avg:40.13ms
step:2268/2330 train_time:91029ms step_avg:40.14ms
step:2269/2330 train_time:91064ms step_avg:40.13ms
step:2270/2330 train_time:91109ms step_avg:40.14ms
step:2271/2330 train_time:91144ms step_avg:40.13ms
step:2272/2330 train_time:91187ms step_avg:40.14ms
step:2273/2330 train_time:91222ms step_avg:40.13ms
step:2274/2330 train_time:91266ms step_avg:40.13ms
step:2275/2330 train_time:91302ms step_avg:40.13ms
step:2276/2330 train_time:91348ms step_avg:40.14ms
step:2277/2330 train_time:91383ms step_avg:40.13ms
step:2278/2330 train_time:91427ms step_avg:40.13ms
step:2279/2330 train_time:91462ms step_avg:40.13ms
step:2280/2330 train_time:91506ms step_avg:40.13ms
step:2281/2330 train_time:91540ms step_avg:40.13ms
step:2282/2330 train_time:91584ms step_avg:40.13ms
step:2283/2330 train_time:91619ms step_avg:40.13ms
step:2284/2330 train_time:91663ms step_avg:40.13ms
step:2285/2330 train_time:91699ms step_avg:40.13ms
step:2286/2330 train_time:91745ms step_avg:40.13ms
step:2287/2330 train_time:91780ms step_avg:40.13ms
step:2288/2330 train_time:91825ms step_avg:40.13ms
step:2289/2330 train_time:91861ms step_avg:40.13ms
step:2290/2330 train_time:91906ms step_avg:40.13ms
step:2291/2330 train_time:91941ms step_avg:40.13ms
step:2292/2330 train_time:91985ms step_avg:40.13ms
step:2293/2330 train_time:92020ms step_avg:40.13ms
step:2294/2330 train_time:92064ms step_avg:40.13ms
step:2295/2330 train_time:92099ms step_avg:40.13ms
step:2296/2330 train_time:92143ms step_avg:40.13ms
step:2297/2330 train_time:92178ms step_avg:40.13ms
step:2298/2330 train_time:92222ms step_avg:40.13ms
step:2299/2330 train_time:92257ms step_avg:40.13ms
step:2300/2330 train_time:92301ms step_avg:40.13ms
step:2301/2330 train_time:92336ms step_avg:40.13ms
step:2302/2330 train_time:92380ms step_avg:40.13ms
step:2303/2330 train_time:92415ms step_avg:40.13ms
step:2304/2330 train_time:92459ms step_avg:40.13ms
step:2305/2330 train_time:92494ms step_avg:40.13ms
step:2306/2330 train_time:92538ms step_avg:40.13ms
step:2307/2330 train_time:92573ms step_avg:40.13ms
step:2308/2330 train_time:92618ms step_avg:40.13ms
step:2309/2330 train_time:92654ms step_avg:40.13ms
step:2310/2330 train_time:92699ms step_avg:40.13ms
step:2311/2330 train_time:92734ms step_avg:40.13ms
step:2312/2330 train_time:92779ms step_avg:40.13ms
step:2313/2330 train_time:92814ms step_avg:40.13ms
step:2314/2330 train_time:92859ms step_avg:40.13ms
step:2315/2330 train_time:92895ms step_avg:40.13ms
step:2316/2330 train_time:92939ms step_avg:40.13ms
step:2317/2330 train_time:92974ms step_avg:40.13ms
step:2318/2330 train_time:93019ms step_avg:40.13ms
step:2319/2330 train_time:93054ms step_avg:40.13ms
step:2320/2330 train_time:93098ms step_avg:40.13ms
step:2321/2330 train_time:93134ms step_avg:40.13ms
step:2322/2330 train_time:93178ms step_avg:40.13ms
step:2323/2330 train_time:93213ms step_avg:40.13ms
step:2324/2330 train_time:93257ms step_avg:40.13ms
step:2325/2330 train_time:93292ms step_avg:40.13ms
step:2326/2330 train_time:93336ms step_avg:40.13ms
step:2327/2330 train_time:93371ms step_avg:40.13ms
step:2328/2330 train_time:93416ms step_avg:40.13ms
step:2329/2330 train_time:93451ms step_avg:40.12ms
step:2330/2330 train_time:93495ms step_avg:40.13ms
step:2330/2330 val_loss:5.1188 train_time:93582ms step_avg:40.16ms
peak memory allocated: 29494 MiB reserved: 38888 MiB
