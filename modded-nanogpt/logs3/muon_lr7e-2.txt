import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                # v_chunk = divide_by_norm(batched)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                # v_chunk = divide_by_norm(batched_update_grads)
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "muon_lr7e-2"
    os.makedirs("logs3", exist_ok=True)
    logfile = f"logs3/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
# embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=7e-2, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    return 1.0

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum_min + 0.5 * (momentum_max - momentum_min)

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs3/{run_id}", exist_ok=True)
            torch.save(log, f"logs3/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 07:32:46 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   33C    P0             112W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   31C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   28C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   31C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   32C    P0             115W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:81ms step_avg:80.65ms
step:2/2330 train_time:205ms step_avg:102.44ms
step:3/2330 train_time:228ms step_avg:75.95ms
step:4/2330 train_time:262ms step_avg:65.50ms
step:5/2330 train_time:319ms step_avg:63.86ms
step:6/2330 train_time:381ms step_avg:63.43ms
step:7/2330 train_time:439ms step_avg:62.75ms
step:8/2330 train_time:501ms step_avg:62.64ms
step:9/2330 train_time:560ms step_avg:62.21ms
step:10/2330 train_time:622ms step_avg:62.20ms
step:11/2330 train_time:680ms step_avg:61.83ms
step:12/2330 train_time:742ms step_avg:61.82ms
step:13/2330 train_time:801ms step_avg:61.60ms
step:14/2330 train_time:863ms step_avg:61.63ms
step:15/2330 train_time:921ms step_avg:61.42ms
step:16/2330 train_time:983ms step_avg:61.46ms
step:17/2330 train_time:1043ms step_avg:61.34ms
step:18/2330 train_time:1107ms step_avg:61.52ms
step:19/2330 train_time:1171ms step_avg:61.61ms
step:20/2330 train_time:1234ms step_avg:61.70ms
step:21/2330 train_time:1294ms step_avg:61.63ms
step:22/2330 train_time:1357ms step_avg:61.67ms
step:23/2330 train_time:1416ms step_avg:61.57ms
step:24/2330 train_time:1478ms step_avg:61.59ms
step:25/2330 train_time:1538ms step_avg:61.50ms
step:26/2330 train_time:1600ms step_avg:61.52ms
step:27/2330 train_time:1658ms step_avg:61.40ms
step:28/2330 train_time:1719ms step_avg:61.41ms
step:29/2330 train_time:1779ms step_avg:61.33ms
step:30/2330 train_time:1841ms step_avg:61.35ms
step:31/2330 train_time:1900ms step_avg:61.28ms
step:32/2330 train_time:1962ms step_avg:61.32ms
step:33/2330 train_time:2022ms step_avg:61.27ms
step:34/2330 train_time:2086ms step_avg:61.34ms
step:35/2330 train_time:2146ms step_avg:61.31ms
step:36/2330 train_time:2209ms step_avg:61.36ms
step:37/2330 train_time:2269ms step_avg:61.33ms
step:38/2330 train_time:2331ms step_avg:61.35ms
step:39/2330 train_time:2391ms step_avg:61.31ms
step:40/2330 train_time:2453ms step_avg:61.34ms
step:41/2330 train_time:2513ms step_avg:61.29ms
step:42/2330 train_time:2575ms step_avg:61.31ms
step:43/2330 train_time:2634ms step_avg:61.25ms
step:44/2330 train_time:2695ms step_avg:61.26ms
step:45/2330 train_time:2755ms step_avg:61.23ms
step:46/2330 train_time:2818ms step_avg:61.25ms
step:47/2330 train_time:2877ms step_avg:61.21ms
step:48/2330 train_time:2939ms step_avg:61.23ms
step:49/2330 train_time:2998ms step_avg:61.19ms
step:50/2330 train_time:3061ms step_avg:61.23ms
step:51/2330 train_time:3121ms step_avg:61.20ms
step:52/2330 train_time:3185ms step_avg:61.25ms
step:53/2330 train_time:3245ms step_avg:61.22ms
step:54/2330 train_time:3307ms step_avg:61.25ms
step:55/2330 train_time:3366ms step_avg:61.21ms
step:56/2330 train_time:3429ms step_avg:61.22ms
step:57/2330 train_time:3488ms step_avg:61.19ms
step:58/2330 train_time:3549ms step_avg:61.20ms
step:59/2330 train_time:3610ms step_avg:61.18ms
step:60/2330 train_time:3672ms step_avg:61.20ms
step:61/2330 train_time:3731ms step_avg:61.17ms
step:62/2330 train_time:3793ms step_avg:61.18ms
step:63/2330 train_time:3852ms step_avg:61.15ms
step:64/2330 train_time:3916ms step_avg:61.19ms
step:65/2330 train_time:3976ms step_avg:61.17ms
step:66/2330 train_time:4038ms step_avg:61.19ms
step:67/2330 train_time:4098ms step_avg:61.16ms
step:68/2330 train_time:4159ms step_avg:61.17ms
step:69/2330 train_time:4218ms step_avg:61.13ms
step:70/2330 train_time:4280ms step_avg:61.15ms
step:71/2330 train_time:4339ms step_avg:61.12ms
step:72/2330 train_time:4403ms step_avg:61.15ms
step:73/2330 train_time:4462ms step_avg:61.13ms
step:74/2330 train_time:4524ms step_avg:61.14ms
step:75/2330 train_time:4583ms step_avg:61.11ms
step:76/2330 train_time:4645ms step_avg:61.12ms
step:77/2330 train_time:4704ms step_avg:61.10ms
step:78/2330 train_time:4766ms step_avg:61.11ms
step:79/2330 train_time:4825ms step_avg:61.08ms
step:80/2330 train_time:4888ms step_avg:61.10ms
step:81/2330 train_time:4947ms step_avg:61.07ms
step:82/2330 train_time:5009ms step_avg:61.09ms
step:83/2330 train_time:5069ms step_avg:61.08ms
step:84/2330 train_time:5131ms step_avg:61.09ms
step:85/2330 train_time:5192ms step_avg:61.08ms
step:86/2330 train_time:5254ms step_avg:61.09ms
step:87/2330 train_time:5315ms step_avg:61.09ms
step:88/2330 train_time:5377ms step_avg:61.10ms
step:89/2330 train_time:5437ms step_avg:61.09ms
step:90/2330 train_time:5499ms step_avg:61.10ms
step:91/2330 train_time:5558ms step_avg:61.07ms
step:92/2330 train_time:5620ms step_avg:61.08ms
step:93/2330 train_time:5679ms step_avg:61.06ms
step:94/2330 train_time:5741ms step_avg:61.08ms
step:95/2330 train_time:5800ms step_avg:61.06ms
step:96/2330 train_time:5863ms step_avg:61.08ms
step:97/2330 train_time:5923ms step_avg:61.06ms
step:98/2330 train_time:5985ms step_avg:61.07ms
step:99/2330 train_time:6043ms step_avg:61.04ms
step:100/2330 train_time:6106ms step_avg:61.06ms
step:101/2330 train_time:6165ms step_avg:61.04ms
step:102/2330 train_time:6227ms step_avg:61.04ms
step:103/2330 train_time:6286ms step_avg:61.02ms
step:104/2330 train_time:6347ms step_avg:61.03ms
step:105/2330 train_time:6407ms step_avg:61.01ms
step:106/2330 train_time:6469ms step_avg:61.03ms
step:107/2330 train_time:6528ms step_avg:61.01ms
step:108/2330 train_time:6591ms step_avg:61.02ms
step:109/2330 train_time:6650ms step_avg:61.01ms
step:110/2330 train_time:6712ms step_avg:61.02ms
step:111/2330 train_time:6771ms step_avg:61.00ms
step:112/2330 train_time:6833ms step_avg:61.01ms
step:113/2330 train_time:6893ms step_avg:61.00ms
step:114/2330 train_time:6955ms step_avg:61.01ms
step:115/2330 train_time:7015ms step_avg:61.00ms
step:116/2330 train_time:7078ms step_avg:61.01ms
step:117/2330 train_time:7137ms step_avg:61.00ms
step:118/2330 train_time:7199ms step_avg:61.01ms
step:119/2330 train_time:7258ms step_avg:60.99ms
step:120/2330 train_time:7320ms step_avg:61.00ms
step:121/2330 train_time:7379ms step_avg:60.98ms
step:122/2330 train_time:7440ms step_avg:60.99ms
step:123/2330 train_time:7499ms step_avg:60.97ms
step:124/2330 train_time:7562ms step_avg:60.98ms
step:125/2330 train_time:7621ms step_avg:60.97ms
step:126/2330 train_time:7683ms step_avg:60.98ms
step:127/2330 train_time:7743ms step_avg:60.96ms
step:128/2330 train_time:7804ms step_avg:60.97ms
step:129/2330 train_time:7863ms step_avg:60.96ms
step:130/2330 train_time:7925ms step_avg:60.96ms
step:131/2330 train_time:7984ms step_avg:60.95ms
step:132/2330 train_time:8046ms step_avg:60.96ms
step:133/2330 train_time:8105ms step_avg:60.94ms
step:134/2330 train_time:8168ms step_avg:60.95ms
step:135/2330 train_time:8227ms step_avg:60.94ms
step:136/2330 train_time:8288ms step_avg:60.94ms
step:137/2330 train_time:8347ms step_avg:60.93ms
step:138/2330 train_time:8409ms step_avg:60.94ms
step:139/2330 train_time:8469ms step_avg:60.93ms
step:140/2330 train_time:8531ms step_avg:60.94ms
step:141/2330 train_time:8590ms step_avg:60.92ms
step:142/2330 train_time:8652ms step_avg:60.93ms
step:143/2330 train_time:8712ms step_avg:60.92ms
step:144/2330 train_time:8775ms step_avg:60.94ms
step:145/2330 train_time:8834ms step_avg:60.93ms
step:146/2330 train_time:8896ms step_avg:60.93ms
step:147/2330 train_time:8956ms step_avg:60.92ms
step:148/2330 train_time:9018ms step_avg:60.93ms
step:149/2330 train_time:9077ms step_avg:60.92ms
step:150/2330 train_time:9138ms step_avg:60.92ms
step:151/2330 train_time:9197ms step_avg:60.91ms
step:152/2330 train_time:9260ms step_avg:60.92ms
step:153/2330 train_time:9319ms step_avg:60.91ms
step:154/2330 train_time:9381ms step_avg:60.92ms
step:155/2330 train_time:9441ms step_avg:60.91ms
step:156/2330 train_time:9504ms step_avg:60.92ms
step:157/2330 train_time:9564ms step_avg:60.91ms
step:158/2330 train_time:9626ms step_avg:60.92ms
step:159/2330 train_time:9685ms step_avg:60.91ms
step:160/2330 train_time:9747ms step_avg:60.92ms
step:161/2330 train_time:9806ms step_avg:60.91ms
step:162/2330 train_time:9868ms step_avg:60.92ms
step:163/2330 train_time:9927ms step_avg:60.90ms
step:164/2330 train_time:9989ms step_avg:60.91ms
step:165/2330 train_time:10048ms step_avg:60.90ms
step:166/2330 train_time:10110ms step_avg:60.91ms
step:167/2330 train_time:10169ms step_avg:60.89ms
step:168/2330 train_time:10231ms step_avg:60.90ms
step:169/2330 train_time:10290ms step_avg:60.89ms
step:170/2330 train_time:10352ms step_avg:60.90ms
step:171/2330 train_time:10411ms step_avg:60.89ms
step:172/2330 train_time:10474ms step_avg:60.89ms
step:173/2330 train_time:10532ms step_avg:60.88ms
step:174/2330 train_time:10595ms step_avg:60.89ms
step:175/2330 train_time:10654ms step_avg:60.88ms
step:176/2330 train_time:10717ms step_avg:60.89ms
step:177/2330 train_time:10777ms step_avg:60.89ms
step:178/2330 train_time:10839ms step_avg:60.89ms
step:179/2330 train_time:10897ms step_avg:60.88ms
step:180/2330 train_time:10959ms step_avg:60.88ms
step:181/2330 train_time:11018ms step_avg:60.87ms
step:182/2330 train_time:11080ms step_avg:60.88ms
step:183/2330 train_time:11139ms step_avg:60.87ms
step:184/2330 train_time:11201ms step_avg:60.88ms
step:185/2330 train_time:11260ms step_avg:60.86ms
step:186/2330 train_time:11323ms step_avg:60.87ms
step:187/2330 train_time:11382ms step_avg:60.87ms
step:188/2330 train_time:11444ms step_avg:60.87ms
step:189/2330 train_time:11503ms step_avg:60.86ms
step:190/2330 train_time:11566ms step_avg:60.87ms
step:191/2330 train_time:11625ms step_avg:60.86ms
step:192/2330 train_time:11687ms step_avg:60.87ms
step:193/2330 train_time:11746ms step_avg:60.86ms
step:194/2330 train_time:11808ms step_avg:60.87ms
step:195/2330 train_time:11867ms step_avg:60.86ms
step:196/2330 train_time:11929ms step_avg:60.86ms
step:197/2330 train_time:11988ms step_avg:60.85ms
step:198/2330 train_time:12050ms step_avg:60.86ms
step:199/2330 train_time:12109ms step_avg:60.85ms
step:200/2330 train_time:12171ms step_avg:60.86ms
step:201/2330 train_time:12230ms step_avg:60.85ms
step:202/2330 train_time:12293ms step_avg:60.85ms
step:203/2330 train_time:12351ms step_avg:60.84ms
step:204/2330 train_time:12414ms step_avg:60.85ms
step:205/2330 train_time:12473ms step_avg:60.84ms
step:206/2330 train_time:12536ms step_avg:60.85ms
step:207/2330 train_time:12595ms step_avg:60.84ms
step:208/2330 train_time:12657ms step_avg:60.85ms
step:209/2330 train_time:12716ms step_avg:60.84ms
step:210/2330 train_time:12778ms step_avg:60.85ms
step:211/2330 train_time:12837ms step_avg:60.84ms
step:212/2330 train_time:12899ms step_avg:60.84ms
step:213/2330 train_time:12958ms step_avg:60.83ms
step:214/2330 train_time:13020ms step_avg:60.84ms
step:215/2330 train_time:13080ms step_avg:60.83ms
step:216/2330 train_time:13142ms step_avg:60.84ms
step:217/2330 train_time:13200ms step_avg:60.83ms
step:218/2330 train_time:13263ms step_avg:60.84ms
step:219/2330 train_time:13323ms step_avg:60.83ms
step:220/2330 train_time:13385ms step_avg:60.84ms
step:221/2330 train_time:13443ms step_avg:60.83ms
step:222/2330 train_time:13505ms step_avg:60.83ms
step:223/2330 train_time:13564ms step_avg:60.83ms
step:224/2330 train_time:13626ms step_avg:60.83ms
step:225/2330 train_time:13685ms step_avg:60.82ms
step:226/2330 train_time:13747ms step_avg:60.83ms
step:227/2330 train_time:13806ms step_avg:60.82ms
step:228/2330 train_time:13869ms step_avg:60.83ms
step:229/2330 train_time:13928ms step_avg:60.82ms
step:230/2330 train_time:13990ms step_avg:60.83ms
step:231/2330 train_time:14049ms step_avg:60.82ms
step:232/2330 train_time:14111ms step_avg:60.82ms
step:233/2330 train_time:14171ms step_avg:60.82ms
step:234/2330 train_time:14233ms step_avg:60.82ms
step:235/2330 train_time:14291ms step_avg:60.81ms
step:236/2330 train_time:14354ms step_avg:60.82ms
step:237/2330 train_time:14414ms step_avg:60.82ms
step:238/2330 train_time:14476ms step_avg:60.82ms
step:239/2330 train_time:14535ms step_avg:60.82ms
step:240/2330 train_time:14597ms step_avg:60.82ms
step:241/2330 train_time:14656ms step_avg:60.81ms
step:242/2330 train_time:14718ms step_avg:60.82ms
step:243/2330 train_time:14777ms step_avg:60.81ms
step:244/2330 train_time:14839ms step_avg:60.82ms
step:245/2330 train_time:14898ms step_avg:60.81ms
step:246/2330 train_time:14961ms step_avg:60.82ms
step:247/2330 train_time:15020ms step_avg:60.81ms
step:248/2330 train_time:15082ms step_avg:60.81ms
step:249/2330 train_time:15142ms step_avg:60.81ms
step:250/2330 train_time:15204ms step_avg:60.82ms
step:250/2330 val_loss:4.1163 train_time:15268ms step_avg:61.07ms
step:251/2330 train_time:15294ms step_avg:60.93ms
step:252/2330 train_time:15328ms step_avg:60.83ms
step:253/2330 train_time:15393ms step_avg:60.84ms
step:254/2330 train_time:15459ms step_avg:60.86ms
step:255/2330 train_time:15518ms step_avg:60.85ms
step:256/2330 train_time:15580ms step_avg:60.86ms
step:257/2330 train_time:15639ms step_avg:60.85ms
step:258/2330 train_time:15700ms step_avg:60.85ms
step:259/2330 train_time:15758ms step_avg:60.84ms
step:260/2330 train_time:15819ms step_avg:60.84ms
step:261/2330 train_time:15878ms step_avg:60.83ms
step:262/2330 train_time:15939ms step_avg:60.84ms
step:263/2330 train_time:15998ms step_avg:60.83ms
step:264/2330 train_time:16059ms step_avg:60.83ms
step:265/2330 train_time:16117ms step_avg:60.82ms
step:266/2330 train_time:16179ms step_avg:60.82ms
step:267/2330 train_time:16238ms step_avg:60.82ms
step:268/2330 train_time:16301ms step_avg:60.83ms
step:269/2330 train_time:16363ms step_avg:60.83ms
step:270/2330 train_time:16426ms step_avg:60.84ms
step:271/2330 train_time:16485ms step_avg:60.83ms
step:272/2330 train_time:16548ms step_avg:60.84ms
step:273/2330 train_time:16607ms step_avg:60.83ms
step:274/2330 train_time:16669ms step_avg:60.84ms
step:275/2330 train_time:16728ms step_avg:60.83ms
step:276/2330 train_time:16790ms step_avg:60.83ms
step:277/2330 train_time:16848ms step_avg:60.82ms
step:278/2330 train_time:16911ms step_avg:60.83ms
step:279/2330 train_time:16970ms step_avg:60.82ms
step:280/2330 train_time:17032ms step_avg:60.83ms
step:281/2330 train_time:17091ms step_avg:60.82ms
step:282/2330 train_time:17152ms step_avg:60.82ms
step:283/2330 train_time:17211ms step_avg:60.82ms
step:284/2330 train_time:17273ms step_avg:60.82ms
step:285/2330 train_time:17333ms step_avg:60.82ms
step:286/2330 train_time:17396ms step_avg:60.82ms
step:287/2330 train_time:17455ms step_avg:60.82ms
step:288/2330 train_time:17518ms step_avg:60.82ms
step:289/2330 train_time:17577ms step_avg:60.82ms
step:290/2330 train_time:17640ms step_avg:60.83ms
step:291/2330 train_time:17699ms step_avg:60.82ms
step:292/2330 train_time:17762ms step_avg:60.83ms
step:293/2330 train_time:17821ms step_avg:60.82ms
step:294/2330 train_time:17882ms step_avg:60.82ms
step:295/2330 train_time:17942ms step_avg:60.82ms
step:296/2330 train_time:18003ms step_avg:60.82ms
step:297/2330 train_time:18062ms step_avg:60.81ms
step:298/2330 train_time:18124ms step_avg:60.82ms
step:299/2330 train_time:18183ms step_avg:60.81ms
step:300/2330 train_time:18245ms step_avg:60.82ms
step:301/2330 train_time:18304ms step_avg:60.81ms
step:302/2330 train_time:18367ms step_avg:60.82ms
step:303/2330 train_time:18426ms step_avg:60.81ms
step:304/2330 train_time:18488ms step_avg:60.82ms
step:305/2330 train_time:18548ms step_avg:60.81ms
step:306/2330 train_time:18611ms step_avg:60.82ms
step:307/2330 train_time:18670ms step_avg:60.82ms
step:308/2330 train_time:18733ms step_avg:60.82ms
step:309/2330 train_time:18792ms step_avg:60.81ms
step:310/2330 train_time:18854ms step_avg:60.82ms
step:311/2330 train_time:18913ms step_avg:60.81ms
step:312/2330 train_time:18975ms step_avg:60.82ms
step:313/2330 train_time:19034ms step_avg:60.81ms
step:314/2330 train_time:19095ms step_avg:60.81ms
step:315/2330 train_time:19154ms step_avg:60.81ms
step:316/2330 train_time:19216ms step_avg:60.81ms
step:317/2330 train_time:19276ms step_avg:60.81ms
step:318/2330 train_time:19339ms step_avg:60.82ms
step:319/2330 train_time:19398ms step_avg:60.81ms
step:320/2330 train_time:19460ms step_avg:60.81ms
step:321/2330 train_time:19519ms step_avg:60.81ms
step:322/2330 train_time:19582ms step_avg:60.81ms
step:323/2330 train_time:19641ms step_avg:60.81ms
step:324/2330 train_time:19703ms step_avg:60.81ms
step:325/2330 train_time:19762ms step_avg:60.81ms
step:326/2330 train_time:19824ms step_avg:60.81ms
step:327/2330 train_time:19883ms step_avg:60.80ms
step:328/2330 train_time:19945ms step_avg:60.81ms
step:329/2330 train_time:20004ms step_avg:60.80ms
step:330/2330 train_time:20066ms step_avg:60.81ms
step:331/2330 train_time:20126ms step_avg:60.80ms
step:332/2330 train_time:20188ms step_avg:60.81ms
step:333/2330 train_time:20247ms step_avg:60.80ms
step:334/2330 train_time:20310ms step_avg:60.81ms
step:335/2330 train_time:20369ms step_avg:60.80ms
step:336/2330 train_time:20431ms step_avg:60.81ms
step:337/2330 train_time:20490ms step_avg:60.80ms
step:338/2330 train_time:20552ms step_avg:60.81ms
step:339/2330 train_time:20612ms step_avg:60.80ms
step:340/2330 train_time:20674ms step_avg:60.81ms
step:341/2330 train_time:20733ms step_avg:60.80ms
step:342/2330 train_time:20796ms step_avg:60.81ms
step:343/2330 train_time:20855ms step_avg:60.80ms
step:344/2330 train_time:20917ms step_avg:60.81ms
step:345/2330 train_time:20976ms step_avg:60.80ms
step:346/2330 train_time:21038ms step_avg:60.80ms
step:347/2330 train_time:21097ms step_avg:60.80ms
step:348/2330 train_time:21160ms step_avg:60.80ms
step:349/2330 train_time:21219ms step_avg:60.80ms
step:350/2330 train_time:21282ms step_avg:60.81ms
step:351/2330 train_time:21341ms step_avg:60.80ms
step:352/2330 train_time:21403ms step_avg:60.80ms
step:353/2330 train_time:21463ms step_avg:60.80ms
step:354/2330 train_time:21525ms step_avg:60.80ms
step:355/2330 train_time:21584ms step_avg:60.80ms
step:356/2330 train_time:21646ms step_avg:60.80ms
step:357/2330 train_time:21705ms step_avg:60.80ms
step:358/2330 train_time:21767ms step_avg:60.80ms
step:359/2330 train_time:21826ms step_avg:60.80ms
step:360/2330 train_time:21889ms step_avg:60.80ms
step:361/2330 train_time:21948ms step_avg:60.80ms
step:362/2330 train_time:22010ms step_avg:60.80ms
step:363/2330 train_time:22071ms step_avg:60.80ms
step:364/2330 train_time:22134ms step_avg:60.81ms
step:365/2330 train_time:22193ms step_avg:60.80ms
step:366/2330 train_time:22256ms step_avg:60.81ms
step:367/2330 train_time:22316ms step_avg:60.81ms
step:368/2330 train_time:22378ms step_avg:60.81ms
step:369/2330 train_time:22437ms step_avg:60.81ms
step:370/2330 train_time:22499ms step_avg:60.81ms
step:371/2330 train_time:22558ms step_avg:60.80ms
step:372/2330 train_time:22621ms step_avg:60.81ms
step:373/2330 train_time:22679ms step_avg:60.80ms
step:374/2330 train_time:22742ms step_avg:60.81ms
step:375/2330 train_time:22801ms step_avg:60.80ms
step:376/2330 train_time:22864ms step_avg:60.81ms
step:377/2330 train_time:22923ms step_avg:60.80ms
step:378/2330 train_time:22986ms step_avg:60.81ms
step:379/2330 train_time:23045ms step_avg:60.80ms
step:380/2330 train_time:23106ms step_avg:60.81ms
step:381/2330 train_time:23166ms step_avg:60.80ms
step:382/2330 train_time:23227ms step_avg:60.80ms
step:383/2330 train_time:23287ms step_avg:60.80ms
step:384/2330 train_time:23349ms step_avg:60.81ms
step:385/2330 train_time:23409ms step_avg:60.80ms
step:386/2330 train_time:23470ms step_avg:60.80ms
step:387/2330 train_time:23529ms step_avg:60.80ms
step:388/2330 train_time:23593ms step_avg:60.81ms
step:389/2330 train_time:23652ms step_avg:60.80ms
step:390/2330 train_time:23715ms step_avg:60.81ms
step:391/2330 train_time:23775ms step_avg:60.80ms
step:392/2330 train_time:23836ms step_avg:60.81ms
step:393/2330 train_time:23895ms step_avg:60.80ms
step:394/2330 train_time:23958ms step_avg:60.81ms
step:395/2330 train_time:24018ms step_avg:60.80ms
step:396/2330 train_time:24081ms step_avg:60.81ms
step:397/2330 train_time:24141ms step_avg:60.81ms
step:398/2330 train_time:24203ms step_avg:60.81ms
step:399/2330 train_time:24262ms step_avg:60.81ms
step:400/2330 train_time:24324ms step_avg:60.81ms
step:401/2330 train_time:24383ms step_avg:60.80ms
step:402/2330 train_time:24445ms step_avg:60.81ms
step:403/2330 train_time:24505ms step_avg:60.81ms
step:404/2330 train_time:24567ms step_avg:60.81ms
step:405/2330 train_time:24626ms step_avg:60.81ms
step:406/2330 train_time:24689ms step_avg:60.81ms
step:407/2330 train_time:24749ms step_avg:60.81ms
step:408/2330 train_time:24811ms step_avg:60.81ms
step:409/2330 train_time:24870ms step_avg:60.81ms
step:410/2330 train_time:24932ms step_avg:60.81ms
step:411/2330 train_time:24992ms step_avg:60.81ms
step:412/2330 train_time:25055ms step_avg:60.81ms
step:413/2330 train_time:25114ms step_avg:60.81ms
step:414/2330 train_time:25176ms step_avg:60.81ms
step:415/2330 train_time:25235ms step_avg:60.81ms
step:416/2330 train_time:25297ms step_avg:60.81ms
step:417/2330 train_time:25356ms step_avg:60.81ms
step:418/2330 train_time:25419ms step_avg:60.81ms
step:419/2330 train_time:25479ms step_avg:60.81ms
step:420/2330 train_time:25542ms step_avg:60.81ms
step:421/2330 train_time:25601ms step_avg:60.81ms
step:422/2330 train_time:25664ms step_avg:60.81ms
step:423/2330 train_time:25722ms step_avg:60.81ms
step:424/2330 train_time:25784ms step_avg:60.81ms
step:425/2330 train_time:25843ms step_avg:60.81ms
step:426/2330 train_time:25906ms step_avg:60.81ms
step:427/2330 train_time:25965ms step_avg:60.81ms
step:428/2330 train_time:26027ms step_avg:60.81ms
step:429/2330 train_time:26087ms step_avg:60.81ms
step:430/2330 train_time:26149ms step_avg:60.81ms
step:431/2330 train_time:26208ms step_avg:60.81ms
step:432/2330 train_time:26270ms step_avg:60.81ms
step:433/2330 train_time:26330ms step_avg:60.81ms
step:434/2330 train_time:26392ms step_avg:60.81ms
step:435/2330 train_time:26451ms step_avg:60.81ms
step:436/2330 train_time:26514ms step_avg:60.81ms
step:437/2330 train_time:26573ms step_avg:60.81ms
step:438/2330 train_time:26636ms step_avg:60.81ms
step:439/2330 train_time:26696ms step_avg:60.81ms
step:440/2330 train_time:26758ms step_avg:60.81ms
step:441/2330 train_time:26817ms step_avg:60.81ms
step:442/2330 train_time:26880ms step_avg:60.81ms
step:443/2330 train_time:26938ms step_avg:60.81ms
step:444/2330 train_time:27001ms step_avg:60.81ms
step:445/2330 train_time:27060ms step_avg:60.81ms
step:446/2330 train_time:27123ms step_avg:60.81ms
step:447/2330 train_time:27182ms step_avg:60.81ms
step:448/2330 train_time:27245ms step_avg:60.81ms
step:449/2330 train_time:27303ms step_avg:60.81ms
step:450/2330 train_time:27365ms step_avg:60.81ms
step:451/2330 train_time:27425ms step_avg:60.81ms
step:452/2330 train_time:27487ms step_avg:60.81ms
step:453/2330 train_time:27546ms step_avg:60.81ms
step:454/2330 train_time:27608ms step_avg:60.81ms
step:455/2330 train_time:27668ms step_avg:60.81ms
step:456/2330 train_time:27730ms step_avg:60.81ms
step:457/2330 train_time:27789ms step_avg:60.81ms
step:458/2330 train_time:27852ms step_avg:60.81ms
step:459/2330 train_time:27912ms step_avg:60.81ms
step:460/2330 train_time:27974ms step_avg:60.81ms
step:461/2330 train_time:28033ms step_avg:60.81ms
step:462/2330 train_time:28096ms step_avg:60.81ms
step:463/2330 train_time:28155ms step_avg:60.81ms
step:464/2330 train_time:28217ms step_avg:60.81ms
step:465/2330 train_time:28276ms step_avg:60.81ms
step:466/2330 train_time:28340ms step_avg:60.81ms
step:467/2330 train_time:28400ms step_avg:60.81ms
step:468/2330 train_time:28462ms step_avg:60.82ms
step:469/2330 train_time:28521ms step_avg:60.81ms
step:470/2330 train_time:28583ms step_avg:60.81ms
step:471/2330 train_time:28642ms step_avg:60.81ms
step:472/2330 train_time:28705ms step_avg:60.81ms
step:473/2330 train_time:28764ms step_avg:60.81ms
step:474/2330 train_time:28826ms step_avg:60.81ms
step:475/2330 train_time:28885ms step_avg:60.81ms
step:476/2330 train_time:28948ms step_avg:60.81ms
step:477/2330 train_time:29007ms step_avg:60.81ms
step:478/2330 train_time:29070ms step_avg:60.82ms
step:479/2330 train_time:29129ms step_avg:60.81ms
step:480/2330 train_time:29192ms step_avg:60.82ms
step:481/2330 train_time:29251ms step_avg:60.81ms
step:482/2330 train_time:29315ms step_avg:60.82ms
step:483/2330 train_time:29374ms step_avg:60.82ms
step:484/2330 train_time:29436ms step_avg:60.82ms
step:485/2330 train_time:29495ms step_avg:60.81ms
step:486/2330 train_time:29557ms step_avg:60.82ms
step:487/2330 train_time:29617ms step_avg:60.81ms
step:488/2330 train_time:29679ms step_avg:60.82ms
step:489/2330 train_time:29737ms step_avg:60.81ms
step:490/2330 train_time:29799ms step_avg:60.82ms
step:491/2330 train_time:29858ms step_avg:60.81ms
step:492/2330 train_time:29921ms step_avg:60.81ms
step:493/2330 train_time:29980ms step_avg:60.81ms
step:494/2330 train_time:30043ms step_avg:60.82ms
step:495/2330 train_time:30103ms step_avg:60.81ms
step:496/2330 train_time:30165ms step_avg:60.82ms
step:497/2330 train_time:30224ms step_avg:60.81ms
step:498/2330 train_time:30287ms step_avg:60.82ms
step:499/2330 train_time:30346ms step_avg:60.81ms
step:500/2330 train_time:30408ms step_avg:60.82ms
step:500/2330 val_loss:3.8294 train_time:30472ms step_avg:60.94ms
step:501/2330 train_time:30497ms step_avg:60.87ms
step:502/2330 train_time:30532ms step_avg:60.82ms
step:503/2330 train_time:30596ms step_avg:60.83ms
step:504/2330 train_time:30662ms step_avg:60.84ms
step:505/2330 train_time:30723ms step_avg:60.84ms
step:506/2330 train_time:30785ms step_avg:60.84ms
step:507/2330 train_time:30844ms step_avg:60.84ms
step:508/2330 train_time:30906ms step_avg:60.84ms
step:509/2330 train_time:30965ms step_avg:60.83ms
step:510/2330 train_time:31026ms step_avg:60.84ms
step:511/2330 train_time:31084ms step_avg:60.83ms
step:512/2330 train_time:31146ms step_avg:60.83ms
step:513/2330 train_time:31204ms step_avg:60.83ms
step:514/2330 train_time:31265ms step_avg:60.83ms
step:515/2330 train_time:31324ms step_avg:60.82ms
step:516/2330 train_time:31386ms step_avg:60.82ms
step:517/2330 train_time:31445ms step_avg:60.82ms
step:518/2330 train_time:31508ms step_avg:60.83ms
step:519/2330 train_time:31569ms step_avg:60.83ms
step:520/2330 train_time:31632ms step_avg:60.83ms
step:521/2330 train_time:31692ms step_avg:60.83ms
step:522/2330 train_time:31755ms step_avg:60.83ms
step:523/2330 train_time:31814ms step_avg:60.83ms
step:524/2330 train_time:31877ms step_avg:60.83ms
step:525/2330 train_time:31936ms step_avg:60.83ms
step:526/2330 train_time:31998ms step_avg:60.83ms
step:527/2330 train_time:32057ms step_avg:60.83ms
step:528/2330 train_time:32120ms step_avg:60.83ms
step:529/2330 train_time:32179ms step_avg:60.83ms
step:530/2330 train_time:32240ms step_avg:60.83ms
step:531/2330 train_time:32299ms step_avg:60.83ms
step:532/2330 train_time:32362ms step_avg:60.83ms
step:533/2330 train_time:32421ms step_avg:60.83ms
step:534/2330 train_time:32484ms step_avg:60.83ms
step:535/2330 train_time:32544ms step_avg:60.83ms
step:536/2330 train_time:32606ms step_avg:60.83ms
step:537/2330 train_time:32666ms step_avg:60.83ms
step:538/2330 train_time:32728ms step_avg:60.83ms
step:539/2330 train_time:32788ms step_avg:60.83ms
step:540/2330 train_time:32850ms step_avg:60.83ms
step:541/2330 train_time:32909ms step_avg:60.83ms
step:542/2330 train_time:32971ms step_avg:60.83ms
step:543/2330 train_time:33030ms step_avg:60.83ms
step:544/2330 train_time:33093ms step_avg:60.83ms
step:545/2330 train_time:33151ms step_avg:60.83ms
step:546/2330 train_time:33214ms step_avg:60.83ms
step:547/2330 train_time:33272ms step_avg:60.83ms
step:548/2330 train_time:33335ms step_avg:60.83ms
step:549/2330 train_time:33395ms step_avg:60.83ms
step:550/2330 train_time:33457ms step_avg:60.83ms
step:551/2330 train_time:33516ms step_avg:60.83ms
step:552/2330 train_time:33579ms step_avg:60.83ms
step:553/2330 train_time:33639ms step_avg:60.83ms
step:554/2330 train_time:33702ms step_avg:60.83ms
step:555/2330 train_time:33761ms step_avg:60.83ms
step:556/2330 train_time:33823ms step_avg:60.83ms
step:557/2330 train_time:33882ms step_avg:60.83ms
step:558/2330 train_time:33945ms step_avg:60.83ms
step:559/2330 train_time:34004ms step_avg:60.83ms
step:560/2330 train_time:34066ms step_avg:60.83ms
step:561/2330 train_time:34124ms step_avg:60.83ms
step:562/2330 train_time:34187ms step_avg:60.83ms
step:563/2330 train_time:34246ms step_avg:60.83ms
step:564/2330 train_time:34308ms step_avg:60.83ms
step:565/2330 train_time:34368ms step_avg:60.83ms
step:566/2330 train_time:34430ms step_avg:60.83ms
step:567/2330 train_time:34489ms step_avg:60.83ms
step:568/2330 train_time:34552ms step_avg:60.83ms
step:569/2330 train_time:34611ms step_avg:60.83ms
step:570/2330 train_time:34674ms step_avg:60.83ms
step:571/2330 train_time:34734ms step_avg:60.83ms
step:572/2330 train_time:34796ms step_avg:60.83ms
step:573/2330 train_time:34855ms step_avg:60.83ms
step:574/2330 train_time:34918ms step_avg:60.83ms
step:575/2330 train_time:34977ms step_avg:60.83ms
step:576/2330 train_time:35040ms step_avg:60.83ms
step:577/2330 train_time:35099ms step_avg:60.83ms
step:578/2330 train_time:35160ms step_avg:60.83ms
step:579/2330 train_time:35219ms step_avg:60.83ms
step:580/2330 train_time:35282ms step_avg:60.83ms
step:581/2330 train_time:35342ms step_avg:60.83ms
step:582/2330 train_time:35404ms step_avg:60.83ms
step:583/2330 train_time:35464ms step_avg:60.83ms
step:584/2330 train_time:35526ms step_avg:60.83ms
step:585/2330 train_time:35586ms step_avg:60.83ms
step:586/2330 train_time:35648ms step_avg:60.83ms
step:587/2330 train_time:35707ms step_avg:60.83ms
step:588/2330 train_time:35769ms step_avg:60.83ms
step:589/2330 train_time:35829ms step_avg:60.83ms
step:590/2330 train_time:35891ms step_avg:60.83ms
step:591/2330 train_time:35950ms step_avg:60.83ms
step:592/2330 train_time:36013ms step_avg:60.83ms
step:593/2330 train_time:36073ms step_avg:60.83ms
step:594/2330 train_time:36136ms step_avg:60.83ms
step:595/2330 train_time:36194ms step_avg:60.83ms
step:596/2330 train_time:36256ms step_avg:60.83ms
step:597/2330 train_time:36316ms step_avg:60.83ms
step:598/2330 train_time:36379ms step_avg:60.83ms
step:599/2330 train_time:36439ms step_avg:60.83ms
step:600/2330 train_time:36502ms step_avg:60.84ms
step:601/2330 train_time:36561ms step_avg:60.83ms
step:602/2330 train_time:36623ms step_avg:60.84ms
step:603/2330 train_time:36683ms step_avg:60.83ms
step:604/2330 train_time:36745ms step_avg:60.84ms
step:605/2330 train_time:36804ms step_avg:60.83ms
step:606/2330 train_time:36866ms step_avg:60.84ms
step:607/2330 train_time:36925ms step_avg:60.83ms
step:608/2330 train_time:36988ms step_avg:60.84ms
step:609/2330 train_time:37047ms step_avg:60.83ms
step:610/2330 train_time:37109ms step_avg:60.83ms
step:611/2330 train_time:37169ms step_avg:60.83ms
step:612/2330 train_time:37230ms step_avg:60.83ms
step:613/2330 train_time:37289ms step_avg:60.83ms
step:614/2330 train_time:37351ms step_avg:60.83ms
step:615/2330 train_time:37411ms step_avg:60.83ms
step:616/2330 train_time:37474ms step_avg:60.83ms
step:617/2330 train_time:37533ms step_avg:60.83ms
step:618/2330 train_time:37596ms step_avg:60.83ms
step:619/2330 train_time:37654ms step_avg:60.83ms
step:620/2330 train_time:37716ms step_avg:60.83ms
step:621/2330 train_time:37776ms step_avg:60.83ms
step:622/2330 train_time:37839ms step_avg:60.83ms
step:623/2330 train_time:37898ms step_avg:60.83ms
step:624/2330 train_time:37960ms step_avg:60.83ms
step:625/2330 train_time:38022ms step_avg:60.83ms
step:626/2330 train_time:38085ms step_avg:60.84ms
step:627/2330 train_time:38143ms step_avg:60.83ms
step:628/2330 train_time:38205ms step_avg:60.84ms
step:629/2330 train_time:38265ms step_avg:60.83ms
step:630/2330 train_time:38327ms step_avg:60.84ms
step:631/2330 train_time:38386ms step_avg:60.83ms
step:632/2330 train_time:38448ms step_avg:60.83ms
step:633/2330 train_time:38507ms step_avg:60.83ms
step:634/2330 train_time:38569ms step_avg:60.83ms
step:635/2330 train_time:38629ms step_avg:60.83ms
step:636/2330 train_time:38691ms step_avg:60.83ms
step:637/2330 train_time:38750ms step_avg:60.83ms
step:638/2330 train_time:38813ms step_avg:60.84ms
step:639/2330 train_time:38873ms step_avg:60.83ms
step:640/2330 train_time:38936ms step_avg:60.84ms
step:641/2330 train_time:38995ms step_avg:60.83ms
step:642/2330 train_time:39058ms step_avg:60.84ms
step:643/2330 train_time:39117ms step_avg:60.84ms
step:644/2330 train_time:39180ms step_avg:60.84ms
step:645/2330 train_time:39240ms step_avg:60.84ms
step:646/2330 train_time:39302ms step_avg:60.84ms
step:647/2330 train_time:39362ms step_avg:60.84ms
step:648/2330 train_time:39424ms step_avg:60.84ms
step:649/2330 train_time:39483ms step_avg:60.84ms
step:650/2330 train_time:39545ms step_avg:60.84ms
step:651/2330 train_time:39604ms step_avg:60.84ms
step:652/2330 train_time:39667ms step_avg:60.84ms
step:653/2330 train_time:39727ms step_avg:60.84ms
step:654/2330 train_time:39789ms step_avg:60.84ms
step:655/2330 train_time:39848ms step_avg:60.84ms
step:656/2330 train_time:39911ms step_avg:60.84ms
step:657/2330 train_time:39970ms step_avg:60.84ms
step:658/2330 train_time:40033ms step_avg:60.84ms
step:659/2330 train_time:40093ms step_avg:60.84ms
step:660/2330 train_time:40155ms step_avg:60.84ms
step:661/2330 train_time:40214ms step_avg:60.84ms
step:662/2330 train_time:40276ms step_avg:60.84ms
step:663/2330 train_time:40335ms step_avg:60.84ms
step:664/2330 train_time:40397ms step_avg:60.84ms
step:665/2330 train_time:40457ms step_avg:60.84ms
step:666/2330 train_time:40519ms step_avg:60.84ms
step:667/2330 train_time:40578ms step_avg:60.84ms
step:668/2330 train_time:40642ms step_avg:60.84ms
step:669/2330 train_time:40701ms step_avg:60.84ms
step:670/2330 train_time:40764ms step_avg:60.84ms
step:671/2330 train_time:40823ms step_avg:60.84ms
step:672/2330 train_time:40886ms step_avg:60.84ms
step:673/2330 train_time:40945ms step_avg:60.84ms
step:674/2330 train_time:41007ms step_avg:60.84ms
step:675/2330 train_time:41066ms step_avg:60.84ms
step:676/2330 train_time:41129ms step_avg:60.84ms
step:677/2330 train_time:41188ms step_avg:60.84ms
step:678/2330 train_time:41250ms step_avg:60.84ms
step:679/2330 train_time:41310ms step_avg:60.84ms
step:680/2330 train_time:41372ms step_avg:60.84ms
step:681/2330 train_time:41432ms step_avg:60.84ms
step:682/2330 train_time:41494ms step_avg:60.84ms
step:683/2330 train_time:41554ms step_avg:60.84ms
step:684/2330 train_time:41616ms step_avg:60.84ms
step:685/2330 train_time:41675ms step_avg:60.84ms
step:686/2330 train_time:41737ms step_avg:60.84ms
step:687/2330 train_time:41797ms step_avg:60.84ms
step:688/2330 train_time:41860ms step_avg:60.84ms
step:689/2330 train_time:41919ms step_avg:60.84ms
step:690/2330 train_time:41982ms step_avg:60.84ms
step:691/2330 train_time:42041ms step_avg:60.84ms
step:692/2330 train_time:42104ms step_avg:60.84ms
step:693/2330 train_time:42164ms step_avg:60.84ms
step:694/2330 train_time:42226ms step_avg:60.84ms
step:695/2330 train_time:42285ms step_avg:60.84ms
step:696/2330 train_time:42347ms step_avg:60.84ms
step:697/2330 train_time:42406ms step_avg:60.84ms
step:698/2330 train_time:42468ms step_avg:60.84ms
step:699/2330 train_time:42526ms step_avg:60.84ms
step:700/2330 train_time:42588ms step_avg:60.84ms
step:701/2330 train_time:42648ms step_avg:60.84ms
step:702/2330 train_time:42710ms step_avg:60.84ms
step:703/2330 train_time:42770ms step_avg:60.84ms
step:704/2330 train_time:42832ms step_avg:60.84ms
step:705/2330 train_time:42891ms step_avg:60.84ms
step:706/2330 train_time:42953ms step_avg:60.84ms
step:707/2330 train_time:43013ms step_avg:60.84ms
step:708/2330 train_time:43076ms step_avg:60.84ms
step:709/2330 train_time:43136ms step_avg:60.84ms
step:710/2330 train_time:43198ms step_avg:60.84ms
step:711/2330 train_time:43257ms step_avg:60.84ms
step:712/2330 train_time:43320ms step_avg:60.84ms
step:713/2330 train_time:43380ms step_avg:60.84ms
step:714/2330 train_time:43443ms step_avg:60.84ms
step:715/2330 train_time:43502ms step_avg:60.84ms
step:716/2330 train_time:43564ms step_avg:60.84ms
step:717/2330 train_time:43624ms step_avg:60.84ms
step:718/2330 train_time:43686ms step_avg:60.84ms
step:719/2330 train_time:43745ms step_avg:60.84ms
step:720/2330 train_time:43807ms step_avg:60.84ms
step:721/2330 train_time:43866ms step_avg:60.84ms
step:722/2330 train_time:43929ms step_avg:60.84ms
step:723/2330 train_time:43988ms step_avg:60.84ms
step:724/2330 train_time:44051ms step_avg:60.84ms
step:725/2330 train_time:44111ms step_avg:60.84ms
step:726/2330 train_time:44173ms step_avg:60.84ms
step:727/2330 train_time:44232ms step_avg:60.84ms
step:728/2330 train_time:44294ms step_avg:60.84ms
step:729/2330 train_time:44353ms step_avg:60.84ms
step:730/2330 train_time:44415ms step_avg:60.84ms
step:731/2330 train_time:44474ms step_avg:60.84ms
step:732/2330 train_time:44536ms step_avg:60.84ms
step:733/2330 train_time:44595ms step_avg:60.84ms
step:734/2330 train_time:44658ms step_avg:60.84ms
step:735/2330 train_time:44718ms step_avg:60.84ms
step:736/2330 train_time:44781ms step_avg:60.84ms
step:737/2330 train_time:44840ms step_avg:60.84ms
step:738/2330 train_time:44903ms step_avg:60.84ms
step:739/2330 train_time:44962ms step_avg:60.84ms
step:740/2330 train_time:45024ms step_avg:60.84ms
step:741/2330 train_time:45084ms step_avg:60.84ms
step:742/2330 train_time:45147ms step_avg:60.84ms
step:743/2330 train_time:45206ms step_avg:60.84ms
step:744/2330 train_time:45268ms step_avg:60.84ms
step:745/2330 train_time:45328ms step_avg:60.84ms
step:746/2330 train_time:45390ms step_avg:60.84ms
step:747/2330 train_time:45449ms step_avg:60.84ms
step:748/2330 train_time:45512ms step_avg:60.84ms
step:749/2330 train_time:45573ms step_avg:60.84ms
step:750/2330 train_time:45635ms step_avg:60.85ms
step:750/2330 val_loss:3.7027 train_time:45699ms step_avg:60.93ms
step:751/2330 train_time:45723ms step_avg:60.88ms
step:752/2330 train_time:45760ms step_avg:60.85ms
step:753/2330 train_time:45823ms step_avg:60.85ms
step:754/2330 train_time:45887ms step_avg:60.86ms
step:755/2330 train_time:45946ms step_avg:60.86ms
step:756/2330 train_time:46009ms step_avg:60.86ms
step:757/2330 train_time:46068ms step_avg:60.86ms
step:758/2330 train_time:46130ms step_avg:60.86ms
step:759/2330 train_time:46189ms step_avg:60.85ms
step:760/2330 train_time:46250ms step_avg:60.86ms
step:761/2330 train_time:46309ms step_avg:60.85ms
step:762/2330 train_time:46370ms step_avg:60.85ms
step:763/2330 train_time:46429ms step_avg:60.85ms
step:764/2330 train_time:46490ms step_avg:60.85ms
step:765/2330 train_time:46549ms step_avg:60.85ms
step:766/2330 train_time:46612ms step_avg:60.85ms
step:767/2330 train_time:46674ms step_avg:60.85ms
step:768/2330 train_time:46739ms step_avg:60.86ms
step:769/2330 train_time:46800ms step_avg:60.86ms
step:770/2330 train_time:46863ms step_avg:60.86ms
step:771/2330 train_time:46924ms step_avg:60.86ms
step:772/2330 train_time:46986ms step_avg:60.86ms
step:773/2330 train_time:47046ms step_avg:60.86ms
step:774/2330 train_time:47109ms step_avg:60.86ms
step:775/2330 train_time:47169ms step_avg:60.86ms
step:776/2330 train_time:47231ms step_avg:60.86ms
step:777/2330 train_time:47290ms step_avg:60.86ms
step:778/2330 train_time:47353ms step_avg:60.86ms
step:779/2330 train_time:47412ms step_avg:60.86ms
step:780/2330 train_time:47474ms step_avg:60.86ms
step:781/2330 train_time:47533ms step_avg:60.86ms
step:782/2330 train_time:47596ms step_avg:60.86ms
step:783/2330 train_time:47657ms step_avg:60.87ms
step:784/2330 train_time:47721ms step_avg:60.87ms
step:785/2330 train_time:47781ms step_avg:60.87ms
step:786/2330 train_time:47845ms step_avg:60.87ms
step:787/2330 train_time:47905ms step_avg:60.87ms
step:788/2330 train_time:47968ms step_avg:60.87ms
step:789/2330 train_time:48027ms step_avg:60.87ms
step:790/2330 train_time:48090ms step_avg:60.87ms
step:791/2330 train_time:48150ms step_avg:60.87ms
step:792/2330 train_time:48212ms step_avg:60.87ms
step:793/2330 train_time:48272ms step_avg:60.87ms
step:794/2330 train_time:48334ms step_avg:60.87ms
step:795/2330 train_time:48393ms step_avg:60.87ms
step:796/2330 train_time:48456ms step_avg:60.87ms
step:797/2330 train_time:48516ms step_avg:60.87ms
step:798/2330 train_time:48579ms step_avg:60.88ms
step:799/2330 train_time:48639ms step_avg:60.87ms
step:800/2330 train_time:48702ms step_avg:60.88ms
step:801/2330 train_time:48763ms step_avg:60.88ms
step:802/2330 train_time:48826ms step_avg:60.88ms
step:803/2330 train_time:48886ms step_avg:60.88ms
step:804/2330 train_time:48949ms step_avg:60.88ms
step:805/2330 train_time:49010ms step_avg:60.88ms
step:806/2330 train_time:49072ms step_avg:60.88ms
step:807/2330 train_time:49132ms step_avg:60.88ms
step:808/2330 train_time:49195ms step_avg:60.88ms
step:809/2330 train_time:49255ms step_avg:60.88ms
step:810/2330 train_time:49317ms step_avg:60.89ms
step:811/2330 train_time:49377ms step_avg:60.88ms
step:812/2330 train_time:49439ms step_avg:60.89ms
step:813/2330 train_time:49499ms step_avg:60.88ms
step:814/2330 train_time:49562ms step_avg:60.89ms
step:815/2330 train_time:49622ms step_avg:60.89ms
step:816/2330 train_time:49685ms step_avg:60.89ms
step:817/2330 train_time:49745ms step_avg:60.89ms
step:818/2330 train_time:49809ms step_avg:60.89ms
step:819/2330 train_time:49868ms step_avg:60.89ms
step:820/2330 train_time:49931ms step_avg:60.89ms
step:821/2330 train_time:49991ms step_avg:60.89ms
step:822/2330 train_time:50054ms step_avg:60.89ms
step:823/2330 train_time:50113ms step_avg:60.89ms
step:824/2330 train_time:50177ms step_avg:60.89ms
step:825/2330 train_time:50236ms step_avg:60.89ms
step:826/2330 train_time:50299ms step_avg:60.89ms
step:827/2330 train_time:50358ms step_avg:60.89ms
step:828/2330 train_time:50421ms step_avg:60.90ms
step:829/2330 train_time:50481ms step_avg:60.89ms
step:830/2330 train_time:50544ms step_avg:60.90ms
step:831/2330 train_time:50603ms step_avg:60.89ms
step:832/2330 train_time:50666ms step_avg:60.90ms
step:833/2330 train_time:50726ms step_avg:60.90ms
step:834/2330 train_time:50789ms step_avg:60.90ms
step:835/2330 train_time:50849ms step_avg:60.90ms
step:836/2330 train_time:50912ms step_avg:60.90ms
step:837/2330 train_time:50971ms step_avg:60.90ms
step:838/2330 train_time:51034ms step_avg:60.90ms
step:839/2330 train_time:51093ms step_avg:60.90ms
step:840/2330 train_time:51156ms step_avg:60.90ms
step:841/2330 train_time:51216ms step_avg:60.90ms
step:842/2330 train_time:51278ms step_avg:60.90ms
step:843/2330 train_time:51338ms step_avg:60.90ms
step:844/2330 train_time:51401ms step_avg:60.90ms
step:845/2330 train_time:51461ms step_avg:60.90ms
step:846/2330 train_time:51523ms step_avg:60.90ms
step:847/2330 train_time:51584ms step_avg:60.90ms
step:848/2330 train_time:51647ms step_avg:60.90ms
step:849/2330 train_time:51707ms step_avg:60.90ms
step:850/2330 train_time:51770ms step_avg:60.91ms
step:851/2330 train_time:51829ms step_avg:60.90ms
step:852/2330 train_time:51892ms step_avg:60.91ms
step:853/2330 train_time:51952ms step_avg:60.91ms
step:854/2330 train_time:52015ms step_avg:60.91ms
step:855/2330 train_time:52075ms step_avg:60.91ms
step:856/2330 train_time:52137ms step_avg:60.91ms
step:857/2330 train_time:52197ms step_avg:60.91ms
step:858/2330 train_time:52260ms step_avg:60.91ms
step:859/2330 train_time:52319ms step_avg:60.91ms
step:860/2330 train_time:52382ms step_avg:60.91ms
step:861/2330 train_time:52442ms step_avg:60.91ms
step:862/2330 train_time:52504ms step_avg:60.91ms
step:863/2330 train_time:52565ms step_avg:60.91ms
step:864/2330 train_time:52627ms step_avg:60.91ms
step:865/2330 train_time:52687ms step_avg:60.91ms
step:866/2330 train_time:52750ms step_avg:60.91ms
step:867/2330 train_time:52810ms step_avg:60.91ms
step:868/2330 train_time:52873ms step_avg:60.91ms
step:869/2330 train_time:52933ms step_avg:60.91ms
step:870/2330 train_time:52996ms step_avg:60.91ms
step:871/2330 train_time:53055ms step_avg:60.91ms
step:872/2330 train_time:53119ms step_avg:60.92ms
step:873/2330 train_time:53179ms step_avg:60.91ms
step:874/2330 train_time:53241ms step_avg:60.92ms
step:875/2330 train_time:53302ms step_avg:60.92ms
step:876/2330 train_time:53365ms step_avg:60.92ms
step:877/2330 train_time:53425ms step_avg:60.92ms
step:878/2330 train_time:53487ms step_avg:60.92ms
step:879/2330 train_time:53547ms step_avg:60.92ms
step:880/2330 train_time:53610ms step_avg:60.92ms
step:881/2330 train_time:53670ms step_avg:60.92ms
step:882/2330 train_time:53732ms step_avg:60.92ms
step:883/2330 train_time:53792ms step_avg:60.92ms
step:884/2330 train_time:53855ms step_avg:60.92ms
step:885/2330 train_time:53915ms step_avg:60.92ms
step:886/2330 train_time:53978ms step_avg:60.92ms
step:887/2330 train_time:54037ms step_avg:60.92ms
step:888/2330 train_time:54100ms step_avg:60.92ms
step:889/2330 train_time:54159ms step_avg:60.92ms
step:890/2330 train_time:54222ms step_avg:60.92ms
step:891/2330 train_time:54283ms step_avg:60.92ms
step:892/2330 train_time:54345ms step_avg:60.93ms
step:893/2330 train_time:54405ms step_avg:60.92ms
step:894/2330 train_time:54468ms step_avg:60.93ms
step:895/2330 train_time:54528ms step_avg:60.92ms
step:896/2330 train_time:54590ms step_avg:60.93ms
step:897/2330 train_time:54650ms step_avg:60.92ms
step:898/2330 train_time:54712ms step_avg:60.93ms
step:899/2330 train_time:54772ms step_avg:60.93ms
step:900/2330 train_time:54835ms step_avg:60.93ms
step:901/2330 train_time:54894ms step_avg:60.93ms
step:902/2330 train_time:54957ms step_avg:60.93ms
step:903/2330 train_time:55018ms step_avg:60.93ms
step:904/2330 train_time:55080ms step_avg:60.93ms
step:905/2330 train_time:55140ms step_avg:60.93ms
step:906/2330 train_time:55203ms step_avg:60.93ms
step:907/2330 train_time:55263ms step_avg:60.93ms
step:908/2330 train_time:55326ms step_avg:60.93ms
step:909/2330 train_time:55385ms step_avg:60.93ms
step:910/2330 train_time:55448ms step_avg:60.93ms
step:911/2330 train_time:55509ms step_avg:60.93ms
step:912/2330 train_time:55572ms step_avg:60.93ms
step:913/2330 train_time:55632ms step_avg:60.93ms
step:914/2330 train_time:55694ms step_avg:60.93ms
step:915/2330 train_time:55754ms step_avg:60.93ms
step:916/2330 train_time:55817ms step_avg:60.94ms
step:917/2330 train_time:55877ms step_avg:60.93ms
step:918/2330 train_time:55939ms step_avg:60.94ms
step:919/2330 train_time:55999ms step_avg:60.93ms
step:920/2330 train_time:56062ms step_avg:60.94ms
step:921/2330 train_time:56122ms step_avg:60.94ms
step:922/2330 train_time:56185ms step_avg:60.94ms
step:923/2330 train_time:56244ms step_avg:60.94ms
step:924/2330 train_time:56307ms step_avg:60.94ms
step:925/2330 train_time:56367ms step_avg:60.94ms
step:926/2330 train_time:56429ms step_avg:60.94ms
step:927/2330 train_time:56489ms step_avg:60.94ms
step:928/2330 train_time:56552ms step_avg:60.94ms
step:929/2330 train_time:56612ms step_avg:60.94ms
step:930/2330 train_time:56674ms step_avg:60.94ms
step:931/2330 train_time:56734ms step_avg:60.94ms
step:932/2330 train_time:56797ms step_avg:60.94ms
step:933/2330 train_time:56856ms step_avg:60.94ms
step:934/2330 train_time:56919ms step_avg:60.94ms
step:935/2330 train_time:56979ms step_avg:60.94ms
step:936/2330 train_time:57042ms step_avg:60.94ms
step:937/2330 train_time:57102ms step_avg:60.94ms
step:938/2330 train_time:57165ms step_avg:60.94ms
step:939/2330 train_time:57225ms step_avg:60.94ms
step:940/2330 train_time:57287ms step_avg:60.94ms
step:941/2330 train_time:57347ms step_avg:60.94ms
step:942/2330 train_time:57410ms step_avg:60.94ms
step:943/2330 train_time:57470ms step_avg:60.94ms
step:944/2330 train_time:57533ms step_avg:60.95ms
step:945/2330 train_time:57593ms step_avg:60.94ms
step:946/2330 train_time:57656ms step_avg:60.95ms
step:947/2330 train_time:57715ms step_avg:60.95ms
step:948/2330 train_time:57778ms step_avg:60.95ms
step:949/2330 train_time:57838ms step_avg:60.95ms
step:950/2330 train_time:57901ms step_avg:60.95ms
step:951/2330 train_time:57960ms step_avg:60.95ms
step:952/2330 train_time:58023ms step_avg:60.95ms
step:953/2330 train_time:58084ms step_avg:60.95ms
step:954/2330 train_time:58146ms step_avg:60.95ms
step:955/2330 train_time:58206ms step_avg:60.95ms
step:956/2330 train_time:58270ms step_avg:60.95ms
step:957/2330 train_time:58329ms step_avg:60.95ms
step:958/2330 train_time:58392ms step_avg:60.95ms
step:959/2330 train_time:58452ms step_avg:60.95ms
step:960/2330 train_time:58515ms step_avg:60.95ms
step:961/2330 train_time:58575ms step_avg:60.95ms
step:962/2330 train_time:58638ms step_avg:60.95ms
step:963/2330 train_time:58697ms step_avg:60.95ms
step:964/2330 train_time:58760ms step_avg:60.95ms
step:965/2330 train_time:58819ms step_avg:60.95ms
step:966/2330 train_time:58882ms step_avg:60.95ms
step:967/2330 train_time:58942ms step_avg:60.95ms
step:968/2330 train_time:59004ms step_avg:60.95ms
step:969/2330 train_time:59064ms step_avg:60.95ms
step:970/2330 train_time:59127ms step_avg:60.96ms
step:971/2330 train_time:59187ms step_avg:60.95ms
step:972/2330 train_time:59250ms step_avg:60.96ms
step:973/2330 train_time:59310ms step_avg:60.96ms
step:974/2330 train_time:59373ms step_avg:60.96ms
step:975/2330 train_time:59433ms step_avg:60.96ms
step:976/2330 train_time:59495ms step_avg:60.96ms
step:977/2330 train_time:59555ms step_avg:60.96ms
step:978/2330 train_time:59618ms step_avg:60.96ms
step:979/2330 train_time:59678ms step_avg:60.96ms
step:980/2330 train_time:59741ms step_avg:60.96ms
step:981/2330 train_time:59801ms step_avg:60.96ms
step:982/2330 train_time:59864ms step_avg:60.96ms
step:983/2330 train_time:59924ms step_avg:60.96ms
step:984/2330 train_time:59986ms step_avg:60.96ms
step:985/2330 train_time:60046ms step_avg:60.96ms
step:986/2330 train_time:60109ms step_avg:60.96ms
step:987/2330 train_time:60169ms step_avg:60.96ms
step:988/2330 train_time:60232ms step_avg:60.96ms
step:989/2330 train_time:60291ms step_avg:60.96ms
step:990/2330 train_time:60354ms step_avg:60.96ms
step:991/2330 train_time:60414ms step_avg:60.96ms
step:992/2330 train_time:60477ms step_avg:60.96ms
step:993/2330 train_time:60536ms step_avg:60.96ms
step:994/2330 train_time:60599ms step_avg:60.97ms
step:995/2330 train_time:60659ms step_avg:60.96ms
step:996/2330 train_time:60722ms step_avg:60.97ms
step:997/2330 train_time:60782ms step_avg:60.96ms
step:998/2330 train_time:60844ms step_avg:60.97ms
step:999/2330 train_time:60904ms step_avg:60.97ms
step:1000/2330 train_time:60968ms step_avg:60.97ms
step:1000/2330 val_loss:3.5901 train_time:61032ms step_avg:61.03ms
step:1001/2330 train_time:61055ms step_avg:60.99ms
step:1002/2330 train_time:61093ms step_avg:60.97ms
step:1003/2330 train_time:61159ms step_avg:60.98ms
step:1004/2330 train_time:61227ms step_avg:60.98ms
step:1005/2330 train_time:61287ms step_avg:60.98ms
step:1006/2330 train_time:61350ms step_avg:60.98ms
step:1007/2330 train_time:61409ms step_avg:60.98ms
step:1008/2330 train_time:61471ms step_avg:60.98ms
step:1009/2330 train_time:61531ms step_avg:60.98ms
step:1010/2330 train_time:61593ms step_avg:60.98ms
step:1011/2330 train_time:61653ms step_avg:60.98ms
step:1012/2330 train_time:61715ms step_avg:60.98ms
step:1013/2330 train_time:61774ms step_avg:60.98ms
step:1014/2330 train_time:61836ms step_avg:60.98ms
step:1015/2330 train_time:61894ms step_avg:60.98ms
step:1016/2330 train_time:61959ms step_avg:60.98ms
step:1017/2330 train_time:62021ms step_avg:60.98ms
step:1018/2330 train_time:62085ms step_avg:60.99ms
step:1019/2330 train_time:62147ms step_avg:60.99ms
step:1020/2330 train_time:62210ms step_avg:60.99ms
step:1021/2330 train_time:62270ms step_avg:60.99ms
step:1022/2330 train_time:62333ms step_avg:60.99ms
step:1023/2330 train_time:62392ms step_avg:60.99ms
step:1024/2330 train_time:62455ms step_avg:60.99ms
step:1025/2330 train_time:62515ms step_avg:60.99ms
step:1026/2330 train_time:62577ms step_avg:60.99ms
step:1027/2330 train_time:62636ms step_avg:60.99ms
step:1028/2330 train_time:62698ms step_avg:60.99ms
step:1029/2330 train_time:62757ms step_avg:60.99ms
step:1030/2330 train_time:62820ms step_avg:60.99ms
step:1031/2330 train_time:62881ms step_avg:60.99ms
step:1032/2330 train_time:62944ms step_avg:60.99ms
step:1033/2330 train_time:63004ms step_avg:60.99ms
step:1034/2330 train_time:63068ms step_avg:60.99ms
step:1035/2330 train_time:63129ms step_avg:60.99ms
step:1036/2330 train_time:63193ms step_avg:61.00ms
step:1037/2330 train_time:63253ms step_avg:61.00ms
step:1038/2330 train_time:63317ms step_avg:61.00ms
step:1039/2330 train_time:63378ms step_avg:61.00ms
step:1040/2330 train_time:63441ms step_avg:61.00ms
step:1041/2330 train_time:63501ms step_avg:61.00ms
step:1042/2330 train_time:63563ms step_avg:61.00ms
step:1043/2330 train_time:63623ms step_avg:61.00ms
step:1044/2330 train_time:63686ms step_avg:61.00ms
step:1045/2330 train_time:63745ms step_avg:61.00ms
step:1046/2330 train_time:63808ms step_avg:61.00ms
step:1047/2330 train_time:63868ms step_avg:61.00ms
step:1048/2330 train_time:63931ms step_avg:61.00ms
step:1049/2330 train_time:63991ms step_avg:61.00ms
step:1050/2330 train_time:64054ms step_avg:61.00ms
step:1051/2330 train_time:64114ms step_avg:61.00ms
step:1052/2330 train_time:64177ms step_avg:61.01ms
step:1053/2330 train_time:64238ms step_avg:61.00ms
step:1054/2330 train_time:64302ms step_avg:61.01ms
step:1055/2330 train_time:64362ms step_avg:61.01ms
step:1056/2330 train_time:64425ms step_avg:61.01ms
step:1057/2330 train_time:64485ms step_avg:61.01ms
step:1058/2330 train_time:64548ms step_avg:61.01ms
step:1059/2330 train_time:64608ms step_avg:61.01ms
step:1060/2330 train_time:64671ms step_avg:61.01ms
step:1061/2330 train_time:64730ms step_avg:61.01ms
step:1062/2330 train_time:64792ms step_avg:61.01ms
step:1063/2330 train_time:64852ms step_avg:61.01ms
step:1064/2330 train_time:64915ms step_avg:61.01ms
step:1065/2330 train_time:64974ms step_avg:61.01ms
step:1066/2330 train_time:65037ms step_avg:61.01ms
step:1067/2330 train_time:65098ms step_avg:61.01ms
step:1068/2330 train_time:65161ms step_avg:61.01ms
step:1069/2330 train_time:65221ms step_avg:61.01ms
step:1070/2330 train_time:65284ms step_avg:61.01ms
step:1071/2330 train_time:65344ms step_avg:61.01ms
step:1072/2330 train_time:65407ms step_avg:61.01ms
step:1073/2330 train_time:65467ms step_avg:61.01ms
step:1074/2330 train_time:65529ms step_avg:61.01ms
step:1075/2330 train_time:65589ms step_avg:61.01ms
step:1076/2330 train_time:65652ms step_avg:61.02ms
step:1077/2330 train_time:65712ms step_avg:61.01ms
step:1078/2330 train_time:65775ms step_avg:61.02ms
step:1079/2330 train_time:65834ms step_avg:61.01ms
step:1080/2330 train_time:65898ms step_avg:61.02ms
step:1081/2330 train_time:65958ms step_avg:61.02ms
step:1082/2330 train_time:66020ms step_avg:61.02ms
step:1083/2330 train_time:66080ms step_avg:61.02ms
step:1084/2330 train_time:66144ms step_avg:61.02ms
step:1085/2330 train_time:66204ms step_avg:61.02ms
step:1086/2330 train_time:66268ms step_avg:61.02ms
step:1087/2330 train_time:66327ms step_avg:61.02ms
step:1088/2330 train_time:66390ms step_avg:61.02ms
step:1089/2330 train_time:66450ms step_avg:61.02ms
step:1090/2330 train_time:66514ms step_avg:61.02ms
step:1091/2330 train_time:66573ms step_avg:61.02ms
step:1092/2330 train_time:66637ms step_avg:61.02ms
step:1093/2330 train_time:66697ms step_avg:61.02ms
step:1094/2330 train_time:66759ms step_avg:61.02ms
step:1095/2330 train_time:66819ms step_avg:61.02ms
step:1096/2330 train_time:66882ms step_avg:61.02ms
step:1097/2330 train_time:66942ms step_avg:61.02ms
step:1098/2330 train_time:67006ms step_avg:61.03ms
step:1099/2330 train_time:67066ms step_avg:61.02ms
step:1100/2330 train_time:67129ms step_avg:61.03ms
step:1101/2330 train_time:67190ms step_avg:61.03ms
step:1102/2330 train_time:67254ms step_avg:61.03ms
step:1103/2330 train_time:67314ms step_avg:61.03ms
step:1104/2330 train_time:67378ms step_avg:61.03ms
step:1105/2330 train_time:67438ms step_avg:61.03ms
step:1106/2330 train_time:67502ms step_avg:61.03ms
step:1107/2330 train_time:67562ms step_avg:61.03ms
step:1108/2330 train_time:67625ms step_avg:61.03ms
step:1109/2330 train_time:67685ms step_avg:61.03ms
step:1110/2330 train_time:67749ms step_avg:61.03ms
step:1111/2330 train_time:67809ms step_avg:61.03ms
step:1112/2330 train_time:67871ms step_avg:61.04ms
step:1113/2330 train_time:67932ms step_avg:61.03ms
step:1114/2330 train_time:67994ms step_avg:61.04ms
step:1115/2330 train_time:68055ms step_avg:61.04ms
step:1116/2330 train_time:68117ms step_avg:61.04ms
step:1117/2330 train_time:68177ms step_avg:61.04ms
step:1118/2330 train_time:68240ms step_avg:61.04ms
step:1119/2330 train_time:68299ms step_avg:61.04ms
step:1120/2330 train_time:68363ms step_avg:61.04ms
step:1121/2330 train_time:68423ms step_avg:61.04ms
step:1122/2330 train_time:68486ms step_avg:61.04ms
step:1123/2330 train_time:68546ms step_avg:61.04ms
step:1124/2330 train_time:68609ms step_avg:61.04ms
step:1125/2330 train_time:68669ms step_avg:61.04ms
step:1126/2330 train_time:68733ms step_avg:61.04ms
step:1127/2330 train_time:68793ms step_avg:61.04ms
step:1128/2330 train_time:68856ms step_avg:61.04ms
step:1129/2330 train_time:68915ms step_avg:61.04ms
step:1130/2330 train_time:68977ms step_avg:61.04ms
step:1131/2330 train_time:69037ms step_avg:61.04ms
step:1132/2330 train_time:69100ms step_avg:61.04ms
step:1133/2330 train_time:69160ms step_avg:61.04ms
step:1134/2330 train_time:69222ms step_avg:61.04ms
step:1135/2330 train_time:69282ms step_avg:61.04ms
step:1136/2330 train_time:69346ms step_avg:61.04ms
step:1137/2330 train_time:69406ms step_avg:61.04ms
step:1138/2330 train_time:69469ms step_avg:61.04ms
step:1139/2330 train_time:69528ms step_avg:61.04ms
step:1140/2330 train_time:69591ms step_avg:61.05ms
step:1141/2330 train_time:69651ms step_avg:61.04ms
step:1142/2330 train_time:69714ms step_avg:61.05ms
step:1143/2330 train_time:69774ms step_avg:61.04ms
step:1144/2330 train_time:69837ms step_avg:61.05ms
step:1145/2330 train_time:69896ms step_avg:61.04ms
step:1146/2330 train_time:69959ms step_avg:61.05ms
step:1147/2330 train_time:70019ms step_avg:61.05ms
step:1148/2330 train_time:70082ms step_avg:61.05ms
step:1149/2330 train_time:70142ms step_avg:61.05ms
step:1150/2330 train_time:70205ms step_avg:61.05ms
step:1151/2330 train_time:70265ms step_avg:61.05ms
step:1152/2330 train_time:70329ms step_avg:61.05ms
step:1153/2330 train_time:70389ms step_avg:61.05ms
step:1154/2330 train_time:70452ms step_avg:61.05ms
step:1155/2330 train_time:70512ms step_avg:61.05ms
step:1156/2330 train_time:70575ms step_avg:61.05ms
step:1157/2330 train_time:70635ms step_avg:61.05ms
step:1158/2330 train_time:70697ms step_avg:61.05ms
step:1159/2330 train_time:70757ms step_avg:61.05ms
step:1160/2330 train_time:70820ms step_avg:61.05ms
step:1161/2330 train_time:70880ms step_avg:61.05ms
step:1162/2330 train_time:70943ms step_avg:61.05ms
step:1163/2330 train_time:71003ms step_avg:61.05ms
step:1164/2330 train_time:71066ms step_avg:61.05ms
step:1165/2330 train_time:71126ms step_avg:61.05ms
step:1166/2330 train_time:71189ms step_avg:61.05ms
step:1167/2330 train_time:71250ms step_avg:61.05ms
step:1168/2330 train_time:71313ms step_avg:61.06ms
step:1169/2330 train_time:71373ms step_avg:61.05ms
step:1170/2330 train_time:71435ms step_avg:61.06ms
step:1171/2330 train_time:71495ms step_avg:61.05ms
step:1172/2330 train_time:71558ms step_avg:61.06ms
step:1173/2330 train_time:71618ms step_avg:61.06ms
step:1174/2330 train_time:71682ms step_avg:61.06ms
step:1175/2330 train_time:71742ms step_avg:61.06ms
step:1176/2330 train_time:71805ms step_avg:61.06ms
step:1177/2330 train_time:71864ms step_avg:61.06ms
step:1178/2330 train_time:71927ms step_avg:61.06ms
step:1179/2330 train_time:71987ms step_avg:61.06ms
step:1180/2330 train_time:72051ms step_avg:61.06ms
step:1181/2330 train_time:72110ms step_avg:61.06ms
step:1182/2330 train_time:72173ms step_avg:61.06ms
step:1183/2330 train_time:72232ms step_avg:61.06ms
step:1184/2330 train_time:72295ms step_avg:61.06ms
step:1185/2330 train_time:72355ms step_avg:61.06ms
step:1186/2330 train_time:72418ms step_avg:61.06ms
step:1187/2330 train_time:72478ms step_avg:61.06ms
step:1188/2330 train_time:72541ms step_avg:61.06ms
step:1189/2330 train_time:72601ms step_avg:61.06ms
step:1190/2330 train_time:72664ms step_avg:61.06ms
step:1191/2330 train_time:72724ms step_avg:61.06ms
step:1192/2330 train_time:72788ms step_avg:61.06ms
step:1193/2330 train_time:72848ms step_avg:61.06ms
step:1194/2330 train_time:72911ms step_avg:61.06ms
step:1195/2330 train_time:72971ms step_avg:61.06ms
step:1196/2330 train_time:73034ms step_avg:61.06ms
step:1197/2330 train_time:73093ms step_avg:61.06ms
step:1198/2330 train_time:73156ms step_avg:61.07ms
step:1199/2330 train_time:73216ms step_avg:61.06ms
step:1200/2330 train_time:73279ms step_avg:61.07ms
step:1201/2330 train_time:73339ms step_avg:61.06ms
step:1202/2330 train_time:73402ms step_avg:61.07ms
step:1203/2330 train_time:73462ms step_avg:61.07ms
step:1204/2330 train_time:73525ms step_avg:61.07ms
step:1205/2330 train_time:73585ms step_avg:61.07ms
step:1206/2330 train_time:73648ms step_avg:61.07ms
step:1207/2330 train_time:73708ms step_avg:61.07ms
step:1208/2330 train_time:73770ms step_avg:61.07ms
step:1209/2330 train_time:73830ms step_avg:61.07ms
step:1210/2330 train_time:73893ms step_avg:61.07ms
step:1211/2330 train_time:73953ms step_avg:61.07ms
step:1212/2330 train_time:74016ms step_avg:61.07ms
step:1213/2330 train_time:74076ms step_avg:61.07ms
step:1214/2330 train_time:74139ms step_avg:61.07ms
step:1215/2330 train_time:74199ms step_avg:61.07ms
step:1216/2330 train_time:74263ms step_avg:61.07ms
step:1217/2330 train_time:74323ms step_avg:61.07ms
step:1218/2330 train_time:74386ms step_avg:61.07ms
step:1219/2330 train_time:74445ms step_avg:61.07ms
step:1220/2330 train_time:74509ms step_avg:61.07ms
step:1221/2330 train_time:74569ms step_avg:61.07ms
step:1222/2330 train_time:74632ms step_avg:61.07ms
step:1223/2330 train_time:74692ms step_avg:61.07ms
step:1224/2330 train_time:74755ms step_avg:61.07ms
step:1225/2330 train_time:74814ms step_avg:61.07ms
step:1226/2330 train_time:74877ms step_avg:61.07ms
step:1227/2330 train_time:74936ms step_avg:61.07ms
step:1228/2330 train_time:74999ms step_avg:61.07ms
step:1229/2330 train_time:75059ms step_avg:61.07ms
step:1230/2330 train_time:75122ms step_avg:61.07ms
step:1231/2330 train_time:75182ms step_avg:61.07ms
step:1232/2330 train_time:75246ms step_avg:61.08ms
step:1233/2330 train_time:75306ms step_avg:61.08ms
step:1234/2330 train_time:75368ms step_avg:61.08ms
step:1235/2330 train_time:75428ms step_avg:61.08ms
step:1236/2330 train_time:75491ms step_avg:61.08ms
step:1237/2330 train_time:75550ms step_avg:61.08ms
step:1238/2330 train_time:75613ms step_avg:61.08ms
step:1239/2330 train_time:75673ms step_avg:61.08ms
step:1240/2330 train_time:75735ms step_avg:61.08ms
step:1241/2330 train_time:75796ms step_avg:61.08ms
step:1242/2330 train_time:75859ms step_avg:61.08ms
step:1243/2330 train_time:75918ms step_avg:61.08ms
step:1244/2330 train_time:75981ms step_avg:61.08ms
step:1245/2330 train_time:76041ms step_avg:61.08ms
step:1246/2330 train_time:76104ms step_avg:61.08ms
step:1247/2330 train_time:76164ms step_avg:61.08ms
step:1248/2330 train_time:76228ms step_avg:61.08ms
step:1249/2330 train_time:76288ms step_avg:61.08ms
step:1250/2330 train_time:76351ms step_avg:61.08ms
step:1250/2330 val_loss:3.5334 train_time:76416ms step_avg:61.13ms
step:1251/2330 train_time:76438ms step_avg:61.10ms
step:1252/2330 train_time:76478ms step_avg:61.08ms
step:1253/2330 train_time:76542ms step_avg:61.09ms
step:1254/2330 train_time:76605ms step_avg:61.09ms
step:1255/2330 train_time:76665ms step_avg:61.09ms
step:1256/2330 train_time:76729ms step_avg:61.09ms
step:1257/2330 train_time:76788ms step_avg:61.09ms
step:1258/2330 train_time:76851ms step_avg:61.09ms
step:1259/2330 train_time:76911ms step_avg:61.09ms
step:1260/2330 train_time:76974ms step_avg:61.09ms
step:1261/2330 train_time:77034ms step_avg:61.09ms
step:1262/2330 train_time:77096ms step_avg:61.09ms
step:1263/2330 train_time:77156ms step_avg:61.09ms
step:1264/2330 train_time:77218ms step_avg:61.09ms
step:1265/2330 train_time:77278ms step_avg:61.09ms
step:1266/2330 train_time:77340ms step_avg:61.09ms
step:1267/2330 train_time:77400ms step_avg:61.09ms
step:1268/2330 train_time:77464ms step_avg:61.09ms
step:1269/2330 train_time:77526ms step_avg:61.09ms
step:1270/2330 train_time:77590ms step_avg:61.09ms
step:1271/2330 train_time:77650ms step_avg:61.09ms
step:1272/2330 train_time:77713ms step_avg:61.10ms
step:1273/2330 train_time:77773ms step_avg:61.09ms
step:1274/2330 train_time:77836ms step_avg:61.10ms
step:1275/2330 train_time:77896ms step_avg:61.09ms
step:1276/2330 train_time:77959ms step_avg:61.10ms
step:1277/2330 train_time:78018ms step_avg:61.09ms
step:1278/2330 train_time:78081ms step_avg:61.10ms
step:1279/2330 train_time:78140ms step_avg:61.09ms
step:1280/2330 train_time:78202ms step_avg:61.10ms
step:1281/2330 train_time:78262ms step_avg:61.09ms
step:1282/2330 train_time:78326ms step_avg:61.10ms
step:1283/2330 train_time:78387ms step_avg:61.10ms
step:1284/2330 train_time:78450ms step_avg:61.10ms
step:1285/2330 train_time:78511ms step_avg:61.10ms
step:1286/2330 train_time:78575ms step_avg:61.10ms
step:1287/2330 train_time:78636ms step_avg:61.10ms
step:1288/2330 train_time:78699ms step_avg:61.10ms
step:1289/2330 train_time:78759ms step_avg:61.10ms
step:1290/2330 train_time:78822ms step_avg:61.10ms
step:1291/2330 train_time:78882ms step_avg:61.10ms
step:1292/2330 train_time:78945ms step_avg:61.10ms
step:1293/2330 train_time:79004ms step_avg:61.10ms
step:1294/2330 train_time:79067ms step_avg:61.10ms
step:1295/2330 train_time:79127ms step_avg:61.10ms
step:1296/2330 train_time:79190ms step_avg:61.10ms
step:1297/2330 train_time:79250ms step_avg:61.10ms
step:1298/2330 train_time:79313ms step_avg:61.10ms
step:1299/2330 train_time:79373ms step_avg:61.10ms
step:1300/2330 train_time:79437ms step_avg:61.11ms
step:1301/2330 train_time:79497ms step_avg:61.10ms
step:1302/2330 train_time:79560ms step_avg:61.11ms
step:1303/2330 train_time:79620ms step_avg:61.11ms
step:1304/2330 train_time:79683ms step_avg:61.11ms
step:1305/2330 train_time:79744ms step_avg:61.11ms
step:1306/2330 train_time:79807ms step_avg:61.11ms
step:1307/2330 train_time:79867ms step_avg:61.11ms
step:1308/2330 train_time:79929ms step_avg:61.11ms
step:1309/2330 train_time:79989ms step_avg:61.11ms
step:1310/2330 train_time:80052ms step_avg:61.11ms
step:1311/2330 train_time:80111ms step_avg:61.11ms
step:1312/2330 train_time:80175ms step_avg:61.11ms
step:1313/2330 train_time:80235ms step_avg:61.11ms
step:1314/2330 train_time:80298ms step_avg:61.11ms
step:1315/2330 train_time:80357ms step_avg:61.11ms
step:1316/2330 train_time:80421ms step_avg:61.11ms
step:1317/2330 train_time:80481ms step_avg:61.11ms
step:1318/2330 train_time:80544ms step_avg:61.11ms
step:1319/2330 train_time:80604ms step_avg:61.11ms
step:1320/2330 train_time:80668ms step_avg:61.11ms
step:1321/2330 train_time:80729ms step_avg:61.11ms
step:1322/2330 train_time:80792ms step_avg:61.11ms
step:1323/2330 train_time:80852ms step_avg:61.11ms
step:1324/2330 train_time:80915ms step_avg:61.11ms
step:1325/2330 train_time:80974ms step_avg:61.11ms
step:1326/2330 train_time:81037ms step_avg:61.11ms
step:1327/2330 train_time:81096ms step_avg:61.11ms
step:1328/2330 train_time:81159ms step_avg:61.11ms
step:1329/2330 train_time:81218ms step_avg:61.11ms
step:1330/2330 train_time:81281ms step_avg:61.11ms
step:1331/2330 train_time:81341ms step_avg:61.11ms
step:1332/2330 train_time:81404ms step_avg:61.11ms
step:1333/2330 train_time:81464ms step_avg:61.11ms
step:1334/2330 train_time:81528ms step_avg:61.12ms
step:1335/2330 train_time:81588ms step_avg:61.11ms
step:1336/2330 train_time:81651ms step_avg:61.12ms
step:1337/2330 train_time:81711ms step_avg:61.12ms
step:1338/2330 train_time:81775ms step_avg:61.12ms
step:1339/2330 train_time:81836ms step_avg:61.12ms
step:1340/2330 train_time:81899ms step_avg:61.12ms
step:1341/2330 train_time:81958ms step_avg:61.12ms
step:1342/2330 train_time:82021ms step_avg:61.12ms
step:1343/2330 train_time:82081ms step_avg:61.12ms
step:1344/2330 train_time:82143ms step_avg:61.12ms
step:1345/2330 train_time:82202ms step_avg:61.12ms
step:1346/2330 train_time:82266ms step_avg:61.12ms
step:1347/2330 train_time:82326ms step_avg:61.12ms
step:1348/2330 train_time:82389ms step_avg:61.12ms
step:1349/2330 train_time:82450ms step_avg:61.12ms
step:1350/2330 train_time:82513ms step_avg:61.12ms
step:1351/2330 train_time:82574ms step_avg:61.12ms
step:1352/2330 train_time:82637ms step_avg:61.12ms
step:1353/2330 train_time:82696ms step_avg:61.12ms
step:1354/2330 train_time:82759ms step_avg:61.12ms
step:1355/2330 train_time:82819ms step_avg:61.12ms
step:1356/2330 train_time:82883ms step_avg:61.12ms
step:1357/2330 train_time:82942ms step_avg:61.12ms
step:1358/2330 train_time:83005ms step_avg:61.12ms
step:1359/2330 train_time:83065ms step_avg:61.12ms
step:1360/2330 train_time:83128ms step_avg:61.12ms
step:1361/2330 train_time:83187ms step_avg:61.12ms
step:1362/2330 train_time:83250ms step_avg:61.12ms
step:1363/2330 train_time:83310ms step_avg:61.12ms
step:1364/2330 train_time:83373ms step_avg:61.12ms
step:1365/2330 train_time:83433ms step_avg:61.12ms
step:1366/2330 train_time:83497ms step_avg:61.12ms
step:1367/2330 train_time:83556ms step_avg:61.12ms
step:1368/2330 train_time:83619ms step_avg:61.13ms
step:1369/2330 train_time:83679ms step_avg:61.12ms
step:1370/2330 train_time:83742ms step_avg:61.13ms
step:1371/2330 train_time:83802ms step_avg:61.12ms
step:1372/2330 train_time:83865ms step_avg:61.13ms
step:1373/2330 train_time:83925ms step_avg:61.13ms
step:1374/2330 train_time:83989ms step_avg:61.13ms
step:1375/2330 train_time:84048ms step_avg:61.13ms
step:1376/2330 train_time:84112ms step_avg:61.13ms
step:1377/2330 train_time:84172ms step_avg:61.13ms
step:1378/2330 train_time:84235ms step_avg:61.13ms
step:1379/2330 train_time:84295ms step_avg:61.13ms
step:1380/2330 train_time:84358ms step_avg:61.13ms
step:1381/2330 train_time:84417ms step_avg:61.13ms
step:1382/2330 train_time:84480ms step_avg:61.13ms
step:1383/2330 train_time:84540ms step_avg:61.13ms
step:1384/2330 train_time:84603ms step_avg:61.13ms
step:1385/2330 train_time:84663ms step_avg:61.13ms
step:1386/2330 train_time:84726ms step_avg:61.13ms
step:1387/2330 train_time:84786ms step_avg:61.13ms
step:1388/2330 train_time:84849ms step_avg:61.13ms
step:1389/2330 train_time:84910ms step_avg:61.13ms
step:1390/2330 train_time:84973ms step_avg:61.13ms
step:1391/2330 train_time:85033ms step_avg:61.13ms
step:1392/2330 train_time:85096ms step_avg:61.13ms
step:1393/2330 train_time:85156ms step_avg:61.13ms
step:1394/2330 train_time:85219ms step_avg:61.13ms
step:1395/2330 train_time:85279ms step_avg:61.13ms
step:1396/2330 train_time:85341ms step_avg:61.13ms
step:1397/2330 train_time:85401ms step_avg:61.13ms
step:1398/2330 train_time:85464ms step_avg:61.13ms
step:1399/2330 train_time:85524ms step_avg:61.13ms
step:1400/2330 train_time:85588ms step_avg:61.13ms
step:1401/2330 train_time:85648ms step_avg:61.13ms
step:1402/2330 train_time:85710ms step_avg:61.13ms
step:1403/2330 train_time:85771ms step_avg:61.13ms
step:1404/2330 train_time:85834ms step_avg:61.13ms
step:1405/2330 train_time:85893ms step_avg:61.13ms
step:1406/2330 train_time:85956ms step_avg:61.14ms
step:1407/2330 train_time:86017ms step_avg:61.13ms
step:1408/2330 train_time:86080ms step_avg:61.14ms
step:1409/2330 train_time:86139ms step_avg:61.13ms
step:1410/2330 train_time:86202ms step_avg:61.14ms
step:1411/2330 train_time:86262ms step_avg:61.14ms
step:1412/2330 train_time:86325ms step_avg:61.14ms
step:1413/2330 train_time:86385ms step_avg:61.14ms
step:1414/2330 train_time:86448ms step_avg:61.14ms
step:1415/2330 train_time:86508ms step_avg:61.14ms
step:1416/2330 train_time:86572ms step_avg:61.14ms
step:1417/2330 train_time:86632ms step_avg:61.14ms
step:1418/2330 train_time:86695ms step_avg:61.14ms
step:1419/2330 train_time:86755ms step_avg:61.14ms
step:1420/2330 train_time:86819ms step_avg:61.14ms
step:1421/2330 train_time:86878ms step_avg:61.14ms
step:1422/2330 train_time:86941ms step_avg:61.14ms
step:1423/2330 train_time:87000ms step_avg:61.14ms
step:1424/2330 train_time:87063ms step_avg:61.14ms
step:1425/2330 train_time:87123ms step_avg:61.14ms
step:1426/2330 train_time:87187ms step_avg:61.14ms
step:1427/2330 train_time:87246ms step_avg:61.14ms
step:1428/2330 train_time:87309ms step_avg:61.14ms
step:1429/2330 train_time:87369ms step_avg:61.14ms
step:1430/2330 train_time:87433ms step_avg:61.14ms
step:1431/2330 train_time:87493ms step_avg:61.14ms
step:1432/2330 train_time:87556ms step_avg:61.14ms
step:1433/2330 train_time:87616ms step_avg:61.14ms
step:1434/2330 train_time:87680ms step_avg:61.14ms
step:1435/2330 train_time:87740ms step_avg:61.14ms
step:1436/2330 train_time:87803ms step_avg:61.14ms
step:1437/2330 train_time:87863ms step_avg:61.14ms
step:1438/2330 train_time:87927ms step_avg:61.15ms
step:1439/2330 train_time:87986ms step_avg:61.14ms
step:1440/2330 train_time:88049ms step_avg:61.15ms
step:1441/2330 train_time:88109ms step_avg:61.14ms
step:1442/2330 train_time:88172ms step_avg:61.15ms
step:1443/2330 train_time:88232ms step_avg:61.15ms
step:1444/2330 train_time:88295ms step_avg:61.15ms
step:1445/2330 train_time:88355ms step_avg:61.15ms
step:1446/2330 train_time:88418ms step_avg:61.15ms
step:1447/2330 train_time:88477ms step_avg:61.15ms
step:1448/2330 train_time:88540ms step_avg:61.15ms
step:1449/2330 train_time:88600ms step_avg:61.15ms
step:1450/2330 train_time:88663ms step_avg:61.15ms
step:1451/2330 train_time:88723ms step_avg:61.15ms
step:1452/2330 train_time:88785ms step_avg:61.15ms
step:1453/2330 train_time:88845ms step_avg:61.15ms
step:1454/2330 train_time:88908ms step_avg:61.15ms
step:1455/2330 train_time:88968ms step_avg:61.15ms
step:1456/2330 train_time:89031ms step_avg:61.15ms
step:1457/2330 train_time:89092ms step_avg:61.15ms
step:1458/2330 train_time:89155ms step_avg:61.15ms
step:1459/2330 train_time:89215ms step_avg:61.15ms
step:1460/2330 train_time:89278ms step_avg:61.15ms
step:1461/2330 train_time:89338ms step_avg:61.15ms
step:1462/2330 train_time:89401ms step_avg:61.15ms
step:1463/2330 train_time:89460ms step_avg:61.15ms
step:1464/2330 train_time:89523ms step_avg:61.15ms
step:1465/2330 train_time:89582ms step_avg:61.15ms
step:1466/2330 train_time:89645ms step_avg:61.15ms
step:1467/2330 train_time:89705ms step_avg:61.15ms
step:1468/2330 train_time:89768ms step_avg:61.15ms
step:1469/2330 train_time:89828ms step_avg:61.15ms
step:1470/2330 train_time:89891ms step_avg:61.15ms
step:1471/2330 train_time:89951ms step_avg:61.15ms
step:1472/2330 train_time:90015ms step_avg:61.15ms
step:1473/2330 train_time:90076ms step_avg:61.15ms
step:1474/2330 train_time:90139ms step_avg:61.15ms
step:1475/2330 train_time:90199ms step_avg:61.15ms
step:1476/2330 train_time:90261ms step_avg:61.15ms
step:1477/2330 train_time:90321ms step_avg:61.15ms
step:1478/2330 train_time:90384ms step_avg:61.15ms
step:1479/2330 train_time:90445ms step_avg:61.15ms
step:1480/2330 train_time:90508ms step_avg:61.15ms
step:1481/2330 train_time:90568ms step_avg:61.15ms
step:1482/2330 train_time:90632ms step_avg:61.16ms
step:1483/2330 train_time:90692ms step_avg:61.15ms
step:1484/2330 train_time:90754ms step_avg:61.16ms
step:1485/2330 train_time:90814ms step_avg:61.15ms
step:1486/2330 train_time:90877ms step_avg:61.16ms
step:1487/2330 train_time:90937ms step_avg:61.15ms
step:1488/2330 train_time:90999ms step_avg:61.16ms
step:1489/2330 train_time:91059ms step_avg:61.15ms
step:1490/2330 train_time:91122ms step_avg:61.16ms
step:1491/2330 train_time:91182ms step_avg:61.16ms
step:1492/2330 train_time:91245ms step_avg:61.16ms
step:1493/2330 train_time:91304ms step_avg:61.15ms
step:1494/2330 train_time:91368ms step_avg:61.16ms
step:1495/2330 train_time:91429ms step_avg:61.16ms
step:1496/2330 train_time:91492ms step_avg:61.16ms
step:1497/2330 train_time:91552ms step_avg:61.16ms
step:1498/2330 train_time:91615ms step_avg:61.16ms
step:1499/2330 train_time:91675ms step_avg:61.16ms
step:1500/2330 train_time:91738ms step_avg:61.16ms
step:1500/2330 val_loss:3.4898 train_time:91802ms step_avg:61.20ms
step:1501/2330 train_time:91824ms step_avg:61.18ms
step:1502/2330 train_time:91866ms step_avg:61.16ms
step:1503/2330 train_time:91930ms step_avg:61.16ms
step:1504/2330 train_time:91994ms step_avg:61.17ms
step:1505/2330 train_time:92054ms step_avg:61.17ms
step:1506/2330 train_time:92117ms step_avg:61.17ms
step:1507/2330 train_time:92176ms step_avg:61.17ms
step:1508/2330 train_time:92239ms step_avg:61.17ms
step:1509/2330 train_time:92298ms step_avg:61.17ms
step:1510/2330 train_time:92361ms step_avg:61.17ms
step:1511/2330 train_time:92421ms step_avg:61.17ms
step:1512/2330 train_time:92483ms step_avg:61.17ms
step:1513/2330 train_time:92542ms step_avg:61.16ms
step:1514/2330 train_time:92605ms step_avg:61.17ms
step:1515/2330 train_time:92663ms step_avg:61.16ms
step:1516/2330 train_time:92726ms step_avg:61.17ms
step:1517/2330 train_time:92788ms step_avg:61.17ms
step:1518/2330 train_time:92853ms step_avg:61.17ms
step:1519/2330 train_time:92915ms step_avg:61.17ms
step:1520/2330 train_time:92978ms step_avg:61.17ms
step:1521/2330 train_time:93038ms step_avg:61.17ms
step:1522/2330 train_time:93101ms step_avg:61.17ms
step:1523/2330 train_time:93161ms step_avg:61.17ms
step:1524/2330 train_time:93223ms step_avg:61.17ms
step:1525/2330 train_time:93283ms step_avg:61.17ms
step:1526/2330 train_time:93345ms step_avg:61.17ms
step:1527/2330 train_time:93405ms step_avg:61.17ms
step:1528/2330 train_time:93468ms step_avg:61.17ms
step:1529/2330 train_time:93527ms step_avg:61.17ms
step:1530/2330 train_time:93590ms step_avg:61.17ms
step:1531/2330 train_time:93650ms step_avg:61.17ms
step:1532/2330 train_time:93714ms step_avg:61.17ms
step:1533/2330 train_time:93774ms step_avg:61.17ms
step:1534/2330 train_time:93838ms step_avg:61.17ms
step:1535/2330 train_time:93899ms step_avg:61.17ms
step:1536/2330 train_time:93963ms step_avg:61.17ms
step:1537/2330 train_time:94024ms step_avg:61.17ms
step:1538/2330 train_time:94087ms step_avg:61.17ms
step:1539/2330 train_time:94148ms step_avg:61.17ms
step:1540/2330 train_time:94211ms step_avg:61.18ms
step:1541/2330 train_time:94271ms step_avg:61.18ms
step:1542/2330 train_time:94335ms step_avg:61.18ms
step:1543/2330 train_time:94396ms step_avg:61.18ms
step:1544/2330 train_time:94459ms step_avg:61.18ms
step:1545/2330 train_time:94518ms step_avg:61.18ms
step:1546/2330 train_time:94581ms step_avg:61.18ms
step:1547/2330 train_time:94641ms step_avg:61.18ms
step:1548/2330 train_time:94704ms step_avg:61.18ms
step:1549/2330 train_time:94765ms step_avg:61.18ms
step:1550/2330 train_time:94829ms step_avg:61.18ms
step:1551/2330 train_time:94890ms step_avg:61.18ms
step:1552/2330 train_time:94955ms step_avg:61.18ms
step:1553/2330 train_time:95015ms step_avg:61.18ms
step:1554/2330 train_time:95079ms step_avg:61.18ms
step:1555/2330 train_time:95139ms step_avg:61.18ms
step:1556/2330 train_time:95203ms step_avg:61.18ms
step:1557/2330 train_time:95263ms step_avg:61.18ms
step:1558/2330 train_time:95326ms step_avg:61.19ms
step:1559/2330 train_time:95387ms step_avg:61.18ms
step:1560/2330 train_time:95450ms step_avg:61.19ms
step:1561/2330 train_time:95511ms step_avg:61.19ms
step:1562/2330 train_time:95575ms step_avg:61.19ms
step:1563/2330 train_time:95635ms step_avg:61.19ms
step:1564/2330 train_time:95699ms step_avg:61.19ms
step:1565/2330 train_time:95759ms step_avg:61.19ms
step:1566/2330 train_time:95822ms step_avg:61.19ms
step:1567/2330 train_time:95883ms step_avg:61.19ms
step:1568/2330 train_time:95946ms step_avg:61.19ms
step:1569/2330 train_time:96007ms step_avg:61.19ms
step:1570/2330 train_time:96072ms step_avg:61.19ms
step:1571/2330 train_time:96132ms step_avg:61.19ms
step:1572/2330 train_time:96196ms step_avg:61.19ms
step:1573/2330 train_time:96256ms step_avg:61.19ms
step:1574/2330 train_time:96320ms step_avg:61.19ms
step:1575/2330 train_time:96381ms step_avg:61.19ms
step:1576/2330 train_time:96445ms step_avg:61.20ms
step:1577/2330 train_time:96506ms step_avg:61.20ms
step:1578/2330 train_time:96569ms step_avg:61.20ms
step:1579/2330 train_time:96629ms step_avg:61.20ms
step:1580/2330 train_time:96693ms step_avg:61.20ms
step:1581/2330 train_time:96753ms step_avg:61.20ms
step:1582/2330 train_time:96817ms step_avg:61.20ms
step:1583/2330 train_time:96878ms step_avg:61.20ms
step:1584/2330 train_time:96941ms step_avg:61.20ms
step:1585/2330 train_time:97002ms step_avg:61.20ms
step:1586/2330 train_time:97066ms step_avg:61.20ms
step:1587/2330 train_time:97126ms step_avg:61.20ms
step:1588/2330 train_time:97190ms step_avg:61.20ms
step:1589/2330 train_time:97251ms step_avg:61.20ms
step:1590/2330 train_time:97314ms step_avg:61.20ms
step:1591/2330 train_time:97375ms step_avg:61.20ms
step:1592/2330 train_time:97438ms step_avg:61.20ms
step:1593/2330 train_time:97499ms step_avg:61.20ms
step:1594/2330 train_time:97562ms step_avg:61.21ms
step:1595/2330 train_time:97622ms step_avg:61.20ms
step:1596/2330 train_time:97685ms step_avg:61.21ms
step:1597/2330 train_time:97746ms step_avg:61.21ms
step:1598/2330 train_time:97809ms step_avg:61.21ms
step:1599/2330 train_time:97870ms step_avg:61.21ms
step:1600/2330 train_time:97933ms step_avg:61.21ms
step:1601/2330 train_time:97994ms step_avg:61.21ms
step:1602/2330 train_time:98057ms step_avg:61.21ms
step:1603/2330 train_time:98118ms step_avg:61.21ms
step:1604/2330 train_time:98182ms step_avg:61.21ms
step:1605/2330 train_time:98242ms step_avg:61.21ms
step:1606/2330 train_time:98305ms step_avg:61.21ms
step:1607/2330 train_time:98366ms step_avg:61.21ms
step:1608/2330 train_time:98430ms step_avg:61.21ms
step:1609/2330 train_time:98490ms step_avg:61.21ms
step:1610/2330 train_time:98554ms step_avg:61.21ms
step:1611/2330 train_time:98613ms step_avg:61.21ms
step:1612/2330 train_time:98677ms step_avg:61.21ms
step:1613/2330 train_time:98738ms step_avg:61.21ms
step:1614/2330 train_time:98801ms step_avg:61.22ms
step:1615/2330 train_time:98861ms step_avg:61.21ms
step:1616/2330 train_time:98924ms step_avg:61.22ms
step:1617/2330 train_time:98985ms step_avg:61.22ms
step:1618/2330 train_time:99048ms step_avg:61.22ms
step:1619/2330 train_time:99109ms step_avg:61.22ms
step:1620/2330 train_time:99174ms step_avg:61.22ms
step:1621/2330 train_time:99234ms step_avg:61.22ms
step:1622/2330 train_time:99298ms step_avg:61.22ms
step:1623/2330 train_time:99358ms step_avg:61.22ms
step:1624/2330 train_time:99421ms step_avg:61.22ms
step:1625/2330 train_time:99482ms step_avg:61.22ms
step:1626/2330 train_time:99545ms step_avg:61.22ms
step:1627/2330 train_time:99606ms step_avg:61.22ms
step:1628/2330 train_time:99668ms step_avg:61.22ms
step:1629/2330 train_time:99729ms step_avg:61.22ms
step:1630/2330 train_time:99793ms step_avg:61.22ms
step:1631/2330 train_time:99853ms step_avg:61.22ms
step:1632/2330 train_time:99916ms step_avg:61.22ms
step:1633/2330 train_time:99977ms step_avg:61.22ms
step:1634/2330 train_time:100041ms step_avg:61.22ms
step:1635/2330 train_time:100101ms step_avg:61.22ms
step:1636/2330 train_time:100165ms step_avg:61.23ms
step:1637/2330 train_time:100225ms step_avg:61.22ms
step:1638/2330 train_time:100289ms step_avg:61.23ms
step:1639/2330 train_time:100351ms step_avg:61.23ms
step:1640/2330 train_time:100415ms step_avg:61.23ms
step:1641/2330 train_time:100475ms step_avg:61.23ms
step:1642/2330 train_time:100539ms step_avg:61.23ms
step:1643/2330 train_time:100600ms step_avg:61.23ms
step:1644/2330 train_time:100664ms step_avg:61.23ms
step:1645/2330 train_time:100723ms step_avg:61.23ms
step:1646/2330 train_time:100787ms step_avg:61.23ms
step:1647/2330 train_time:100847ms step_avg:61.23ms
step:1648/2330 train_time:100910ms step_avg:61.23ms
step:1649/2330 train_time:100971ms step_avg:61.23ms
step:1650/2330 train_time:101034ms step_avg:61.23ms
step:1651/2330 train_time:101095ms step_avg:61.23ms
step:1652/2330 train_time:101158ms step_avg:61.23ms
step:1653/2330 train_time:101219ms step_avg:61.23ms
step:1654/2330 train_time:101282ms step_avg:61.23ms
step:1655/2330 train_time:101343ms step_avg:61.23ms
step:1656/2330 train_time:101407ms step_avg:61.24ms
step:1657/2330 train_time:101468ms step_avg:61.24ms
step:1658/2330 train_time:101532ms step_avg:61.24ms
step:1659/2330 train_time:101592ms step_avg:61.24ms
step:1660/2330 train_time:101655ms step_avg:61.24ms
step:1661/2330 train_time:101716ms step_avg:61.24ms
step:1662/2330 train_time:101780ms step_avg:61.24ms
step:1663/2330 train_time:101840ms step_avg:61.24ms
step:1664/2330 train_time:101903ms step_avg:61.24ms
step:1665/2330 train_time:101963ms step_avg:61.24ms
step:1666/2330 train_time:102027ms step_avg:61.24ms
step:1667/2330 train_time:102087ms step_avg:61.24ms
step:1668/2330 train_time:102151ms step_avg:61.24ms
step:1669/2330 train_time:102212ms step_avg:61.24ms
step:1670/2330 train_time:102275ms step_avg:61.24ms
step:1671/2330 train_time:102335ms step_avg:61.24ms
step:1672/2330 train_time:102399ms step_avg:61.24ms
step:1673/2330 train_time:102458ms step_avg:61.24ms
step:1674/2330 train_time:102522ms step_avg:61.24ms
step:1675/2330 train_time:102583ms step_avg:61.24ms
step:1676/2330 train_time:102646ms step_avg:61.24ms
step:1677/2330 train_time:102707ms step_avg:61.24ms
step:1678/2330 train_time:102770ms step_avg:61.25ms
step:1679/2330 train_time:102830ms step_avg:61.24ms
step:1680/2330 train_time:102894ms step_avg:61.25ms
step:1681/2330 train_time:102954ms step_avg:61.25ms
step:1682/2330 train_time:103018ms step_avg:61.25ms
step:1683/2330 train_time:103078ms step_avg:61.25ms
step:1684/2330 train_time:103142ms step_avg:61.25ms
step:1685/2330 train_time:103202ms step_avg:61.25ms
step:1686/2330 train_time:103265ms step_avg:61.25ms
step:1687/2330 train_time:103326ms step_avg:61.25ms
step:1688/2330 train_time:103389ms step_avg:61.25ms
step:1689/2330 train_time:103450ms step_avg:61.25ms
step:1690/2330 train_time:103513ms step_avg:61.25ms
step:1691/2330 train_time:103574ms step_avg:61.25ms
step:1692/2330 train_time:103637ms step_avg:61.25ms
step:1693/2330 train_time:103697ms step_avg:61.25ms
step:1694/2330 train_time:103761ms step_avg:61.25ms
step:1695/2330 train_time:103821ms step_avg:61.25ms
step:1696/2330 train_time:103884ms step_avg:61.25ms
step:1697/2330 train_time:103944ms step_avg:61.25ms
step:1698/2330 train_time:104008ms step_avg:61.25ms
step:1699/2330 train_time:104068ms step_avg:61.25ms
step:1700/2330 train_time:104132ms step_avg:61.25ms
step:1701/2330 train_time:104193ms step_avg:61.25ms
step:1702/2330 train_time:104257ms step_avg:61.26ms
step:1703/2330 train_time:104317ms step_avg:61.26ms
step:1704/2330 train_time:104381ms step_avg:61.26ms
step:1705/2330 train_time:104441ms step_avg:61.26ms
step:1706/2330 train_time:104504ms step_avg:61.26ms
step:1707/2330 train_time:104565ms step_avg:61.26ms
step:1708/2330 train_time:104629ms step_avg:61.26ms
step:1709/2330 train_time:104689ms step_avg:61.26ms
step:1710/2330 train_time:104753ms step_avg:61.26ms
step:1711/2330 train_time:104813ms step_avg:61.26ms
step:1712/2330 train_time:104877ms step_avg:61.26ms
step:1713/2330 train_time:104937ms step_avg:61.26ms
step:1714/2330 train_time:105001ms step_avg:61.26ms
step:1715/2330 train_time:105061ms step_avg:61.26ms
step:1716/2330 train_time:105124ms step_avg:61.26ms
step:1717/2330 train_time:105184ms step_avg:61.26ms
step:1718/2330 train_time:105247ms step_avg:61.26ms
step:1719/2330 train_time:105308ms step_avg:61.26ms
step:1720/2330 train_time:105371ms step_avg:61.26ms
step:1721/2330 train_time:105431ms step_avg:61.26ms
step:1722/2330 train_time:105495ms step_avg:61.26ms
step:1723/2330 train_time:105555ms step_avg:61.26ms
step:1724/2330 train_time:105619ms step_avg:61.26ms
step:1725/2330 train_time:105679ms step_avg:61.26ms
step:1726/2330 train_time:105743ms step_avg:61.26ms
step:1727/2330 train_time:105804ms step_avg:61.26ms
step:1728/2330 train_time:105867ms step_avg:61.27ms
step:1729/2330 train_time:105928ms step_avg:61.27ms
step:1730/2330 train_time:105992ms step_avg:61.27ms
step:1731/2330 train_time:106053ms step_avg:61.27ms
step:1732/2330 train_time:106116ms step_avg:61.27ms
step:1733/2330 train_time:106176ms step_avg:61.27ms
step:1734/2330 train_time:106240ms step_avg:61.27ms
step:1735/2330 train_time:106300ms step_avg:61.27ms
step:1736/2330 train_time:106364ms step_avg:61.27ms
step:1737/2330 train_time:106424ms step_avg:61.27ms
step:1738/2330 train_time:106487ms step_avg:61.27ms
step:1739/2330 train_time:106548ms step_avg:61.27ms
step:1740/2330 train_time:106611ms step_avg:61.27ms
step:1741/2330 train_time:106672ms step_avg:61.27ms
step:1742/2330 train_time:106736ms step_avg:61.27ms
step:1743/2330 train_time:106797ms step_avg:61.27ms
step:1744/2330 train_time:106861ms step_avg:61.27ms
step:1745/2330 train_time:106921ms step_avg:61.27ms
step:1746/2330 train_time:106985ms step_avg:61.27ms
step:1747/2330 train_time:107045ms step_avg:61.27ms
step:1748/2330 train_time:107109ms step_avg:61.27ms
step:1749/2330 train_time:107169ms step_avg:61.27ms
step:1750/2330 train_time:107233ms step_avg:61.28ms
step:1750/2330 val_loss:3.4465 train_time:107298ms step_avg:61.31ms
step:1751/2330 train_time:107321ms step_avg:61.29ms
step:1752/2330 train_time:107359ms step_avg:61.28ms
step:1753/2330 train_time:107427ms step_avg:61.28ms
step:1754/2330 train_time:107494ms step_avg:61.28ms
step:1755/2330 train_time:107554ms step_avg:61.28ms
step:1756/2330 train_time:107617ms step_avg:61.29ms
step:1757/2330 train_time:107677ms step_avg:61.28ms
step:1758/2330 train_time:107739ms step_avg:61.29ms
step:1759/2330 train_time:107798ms step_avg:61.28ms
step:1760/2330 train_time:107861ms step_avg:61.28ms
step:1761/2330 train_time:107921ms step_avg:61.28ms
step:1762/2330 train_time:107983ms step_avg:61.28ms
step:1763/2330 train_time:108042ms step_avg:61.28ms
step:1764/2330 train_time:108105ms step_avg:61.28ms
step:1765/2330 train_time:108164ms step_avg:61.28ms
step:1766/2330 train_time:108229ms step_avg:61.28ms
step:1767/2330 train_time:108292ms step_avg:61.29ms
step:1768/2330 train_time:108356ms step_avg:61.29ms
step:1769/2330 train_time:108418ms step_avg:61.29ms
step:1770/2330 train_time:108482ms step_avg:61.29ms
step:1771/2330 train_time:108542ms step_avg:61.29ms
step:1772/2330 train_time:108606ms step_avg:61.29ms
step:1773/2330 train_time:108666ms step_avg:61.29ms
step:1774/2330 train_time:108730ms step_avg:61.29ms
step:1775/2330 train_time:108790ms step_avg:61.29ms
step:1776/2330 train_time:108853ms step_avg:61.29ms
step:1777/2330 train_time:108912ms step_avg:61.29ms
step:1778/2330 train_time:108976ms step_avg:61.29ms
step:1779/2330 train_time:109035ms step_avg:61.29ms
step:1780/2330 train_time:109098ms step_avg:61.29ms
step:1781/2330 train_time:109158ms step_avg:61.29ms
step:1782/2330 train_time:109223ms step_avg:61.29ms
step:1783/2330 train_time:109284ms step_avg:61.29ms
step:1784/2330 train_time:109348ms step_avg:61.29ms
step:1785/2330 train_time:109408ms step_avg:61.29ms
step:1786/2330 train_time:109472ms step_avg:61.29ms
step:1787/2330 train_time:109533ms step_avg:61.29ms
step:1788/2330 train_time:109597ms step_avg:61.30ms
step:1789/2330 train_time:109658ms step_avg:61.30ms
step:1790/2330 train_time:109722ms step_avg:61.30ms
step:1791/2330 train_time:109782ms step_avg:61.30ms
step:1792/2330 train_time:109845ms step_avg:61.30ms
step:1793/2330 train_time:109905ms step_avg:61.30ms
step:1794/2330 train_time:109968ms step_avg:61.30ms
step:1795/2330 train_time:110029ms step_avg:61.30ms
step:1796/2330 train_time:110092ms step_avg:61.30ms
step:1797/2330 train_time:110152ms step_avg:61.30ms
step:1798/2330 train_time:110215ms step_avg:61.30ms
step:1799/2330 train_time:110275ms step_avg:61.30ms
step:1800/2330 train_time:110339ms step_avg:61.30ms
step:1801/2330 train_time:110400ms step_avg:61.30ms
step:1802/2330 train_time:110463ms step_avg:61.30ms
step:1803/2330 train_time:110524ms step_avg:61.30ms
step:1804/2330 train_time:110588ms step_avg:61.30ms
step:1805/2330 train_time:110648ms step_avg:61.30ms
step:1806/2330 train_time:110712ms step_avg:61.30ms
step:1807/2330 train_time:110773ms step_avg:61.30ms
step:1808/2330 train_time:110836ms step_avg:61.30ms
step:1809/2330 train_time:110896ms step_avg:61.30ms
step:1810/2330 train_time:110961ms step_avg:61.30ms
step:1811/2330 train_time:111022ms step_avg:61.30ms
step:1812/2330 train_time:111085ms step_avg:61.31ms
step:1813/2330 train_time:111145ms step_avg:61.30ms
step:1814/2330 train_time:111209ms step_avg:61.31ms
step:1815/2330 train_time:111269ms step_avg:61.31ms
step:1816/2330 train_time:111333ms step_avg:61.31ms
step:1817/2330 train_time:111393ms step_avg:61.31ms
step:1818/2330 train_time:111457ms step_avg:61.31ms
step:1819/2330 train_time:111517ms step_avg:61.31ms
step:1820/2330 train_time:111580ms step_avg:61.31ms
step:1821/2330 train_time:111641ms step_avg:61.31ms
step:1822/2330 train_time:111704ms step_avg:61.31ms
step:1823/2330 train_time:111764ms step_avg:61.31ms
step:1824/2330 train_time:111828ms step_avg:61.31ms
step:1825/2330 train_time:111888ms step_avg:61.31ms
step:1826/2330 train_time:111951ms step_avg:61.31ms
step:1827/2330 train_time:112011ms step_avg:61.31ms
step:1828/2330 train_time:112074ms step_avg:61.31ms
step:1829/2330 train_time:112135ms step_avg:61.31ms
step:1830/2330 train_time:112198ms step_avg:61.31ms
step:1831/2330 train_time:112259ms step_avg:61.31ms
step:1832/2330 train_time:112322ms step_avg:61.31ms
step:1833/2330 train_time:112383ms step_avg:61.31ms
step:1834/2330 train_time:112446ms step_avg:61.31ms
step:1835/2330 train_time:112506ms step_avg:61.31ms
step:1836/2330 train_time:112569ms step_avg:61.31ms
step:1837/2330 train_time:112630ms step_avg:61.31ms
step:1838/2330 train_time:112694ms step_avg:61.31ms
step:1839/2330 train_time:112754ms step_avg:61.31ms
step:1840/2330 train_time:112818ms step_avg:61.31ms
step:1841/2330 train_time:112878ms step_avg:61.31ms
step:1842/2330 train_time:112941ms step_avg:61.31ms
step:1843/2330 train_time:113001ms step_avg:61.31ms
step:1844/2330 train_time:113065ms step_avg:61.31ms
step:1845/2330 train_time:113125ms step_avg:61.31ms
step:1846/2330 train_time:113188ms step_avg:61.32ms
step:1847/2330 train_time:113249ms step_avg:61.31ms
step:1848/2330 train_time:113312ms step_avg:61.32ms
step:1849/2330 train_time:113372ms step_avg:61.32ms
step:1850/2330 train_time:113436ms step_avg:61.32ms
step:1851/2330 train_time:113496ms step_avg:61.32ms
step:1852/2330 train_time:113559ms step_avg:61.32ms
step:1853/2330 train_time:113620ms step_avg:61.32ms
step:1854/2330 train_time:113684ms step_avg:61.32ms
step:1855/2330 train_time:113744ms step_avg:61.32ms
step:1856/2330 train_time:113807ms step_avg:61.32ms
step:1857/2330 train_time:113868ms step_avg:61.32ms
step:1858/2330 train_time:113931ms step_avg:61.32ms
step:1859/2330 train_time:113992ms step_avg:61.32ms
step:1860/2330 train_time:114055ms step_avg:61.32ms
step:1861/2330 train_time:114115ms step_avg:61.32ms
step:1862/2330 train_time:114178ms step_avg:61.32ms
step:1863/2330 train_time:114239ms step_avg:61.32ms
step:1864/2330 train_time:114303ms step_avg:61.32ms
step:1865/2330 train_time:114363ms step_avg:61.32ms
step:1866/2330 train_time:114426ms step_avg:61.32ms
step:1867/2330 train_time:114487ms step_avg:61.32ms
step:1868/2330 train_time:114550ms step_avg:61.32ms
step:1869/2330 train_time:114611ms step_avg:61.32ms
step:1870/2330 train_time:114675ms step_avg:61.32ms
step:1871/2330 train_time:114735ms step_avg:61.32ms
step:1872/2330 train_time:114799ms step_avg:61.32ms
step:1873/2330 train_time:114859ms step_avg:61.32ms
step:1874/2330 train_time:114922ms step_avg:61.32ms
step:1875/2330 train_time:114982ms step_avg:61.32ms
step:1876/2330 train_time:115045ms step_avg:61.32ms
step:1877/2330 train_time:115105ms step_avg:61.32ms
step:1878/2330 train_time:115168ms step_avg:61.32ms
step:1879/2330 train_time:115229ms step_avg:61.32ms
step:1880/2330 train_time:115292ms step_avg:61.33ms
step:1881/2330 train_time:115353ms step_avg:61.33ms
step:1882/2330 train_time:115416ms step_avg:61.33ms
step:1883/2330 train_time:115476ms step_avg:61.33ms
step:1884/2330 train_time:115540ms step_avg:61.33ms
step:1885/2330 train_time:115600ms step_avg:61.33ms
step:1886/2330 train_time:115664ms step_avg:61.33ms
step:1887/2330 train_time:115724ms step_avg:61.33ms
step:1888/2330 train_time:115787ms step_avg:61.33ms
step:1889/2330 train_time:115848ms step_avg:61.33ms
step:1890/2330 train_time:115911ms step_avg:61.33ms
step:1891/2330 train_time:115971ms step_avg:61.33ms
step:1892/2330 train_time:116035ms step_avg:61.33ms
step:1893/2330 train_time:116095ms step_avg:61.33ms
step:1894/2330 train_time:116159ms step_avg:61.33ms
step:1895/2330 train_time:116220ms step_avg:61.33ms
step:1896/2330 train_time:116283ms step_avg:61.33ms
step:1897/2330 train_time:116343ms step_avg:61.33ms
step:1898/2330 train_time:116406ms step_avg:61.33ms
step:1899/2330 train_time:116466ms step_avg:61.33ms
step:1900/2330 train_time:116530ms step_avg:61.33ms
step:1901/2330 train_time:116591ms step_avg:61.33ms
step:1902/2330 train_time:116653ms step_avg:61.33ms
step:1903/2330 train_time:116713ms step_avg:61.33ms
step:1904/2330 train_time:116776ms step_avg:61.33ms
step:1905/2330 train_time:116836ms step_avg:61.33ms
step:1906/2330 train_time:116900ms step_avg:61.33ms
step:1907/2330 train_time:116960ms step_avg:61.33ms
step:1908/2330 train_time:117024ms step_avg:61.33ms
step:1909/2330 train_time:117084ms step_avg:61.33ms
step:1910/2330 train_time:117147ms step_avg:61.33ms
step:1911/2330 train_time:117208ms step_avg:61.33ms
step:1912/2330 train_time:117272ms step_avg:61.33ms
step:1913/2330 train_time:117332ms step_avg:61.33ms
step:1914/2330 train_time:117395ms step_avg:61.34ms
step:1915/2330 train_time:117456ms step_avg:61.33ms
step:1916/2330 train_time:117520ms step_avg:61.34ms
step:1917/2330 train_time:117579ms step_avg:61.34ms
step:1918/2330 train_time:117643ms step_avg:61.34ms
step:1919/2330 train_time:117703ms step_avg:61.34ms
step:1920/2330 train_time:117767ms step_avg:61.34ms
step:1921/2330 train_time:117828ms step_avg:61.34ms
step:1922/2330 train_time:117891ms step_avg:61.34ms
step:1923/2330 train_time:117952ms step_avg:61.34ms
step:1924/2330 train_time:118015ms step_avg:61.34ms
step:1925/2330 train_time:118075ms step_avg:61.34ms
step:1926/2330 train_time:118139ms step_avg:61.34ms
step:1927/2330 train_time:118199ms step_avg:61.34ms
step:1928/2330 train_time:118262ms step_avg:61.34ms
step:1929/2330 train_time:118322ms step_avg:61.34ms
step:1930/2330 train_time:118386ms step_avg:61.34ms
step:1931/2330 train_time:118446ms step_avg:61.34ms
step:1932/2330 train_time:118509ms step_avg:61.34ms
step:1933/2330 train_time:118570ms step_avg:61.34ms
step:1934/2330 train_time:118634ms step_avg:61.34ms
step:1935/2330 train_time:118694ms step_avg:61.34ms
step:1936/2330 train_time:118758ms step_avg:61.34ms
step:1937/2330 train_time:118819ms step_avg:61.34ms
step:1938/2330 train_time:118882ms step_avg:61.34ms
step:1939/2330 train_time:118943ms step_avg:61.34ms
step:1940/2330 train_time:119006ms step_avg:61.34ms
step:1941/2330 train_time:119066ms step_avg:61.34ms
step:1942/2330 train_time:119130ms step_avg:61.34ms
step:1943/2330 train_time:119190ms step_avg:61.34ms
step:1944/2330 train_time:119253ms step_avg:61.34ms
step:1945/2330 train_time:119314ms step_avg:61.34ms
step:1946/2330 train_time:119377ms step_avg:61.34ms
step:1947/2330 train_time:119438ms step_avg:61.34ms
step:1948/2330 train_time:119502ms step_avg:61.35ms
step:1949/2330 train_time:119562ms step_avg:61.35ms
step:1950/2330 train_time:119625ms step_avg:61.35ms
step:1951/2330 train_time:119685ms step_avg:61.35ms
step:1952/2330 train_time:119748ms step_avg:61.35ms
step:1953/2330 train_time:119809ms step_avg:61.35ms
step:1954/2330 train_time:119873ms step_avg:61.35ms
step:1955/2330 train_time:119933ms step_avg:61.35ms
step:1956/2330 train_time:119996ms step_avg:61.35ms
step:1957/2330 train_time:120057ms step_avg:61.35ms
step:1958/2330 train_time:120122ms step_avg:61.35ms
step:1959/2330 train_time:120181ms step_avg:61.35ms
step:1960/2330 train_time:120245ms step_avg:61.35ms
step:1961/2330 train_time:120305ms step_avg:61.35ms
step:1962/2330 train_time:120368ms step_avg:61.35ms
step:1963/2330 train_time:120428ms step_avg:61.35ms
step:1964/2330 train_time:120492ms step_avg:61.35ms
step:1965/2330 train_time:120552ms step_avg:61.35ms
step:1966/2330 train_time:120616ms step_avg:61.35ms
step:1967/2330 train_time:120676ms step_avg:61.35ms
step:1968/2330 train_time:120740ms step_avg:61.35ms
step:1969/2330 train_time:120800ms step_avg:61.35ms
step:1970/2330 train_time:120864ms step_avg:61.35ms
step:1971/2330 train_time:120924ms step_avg:61.35ms
step:1972/2330 train_time:120988ms step_avg:61.35ms
step:1973/2330 train_time:121049ms step_avg:61.35ms
step:1974/2330 train_time:121112ms step_avg:61.35ms
step:1975/2330 train_time:121172ms step_avg:61.35ms
step:1976/2330 train_time:121235ms step_avg:61.35ms
step:1977/2330 train_time:121296ms step_avg:61.35ms
step:1978/2330 train_time:121359ms step_avg:61.35ms
step:1979/2330 train_time:121420ms step_avg:61.35ms
step:1980/2330 train_time:121483ms step_avg:61.36ms
step:1981/2330 train_time:121544ms step_avg:61.35ms
step:1982/2330 train_time:121607ms step_avg:61.36ms
step:1983/2330 train_time:121668ms step_avg:61.36ms
step:1984/2330 train_time:121731ms step_avg:61.36ms
step:1985/2330 train_time:121792ms step_avg:61.36ms
step:1986/2330 train_time:121855ms step_avg:61.36ms
step:1987/2330 train_time:121915ms step_avg:61.36ms
step:1988/2330 train_time:121979ms step_avg:61.36ms
step:1989/2330 train_time:122040ms step_avg:61.36ms
step:1990/2330 train_time:122104ms step_avg:61.36ms
step:1991/2330 train_time:122164ms step_avg:61.36ms
step:1992/2330 train_time:122228ms step_avg:61.36ms
step:1993/2330 train_time:122288ms step_avg:61.36ms
step:1994/2330 train_time:122351ms step_avg:61.36ms
step:1995/2330 train_time:122411ms step_avg:61.36ms
step:1996/2330 train_time:122476ms step_avg:61.36ms
step:1997/2330 train_time:122536ms step_avg:61.36ms
step:1998/2330 train_time:122600ms step_avg:61.36ms
step:1999/2330 train_time:122661ms step_avg:61.36ms
step:2000/2330 train_time:122724ms step_avg:61.36ms
step:2000/2330 val_loss:3.4176 train_time:122789ms step_avg:61.39ms
step:2001/2330 train_time:122811ms step_avg:61.37ms
step:2002/2330 train_time:122852ms step_avg:61.36ms
step:2003/2330 train_time:122916ms step_avg:61.37ms
step:2004/2330 train_time:122981ms step_avg:61.37ms
step:2005/2330 train_time:123042ms step_avg:61.37ms
step:2006/2330 train_time:123106ms step_avg:61.37ms
step:2007/2330 train_time:123166ms step_avg:61.37ms
step:2008/2330 train_time:123228ms step_avg:61.37ms
step:2009/2330 train_time:123288ms step_avg:61.37ms
step:2010/2330 train_time:123351ms step_avg:61.37ms
step:2011/2330 train_time:123410ms step_avg:61.37ms
step:2012/2330 train_time:123472ms step_avg:61.37ms
step:2013/2330 train_time:123533ms step_avg:61.37ms
step:2014/2330 train_time:123596ms step_avg:61.37ms
step:2015/2330 train_time:123655ms step_avg:61.37ms
step:2016/2330 train_time:123718ms step_avg:61.37ms
step:2017/2330 train_time:123779ms step_avg:61.37ms
step:2018/2330 train_time:123845ms step_avg:61.37ms
step:2019/2330 train_time:123906ms step_avg:61.37ms
step:2020/2330 train_time:123970ms step_avg:61.37ms
step:2021/2330 train_time:124032ms step_avg:61.37ms
step:2022/2330 train_time:124096ms step_avg:61.37ms
step:2023/2330 train_time:124157ms step_avg:61.37ms
step:2024/2330 train_time:124221ms step_avg:61.37ms
step:2025/2330 train_time:124281ms step_avg:61.37ms
step:2026/2330 train_time:124344ms step_avg:61.37ms
step:2027/2330 train_time:124403ms step_avg:61.37ms
step:2028/2330 train_time:124467ms step_avg:61.37ms
step:2029/2330 train_time:124527ms step_avg:61.37ms
step:2030/2330 train_time:124590ms step_avg:61.37ms
step:2031/2330 train_time:124649ms step_avg:61.37ms
step:2032/2330 train_time:124712ms step_avg:61.37ms
step:2033/2330 train_time:124773ms step_avg:61.37ms
step:2034/2330 train_time:124836ms step_avg:61.37ms
step:2035/2330 train_time:124897ms step_avg:61.37ms
step:2036/2330 train_time:124961ms step_avg:61.38ms
step:2037/2330 train_time:125021ms step_avg:61.38ms
step:2038/2330 train_time:125085ms step_avg:61.38ms
step:2039/2330 train_time:125145ms step_avg:61.38ms
step:2040/2330 train_time:125208ms step_avg:61.38ms
step:2041/2330 train_time:125269ms step_avg:61.38ms
step:2042/2330 train_time:125332ms step_avg:61.38ms
step:2043/2330 train_time:125392ms step_avg:61.38ms
step:2044/2330 train_time:125455ms step_avg:61.38ms
step:2045/2330 train_time:125515ms step_avg:61.38ms
step:2046/2330 train_time:125578ms step_avg:61.38ms
step:2047/2330 train_time:125637ms step_avg:61.38ms
step:2048/2330 train_time:125701ms step_avg:61.38ms
step:2049/2330 train_time:125761ms step_avg:61.38ms
step:2050/2330 train_time:125824ms step_avg:61.38ms
step:2051/2330 train_time:125885ms step_avg:61.38ms
step:2052/2330 train_time:125949ms step_avg:61.38ms
step:2053/2330 train_time:126009ms step_avg:61.38ms
step:2054/2330 train_time:126073ms step_avg:61.38ms
step:2055/2330 train_time:126133ms step_avg:61.38ms
step:2056/2330 train_time:126197ms step_avg:61.38ms
step:2057/2330 train_time:126257ms step_avg:61.38ms
step:2058/2330 train_time:126320ms step_avg:61.38ms
step:2059/2330 train_time:126381ms step_avg:61.38ms
step:2060/2330 train_time:126444ms step_avg:61.38ms
step:2061/2330 train_time:126504ms step_avg:61.38ms
step:2062/2330 train_time:126567ms step_avg:61.38ms
step:2063/2330 train_time:126627ms step_avg:61.38ms
step:2064/2330 train_time:126691ms step_avg:61.38ms
step:2065/2330 train_time:126751ms step_avg:61.38ms
step:2066/2330 train_time:126814ms step_avg:61.38ms
step:2067/2330 train_time:126875ms step_avg:61.38ms
step:2068/2330 train_time:126938ms step_avg:61.38ms
step:2069/2330 train_time:126999ms step_avg:61.38ms
step:2070/2330 train_time:127062ms step_avg:61.38ms
step:2071/2330 train_time:127123ms step_avg:61.38ms
step:2072/2330 train_time:127187ms step_avg:61.38ms
step:2073/2330 train_time:127247ms step_avg:61.38ms
step:2074/2330 train_time:127311ms step_avg:61.38ms
step:2075/2330 train_time:127371ms step_avg:61.38ms
step:2076/2330 train_time:127434ms step_avg:61.38ms
step:2077/2330 train_time:127495ms step_avg:61.38ms
step:2078/2330 train_time:127559ms step_avg:61.39ms
step:2079/2330 train_time:127618ms step_avg:61.38ms
step:2080/2330 train_time:127682ms step_avg:61.39ms
step:2081/2330 train_time:127742ms step_avg:61.38ms
step:2082/2330 train_time:127806ms step_avg:61.39ms
step:2083/2330 train_time:127866ms step_avg:61.39ms
step:2084/2330 train_time:127929ms step_avg:61.39ms
step:2085/2330 train_time:127989ms step_avg:61.39ms
step:2086/2330 train_time:128053ms step_avg:61.39ms
step:2087/2330 train_time:128114ms step_avg:61.39ms
step:2088/2330 train_time:128177ms step_avg:61.39ms
step:2089/2330 train_time:128237ms step_avg:61.39ms
step:2090/2330 train_time:128301ms step_avg:61.39ms
step:2091/2330 train_time:128361ms step_avg:61.39ms
step:2092/2330 train_time:128424ms step_avg:61.39ms
step:2093/2330 train_time:128485ms step_avg:61.39ms
step:2094/2330 train_time:128550ms step_avg:61.39ms
step:2095/2330 train_time:128610ms step_avg:61.39ms
step:2096/2330 train_time:128673ms step_avg:61.39ms
step:2097/2330 train_time:128734ms step_avg:61.39ms
step:2098/2330 train_time:128798ms step_avg:61.39ms
step:2099/2330 train_time:128858ms step_avg:61.39ms
step:2100/2330 train_time:128922ms step_avg:61.39ms
step:2101/2330 train_time:128982ms step_avg:61.39ms
step:2102/2330 train_time:129045ms step_avg:61.39ms
step:2103/2330 train_time:129106ms step_avg:61.39ms
step:2104/2330 train_time:129169ms step_avg:61.39ms
step:2105/2330 train_time:129230ms step_avg:61.39ms
step:2106/2330 train_time:129293ms step_avg:61.39ms
step:2107/2330 train_time:129354ms step_avg:61.39ms
step:2108/2330 train_time:129417ms step_avg:61.39ms
step:2109/2330 train_time:129477ms step_avg:61.39ms
step:2110/2330 train_time:129541ms step_avg:61.39ms
step:2111/2330 train_time:129601ms step_avg:61.39ms
step:2112/2330 train_time:129664ms step_avg:61.39ms
step:2113/2330 train_time:129724ms step_avg:61.39ms
step:2114/2330 train_time:129788ms step_avg:61.39ms
step:2115/2330 train_time:129848ms step_avg:61.39ms
step:2116/2330 train_time:129911ms step_avg:61.39ms
step:2117/2330 train_time:129971ms step_avg:61.39ms
step:2118/2330 train_time:130035ms step_avg:61.39ms
step:2119/2330 train_time:130095ms step_avg:61.39ms
step:2120/2330 train_time:130159ms step_avg:61.40ms
step:2121/2330 train_time:130220ms step_avg:61.40ms
step:2122/2330 train_time:130283ms step_avg:61.40ms
step:2123/2330 train_time:130343ms step_avg:61.40ms
step:2124/2330 train_time:130406ms step_avg:61.40ms
step:2125/2330 train_time:130466ms step_avg:61.40ms
step:2126/2330 train_time:130529ms step_avg:61.40ms
step:2127/2330 train_time:130590ms step_avg:61.40ms
step:2128/2330 train_time:130653ms step_avg:61.40ms
step:2129/2330 train_time:130714ms step_avg:61.40ms
step:2130/2330 train_time:130778ms step_avg:61.40ms
step:2131/2330 train_time:130838ms step_avg:61.40ms
step:2132/2330 train_time:130902ms step_avg:61.40ms
step:2133/2330 train_time:130963ms step_avg:61.40ms
step:2134/2330 train_time:131026ms step_avg:61.40ms
step:2135/2330 train_time:131087ms step_avg:61.40ms
step:2136/2330 train_time:131150ms step_avg:61.40ms
step:2137/2330 train_time:131210ms step_avg:61.40ms
step:2138/2330 train_time:131273ms step_avg:61.40ms
step:2139/2330 train_time:131333ms step_avg:61.40ms
step:2140/2330 train_time:131396ms step_avg:61.40ms
step:2141/2330 train_time:131457ms step_avg:61.40ms
step:2142/2330 train_time:131520ms step_avg:61.40ms
step:2143/2330 train_time:131580ms step_avg:61.40ms
step:2144/2330 train_time:131644ms step_avg:61.40ms
step:2145/2330 train_time:131704ms step_avg:61.40ms
step:2146/2330 train_time:131768ms step_avg:61.40ms
step:2147/2330 train_time:131829ms step_avg:61.40ms
step:2148/2330 train_time:131893ms step_avg:61.40ms
step:2149/2330 train_time:131953ms step_avg:61.40ms
step:2150/2330 train_time:132017ms step_avg:61.40ms
step:2151/2330 train_time:132077ms step_avg:61.40ms
step:2152/2330 train_time:132141ms step_avg:61.40ms
step:2153/2330 train_time:132202ms step_avg:61.40ms
step:2154/2330 train_time:132265ms step_avg:61.40ms
step:2155/2330 train_time:132326ms step_avg:61.40ms
step:2156/2330 train_time:132390ms step_avg:61.41ms
step:2157/2330 train_time:132450ms step_avg:61.40ms
step:2158/2330 train_time:132513ms step_avg:61.41ms
step:2159/2330 train_time:132574ms step_avg:61.41ms
step:2160/2330 train_time:132638ms step_avg:61.41ms
step:2161/2330 train_time:132699ms step_avg:61.41ms
step:2162/2330 train_time:132762ms step_avg:61.41ms
step:2163/2330 train_time:132822ms step_avg:61.41ms
step:2164/2330 train_time:132886ms step_avg:61.41ms
step:2165/2330 train_time:132947ms step_avg:61.41ms
step:2166/2330 train_time:133011ms step_avg:61.41ms
step:2167/2330 train_time:133070ms step_avg:61.41ms
step:2168/2330 train_time:133134ms step_avg:61.41ms
step:2169/2330 train_time:133194ms step_avg:61.41ms
step:2170/2330 train_time:133258ms step_avg:61.41ms
step:2171/2330 train_time:133319ms step_avg:61.41ms
step:2172/2330 train_time:133382ms step_avg:61.41ms
step:2173/2330 train_time:133443ms step_avg:61.41ms
step:2174/2330 train_time:133507ms step_avg:61.41ms
step:2175/2330 train_time:133567ms step_avg:61.41ms
step:2176/2330 train_time:133630ms step_avg:61.41ms
step:2177/2330 train_time:133691ms step_avg:61.41ms
step:2178/2330 train_time:133755ms step_avg:61.41ms
step:2179/2330 train_time:133815ms step_avg:61.41ms
step:2180/2330 train_time:133878ms step_avg:61.41ms
step:2181/2330 train_time:133939ms step_avg:61.41ms
step:2182/2330 train_time:134002ms step_avg:61.41ms
step:2183/2330 train_time:134063ms step_avg:61.41ms
step:2184/2330 train_time:134127ms step_avg:61.41ms
step:2185/2330 train_time:134188ms step_avg:61.41ms
step:2186/2330 train_time:134251ms step_avg:61.41ms
step:2187/2330 train_time:134311ms step_avg:61.41ms
step:2188/2330 train_time:134375ms step_avg:61.41ms
step:2189/2330 train_time:134435ms step_avg:61.41ms
step:2190/2330 train_time:134500ms step_avg:61.42ms
step:2191/2330 train_time:134560ms step_avg:61.42ms
step:2192/2330 train_time:134624ms step_avg:61.42ms
step:2193/2330 train_time:134684ms step_avg:61.42ms
step:2194/2330 train_time:134748ms step_avg:61.42ms
step:2195/2330 train_time:134808ms step_avg:61.42ms
step:2196/2330 train_time:134871ms step_avg:61.42ms
step:2197/2330 train_time:134932ms step_avg:61.42ms
step:2198/2330 train_time:134995ms step_avg:61.42ms
step:2199/2330 train_time:135057ms step_avg:61.42ms
step:2200/2330 train_time:135120ms step_avg:61.42ms
step:2201/2330 train_time:135180ms step_avg:61.42ms
step:2202/2330 train_time:135243ms step_avg:61.42ms
step:2203/2330 train_time:135304ms step_avg:61.42ms
step:2204/2330 train_time:135368ms step_avg:61.42ms
step:2205/2330 train_time:135428ms step_avg:61.42ms
step:2206/2330 train_time:135491ms step_avg:61.42ms
step:2207/2330 train_time:135552ms step_avg:61.42ms
step:2208/2330 train_time:135615ms step_avg:61.42ms
step:2209/2330 train_time:135676ms step_avg:61.42ms
step:2210/2330 train_time:135739ms step_avg:61.42ms
step:2211/2330 train_time:135800ms step_avg:61.42ms
step:2212/2330 train_time:135864ms step_avg:61.42ms
step:2213/2330 train_time:135924ms step_avg:61.42ms
step:2214/2330 train_time:135988ms step_avg:61.42ms
step:2215/2330 train_time:136049ms step_avg:61.42ms
step:2216/2330 train_time:136113ms step_avg:61.42ms
step:2217/2330 train_time:136173ms step_avg:61.42ms
step:2218/2330 train_time:136236ms step_avg:61.42ms
step:2219/2330 train_time:136297ms step_avg:61.42ms
step:2220/2330 train_time:136361ms step_avg:61.42ms
step:2221/2330 train_time:136421ms step_avg:61.42ms
step:2222/2330 train_time:136484ms step_avg:61.42ms
step:2223/2330 train_time:136545ms step_avg:61.42ms
step:2224/2330 train_time:136608ms step_avg:61.42ms
step:2225/2330 train_time:136668ms step_avg:61.42ms
step:2226/2330 train_time:136731ms step_avg:61.42ms
step:2227/2330 train_time:136792ms step_avg:61.42ms
step:2228/2330 train_time:136855ms step_avg:61.42ms
step:2229/2330 train_time:136915ms step_avg:61.42ms
step:2230/2330 train_time:136979ms step_avg:61.43ms
step:2231/2330 train_time:137039ms step_avg:61.42ms
step:2232/2330 train_time:137102ms step_avg:61.43ms
step:2233/2330 train_time:137163ms step_avg:61.43ms
step:2234/2330 train_time:137226ms step_avg:61.43ms
step:2235/2330 train_time:137287ms step_avg:61.43ms
step:2236/2330 train_time:137350ms step_avg:61.43ms
step:2237/2330 train_time:137410ms step_avg:61.43ms
step:2238/2330 train_time:137473ms step_avg:61.43ms
step:2239/2330 train_time:137533ms step_avg:61.43ms
step:2240/2330 train_time:137597ms step_avg:61.43ms
step:2241/2330 train_time:137657ms step_avg:61.43ms
step:2242/2330 train_time:137720ms step_avg:61.43ms
step:2243/2330 train_time:137781ms step_avg:61.43ms
step:2244/2330 train_time:137845ms step_avg:61.43ms
step:2245/2330 train_time:137905ms step_avg:61.43ms
step:2246/2330 train_time:137969ms step_avg:61.43ms
step:2247/2330 train_time:138029ms step_avg:61.43ms
step:2248/2330 train_time:138092ms step_avg:61.43ms
step:2249/2330 train_time:138153ms step_avg:61.43ms
step:2250/2330 train_time:138217ms step_avg:61.43ms
step:2250/2330 val_loss:3.3971 train_time:138282ms step_avg:61.46ms
step:2251/2330 train_time:138304ms step_avg:61.44ms
step:2252/2330 train_time:138345ms step_avg:61.43ms
step:2253/2330 train_time:138410ms step_avg:61.43ms
step:2254/2330 train_time:138475ms step_avg:61.44ms
step:2255/2330 train_time:138535ms step_avg:61.43ms
step:2256/2330 train_time:138599ms step_avg:61.44ms
step:2257/2330 train_time:138658ms step_avg:61.43ms
step:2258/2330 train_time:138721ms step_avg:61.44ms
step:2259/2330 train_time:138781ms step_avg:61.43ms
step:2260/2330 train_time:138843ms step_avg:61.44ms
step:2261/2330 train_time:138903ms step_avg:61.43ms
step:2262/2330 train_time:138966ms step_avg:61.43ms
step:2263/2330 train_time:139025ms step_avg:61.43ms
step:2264/2330 train_time:139087ms step_avg:61.43ms
step:2265/2330 train_time:139147ms step_avg:61.43ms
step:2266/2330 train_time:139211ms step_avg:61.43ms
step:2267/2330 train_time:139273ms step_avg:61.43ms
step:2268/2330 train_time:139338ms step_avg:61.44ms
step:2269/2330 train_time:139399ms step_avg:61.44ms
step:2270/2330 train_time:139464ms step_avg:61.44ms
step:2271/2330 train_time:139525ms step_avg:61.44ms
step:2272/2330 train_time:139589ms step_avg:61.44ms
step:2273/2330 train_time:139649ms step_avg:61.44ms
step:2274/2330 train_time:139713ms step_avg:61.44ms
step:2275/2330 train_time:139773ms step_avg:61.44ms
step:2276/2330 train_time:139836ms step_avg:61.44ms
step:2277/2330 train_time:139895ms step_avg:61.44ms
step:2278/2330 train_time:139958ms step_avg:61.44ms
step:2279/2330 train_time:140018ms step_avg:61.44ms
step:2280/2330 train_time:140081ms step_avg:61.44ms
step:2281/2330 train_time:140142ms step_avg:61.44ms
step:2282/2330 train_time:140205ms step_avg:61.44ms
step:2283/2330 train_time:140266ms step_avg:61.44ms
step:2284/2330 train_time:140330ms step_avg:61.44ms
step:2285/2330 train_time:140390ms step_avg:61.44ms
step:2286/2330 train_time:140455ms step_avg:61.44ms
step:2287/2330 train_time:140516ms step_avg:61.44ms
step:2288/2330 train_time:140580ms step_avg:61.44ms
step:2289/2330 train_time:140640ms step_avg:61.44ms
step:2290/2330 train_time:140703ms step_avg:61.44ms
step:2291/2330 train_time:140763ms step_avg:61.44ms
step:2292/2330 train_time:140827ms step_avg:61.44ms
step:2293/2330 train_time:140887ms step_avg:61.44ms
step:2294/2330 train_time:140950ms step_avg:61.44ms
step:2295/2330 train_time:141010ms step_avg:61.44ms
step:2296/2330 train_time:141074ms step_avg:61.44ms
step:2297/2330 train_time:141134ms step_avg:61.44ms
step:2298/2330 train_time:141197ms step_avg:61.44ms
step:2299/2330 train_time:141257ms step_avg:61.44ms
step:2300/2330 train_time:141321ms step_avg:61.44ms
step:2301/2330 train_time:141382ms step_avg:61.44ms
step:2302/2330 train_time:141446ms step_avg:61.44ms
step:2303/2330 train_time:141507ms step_avg:61.44ms
step:2304/2330 train_time:141571ms step_avg:61.45ms
step:2305/2330 train_time:141631ms step_avg:61.45ms
step:2306/2330 train_time:141694ms step_avg:61.45ms
step:2307/2330 train_time:141755ms step_avg:61.45ms
step:2308/2330 train_time:141818ms step_avg:61.45ms
step:2309/2330 train_time:141879ms step_avg:61.45ms
step:2310/2330 train_time:141943ms step_avg:61.45ms
step:2311/2330 train_time:142003ms step_avg:61.45ms
step:2312/2330 train_time:142066ms step_avg:61.45ms
step:2313/2330 train_time:142126ms step_avg:61.45ms
step:2314/2330 train_time:142189ms step_avg:61.45ms
step:2315/2330 train_time:142249ms step_avg:61.45ms
step:2316/2330 train_time:142313ms step_avg:61.45ms
step:2317/2330 train_time:142373ms step_avg:61.45ms
step:2318/2330 train_time:142436ms step_avg:61.45ms
step:2319/2330 train_time:142497ms step_avg:61.45ms
step:2320/2330 train_time:142560ms step_avg:61.45ms
step:2321/2330 train_time:142620ms step_avg:61.45ms
step:2322/2330 train_time:142684ms step_avg:61.45ms
step:2323/2330 train_time:142745ms step_avg:61.45ms
step:2324/2330 train_time:142808ms step_avg:61.45ms
step:2325/2330 train_time:142869ms step_avg:61.45ms
step:2326/2330 train_time:142933ms step_avg:61.45ms
step:2327/2330 train_time:142992ms step_avg:61.45ms
step:2328/2330 train_time:143055ms step_avg:61.45ms
step:2329/2330 train_time:143115ms step_avg:61.45ms
step:2330/2330 train_time:143179ms step_avg:61.45ms
step:2330/2330 val_loss:3.3869 train_time:143244ms step_avg:61.48ms
peak memory allocated: 29998 MiB reserved: 44076 MiB
