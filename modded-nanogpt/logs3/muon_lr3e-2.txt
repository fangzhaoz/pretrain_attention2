import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                # v_chunk = divide_by_norm(batched)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                # v_chunk = divide_by_norm(batched_update_grads)
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "muon_lr3e-2"
    os.makedirs("logs3", exist_ok=True)
    logfile = f"logs3/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
# embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=3e-2, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    return 1.0

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum_min + 0.5 * (momentum_max - momentum_min)

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs3/{run_id}", exist_ok=True)
            torch.save(log, f"logs3/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 07:40:47 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   34C    P0             113W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   33C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   30C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   31C    P0             114W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   33C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   30C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   34C    P0             116W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:86ms step_avg:85.53ms
step:2/2330 train_time:184ms step_avg:91.85ms
step:3/2330 train_time:206ms step_avg:68.75ms
step:4/2330 train_time:241ms step_avg:60.31ms
step:5/2330 train_time:299ms step_avg:59.72ms
step:6/2330 train_time:360ms step_avg:60.05ms
step:7/2330 train_time:419ms step_avg:59.84ms
step:8/2330 train_time:481ms step_avg:60.07ms
step:9/2330 train_time:539ms step_avg:59.88ms
step:10/2330 train_time:601ms step_avg:60.05ms
step:11/2330 train_time:659ms step_avg:59.91ms
step:12/2330 train_time:721ms step_avg:60.05ms
step:13/2330 train_time:779ms step_avg:59.93ms
step:14/2330 train_time:841ms step_avg:60.05ms
step:15/2330 train_time:899ms step_avg:59.95ms
step:16/2330 train_time:961ms step_avg:60.08ms
step:17/2330 train_time:1022ms step_avg:60.13ms
step:18/2330 train_time:1088ms step_avg:60.46ms
step:19/2330 train_time:1152ms step_avg:60.62ms
step:20/2330 train_time:1215ms step_avg:60.74ms
step:21/2330 train_time:1275ms step_avg:60.70ms
step:22/2330 train_time:1337ms step_avg:60.77ms
step:23/2330 train_time:1396ms step_avg:60.71ms
step:24/2330 train_time:1459ms step_avg:60.78ms
step:25/2330 train_time:1517ms step_avg:60.70ms
step:26/2330 train_time:1580ms step_avg:60.76ms
step:27/2330 train_time:1639ms step_avg:60.70ms
step:28/2330 train_time:1701ms step_avg:60.74ms
step:29/2330 train_time:1759ms step_avg:60.67ms
step:30/2330 train_time:1821ms step_avg:60.71ms
step:31/2330 train_time:1880ms step_avg:60.64ms
step:32/2330 train_time:1942ms step_avg:60.67ms
step:33/2330 train_time:2002ms step_avg:60.68ms
step:34/2330 train_time:2066ms step_avg:60.76ms
step:35/2330 train_time:2127ms step_avg:60.76ms
step:36/2330 train_time:2191ms step_avg:60.86ms
step:37/2330 train_time:2252ms step_avg:60.87ms
step:38/2330 train_time:2315ms step_avg:60.93ms
step:39/2330 train_time:2375ms step_avg:60.91ms
step:40/2330 train_time:2438ms step_avg:60.94ms
step:41/2330 train_time:2497ms step_avg:60.90ms
step:42/2330 train_time:2559ms step_avg:60.92ms
step:43/2330 train_time:2618ms step_avg:60.88ms
step:44/2330 train_time:2680ms step_avg:60.91ms
step:45/2330 train_time:2738ms step_avg:60.85ms
step:46/2330 train_time:2801ms step_avg:60.89ms
step:47/2330 train_time:2860ms step_avg:60.85ms
step:48/2330 train_time:2923ms step_avg:60.90ms
step:49/2330 train_time:2983ms step_avg:60.89ms
step:50/2330 train_time:3046ms step_avg:60.93ms
step:51/2330 train_time:3106ms step_avg:60.90ms
step:52/2330 train_time:3169ms step_avg:60.95ms
step:53/2330 train_time:3230ms step_avg:60.93ms
step:54/2330 train_time:3292ms step_avg:60.96ms
step:55/2330 train_time:3352ms step_avg:60.95ms
step:56/2330 train_time:3414ms step_avg:60.97ms
step:57/2330 train_time:3474ms step_avg:60.94ms
step:58/2330 train_time:3536ms step_avg:60.97ms
step:59/2330 train_time:3595ms step_avg:60.94ms
step:60/2330 train_time:3658ms step_avg:60.96ms
step:61/2330 train_time:3717ms step_avg:60.94ms
step:62/2330 train_time:3780ms step_avg:60.96ms
step:63/2330 train_time:3839ms step_avg:60.94ms
step:64/2330 train_time:3901ms step_avg:60.95ms
step:65/2330 train_time:3961ms step_avg:60.93ms
step:66/2330 train_time:4024ms step_avg:60.97ms
step:67/2330 train_time:4084ms step_avg:60.95ms
step:68/2330 train_time:4147ms step_avg:60.98ms
step:69/2330 train_time:4208ms step_avg:60.98ms
step:70/2330 train_time:4271ms step_avg:61.01ms
step:71/2330 train_time:4330ms step_avg:60.98ms
step:72/2330 train_time:4393ms step_avg:61.01ms
step:73/2330 train_time:4452ms step_avg:60.98ms
step:74/2330 train_time:4514ms step_avg:61.00ms
step:75/2330 train_time:4573ms step_avg:60.97ms
step:76/2330 train_time:4635ms step_avg:60.99ms
step:77/2330 train_time:4694ms step_avg:60.96ms
step:78/2330 train_time:4757ms step_avg:60.99ms
step:79/2330 train_time:4818ms step_avg:60.98ms
step:80/2330 train_time:4880ms step_avg:60.99ms
step:81/2330 train_time:4938ms step_avg:60.97ms
step:82/2330 train_time:5001ms step_avg:60.99ms
step:83/2330 train_time:5061ms step_avg:60.97ms
step:84/2330 train_time:5124ms step_avg:61.00ms
step:85/2330 train_time:5183ms step_avg:60.98ms
step:86/2330 train_time:5246ms step_avg:61.00ms
step:87/2330 train_time:5306ms step_avg:60.99ms
step:88/2330 train_time:5370ms step_avg:61.02ms
step:89/2330 train_time:5430ms step_avg:61.01ms
step:90/2330 train_time:5492ms step_avg:61.02ms
step:91/2330 train_time:5551ms step_avg:61.00ms
step:92/2330 train_time:5614ms step_avg:61.02ms
step:93/2330 train_time:5673ms step_avg:61.00ms
step:94/2330 train_time:5735ms step_avg:61.01ms
step:95/2330 train_time:5795ms step_avg:60.99ms
step:96/2330 train_time:5858ms step_avg:61.02ms
step:97/2330 train_time:5918ms step_avg:61.01ms
step:98/2330 train_time:5980ms step_avg:61.02ms
step:99/2330 train_time:6039ms step_avg:61.00ms
step:100/2330 train_time:6101ms step_avg:61.01ms
step:101/2330 train_time:6160ms step_avg:60.99ms
step:102/2330 train_time:6223ms step_avg:61.01ms
step:103/2330 train_time:6282ms step_avg:60.99ms
step:104/2330 train_time:6345ms step_avg:61.01ms
step:105/2330 train_time:6405ms step_avg:61.00ms
step:106/2330 train_time:6469ms step_avg:61.03ms
step:107/2330 train_time:6528ms step_avg:61.01ms
step:108/2330 train_time:6591ms step_avg:61.03ms
step:109/2330 train_time:6650ms step_avg:61.01ms
step:110/2330 train_time:6712ms step_avg:61.02ms
step:111/2330 train_time:6771ms step_avg:61.00ms
step:112/2330 train_time:6834ms step_avg:61.02ms
step:113/2330 train_time:6893ms step_avg:61.00ms
step:114/2330 train_time:6956ms step_avg:61.02ms
step:115/2330 train_time:7016ms step_avg:61.01ms
step:116/2330 train_time:7078ms step_avg:61.02ms
step:117/2330 train_time:7138ms step_avg:61.01ms
step:118/2330 train_time:7200ms step_avg:61.02ms
step:119/2330 train_time:7260ms step_avg:61.00ms
step:120/2330 train_time:7322ms step_avg:61.02ms
step:121/2330 train_time:7382ms step_avg:61.01ms
step:122/2330 train_time:7445ms step_avg:61.02ms
step:123/2330 train_time:7504ms step_avg:61.01ms
step:124/2330 train_time:7568ms step_avg:61.03ms
step:125/2330 train_time:7627ms step_avg:61.02ms
step:126/2330 train_time:7690ms step_avg:61.03ms
step:127/2330 train_time:7749ms step_avg:61.02ms
step:128/2330 train_time:7812ms step_avg:61.03ms
step:129/2330 train_time:7872ms step_avg:61.02ms
step:130/2330 train_time:7934ms step_avg:61.03ms
step:131/2330 train_time:7994ms step_avg:61.02ms
step:132/2330 train_time:8057ms step_avg:61.04ms
step:133/2330 train_time:8117ms step_avg:61.03ms
step:134/2330 train_time:8179ms step_avg:61.04ms
step:135/2330 train_time:8238ms step_avg:61.02ms
step:136/2330 train_time:8302ms step_avg:61.04ms
step:137/2330 train_time:8362ms step_avg:61.03ms
step:138/2330 train_time:8424ms step_avg:61.04ms
step:139/2330 train_time:8484ms step_avg:61.03ms
step:140/2330 train_time:8546ms step_avg:61.04ms
step:141/2330 train_time:8605ms step_avg:61.03ms
step:142/2330 train_time:8668ms step_avg:61.04ms
step:143/2330 train_time:8727ms step_avg:61.03ms
step:144/2330 train_time:8789ms step_avg:61.04ms
step:145/2330 train_time:8849ms step_avg:61.03ms
step:146/2330 train_time:8911ms step_avg:61.04ms
step:147/2330 train_time:8971ms step_avg:61.03ms
step:148/2330 train_time:9033ms step_avg:61.03ms
step:149/2330 train_time:9092ms step_avg:61.02ms
step:150/2330 train_time:9155ms step_avg:61.03ms
step:151/2330 train_time:9215ms step_avg:61.03ms
step:152/2330 train_time:9277ms step_avg:61.04ms
step:153/2330 train_time:9338ms step_avg:61.03ms
step:154/2330 train_time:9401ms step_avg:61.04ms
step:155/2330 train_time:9460ms step_avg:61.03ms
step:156/2330 train_time:9522ms step_avg:61.04ms
step:157/2330 train_time:9581ms step_avg:61.03ms
step:158/2330 train_time:9644ms step_avg:61.04ms
step:159/2330 train_time:9703ms step_avg:61.03ms
step:160/2330 train_time:9766ms step_avg:61.04ms
step:161/2330 train_time:9825ms step_avg:61.03ms
step:162/2330 train_time:9889ms step_avg:61.04ms
step:163/2330 train_time:9948ms step_avg:61.03ms
step:164/2330 train_time:10010ms step_avg:61.04ms
step:165/2330 train_time:10069ms step_avg:61.03ms
step:166/2330 train_time:10131ms step_avg:61.03ms
step:167/2330 train_time:10191ms step_avg:61.02ms
step:168/2330 train_time:10253ms step_avg:61.03ms
step:169/2330 train_time:10314ms step_avg:61.03ms
step:170/2330 train_time:10376ms step_avg:61.04ms
step:171/2330 train_time:10436ms step_avg:61.03ms
step:172/2330 train_time:10498ms step_avg:61.03ms
step:173/2330 train_time:10557ms step_avg:61.02ms
step:174/2330 train_time:10620ms step_avg:61.03ms
step:175/2330 train_time:10679ms step_avg:61.02ms
step:176/2330 train_time:10741ms step_avg:61.03ms
step:177/2330 train_time:10802ms step_avg:61.03ms
step:178/2330 train_time:10864ms step_avg:61.04ms
step:179/2330 train_time:10924ms step_avg:61.03ms
step:180/2330 train_time:10987ms step_avg:61.04ms
step:181/2330 train_time:11047ms step_avg:61.03ms
step:182/2330 train_time:11110ms step_avg:61.04ms
step:183/2330 train_time:11169ms step_avg:61.03ms
step:184/2330 train_time:11231ms step_avg:61.04ms
step:185/2330 train_time:11290ms step_avg:61.03ms
step:186/2330 train_time:11352ms step_avg:61.03ms
step:187/2330 train_time:11411ms step_avg:61.02ms
step:188/2330 train_time:11473ms step_avg:61.03ms
step:189/2330 train_time:11534ms step_avg:61.02ms
step:190/2330 train_time:11597ms step_avg:61.04ms
step:191/2330 train_time:11657ms step_avg:61.03ms
step:192/2330 train_time:11720ms step_avg:61.04ms
step:193/2330 train_time:11779ms step_avg:61.03ms
step:194/2330 train_time:11841ms step_avg:61.04ms
step:195/2330 train_time:11900ms step_avg:61.03ms
step:196/2330 train_time:11963ms step_avg:61.03ms
step:197/2330 train_time:12023ms step_avg:61.03ms
step:198/2330 train_time:12085ms step_avg:61.03ms
step:199/2330 train_time:12145ms step_avg:61.03ms
step:200/2330 train_time:12207ms step_avg:61.03ms
step:201/2330 train_time:12267ms step_avg:61.03ms
step:202/2330 train_time:12330ms step_avg:61.04ms
step:203/2330 train_time:12390ms step_avg:61.03ms
step:204/2330 train_time:12452ms step_avg:61.04ms
step:205/2330 train_time:12511ms step_avg:61.03ms
step:206/2330 train_time:12573ms step_avg:61.04ms
step:207/2330 train_time:12633ms step_avg:61.03ms
step:208/2330 train_time:12696ms step_avg:61.04ms
step:209/2330 train_time:12756ms step_avg:61.03ms
step:210/2330 train_time:12819ms step_avg:61.04ms
step:211/2330 train_time:12878ms step_avg:61.04ms
step:212/2330 train_time:12941ms step_avg:61.04ms
step:213/2330 train_time:13000ms step_avg:61.03ms
step:214/2330 train_time:13062ms step_avg:61.04ms
step:215/2330 train_time:13122ms step_avg:61.03ms
step:216/2330 train_time:13184ms step_avg:61.04ms
step:217/2330 train_time:13244ms step_avg:61.03ms
step:218/2330 train_time:13306ms step_avg:61.04ms
step:219/2330 train_time:13366ms step_avg:61.03ms
step:220/2330 train_time:13429ms step_avg:61.04ms
step:221/2330 train_time:13488ms step_avg:61.03ms
step:222/2330 train_time:13551ms step_avg:61.04ms
step:223/2330 train_time:13610ms step_avg:61.03ms
step:224/2330 train_time:13673ms step_avg:61.04ms
step:225/2330 train_time:13732ms step_avg:61.03ms
step:226/2330 train_time:13794ms step_avg:61.03ms
step:227/2330 train_time:13853ms step_avg:61.03ms
step:228/2330 train_time:13917ms step_avg:61.04ms
step:229/2330 train_time:13977ms step_avg:61.04ms
step:230/2330 train_time:14039ms step_avg:61.04ms
step:231/2330 train_time:14099ms step_avg:61.03ms
step:232/2330 train_time:14161ms step_avg:61.04ms
step:233/2330 train_time:14221ms step_avg:61.03ms
step:234/2330 train_time:14283ms step_avg:61.04ms
step:235/2330 train_time:14343ms step_avg:61.03ms
step:236/2330 train_time:14405ms step_avg:61.04ms
step:237/2330 train_time:14466ms step_avg:61.04ms
step:238/2330 train_time:14529ms step_avg:61.05ms
step:239/2330 train_time:14589ms step_avg:61.04ms
step:240/2330 train_time:14651ms step_avg:61.04ms
step:241/2330 train_time:14710ms step_avg:61.04ms
step:242/2330 train_time:14773ms step_avg:61.04ms
step:243/2330 train_time:14831ms step_avg:61.03ms
step:244/2330 train_time:14893ms step_avg:61.04ms
step:245/2330 train_time:14952ms step_avg:61.03ms
step:246/2330 train_time:15016ms step_avg:61.04ms
step:247/2330 train_time:15076ms step_avg:61.03ms
step:248/2330 train_time:15138ms step_avg:61.04ms
step:249/2330 train_time:15198ms step_avg:61.04ms
step:250/2330 train_time:15261ms step_avg:61.04ms
step:250/2330 val_loss:4.0812 train_time:15325ms step_avg:61.30ms
step:251/2330 train_time:15348ms step_avg:61.15ms
step:252/2330 train_time:15385ms step_avg:61.05ms
step:253/2330 train_time:15450ms step_avg:61.07ms
step:254/2330 train_time:15515ms step_avg:61.08ms
step:255/2330 train_time:15574ms step_avg:61.07ms
step:256/2330 train_time:15637ms step_avg:61.08ms
step:257/2330 train_time:15696ms step_avg:61.07ms
step:258/2330 train_time:15758ms step_avg:61.08ms
step:259/2330 train_time:15816ms step_avg:61.06ms
step:260/2330 train_time:15877ms step_avg:61.07ms
step:261/2330 train_time:15936ms step_avg:61.06ms
step:262/2330 train_time:15998ms step_avg:61.06ms
step:263/2330 train_time:16056ms step_avg:61.05ms
step:264/2330 train_time:16118ms step_avg:61.05ms
step:265/2330 train_time:16176ms step_avg:61.04ms
step:266/2330 train_time:16238ms step_avg:61.04ms
step:267/2330 train_time:16297ms step_avg:61.04ms
step:268/2330 train_time:16362ms step_avg:61.05ms
step:269/2330 train_time:16424ms step_avg:61.06ms
step:270/2330 train_time:16488ms step_avg:61.07ms
step:271/2330 train_time:16548ms step_avg:61.06ms
step:272/2330 train_time:16611ms step_avg:61.07ms
step:273/2330 train_time:16670ms step_avg:61.06ms
step:274/2330 train_time:16732ms step_avg:61.06ms
step:275/2330 train_time:16791ms step_avg:61.06ms
step:276/2330 train_time:16852ms step_avg:61.06ms
step:277/2330 train_time:16911ms step_avg:61.05ms
step:278/2330 train_time:16973ms step_avg:61.05ms
step:279/2330 train_time:17032ms step_avg:61.05ms
step:280/2330 train_time:17094ms step_avg:61.05ms
step:281/2330 train_time:17152ms step_avg:61.04ms
step:282/2330 train_time:17214ms step_avg:61.04ms
step:283/2330 train_time:17274ms step_avg:61.04ms
step:284/2330 train_time:17336ms step_avg:61.04ms
step:285/2330 train_time:17396ms step_avg:61.04ms
step:286/2330 train_time:17458ms step_avg:61.04ms
step:287/2330 train_time:17518ms step_avg:61.04ms
step:288/2330 train_time:17582ms step_avg:61.05ms
step:289/2330 train_time:17642ms step_avg:61.05ms
step:290/2330 train_time:17704ms step_avg:61.05ms
step:291/2330 train_time:17764ms step_avg:61.04ms
step:292/2330 train_time:17826ms step_avg:61.05ms
step:293/2330 train_time:17885ms step_avg:61.04ms
step:294/2330 train_time:17947ms step_avg:61.04ms
step:295/2330 train_time:18006ms step_avg:61.04ms
step:296/2330 train_time:18069ms step_avg:61.04ms
step:297/2330 train_time:18128ms step_avg:61.04ms
step:298/2330 train_time:18190ms step_avg:61.04ms
step:299/2330 train_time:18250ms step_avg:61.04ms
step:300/2330 train_time:18312ms step_avg:61.04ms
step:301/2330 train_time:18372ms step_avg:61.04ms
step:302/2330 train_time:18436ms step_avg:61.05ms
step:303/2330 train_time:18496ms step_avg:61.04ms
step:304/2330 train_time:18558ms step_avg:61.05ms
step:305/2330 train_time:18618ms step_avg:61.04ms
step:306/2330 train_time:18681ms step_avg:61.05ms
step:307/2330 train_time:18741ms step_avg:61.04ms
step:308/2330 train_time:18803ms step_avg:61.05ms
step:309/2330 train_time:18863ms step_avg:61.04ms
step:310/2330 train_time:18925ms step_avg:61.05ms
step:311/2330 train_time:18984ms step_avg:61.04ms
step:312/2330 train_time:19046ms step_avg:61.04ms
step:313/2330 train_time:19105ms step_avg:61.04ms
step:314/2330 train_time:19167ms step_avg:61.04ms
step:315/2330 train_time:19226ms step_avg:61.04ms
step:316/2330 train_time:19289ms step_avg:61.04ms
step:317/2330 train_time:19348ms step_avg:61.04ms
step:318/2330 train_time:19411ms step_avg:61.04ms
step:319/2330 train_time:19471ms step_avg:61.04ms
step:320/2330 train_time:19534ms step_avg:61.04ms
step:321/2330 train_time:19594ms step_avg:61.04ms
step:322/2330 train_time:19657ms step_avg:61.05ms
step:323/2330 train_time:19717ms step_avg:61.04ms
step:324/2330 train_time:19779ms step_avg:61.05ms
step:325/2330 train_time:19838ms step_avg:61.04ms
step:326/2330 train_time:19900ms step_avg:61.04ms
step:327/2330 train_time:19959ms step_avg:61.04ms
step:328/2330 train_time:20023ms step_avg:61.05ms
step:329/2330 train_time:20083ms step_avg:61.04ms
step:330/2330 train_time:20145ms step_avg:61.05ms
step:331/2330 train_time:20204ms step_avg:61.04ms
step:332/2330 train_time:20266ms step_avg:61.04ms
step:333/2330 train_time:20326ms step_avg:61.04ms
step:334/2330 train_time:20389ms step_avg:61.04ms
step:335/2330 train_time:20448ms step_avg:61.04ms
step:336/2330 train_time:20511ms step_avg:61.04ms
step:337/2330 train_time:20571ms step_avg:61.04ms
step:338/2330 train_time:20633ms step_avg:61.05ms
step:339/2330 train_time:20693ms step_avg:61.04ms
step:340/2330 train_time:20756ms step_avg:61.05ms
step:341/2330 train_time:20816ms step_avg:61.04ms
step:342/2330 train_time:20878ms step_avg:61.05ms
step:343/2330 train_time:20937ms step_avg:61.04ms
step:344/2330 train_time:20999ms step_avg:61.04ms
step:345/2330 train_time:21058ms step_avg:61.04ms
step:346/2330 train_time:21121ms step_avg:61.04ms
step:347/2330 train_time:21181ms step_avg:61.04ms
step:348/2330 train_time:21243ms step_avg:61.04ms
step:349/2330 train_time:21302ms step_avg:61.04ms
step:350/2330 train_time:21365ms step_avg:61.04ms
step:351/2330 train_time:21424ms step_avg:61.04ms
step:352/2330 train_time:21487ms step_avg:61.04ms
step:353/2330 train_time:21547ms step_avg:61.04ms
step:354/2330 train_time:21609ms step_avg:61.04ms
step:355/2330 train_time:21669ms step_avg:61.04ms
step:356/2330 train_time:21731ms step_avg:61.04ms
step:357/2330 train_time:21791ms step_avg:61.04ms
step:358/2330 train_time:21853ms step_avg:61.04ms
step:359/2330 train_time:21912ms step_avg:61.04ms
step:360/2330 train_time:21975ms step_avg:61.04ms
step:361/2330 train_time:22035ms step_avg:61.04ms
step:362/2330 train_time:22097ms step_avg:61.04ms
step:363/2330 train_time:22156ms step_avg:61.04ms
step:364/2330 train_time:22219ms step_avg:61.04ms
step:365/2330 train_time:22279ms step_avg:61.04ms
step:366/2330 train_time:22342ms step_avg:61.04ms
step:367/2330 train_time:22402ms step_avg:61.04ms
step:368/2330 train_time:22464ms step_avg:61.04ms
step:369/2330 train_time:22524ms step_avg:61.04ms
step:370/2330 train_time:22586ms step_avg:61.04ms
step:371/2330 train_time:22645ms step_avg:61.04ms
step:372/2330 train_time:22707ms step_avg:61.04ms
step:373/2330 train_time:22767ms step_avg:61.04ms
step:374/2330 train_time:22829ms step_avg:61.04ms
step:375/2330 train_time:22889ms step_avg:61.04ms
step:376/2330 train_time:22952ms step_avg:61.04ms
step:377/2330 train_time:23012ms step_avg:61.04ms
step:378/2330 train_time:23074ms step_avg:61.04ms
step:379/2330 train_time:23134ms step_avg:61.04ms
step:380/2330 train_time:23196ms step_avg:61.04ms
step:381/2330 train_time:23256ms step_avg:61.04ms
step:382/2330 train_time:23318ms step_avg:61.04ms
step:383/2330 train_time:23377ms step_avg:61.04ms
step:384/2330 train_time:23440ms step_avg:61.04ms
step:385/2330 train_time:23500ms step_avg:61.04ms
step:386/2330 train_time:23563ms step_avg:61.04ms
step:387/2330 train_time:23623ms step_avg:61.04ms
step:388/2330 train_time:23686ms step_avg:61.05ms
step:389/2330 train_time:23745ms step_avg:61.04ms
step:390/2330 train_time:23808ms step_avg:61.05ms
step:391/2330 train_time:23867ms step_avg:61.04ms
step:392/2330 train_time:23929ms step_avg:61.04ms
step:393/2330 train_time:23989ms step_avg:61.04ms
step:394/2330 train_time:24051ms step_avg:61.04ms
step:395/2330 train_time:24111ms step_avg:61.04ms
step:396/2330 train_time:24174ms step_avg:61.04ms
step:397/2330 train_time:24233ms step_avg:61.04ms
step:398/2330 train_time:24296ms step_avg:61.05ms
step:399/2330 train_time:24356ms step_avg:61.04ms
step:400/2330 train_time:24418ms step_avg:61.05ms
step:401/2330 train_time:24477ms step_avg:61.04ms
step:402/2330 train_time:24540ms step_avg:61.04ms
step:403/2330 train_time:24599ms step_avg:61.04ms
step:404/2330 train_time:24662ms step_avg:61.04ms
step:405/2330 train_time:24723ms step_avg:61.04ms
step:406/2330 train_time:24786ms step_avg:61.05ms
step:407/2330 train_time:24845ms step_avg:61.05ms
step:408/2330 train_time:24908ms step_avg:61.05ms
step:409/2330 train_time:24967ms step_avg:61.04ms
step:410/2330 train_time:25028ms step_avg:61.05ms
step:411/2330 train_time:25088ms step_avg:61.04ms
step:412/2330 train_time:25150ms step_avg:61.04ms
step:413/2330 train_time:25210ms step_avg:61.04ms
step:414/2330 train_time:25272ms step_avg:61.04ms
step:415/2330 train_time:25333ms step_avg:61.04ms
step:416/2330 train_time:25396ms step_avg:61.05ms
step:417/2330 train_time:25456ms step_avg:61.05ms
step:418/2330 train_time:25518ms step_avg:61.05ms
step:419/2330 train_time:25577ms step_avg:61.04ms
step:420/2330 train_time:25640ms step_avg:61.05ms
step:421/2330 train_time:25699ms step_avg:61.04ms
step:422/2330 train_time:25762ms step_avg:61.05ms
step:423/2330 train_time:25822ms step_avg:61.04ms
step:424/2330 train_time:25884ms step_avg:61.05ms
step:425/2330 train_time:25944ms step_avg:61.04ms
step:426/2330 train_time:26006ms step_avg:61.05ms
step:427/2330 train_time:26066ms step_avg:61.04ms
step:428/2330 train_time:26128ms step_avg:61.05ms
step:429/2330 train_time:26188ms step_avg:61.04ms
step:430/2330 train_time:26250ms step_avg:61.05ms
step:431/2330 train_time:26310ms step_avg:61.04ms
step:432/2330 train_time:26372ms step_avg:61.05ms
step:433/2330 train_time:26432ms step_avg:61.04ms
step:434/2330 train_time:26495ms step_avg:61.05ms
step:435/2330 train_time:26555ms step_avg:61.05ms
step:436/2330 train_time:26617ms step_avg:61.05ms
step:437/2330 train_time:26676ms step_avg:61.04ms
step:438/2330 train_time:26739ms step_avg:61.05ms
step:439/2330 train_time:26798ms step_avg:61.04ms
step:440/2330 train_time:26860ms step_avg:61.05ms
step:441/2330 train_time:26920ms step_avg:61.04ms
step:442/2330 train_time:26984ms step_avg:61.05ms
step:443/2330 train_time:27043ms step_avg:61.05ms
step:444/2330 train_time:27105ms step_avg:61.05ms
step:445/2330 train_time:27165ms step_avg:61.04ms
step:446/2330 train_time:27227ms step_avg:61.05ms
step:447/2330 train_time:27287ms step_avg:61.05ms
step:448/2330 train_time:27349ms step_avg:61.05ms
step:449/2330 train_time:27409ms step_avg:61.05ms
step:450/2330 train_time:27472ms step_avg:61.05ms
step:451/2330 train_time:27531ms step_avg:61.04ms
step:452/2330 train_time:27594ms step_avg:61.05ms
step:453/2330 train_time:27654ms step_avg:61.05ms
step:454/2330 train_time:27717ms step_avg:61.05ms
step:455/2330 train_time:27776ms step_avg:61.05ms
step:456/2330 train_time:27840ms step_avg:61.05ms
step:457/2330 train_time:27899ms step_avg:61.05ms
step:458/2330 train_time:27961ms step_avg:61.05ms
step:459/2330 train_time:28021ms step_avg:61.05ms
step:460/2330 train_time:28084ms step_avg:61.05ms
step:461/2330 train_time:28144ms step_avg:61.05ms
step:462/2330 train_time:28206ms step_avg:61.05ms
step:463/2330 train_time:28266ms step_avg:61.05ms
step:464/2330 train_time:28329ms step_avg:61.05ms
step:465/2330 train_time:28388ms step_avg:61.05ms
step:466/2330 train_time:28451ms step_avg:61.05ms
step:467/2330 train_time:28511ms step_avg:61.05ms
step:468/2330 train_time:28574ms step_avg:61.06ms
step:469/2330 train_time:28633ms step_avg:61.05ms
step:470/2330 train_time:28696ms step_avg:61.06ms
step:471/2330 train_time:28756ms step_avg:61.05ms
step:472/2330 train_time:28818ms step_avg:61.06ms
step:473/2330 train_time:28878ms step_avg:61.05ms
step:474/2330 train_time:28941ms step_avg:61.06ms
step:475/2330 train_time:29000ms step_avg:61.05ms
step:476/2330 train_time:29062ms step_avg:61.06ms
step:477/2330 train_time:29121ms step_avg:61.05ms
step:478/2330 train_time:29184ms step_avg:61.05ms
step:479/2330 train_time:29244ms step_avg:61.05ms
step:480/2330 train_time:29307ms step_avg:61.06ms
step:481/2330 train_time:29366ms step_avg:61.05ms
step:482/2330 train_time:29429ms step_avg:61.06ms
step:483/2330 train_time:29488ms step_avg:61.05ms
step:484/2330 train_time:29551ms step_avg:61.05ms
step:485/2330 train_time:29611ms step_avg:61.05ms
step:486/2330 train_time:29673ms step_avg:61.06ms
step:487/2330 train_time:29732ms step_avg:61.05ms
step:488/2330 train_time:29796ms step_avg:61.06ms
step:489/2330 train_time:29856ms step_avg:61.06ms
step:490/2330 train_time:29919ms step_avg:61.06ms
step:491/2330 train_time:29978ms step_avg:61.05ms
step:492/2330 train_time:30041ms step_avg:61.06ms
step:493/2330 train_time:30100ms step_avg:61.06ms
step:494/2330 train_time:30163ms step_avg:61.06ms
step:495/2330 train_time:30223ms step_avg:61.06ms
step:496/2330 train_time:30286ms step_avg:61.06ms
step:497/2330 train_time:30345ms step_avg:61.06ms
step:498/2330 train_time:30407ms step_avg:61.06ms
step:499/2330 train_time:30467ms step_avg:61.06ms
step:500/2330 train_time:30529ms step_avg:61.06ms
step:500/2330 val_loss:3.8173 train_time:30593ms step_avg:61.19ms
step:501/2330 train_time:30617ms step_avg:61.11ms
step:502/2330 train_time:30655ms step_avg:61.07ms
step:503/2330 train_time:30718ms step_avg:61.07ms
step:504/2330 train_time:30781ms step_avg:61.07ms
step:505/2330 train_time:30840ms step_avg:61.07ms
step:506/2330 train_time:30902ms step_avg:61.07ms
step:507/2330 train_time:30961ms step_avg:61.07ms
step:508/2330 train_time:31023ms step_avg:61.07ms
step:509/2330 train_time:31081ms step_avg:61.06ms
step:510/2330 train_time:31143ms step_avg:61.06ms
step:511/2330 train_time:31202ms step_avg:61.06ms
step:512/2330 train_time:31264ms step_avg:61.06ms
step:513/2330 train_time:31322ms step_avg:61.06ms
step:514/2330 train_time:31384ms step_avg:61.06ms
step:515/2330 train_time:31442ms step_avg:61.05ms
step:516/2330 train_time:31504ms step_avg:61.05ms
step:517/2330 train_time:31565ms step_avg:61.05ms
step:518/2330 train_time:31630ms step_avg:61.06ms
step:519/2330 train_time:31691ms step_avg:61.06ms
step:520/2330 train_time:31755ms step_avg:61.07ms
step:521/2330 train_time:31814ms step_avg:61.06ms
step:522/2330 train_time:31876ms step_avg:61.07ms
step:523/2330 train_time:31936ms step_avg:61.06ms
step:524/2330 train_time:31998ms step_avg:61.07ms
step:525/2330 train_time:32057ms step_avg:61.06ms
step:526/2330 train_time:32120ms step_avg:61.06ms
step:527/2330 train_time:32179ms step_avg:61.06ms
step:528/2330 train_time:32240ms step_avg:61.06ms
step:529/2330 train_time:32299ms step_avg:61.06ms
step:530/2330 train_time:32361ms step_avg:61.06ms
step:531/2330 train_time:32420ms step_avg:61.05ms
step:532/2330 train_time:32482ms step_avg:61.06ms
step:533/2330 train_time:32542ms step_avg:61.05ms
step:534/2330 train_time:32605ms step_avg:61.06ms
step:535/2330 train_time:32666ms step_avg:61.06ms
step:536/2330 train_time:32729ms step_avg:61.06ms
step:537/2330 train_time:32789ms step_avg:61.06ms
step:538/2330 train_time:32852ms step_avg:61.06ms
step:539/2330 train_time:32912ms step_avg:61.06ms
step:540/2330 train_time:32974ms step_avg:61.06ms
step:541/2330 train_time:33034ms step_avg:61.06ms
step:542/2330 train_time:33096ms step_avg:61.06ms
step:543/2330 train_time:33155ms step_avg:61.06ms
step:544/2330 train_time:33218ms step_avg:61.06ms
step:545/2330 train_time:33277ms step_avg:61.06ms
step:546/2330 train_time:33339ms step_avg:61.06ms
step:547/2330 train_time:33398ms step_avg:61.06ms
step:548/2330 train_time:33461ms step_avg:61.06ms
step:549/2330 train_time:33521ms step_avg:61.06ms
step:550/2330 train_time:33583ms step_avg:61.06ms
step:551/2330 train_time:33643ms step_avg:61.06ms
step:552/2330 train_time:33705ms step_avg:61.06ms
step:553/2330 train_time:33766ms step_avg:61.06ms
step:554/2330 train_time:33828ms step_avg:61.06ms
step:555/2330 train_time:33888ms step_avg:61.06ms
step:556/2330 train_time:33951ms step_avg:61.06ms
step:557/2330 train_time:34010ms step_avg:61.06ms
step:558/2330 train_time:34073ms step_avg:61.06ms
step:559/2330 train_time:34132ms step_avg:61.06ms
step:560/2330 train_time:34195ms step_avg:61.06ms
step:561/2330 train_time:34254ms step_avg:61.06ms
step:562/2330 train_time:34316ms step_avg:61.06ms
step:563/2330 train_time:34375ms step_avg:61.06ms
step:564/2330 train_time:34438ms step_avg:61.06ms
step:565/2330 train_time:34498ms step_avg:61.06ms
step:566/2330 train_time:34560ms step_avg:61.06ms
step:567/2330 train_time:34620ms step_avg:61.06ms
step:568/2330 train_time:34682ms step_avg:61.06ms
step:569/2330 train_time:34741ms step_avg:61.06ms
step:570/2330 train_time:34803ms step_avg:61.06ms
step:571/2330 train_time:34863ms step_avg:61.06ms
step:572/2330 train_time:34926ms step_avg:61.06ms
step:573/2330 train_time:34986ms step_avg:61.06ms
step:574/2330 train_time:35048ms step_avg:61.06ms
step:575/2330 train_time:35107ms step_avg:61.06ms
step:576/2330 train_time:35170ms step_avg:61.06ms
step:577/2330 train_time:35229ms step_avg:61.06ms
step:578/2330 train_time:35291ms step_avg:61.06ms
step:579/2330 train_time:35351ms step_avg:61.06ms
step:580/2330 train_time:35413ms step_avg:61.06ms
step:581/2330 train_time:35474ms step_avg:61.06ms
step:582/2330 train_time:35536ms step_avg:61.06ms
step:583/2330 train_time:35596ms step_avg:61.06ms
step:584/2330 train_time:35659ms step_avg:61.06ms
step:585/2330 train_time:35718ms step_avg:61.06ms
step:586/2330 train_time:35781ms step_avg:61.06ms
step:587/2330 train_time:35841ms step_avg:61.06ms
step:588/2330 train_time:35903ms step_avg:61.06ms
step:589/2330 train_time:35962ms step_avg:61.06ms
step:590/2330 train_time:36025ms step_avg:61.06ms
step:591/2330 train_time:36084ms step_avg:61.06ms
step:592/2330 train_time:36147ms step_avg:61.06ms
step:593/2330 train_time:36206ms step_avg:61.06ms
step:594/2330 train_time:36269ms step_avg:61.06ms
step:595/2330 train_time:36329ms step_avg:61.06ms
step:596/2330 train_time:36392ms step_avg:61.06ms
step:597/2330 train_time:36451ms step_avg:61.06ms
step:598/2330 train_time:36514ms step_avg:61.06ms
step:599/2330 train_time:36573ms step_avg:61.06ms
step:600/2330 train_time:36635ms step_avg:61.06ms
step:601/2330 train_time:36694ms step_avg:61.06ms
step:602/2330 train_time:36757ms step_avg:61.06ms
step:603/2330 train_time:36817ms step_avg:61.06ms
step:604/2330 train_time:36880ms step_avg:61.06ms
step:605/2330 train_time:36940ms step_avg:61.06ms
step:606/2330 train_time:37002ms step_avg:61.06ms
step:607/2330 train_time:37061ms step_avg:61.06ms
step:608/2330 train_time:37123ms step_avg:61.06ms
step:609/2330 train_time:37182ms step_avg:61.05ms
step:610/2330 train_time:37244ms step_avg:61.06ms
step:611/2330 train_time:37304ms step_avg:61.05ms
step:612/2330 train_time:37366ms step_avg:61.06ms
step:613/2330 train_time:37426ms step_avg:61.05ms
step:614/2330 train_time:37489ms step_avg:61.06ms
step:615/2330 train_time:37549ms step_avg:61.06ms
step:616/2330 train_time:37612ms step_avg:61.06ms
step:617/2330 train_time:37672ms step_avg:61.06ms
step:618/2330 train_time:37734ms step_avg:61.06ms
step:619/2330 train_time:37794ms step_avg:61.06ms
step:620/2330 train_time:37856ms step_avg:61.06ms
step:621/2330 train_time:37916ms step_avg:61.06ms
step:622/2330 train_time:37979ms step_avg:61.06ms
step:623/2330 train_time:38039ms step_avg:61.06ms
step:624/2330 train_time:38102ms step_avg:61.06ms
step:625/2330 train_time:38161ms step_avg:61.06ms
step:626/2330 train_time:38224ms step_avg:61.06ms
step:627/2330 train_time:38283ms step_avg:61.06ms
step:628/2330 train_time:38345ms step_avg:61.06ms
step:629/2330 train_time:38405ms step_avg:61.06ms
step:630/2330 train_time:38467ms step_avg:61.06ms
step:631/2330 train_time:38527ms step_avg:61.06ms
step:632/2330 train_time:38589ms step_avg:61.06ms
step:633/2330 train_time:38649ms step_avg:61.06ms
step:634/2330 train_time:38712ms step_avg:61.06ms
step:635/2330 train_time:38772ms step_avg:61.06ms
step:636/2330 train_time:38834ms step_avg:61.06ms
step:637/2330 train_time:38894ms step_avg:61.06ms
step:638/2330 train_time:38957ms step_avg:61.06ms
step:639/2330 train_time:39017ms step_avg:61.06ms
step:640/2330 train_time:39080ms step_avg:61.06ms
step:641/2330 train_time:39140ms step_avg:61.06ms
step:642/2330 train_time:39202ms step_avg:61.06ms
step:643/2330 train_time:39261ms step_avg:61.06ms
step:644/2330 train_time:39323ms step_avg:61.06ms
step:645/2330 train_time:39382ms step_avg:61.06ms
step:646/2330 train_time:39445ms step_avg:61.06ms
step:647/2330 train_time:39504ms step_avg:61.06ms
step:648/2330 train_time:39567ms step_avg:61.06ms
step:649/2330 train_time:39627ms step_avg:61.06ms
step:650/2330 train_time:39689ms step_avg:61.06ms
step:651/2330 train_time:39749ms step_avg:61.06ms
step:652/2330 train_time:39812ms step_avg:61.06ms
step:653/2330 train_time:39872ms step_avg:61.06ms
step:654/2330 train_time:39934ms step_avg:61.06ms
step:655/2330 train_time:39994ms step_avg:61.06ms
step:656/2330 train_time:40056ms step_avg:61.06ms
step:657/2330 train_time:40115ms step_avg:61.06ms
step:658/2330 train_time:40179ms step_avg:61.06ms
step:659/2330 train_time:40239ms step_avg:61.06ms
step:660/2330 train_time:40302ms step_avg:61.06ms
step:661/2330 train_time:40361ms step_avg:61.06ms
step:662/2330 train_time:40424ms step_avg:61.06ms
step:663/2330 train_time:40483ms step_avg:61.06ms
step:664/2330 train_time:40546ms step_avg:61.06ms
step:665/2330 train_time:40606ms step_avg:61.06ms
step:666/2330 train_time:40668ms step_avg:61.06ms
step:667/2330 train_time:40728ms step_avg:61.06ms
step:668/2330 train_time:40790ms step_avg:61.06ms
step:669/2330 train_time:40850ms step_avg:61.06ms
step:670/2330 train_time:40914ms step_avg:61.07ms
step:671/2330 train_time:40973ms step_avg:61.06ms
step:672/2330 train_time:41035ms step_avg:61.06ms
step:673/2330 train_time:41095ms step_avg:61.06ms
step:674/2330 train_time:41157ms step_avg:61.06ms
step:675/2330 train_time:41217ms step_avg:61.06ms
step:676/2330 train_time:41280ms step_avg:61.06ms
step:677/2330 train_time:41341ms step_avg:61.06ms
step:678/2330 train_time:41403ms step_avg:61.07ms
step:679/2330 train_time:41462ms step_avg:61.06ms
step:680/2330 train_time:41525ms step_avg:61.07ms
step:681/2330 train_time:41583ms step_avg:61.06ms
step:682/2330 train_time:41646ms step_avg:61.06ms
step:683/2330 train_time:41705ms step_avg:61.06ms
step:684/2330 train_time:41767ms step_avg:61.06ms
step:685/2330 train_time:41827ms step_avg:61.06ms
step:686/2330 train_time:41890ms step_avg:61.06ms
step:687/2330 train_time:41949ms step_avg:61.06ms
step:688/2330 train_time:42013ms step_avg:61.07ms
step:689/2330 train_time:42073ms step_avg:61.06ms
step:690/2330 train_time:42135ms step_avg:61.06ms
step:691/2330 train_time:42194ms step_avg:61.06ms
step:692/2330 train_time:42256ms step_avg:61.06ms
step:693/2330 train_time:42316ms step_avg:61.06ms
step:694/2330 train_time:42379ms step_avg:61.06ms
step:695/2330 train_time:42439ms step_avg:61.06ms
step:696/2330 train_time:42501ms step_avg:61.06ms
step:697/2330 train_time:42561ms step_avg:61.06ms
step:698/2330 train_time:42623ms step_avg:61.06ms
step:699/2330 train_time:42682ms step_avg:61.06ms
step:700/2330 train_time:42744ms step_avg:61.06ms
step:701/2330 train_time:42804ms step_avg:61.06ms
step:702/2330 train_time:42867ms step_avg:61.06ms
step:703/2330 train_time:42927ms step_avg:61.06ms
step:704/2330 train_time:42989ms step_avg:61.06ms
step:705/2330 train_time:43049ms step_avg:61.06ms
step:706/2330 train_time:43112ms step_avg:61.07ms
step:707/2330 train_time:43173ms step_avg:61.06ms
step:708/2330 train_time:43234ms step_avg:61.07ms
step:709/2330 train_time:43293ms step_avg:61.06ms
step:710/2330 train_time:43356ms step_avg:61.06ms
step:711/2330 train_time:43415ms step_avg:61.06ms
step:712/2330 train_time:43479ms step_avg:61.07ms
step:713/2330 train_time:43539ms step_avg:61.07ms
step:714/2330 train_time:43602ms step_avg:61.07ms
step:715/2330 train_time:43661ms step_avg:61.07ms
step:716/2330 train_time:43724ms step_avg:61.07ms
step:717/2330 train_time:43783ms step_avg:61.06ms
step:718/2330 train_time:43845ms step_avg:61.07ms
step:719/2330 train_time:43905ms step_avg:61.06ms
step:720/2330 train_time:43967ms step_avg:61.07ms
step:721/2330 train_time:44027ms step_avg:61.06ms
step:722/2330 train_time:44090ms step_avg:61.07ms
step:723/2330 train_time:44149ms step_avg:61.06ms
step:724/2330 train_time:44212ms step_avg:61.07ms
step:725/2330 train_time:44272ms step_avg:61.06ms
step:726/2330 train_time:44334ms step_avg:61.07ms
step:727/2330 train_time:44392ms step_avg:61.06ms
step:728/2330 train_time:44455ms step_avg:61.06ms
step:729/2330 train_time:44515ms step_avg:61.06ms
step:730/2330 train_time:44578ms step_avg:61.07ms
step:731/2330 train_time:44637ms step_avg:61.06ms
step:732/2330 train_time:44700ms step_avg:61.07ms
step:733/2330 train_time:44760ms step_avg:61.06ms
step:734/2330 train_time:44822ms step_avg:61.07ms
step:735/2330 train_time:44881ms step_avg:61.06ms
step:736/2330 train_time:44943ms step_avg:61.06ms
step:737/2330 train_time:45002ms step_avg:61.06ms
step:738/2330 train_time:45065ms step_avg:61.06ms
step:739/2330 train_time:45125ms step_avg:61.06ms
step:740/2330 train_time:45187ms step_avg:61.06ms
step:741/2330 train_time:45247ms step_avg:61.06ms
step:742/2330 train_time:45310ms step_avg:61.06ms
step:743/2330 train_time:45370ms step_avg:61.06ms
step:744/2330 train_time:45433ms step_avg:61.07ms
step:745/2330 train_time:45493ms step_avg:61.06ms
step:746/2330 train_time:45555ms step_avg:61.07ms
step:747/2330 train_time:45615ms step_avg:61.06ms
step:748/2330 train_time:45678ms step_avg:61.07ms
step:749/2330 train_time:45737ms step_avg:61.06ms
step:750/2330 train_time:45800ms step_avg:61.07ms
step:750/2330 val_loss:3.6952 train_time:45865ms step_avg:61.15ms
step:751/2330 train_time:45887ms step_avg:61.10ms
step:752/2330 train_time:45926ms step_avg:61.07ms
step:753/2330 train_time:45989ms step_avg:61.07ms
step:754/2330 train_time:46054ms step_avg:61.08ms
step:755/2330 train_time:46113ms step_avg:61.08ms
step:756/2330 train_time:46175ms step_avg:61.08ms
step:757/2330 train_time:46234ms step_avg:61.08ms
step:758/2330 train_time:46296ms step_avg:61.08ms
step:759/2330 train_time:46354ms step_avg:61.07ms
step:760/2330 train_time:46417ms step_avg:61.07ms
step:761/2330 train_time:46475ms step_avg:61.07ms
step:762/2330 train_time:46537ms step_avg:61.07ms
step:763/2330 train_time:46596ms step_avg:61.07ms
step:764/2330 train_time:46657ms step_avg:61.07ms
step:765/2330 train_time:46716ms step_avg:61.07ms
step:766/2330 train_time:46780ms step_avg:61.07ms
step:767/2330 train_time:46841ms step_avg:61.07ms
step:768/2330 train_time:46905ms step_avg:61.07ms
step:769/2330 train_time:46967ms step_avg:61.08ms
step:770/2330 train_time:47031ms step_avg:61.08ms
step:771/2330 train_time:47092ms step_avg:61.08ms
step:772/2330 train_time:47156ms step_avg:61.08ms
step:773/2330 train_time:47216ms step_avg:61.08ms
step:774/2330 train_time:47279ms step_avg:61.08ms
step:775/2330 train_time:47338ms step_avg:61.08ms
step:776/2330 train_time:47401ms step_avg:61.08ms
step:777/2330 train_time:47460ms step_avg:61.08ms
step:778/2330 train_time:47523ms step_avg:61.08ms
step:779/2330 train_time:47583ms step_avg:61.08ms
step:780/2330 train_time:47645ms step_avg:61.08ms
step:781/2330 train_time:47705ms step_avg:61.08ms
step:782/2330 train_time:47768ms step_avg:61.08ms
step:783/2330 train_time:47828ms step_avg:61.08ms
step:784/2330 train_time:47892ms step_avg:61.09ms
step:785/2330 train_time:47952ms step_avg:61.09ms
step:786/2330 train_time:48016ms step_avg:61.09ms
step:787/2330 train_time:48076ms step_avg:61.09ms
step:788/2330 train_time:48140ms step_avg:61.09ms
step:789/2330 train_time:48200ms step_avg:61.09ms
step:790/2330 train_time:48262ms step_avg:61.09ms
step:791/2330 train_time:48322ms step_avg:61.09ms
step:792/2330 train_time:48385ms step_avg:61.09ms
step:793/2330 train_time:48444ms step_avg:61.09ms
step:794/2330 train_time:48508ms step_avg:61.09ms
step:795/2330 train_time:48569ms step_avg:61.09ms
step:796/2330 train_time:48631ms step_avg:61.09ms
step:797/2330 train_time:48691ms step_avg:61.09ms
step:798/2330 train_time:48754ms step_avg:61.10ms
step:799/2330 train_time:48814ms step_avg:61.09ms
step:800/2330 train_time:48877ms step_avg:61.10ms
step:801/2330 train_time:48937ms step_avg:61.09ms
step:802/2330 train_time:49000ms step_avg:61.10ms
step:803/2330 train_time:49062ms step_avg:61.10ms
step:804/2330 train_time:49125ms step_avg:61.10ms
step:805/2330 train_time:49185ms step_avg:61.10ms
step:806/2330 train_time:49248ms step_avg:61.10ms
step:807/2330 train_time:49308ms step_avg:61.10ms
step:808/2330 train_time:49371ms step_avg:61.10ms
step:809/2330 train_time:49431ms step_avg:61.10ms
step:810/2330 train_time:49494ms step_avg:61.10ms
step:811/2330 train_time:49554ms step_avg:61.10ms
step:812/2330 train_time:49616ms step_avg:61.10ms
step:813/2330 train_time:49676ms step_avg:61.10ms
step:814/2330 train_time:49739ms step_avg:61.10ms
step:815/2330 train_time:49799ms step_avg:61.10ms
step:816/2330 train_time:49862ms step_avg:61.11ms
step:817/2330 train_time:49923ms step_avg:61.10ms
step:818/2330 train_time:49985ms step_avg:61.11ms
step:819/2330 train_time:50046ms step_avg:61.11ms
step:820/2330 train_time:50109ms step_avg:61.11ms
step:821/2330 train_time:50169ms step_avg:61.11ms
step:822/2330 train_time:50232ms step_avg:61.11ms
step:823/2330 train_time:50292ms step_avg:61.11ms
step:824/2330 train_time:50355ms step_avg:61.11ms
step:825/2330 train_time:50415ms step_avg:61.11ms
step:826/2330 train_time:50478ms step_avg:61.11ms
step:827/2330 train_time:50538ms step_avg:61.11ms
step:828/2330 train_time:50601ms step_avg:61.11ms
step:829/2330 train_time:50661ms step_avg:61.11ms
step:830/2330 train_time:50723ms step_avg:61.11ms
step:831/2330 train_time:50784ms step_avg:61.11ms
step:832/2330 train_time:50848ms step_avg:61.12ms
step:833/2330 train_time:50909ms step_avg:61.11ms
step:834/2330 train_time:50971ms step_avg:61.12ms
step:835/2330 train_time:51032ms step_avg:61.12ms
step:836/2330 train_time:51095ms step_avg:61.12ms
step:837/2330 train_time:51156ms step_avg:61.12ms
step:838/2330 train_time:51219ms step_avg:61.12ms
step:839/2330 train_time:51279ms step_avg:61.12ms
step:840/2330 train_time:51342ms step_avg:61.12ms
step:841/2330 train_time:51402ms step_avg:61.12ms
step:842/2330 train_time:51465ms step_avg:61.12ms
step:843/2330 train_time:51525ms step_avg:61.12ms
step:844/2330 train_time:51588ms step_avg:61.12ms
step:845/2330 train_time:51648ms step_avg:61.12ms
step:846/2330 train_time:51712ms step_avg:61.13ms
step:847/2330 train_time:51772ms step_avg:61.12ms
step:848/2330 train_time:51836ms step_avg:61.13ms
step:849/2330 train_time:51896ms step_avg:61.13ms
step:850/2330 train_time:51959ms step_avg:61.13ms
step:851/2330 train_time:52019ms step_avg:61.13ms
step:852/2330 train_time:52081ms step_avg:61.13ms
step:853/2330 train_time:52142ms step_avg:61.13ms
step:854/2330 train_time:52206ms step_avg:61.13ms
step:855/2330 train_time:52266ms step_avg:61.13ms
step:856/2330 train_time:52329ms step_avg:61.13ms
step:857/2330 train_time:52389ms step_avg:61.13ms
step:858/2330 train_time:52452ms step_avg:61.13ms
step:859/2330 train_time:52512ms step_avg:61.13ms
step:860/2330 train_time:52575ms step_avg:61.13ms
step:861/2330 train_time:52635ms step_avg:61.13ms
step:862/2330 train_time:52698ms step_avg:61.13ms
step:863/2330 train_time:52758ms step_avg:61.13ms
step:864/2330 train_time:52821ms step_avg:61.14ms
step:865/2330 train_time:52881ms step_avg:61.13ms
step:866/2330 train_time:52944ms step_avg:61.14ms
step:867/2330 train_time:53004ms step_avg:61.14ms
step:868/2330 train_time:53067ms step_avg:61.14ms
step:869/2330 train_time:53128ms step_avg:61.14ms
step:870/2330 train_time:53191ms step_avg:61.14ms
step:871/2330 train_time:53252ms step_avg:61.14ms
step:872/2330 train_time:53315ms step_avg:61.14ms
step:873/2330 train_time:53374ms step_avg:61.14ms
step:874/2330 train_time:53437ms step_avg:61.14ms
step:875/2330 train_time:53497ms step_avg:61.14ms
step:876/2330 train_time:53560ms step_avg:61.14ms
step:877/2330 train_time:53621ms step_avg:61.14ms
step:878/2330 train_time:53683ms step_avg:61.14ms
step:879/2330 train_time:53743ms step_avg:61.14ms
step:880/2330 train_time:53806ms step_avg:61.14ms
step:881/2330 train_time:53866ms step_avg:61.14ms
step:882/2330 train_time:53930ms step_avg:61.14ms
step:883/2330 train_time:53990ms step_avg:61.14ms
step:884/2330 train_time:54053ms step_avg:61.15ms
step:885/2330 train_time:54113ms step_avg:61.14ms
step:886/2330 train_time:54176ms step_avg:61.15ms
step:887/2330 train_time:54236ms step_avg:61.15ms
step:888/2330 train_time:54299ms step_avg:61.15ms
step:889/2330 train_time:54359ms step_avg:61.15ms
step:890/2330 train_time:54422ms step_avg:61.15ms
step:891/2330 train_time:54482ms step_avg:61.15ms
step:892/2330 train_time:54545ms step_avg:61.15ms
step:893/2330 train_time:54605ms step_avg:61.15ms
step:894/2330 train_time:54668ms step_avg:61.15ms
step:895/2330 train_time:54728ms step_avg:61.15ms
step:896/2330 train_time:54791ms step_avg:61.15ms
step:897/2330 train_time:54851ms step_avg:61.15ms
step:898/2330 train_time:54914ms step_avg:61.15ms
step:899/2330 train_time:54974ms step_avg:61.15ms
step:900/2330 train_time:55037ms step_avg:61.15ms
step:901/2330 train_time:55097ms step_avg:61.15ms
step:902/2330 train_time:55161ms step_avg:61.15ms
step:903/2330 train_time:55221ms step_avg:61.15ms
step:904/2330 train_time:55284ms step_avg:61.16ms
step:905/2330 train_time:55345ms step_avg:61.15ms
step:906/2330 train_time:55408ms step_avg:61.16ms
step:907/2330 train_time:55469ms step_avg:61.16ms
step:908/2330 train_time:55532ms step_avg:61.16ms
step:909/2330 train_time:55592ms step_avg:61.16ms
step:910/2330 train_time:55656ms step_avg:61.16ms
step:911/2330 train_time:55717ms step_avg:61.16ms
step:912/2330 train_time:55780ms step_avg:61.16ms
step:913/2330 train_time:55840ms step_avg:61.16ms
step:914/2330 train_time:55903ms step_avg:61.16ms
step:915/2330 train_time:55963ms step_avg:61.16ms
step:916/2330 train_time:56027ms step_avg:61.16ms
step:917/2330 train_time:56088ms step_avg:61.16ms
step:918/2330 train_time:56151ms step_avg:61.17ms
step:919/2330 train_time:56212ms step_avg:61.17ms
step:920/2330 train_time:56275ms step_avg:61.17ms
step:921/2330 train_time:56335ms step_avg:61.17ms
step:922/2330 train_time:56398ms step_avg:61.17ms
step:923/2330 train_time:56459ms step_avg:61.17ms
step:924/2330 train_time:56522ms step_avg:61.17ms
step:925/2330 train_time:56582ms step_avg:61.17ms
step:926/2330 train_time:56645ms step_avg:61.17ms
step:927/2330 train_time:56705ms step_avg:61.17ms
step:928/2330 train_time:56768ms step_avg:61.17ms
step:929/2330 train_time:56827ms step_avg:61.17ms
step:930/2330 train_time:56890ms step_avg:61.17ms
step:931/2330 train_time:56950ms step_avg:61.17ms
step:932/2330 train_time:57013ms step_avg:61.17ms
step:933/2330 train_time:57073ms step_avg:61.17ms
step:934/2330 train_time:57135ms step_avg:61.17ms
step:935/2330 train_time:57195ms step_avg:61.17ms
step:936/2330 train_time:57258ms step_avg:61.17ms
step:937/2330 train_time:57318ms step_avg:61.17ms
step:938/2330 train_time:57380ms step_avg:61.17ms
step:939/2330 train_time:57440ms step_avg:61.17ms
step:940/2330 train_time:57503ms step_avg:61.17ms
step:941/2330 train_time:57563ms step_avg:61.17ms
step:942/2330 train_time:57627ms step_avg:61.17ms
step:943/2330 train_time:57687ms step_avg:61.17ms
step:944/2330 train_time:57750ms step_avg:61.18ms
step:945/2330 train_time:57810ms step_avg:61.17ms
step:946/2330 train_time:57873ms step_avg:61.18ms
step:947/2330 train_time:57932ms step_avg:61.17ms
step:948/2330 train_time:57995ms step_avg:61.18ms
step:949/2330 train_time:58056ms step_avg:61.18ms
step:950/2330 train_time:58119ms step_avg:61.18ms
step:951/2330 train_time:58178ms step_avg:61.18ms
step:952/2330 train_time:58241ms step_avg:61.18ms
step:953/2330 train_time:58301ms step_avg:61.18ms
step:954/2330 train_time:58363ms step_avg:61.18ms
step:955/2330 train_time:58423ms step_avg:61.18ms
step:956/2330 train_time:58486ms step_avg:61.18ms
step:957/2330 train_time:58547ms step_avg:61.18ms
step:958/2330 train_time:58610ms step_avg:61.18ms
step:959/2330 train_time:58671ms step_avg:61.18ms
step:960/2330 train_time:58734ms step_avg:61.18ms
step:961/2330 train_time:58794ms step_avg:61.18ms
step:962/2330 train_time:58857ms step_avg:61.18ms
step:963/2330 train_time:58917ms step_avg:61.18ms
step:964/2330 train_time:58980ms step_avg:61.18ms
step:965/2330 train_time:59040ms step_avg:61.18ms
step:966/2330 train_time:59102ms step_avg:61.18ms
step:967/2330 train_time:59163ms step_avg:61.18ms
step:968/2330 train_time:59226ms step_avg:61.18ms
step:969/2330 train_time:59286ms step_avg:61.18ms
step:970/2330 train_time:59349ms step_avg:61.18ms
step:971/2330 train_time:59410ms step_avg:61.18ms
step:972/2330 train_time:59472ms step_avg:61.19ms
step:973/2330 train_time:59532ms step_avg:61.18ms
step:974/2330 train_time:59595ms step_avg:61.19ms
step:975/2330 train_time:59656ms step_avg:61.19ms
step:976/2330 train_time:59719ms step_avg:61.19ms
step:977/2330 train_time:59779ms step_avg:61.19ms
step:978/2330 train_time:59842ms step_avg:61.19ms
step:979/2330 train_time:59902ms step_avg:61.19ms
step:980/2330 train_time:59965ms step_avg:61.19ms
step:981/2330 train_time:60026ms step_avg:61.19ms
step:982/2330 train_time:60089ms step_avg:61.19ms
step:983/2330 train_time:60150ms step_avg:61.19ms
step:984/2330 train_time:60213ms step_avg:61.19ms
step:985/2330 train_time:60272ms step_avg:61.19ms
step:986/2330 train_time:60336ms step_avg:61.19ms
step:987/2330 train_time:60396ms step_avg:61.19ms
step:988/2330 train_time:60459ms step_avg:61.19ms
step:989/2330 train_time:60519ms step_avg:61.19ms
step:990/2330 train_time:60582ms step_avg:61.19ms
step:991/2330 train_time:60643ms step_avg:61.19ms
step:992/2330 train_time:60706ms step_avg:61.20ms
step:993/2330 train_time:60766ms step_avg:61.19ms
step:994/2330 train_time:60830ms step_avg:61.20ms
step:995/2330 train_time:60889ms step_avg:61.20ms
step:996/2330 train_time:60952ms step_avg:61.20ms
step:997/2330 train_time:61012ms step_avg:61.20ms
step:998/2330 train_time:61075ms step_avg:61.20ms
step:999/2330 train_time:61135ms step_avg:61.20ms
step:1000/2330 train_time:61198ms step_avg:61.20ms
step:1000/2330 val_loss:3.5802 train_time:61262ms step_avg:61.26ms
step:1001/2330 train_time:61285ms step_avg:61.22ms
step:1002/2330 train_time:61324ms step_avg:61.20ms
step:1003/2330 train_time:61388ms step_avg:61.20ms
step:1004/2330 train_time:61452ms step_avg:61.21ms
step:1005/2330 train_time:61512ms step_avg:61.21ms
step:1006/2330 train_time:61576ms step_avg:61.21ms
step:1007/2330 train_time:61635ms step_avg:61.21ms
step:1008/2330 train_time:61698ms step_avg:61.21ms
step:1009/2330 train_time:61757ms step_avg:61.21ms
step:1010/2330 train_time:61820ms step_avg:61.21ms
step:1011/2330 train_time:61879ms step_avg:61.21ms
step:1012/2330 train_time:61941ms step_avg:61.21ms
step:1013/2330 train_time:62000ms step_avg:61.20ms
step:1014/2330 train_time:62063ms step_avg:61.21ms
step:1015/2330 train_time:62122ms step_avg:61.20ms
step:1016/2330 train_time:62188ms step_avg:61.21ms
step:1017/2330 train_time:62251ms step_avg:61.21ms
step:1018/2330 train_time:62316ms step_avg:61.21ms
step:1019/2330 train_time:62379ms step_avg:61.22ms
step:1020/2330 train_time:62443ms step_avg:61.22ms
step:1021/2330 train_time:62503ms step_avg:61.22ms
step:1022/2330 train_time:62566ms step_avg:61.22ms
step:1023/2330 train_time:62625ms step_avg:61.22ms
step:1024/2330 train_time:62688ms step_avg:61.22ms
step:1025/2330 train_time:62748ms step_avg:61.22ms
step:1026/2330 train_time:62811ms step_avg:61.22ms
step:1027/2330 train_time:62871ms step_avg:61.22ms
step:1028/2330 train_time:62934ms step_avg:61.22ms
step:1029/2330 train_time:62993ms step_avg:61.22ms
step:1030/2330 train_time:63056ms step_avg:61.22ms
step:1031/2330 train_time:63115ms step_avg:61.22ms
step:1032/2330 train_time:63179ms step_avg:61.22ms
step:1033/2330 train_time:63241ms step_avg:61.22ms
step:1034/2330 train_time:63305ms step_avg:61.22ms
step:1035/2330 train_time:63367ms step_avg:61.22ms
step:1036/2330 train_time:63430ms step_avg:61.23ms
step:1037/2330 train_time:63490ms step_avg:61.22ms
step:1038/2330 train_time:63553ms step_avg:61.23ms
step:1039/2330 train_time:63613ms step_avg:61.23ms
step:1040/2330 train_time:63676ms step_avg:61.23ms
step:1041/2330 train_time:63736ms step_avg:61.23ms
step:1042/2330 train_time:63799ms step_avg:61.23ms
step:1043/2330 train_time:63859ms step_avg:61.23ms
step:1044/2330 train_time:63922ms step_avg:61.23ms
step:1045/2330 train_time:63981ms step_avg:61.23ms
step:1046/2330 train_time:64043ms step_avg:61.23ms
step:1047/2330 train_time:64103ms step_avg:61.23ms
step:1048/2330 train_time:64166ms step_avg:61.23ms
step:1049/2330 train_time:64227ms step_avg:61.23ms
step:1050/2330 train_time:64290ms step_avg:61.23ms
step:1051/2330 train_time:64351ms step_avg:61.23ms
step:1052/2330 train_time:64414ms step_avg:61.23ms
step:1053/2330 train_time:64475ms step_avg:61.23ms
step:1054/2330 train_time:64539ms step_avg:61.23ms
step:1055/2330 train_time:64600ms step_avg:61.23ms
step:1056/2330 train_time:64663ms step_avg:61.23ms
step:1057/2330 train_time:64723ms step_avg:61.23ms
step:1058/2330 train_time:64785ms step_avg:61.23ms
step:1059/2330 train_time:64845ms step_avg:61.23ms
step:1060/2330 train_time:64908ms step_avg:61.23ms
step:1061/2330 train_time:64968ms step_avg:61.23ms
step:1062/2330 train_time:65030ms step_avg:61.23ms
step:1063/2330 train_time:65090ms step_avg:61.23ms
step:1064/2330 train_time:65153ms step_avg:61.23ms
step:1065/2330 train_time:65214ms step_avg:61.23ms
step:1066/2330 train_time:65276ms step_avg:61.23ms
step:1067/2330 train_time:65336ms step_avg:61.23ms
step:1068/2330 train_time:65400ms step_avg:61.24ms
step:1069/2330 train_time:65461ms step_avg:61.24ms
step:1070/2330 train_time:65524ms step_avg:61.24ms
step:1071/2330 train_time:65584ms step_avg:61.24ms
step:1072/2330 train_time:65647ms step_avg:61.24ms
step:1073/2330 train_time:65707ms step_avg:61.24ms
step:1074/2330 train_time:65769ms step_avg:61.24ms
step:1075/2330 train_time:65829ms step_avg:61.24ms
step:1076/2330 train_time:65892ms step_avg:61.24ms
step:1077/2330 train_time:65952ms step_avg:61.24ms
step:1078/2330 train_time:66015ms step_avg:61.24ms
step:1079/2330 train_time:66075ms step_avg:61.24ms
step:1080/2330 train_time:66138ms step_avg:61.24ms
step:1081/2330 train_time:66198ms step_avg:61.24ms
step:1082/2330 train_time:66261ms step_avg:61.24ms
step:1083/2330 train_time:66322ms step_avg:61.24ms
step:1084/2330 train_time:66385ms step_avg:61.24ms
step:1085/2330 train_time:66445ms step_avg:61.24ms
step:1086/2330 train_time:66508ms step_avg:61.24ms
step:1087/2330 train_time:66569ms step_avg:61.24ms
step:1088/2330 train_time:66632ms step_avg:61.24ms
step:1089/2330 train_time:66691ms step_avg:61.24ms
step:1090/2330 train_time:66754ms step_avg:61.24ms
step:1091/2330 train_time:66814ms step_avg:61.24ms
step:1092/2330 train_time:66877ms step_avg:61.24ms
step:1093/2330 train_time:66937ms step_avg:61.24ms
step:1094/2330 train_time:67001ms step_avg:61.24ms
step:1095/2330 train_time:67061ms step_avg:61.24ms
step:1096/2330 train_time:67124ms step_avg:61.24ms
step:1097/2330 train_time:67184ms step_avg:61.24ms
step:1098/2330 train_time:67247ms step_avg:61.24ms
step:1099/2330 train_time:67307ms step_avg:61.24ms
step:1100/2330 train_time:67370ms step_avg:61.25ms
step:1101/2330 train_time:67430ms step_avg:61.24ms
step:1102/2330 train_time:67494ms step_avg:61.25ms
step:1103/2330 train_time:67554ms step_avg:61.25ms
step:1104/2330 train_time:67618ms step_avg:61.25ms
step:1105/2330 train_time:67677ms step_avg:61.25ms
step:1106/2330 train_time:67741ms step_avg:61.25ms
step:1107/2330 train_time:67801ms step_avg:61.25ms
step:1108/2330 train_time:67864ms step_avg:61.25ms
step:1109/2330 train_time:67924ms step_avg:61.25ms
step:1110/2330 train_time:67986ms step_avg:61.25ms
step:1111/2330 train_time:68047ms step_avg:61.25ms
step:1112/2330 train_time:68110ms step_avg:61.25ms
step:1113/2330 train_time:68170ms step_avg:61.25ms
step:1114/2330 train_time:68233ms step_avg:61.25ms
step:1115/2330 train_time:68293ms step_avg:61.25ms
step:1116/2330 train_time:68357ms step_avg:61.25ms
step:1117/2330 train_time:68417ms step_avg:61.25ms
step:1118/2330 train_time:68481ms step_avg:61.25ms
step:1119/2330 train_time:68541ms step_avg:61.25ms
step:1120/2330 train_time:68605ms step_avg:61.25ms
step:1121/2330 train_time:68665ms step_avg:61.25ms
step:1122/2330 train_time:68728ms step_avg:61.25ms
step:1123/2330 train_time:68788ms step_avg:61.25ms
step:1124/2330 train_time:68852ms step_avg:61.26ms
step:1125/2330 train_time:68912ms step_avg:61.26ms
step:1126/2330 train_time:68975ms step_avg:61.26ms
step:1127/2330 train_time:69036ms step_avg:61.26ms
step:1128/2330 train_time:69100ms step_avg:61.26ms
step:1129/2330 train_time:69160ms step_avg:61.26ms
step:1130/2330 train_time:69223ms step_avg:61.26ms
step:1131/2330 train_time:69283ms step_avg:61.26ms
step:1132/2330 train_time:69346ms step_avg:61.26ms
step:1133/2330 train_time:69406ms step_avg:61.26ms
step:1134/2330 train_time:69469ms step_avg:61.26ms
step:1135/2330 train_time:69529ms step_avg:61.26ms
step:1136/2330 train_time:69592ms step_avg:61.26ms
step:1137/2330 train_time:69652ms step_avg:61.26ms
step:1138/2330 train_time:69715ms step_avg:61.26ms
step:1139/2330 train_time:69776ms step_avg:61.26ms
step:1140/2330 train_time:69839ms step_avg:61.26ms
step:1141/2330 train_time:69898ms step_avg:61.26ms
step:1142/2330 train_time:69962ms step_avg:61.26ms
step:1143/2330 train_time:70022ms step_avg:61.26ms
step:1144/2330 train_time:70085ms step_avg:61.26ms
step:1145/2330 train_time:70145ms step_avg:61.26ms
step:1146/2330 train_time:70208ms step_avg:61.26ms
step:1147/2330 train_time:70267ms step_avg:61.26ms
step:1148/2330 train_time:70330ms step_avg:61.26ms
step:1149/2330 train_time:70390ms step_avg:61.26ms
step:1150/2330 train_time:70453ms step_avg:61.26ms
step:1151/2330 train_time:70513ms step_avg:61.26ms
step:1152/2330 train_time:70577ms step_avg:61.26ms
step:1153/2330 train_time:70636ms step_avg:61.26ms
step:1154/2330 train_time:70699ms step_avg:61.26ms
step:1155/2330 train_time:70760ms step_avg:61.26ms
step:1156/2330 train_time:70823ms step_avg:61.27ms
step:1157/2330 train_time:70883ms step_avg:61.26ms
step:1158/2330 train_time:70946ms step_avg:61.27ms
step:1159/2330 train_time:71006ms step_avg:61.26ms
step:1160/2330 train_time:71069ms step_avg:61.27ms
step:1161/2330 train_time:71129ms step_avg:61.27ms
step:1162/2330 train_time:71192ms step_avg:61.27ms
step:1163/2330 train_time:71251ms step_avg:61.26ms
step:1164/2330 train_time:71314ms step_avg:61.27ms
step:1165/2330 train_time:71375ms step_avg:61.27ms
step:1166/2330 train_time:71439ms step_avg:61.27ms
step:1167/2330 train_time:71499ms step_avg:61.27ms
step:1168/2330 train_time:71562ms step_avg:61.27ms
step:1169/2330 train_time:71622ms step_avg:61.27ms
step:1170/2330 train_time:71684ms step_avg:61.27ms
step:1171/2330 train_time:71744ms step_avg:61.27ms
step:1172/2330 train_time:71807ms step_avg:61.27ms
step:1173/2330 train_time:71867ms step_avg:61.27ms
step:1174/2330 train_time:71931ms step_avg:61.27ms
step:1175/2330 train_time:71990ms step_avg:61.27ms
step:1176/2330 train_time:72053ms step_avg:61.27ms
step:1177/2330 train_time:72113ms step_avg:61.27ms
step:1178/2330 train_time:72176ms step_avg:61.27ms
step:1179/2330 train_time:72236ms step_avg:61.27ms
step:1180/2330 train_time:72300ms step_avg:61.27ms
step:1181/2330 train_time:72360ms step_avg:61.27ms
step:1182/2330 train_time:72423ms step_avg:61.27ms
step:1183/2330 train_time:72483ms step_avg:61.27ms
step:1184/2330 train_time:72546ms step_avg:61.27ms
step:1185/2330 train_time:72606ms step_avg:61.27ms
step:1186/2330 train_time:72668ms step_avg:61.27ms
step:1187/2330 train_time:72728ms step_avg:61.27ms
step:1188/2330 train_time:72791ms step_avg:61.27ms
step:1189/2330 train_time:72851ms step_avg:61.27ms
step:1190/2330 train_time:72915ms step_avg:61.27ms
step:1191/2330 train_time:72975ms step_avg:61.27ms
step:1192/2330 train_time:73038ms step_avg:61.27ms
step:1193/2330 train_time:73098ms step_avg:61.27ms
step:1194/2330 train_time:73161ms step_avg:61.27ms
step:1195/2330 train_time:73221ms step_avg:61.27ms
step:1196/2330 train_time:73285ms step_avg:61.27ms
step:1197/2330 train_time:73344ms step_avg:61.27ms
step:1198/2330 train_time:73408ms step_avg:61.28ms
step:1199/2330 train_time:73467ms step_avg:61.27ms
step:1200/2330 train_time:73531ms step_avg:61.28ms
step:1201/2330 train_time:73590ms step_avg:61.27ms
step:1202/2330 train_time:73653ms step_avg:61.28ms
step:1203/2330 train_time:73713ms step_avg:61.27ms
step:1204/2330 train_time:73776ms step_avg:61.28ms
step:1205/2330 train_time:73836ms step_avg:61.27ms
step:1206/2330 train_time:73899ms step_avg:61.28ms
step:1207/2330 train_time:73959ms step_avg:61.28ms
step:1208/2330 train_time:74023ms step_avg:61.28ms
step:1209/2330 train_time:74083ms step_avg:61.28ms
step:1210/2330 train_time:74146ms step_avg:61.28ms
step:1211/2330 train_time:74206ms step_avg:61.28ms
step:1212/2330 train_time:74269ms step_avg:61.28ms
step:1213/2330 train_time:74328ms step_avg:61.28ms
step:1214/2330 train_time:74392ms step_avg:61.28ms
step:1215/2330 train_time:74451ms step_avg:61.28ms
step:1216/2330 train_time:74515ms step_avg:61.28ms
step:1217/2330 train_time:74575ms step_avg:61.28ms
step:1218/2330 train_time:74638ms step_avg:61.28ms
step:1219/2330 train_time:74698ms step_avg:61.28ms
step:1220/2330 train_time:74762ms step_avg:61.28ms
step:1221/2330 train_time:74823ms step_avg:61.28ms
step:1222/2330 train_time:74886ms step_avg:61.28ms
step:1223/2330 train_time:74946ms step_avg:61.28ms
step:1224/2330 train_time:75010ms step_avg:61.28ms
step:1225/2330 train_time:75070ms step_avg:61.28ms
step:1226/2330 train_time:75133ms step_avg:61.28ms
step:1227/2330 train_time:75193ms step_avg:61.28ms
step:1228/2330 train_time:75257ms step_avg:61.28ms
step:1229/2330 train_time:75317ms step_avg:61.28ms
step:1230/2330 train_time:75381ms step_avg:61.29ms
step:1231/2330 train_time:75441ms step_avg:61.28ms
step:1232/2330 train_time:75504ms step_avg:61.29ms
step:1233/2330 train_time:75564ms step_avg:61.28ms
step:1234/2330 train_time:75626ms step_avg:61.29ms
step:1235/2330 train_time:75687ms step_avg:61.29ms
step:1236/2330 train_time:75750ms step_avg:61.29ms
step:1237/2330 train_time:75811ms step_avg:61.29ms
step:1238/2330 train_time:75874ms step_avg:61.29ms
step:1239/2330 train_time:75934ms step_avg:61.29ms
step:1240/2330 train_time:75998ms step_avg:61.29ms
step:1241/2330 train_time:76058ms step_avg:61.29ms
step:1242/2330 train_time:76122ms step_avg:61.29ms
step:1243/2330 train_time:76182ms step_avg:61.29ms
step:1244/2330 train_time:76245ms step_avg:61.29ms
step:1245/2330 train_time:76305ms step_avg:61.29ms
step:1246/2330 train_time:76368ms step_avg:61.29ms
step:1247/2330 train_time:76428ms step_avg:61.29ms
step:1248/2330 train_time:76491ms step_avg:61.29ms
step:1249/2330 train_time:76550ms step_avg:61.29ms
step:1250/2330 train_time:76613ms step_avg:61.29ms
step:1250/2330 val_loss:3.5281 train_time:76678ms step_avg:61.34ms
step:1251/2330 train_time:76700ms step_avg:61.31ms
step:1252/2330 train_time:76741ms step_avg:61.29ms
step:1253/2330 train_time:76808ms step_avg:61.30ms
step:1254/2330 train_time:76874ms step_avg:61.30ms
step:1255/2330 train_time:76934ms step_avg:61.30ms
step:1256/2330 train_time:76997ms step_avg:61.30ms
step:1257/2330 train_time:77056ms step_avg:61.30ms
step:1258/2330 train_time:77118ms step_avg:61.30ms
step:1259/2330 train_time:77178ms step_avg:61.30ms
step:1260/2330 train_time:77241ms step_avg:61.30ms
step:1261/2330 train_time:77300ms step_avg:61.30ms
step:1262/2330 train_time:77363ms step_avg:61.30ms
step:1263/2330 train_time:77422ms step_avg:61.30ms
step:1264/2330 train_time:77485ms step_avg:61.30ms
step:1265/2330 train_time:77544ms step_avg:61.30ms
step:1266/2330 train_time:77607ms step_avg:61.30ms
step:1267/2330 train_time:77667ms step_avg:61.30ms
step:1268/2330 train_time:77732ms step_avg:61.30ms
step:1269/2330 train_time:77794ms step_avg:61.30ms
step:1270/2330 train_time:77858ms step_avg:61.31ms
step:1271/2330 train_time:77920ms step_avg:61.31ms
step:1272/2330 train_time:77983ms step_avg:61.31ms
step:1273/2330 train_time:78043ms step_avg:61.31ms
step:1274/2330 train_time:78106ms step_avg:61.31ms
step:1275/2330 train_time:78166ms step_avg:61.31ms
step:1276/2330 train_time:78229ms step_avg:61.31ms
step:1277/2330 train_time:78288ms step_avg:61.31ms
step:1278/2330 train_time:78351ms step_avg:61.31ms
step:1279/2330 train_time:78410ms step_avg:61.31ms
step:1280/2330 train_time:78473ms step_avg:61.31ms
step:1281/2330 train_time:78533ms step_avg:61.31ms
step:1282/2330 train_time:78596ms step_avg:61.31ms
step:1283/2330 train_time:78656ms step_avg:61.31ms
step:1284/2330 train_time:78720ms step_avg:61.31ms
step:1285/2330 train_time:78782ms step_avg:61.31ms
step:1286/2330 train_time:78845ms step_avg:61.31ms
step:1287/2330 train_time:78906ms step_avg:61.31ms
step:1288/2330 train_time:78969ms step_avg:61.31ms
step:1289/2330 train_time:79029ms step_avg:61.31ms
step:1290/2330 train_time:79093ms step_avg:61.31ms
step:1291/2330 train_time:79153ms step_avg:61.31ms
step:1292/2330 train_time:79215ms step_avg:61.31ms
step:1293/2330 train_time:79275ms step_avg:61.31ms
step:1294/2330 train_time:79338ms step_avg:61.31ms
step:1295/2330 train_time:79398ms step_avg:61.31ms
step:1296/2330 train_time:79461ms step_avg:61.31ms
step:1297/2330 train_time:79521ms step_avg:61.31ms
step:1298/2330 train_time:79585ms step_avg:61.31ms
step:1299/2330 train_time:79644ms step_avg:61.31ms
step:1300/2330 train_time:79708ms step_avg:61.31ms
step:1301/2330 train_time:79768ms step_avg:61.31ms
step:1302/2330 train_time:79831ms step_avg:61.31ms
step:1303/2330 train_time:79892ms step_avg:61.31ms
step:1304/2330 train_time:79956ms step_avg:61.32ms
step:1305/2330 train_time:80016ms step_avg:61.32ms
step:1306/2330 train_time:80080ms step_avg:61.32ms
step:1307/2330 train_time:80139ms step_avg:61.32ms
step:1308/2330 train_time:80202ms step_avg:61.32ms
step:1309/2330 train_time:80262ms step_avg:61.32ms
step:1310/2330 train_time:80325ms step_avg:61.32ms
step:1311/2330 train_time:80385ms step_avg:61.32ms
step:1312/2330 train_time:80448ms step_avg:61.32ms
step:1313/2330 train_time:80508ms step_avg:61.32ms
step:1314/2330 train_time:80571ms step_avg:61.32ms
step:1315/2330 train_time:80631ms step_avg:61.32ms
step:1316/2330 train_time:80695ms step_avg:61.32ms
step:1317/2330 train_time:80755ms step_avg:61.32ms
step:1318/2330 train_time:80819ms step_avg:61.32ms
step:1319/2330 train_time:80880ms step_avg:61.32ms
step:1320/2330 train_time:80943ms step_avg:61.32ms
step:1321/2330 train_time:81004ms step_avg:61.32ms
step:1322/2330 train_time:81067ms step_avg:61.32ms
step:1323/2330 train_time:81127ms step_avg:61.32ms
step:1324/2330 train_time:81190ms step_avg:61.32ms
step:1325/2330 train_time:81250ms step_avg:61.32ms
step:1326/2330 train_time:81313ms step_avg:61.32ms
step:1327/2330 train_time:81373ms step_avg:61.32ms
step:1328/2330 train_time:81437ms step_avg:61.32ms
step:1329/2330 train_time:81496ms step_avg:61.32ms
step:1330/2330 train_time:81560ms step_avg:61.32ms
step:1331/2330 train_time:81620ms step_avg:61.32ms
step:1332/2330 train_time:81684ms step_avg:61.32ms
step:1333/2330 train_time:81744ms step_avg:61.32ms
step:1334/2330 train_time:81808ms step_avg:61.32ms
step:1335/2330 train_time:81868ms step_avg:61.32ms
step:1336/2330 train_time:81932ms step_avg:61.33ms
step:1337/2330 train_time:81992ms step_avg:61.33ms
step:1338/2330 train_time:82055ms step_avg:61.33ms
step:1339/2330 train_time:82116ms step_avg:61.33ms
step:1340/2330 train_time:82180ms step_avg:61.33ms
step:1341/2330 train_time:82240ms step_avg:61.33ms
step:1342/2330 train_time:82304ms step_avg:61.33ms
step:1343/2330 train_time:82364ms step_avg:61.33ms
step:1344/2330 train_time:82426ms step_avg:61.33ms
step:1345/2330 train_time:82486ms step_avg:61.33ms
step:1346/2330 train_time:82549ms step_avg:61.33ms
step:1347/2330 train_time:82609ms step_avg:61.33ms
step:1348/2330 train_time:82673ms step_avg:61.33ms
step:1349/2330 train_time:82733ms step_avg:61.33ms
step:1350/2330 train_time:82796ms step_avg:61.33ms
step:1351/2330 train_time:82856ms step_avg:61.33ms
step:1352/2330 train_time:82920ms step_avg:61.33ms
step:1353/2330 train_time:82980ms step_avg:61.33ms
step:1354/2330 train_time:83043ms step_avg:61.33ms
step:1355/2330 train_time:83104ms step_avg:61.33ms
step:1356/2330 train_time:83167ms step_avg:61.33ms
step:1357/2330 train_time:83227ms step_avg:61.33ms
step:1358/2330 train_time:83290ms step_avg:61.33ms
step:1359/2330 train_time:83350ms step_avg:61.33ms
step:1360/2330 train_time:83413ms step_avg:61.33ms
step:1361/2330 train_time:83473ms step_avg:61.33ms
step:1362/2330 train_time:83537ms step_avg:61.33ms
step:1363/2330 train_time:83596ms step_avg:61.33ms
step:1364/2330 train_time:83659ms step_avg:61.33ms
step:1365/2330 train_time:83720ms step_avg:61.33ms
step:1366/2330 train_time:83784ms step_avg:61.34ms
step:1367/2330 train_time:83844ms step_avg:61.33ms
step:1368/2330 train_time:83907ms step_avg:61.34ms
step:1369/2330 train_time:83966ms step_avg:61.33ms
step:1370/2330 train_time:84030ms step_avg:61.34ms
step:1371/2330 train_time:84090ms step_avg:61.34ms
step:1372/2330 train_time:84153ms step_avg:61.34ms
step:1373/2330 train_time:84213ms step_avg:61.33ms
step:1374/2330 train_time:84275ms step_avg:61.34ms
step:1375/2330 train_time:84336ms step_avg:61.34ms
step:1376/2330 train_time:84399ms step_avg:61.34ms
step:1377/2330 train_time:84459ms step_avg:61.34ms
step:1378/2330 train_time:84522ms step_avg:61.34ms
step:1379/2330 train_time:84582ms step_avg:61.34ms
step:1380/2330 train_time:84645ms step_avg:61.34ms
step:1381/2330 train_time:84705ms step_avg:61.34ms
step:1382/2330 train_time:84768ms step_avg:61.34ms
step:1383/2330 train_time:84828ms step_avg:61.34ms
step:1384/2330 train_time:84892ms step_avg:61.34ms
step:1385/2330 train_time:84952ms step_avg:61.34ms
step:1386/2330 train_time:85015ms step_avg:61.34ms
step:1387/2330 train_time:85076ms step_avg:61.34ms
step:1388/2330 train_time:85140ms step_avg:61.34ms
step:1389/2330 train_time:85200ms step_avg:61.34ms
step:1390/2330 train_time:85263ms step_avg:61.34ms
step:1391/2330 train_time:85323ms step_avg:61.34ms
step:1392/2330 train_time:85387ms step_avg:61.34ms
step:1393/2330 train_time:85447ms step_avg:61.34ms
step:1394/2330 train_time:85510ms step_avg:61.34ms
step:1395/2330 train_time:85570ms step_avg:61.34ms
step:1396/2330 train_time:85633ms step_avg:61.34ms
step:1397/2330 train_time:85692ms step_avg:61.34ms
step:1398/2330 train_time:85756ms step_avg:61.34ms
step:1399/2330 train_time:85816ms step_avg:61.34ms
step:1400/2330 train_time:85879ms step_avg:61.34ms
step:1401/2330 train_time:85940ms step_avg:61.34ms
step:1402/2330 train_time:86004ms step_avg:61.34ms
step:1403/2330 train_time:86065ms step_avg:61.34ms
step:1404/2330 train_time:86128ms step_avg:61.34ms
step:1405/2330 train_time:86188ms step_avg:61.34ms
step:1406/2330 train_time:86250ms step_avg:61.34ms
step:1407/2330 train_time:86310ms step_avg:61.34ms
step:1408/2330 train_time:86373ms step_avg:61.34ms
step:1409/2330 train_time:86432ms step_avg:61.34ms
step:1410/2330 train_time:86496ms step_avg:61.34ms
step:1411/2330 train_time:86556ms step_avg:61.34ms
step:1412/2330 train_time:86620ms step_avg:61.35ms
step:1413/2330 train_time:86681ms step_avg:61.35ms
step:1414/2330 train_time:86745ms step_avg:61.35ms
step:1415/2330 train_time:86804ms step_avg:61.35ms
step:1416/2330 train_time:86867ms step_avg:61.35ms
step:1417/2330 train_time:86928ms step_avg:61.35ms
step:1418/2330 train_time:86991ms step_avg:61.35ms
step:1419/2330 train_time:87050ms step_avg:61.35ms
step:1420/2330 train_time:87113ms step_avg:61.35ms
step:1421/2330 train_time:87173ms step_avg:61.35ms
step:1422/2330 train_time:87237ms step_avg:61.35ms
step:1423/2330 train_time:87296ms step_avg:61.35ms
step:1424/2330 train_time:87359ms step_avg:61.35ms
step:1425/2330 train_time:87419ms step_avg:61.35ms
step:1426/2330 train_time:87483ms step_avg:61.35ms
step:1427/2330 train_time:87543ms step_avg:61.35ms
step:1428/2330 train_time:87606ms step_avg:61.35ms
step:1429/2330 train_time:87666ms step_avg:61.35ms
step:1430/2330 train_time:87729ms step_avg:61.35ms
step:1431/2330 train_time:87789ms step_avg:61.35ms
step:1432/2330 train_time:87852ms step_avg:61.35ms
step:1433/2330 train_time:87912ms step_avg:61.35ms
step:1434/2330 train_time:87975ms step_avg:61.35ms
step:1435/2330 train_time:88035ms step_avg:61.35ms
step:1436/2330 train_time:88099ms step_avg:61.35ms
step:1437/2330 train_time:88159ms step_avg:61.35ms
step:1438/2330 train_time:88223ms step_avg:61.35ms
step:1439/2330 train_time:88283ms step_avg:61.35ms
step:1440/2330 train_time:88345ms step_avg:61.35ms
step:1441/2330 train_time:88405ms step_avg:61.35ms
step:1442/2330 train_time:88468ms step_avg:61.35ms
step:1443/2330 train_time:88528ms step_avg:61.35ms
step:1444/2330 train_time:88591ms step_avg:61.35ms
step:1445/2330 train_time:88652ms step_avg:61.35ms
step:1446/2330 train_time:88715ms step_avg:61.35ms
step:1447/2330 train_time:88775ms step_avg:61.35ms
step:1448/2330 train_time:88838ms step_avg:61.35ms
step:1449/2330 train_time:88899ms step_avg:61.35ms
step:1450/2330 train_time:88962ms step_avg:61.35ms
step:1451/2330 train_time:89022ms step_avg:61.35ms
step:1452/2330 train_time:89085ms step_avg:61.35ms
step:1453/2330 train_time:89145ms step_avg:61.35ms
step:1454/2330 train_time:89208ms step_avg:61.35ms
step:1455/2330 train_time:89268ms step_avg:61.35ms
step:1456/2330 train_time:89331ms step_avg:61.35ms
step:1457/2330 train_time:89391ms step_avg:61.35ms
step:1458/2330 train_time:89453ms step_avg:61.35ms
step:1459/2330 train_time:89513ms step_avg:61.35ms
step:1460/2330 train_time:89576ms step_avg:61.35ms
step:1461/2330 train_time:89637ms step_avg:61.35ms
step:1462/2330 train_time:89700ms step_avg:61.35ms
step:1463/2330 train_time:89761ms step_avg:61.35ms
step:1464/2330 train_time:89824ms step_avg:61.36ms
step:1465/2330 train_time:89885ms step_avg:61.35ms
step:1466/2330 train_time:89947ms step_avg:61.36ms
step:1467/2330 train_time:90007ms step_avg:61.35ms
step:1468/2330 train_time:90070ms step_avg:61.36ms
step:1469/2330 train_time:90131ms step_avg:61.36ms
step:1470/2330 train_time:90194ms step_avg:61.36ms
step:1471/2330 train_time:90253ms step_avg:61.35ms
step:1472/2330 train_time:90316ms step_avg:61.36ms
step:1473/2330 train_time:90376ms step_avg:61.36ms
step:1474/2330 train_time:90440ms step_avg:61.36ms
step:1475/2330 train_time:90500ms step_avg:61.36ms
step:1476/2330 train_time:90564ms step_avg:61.36ms
step:1477/2330 train_time:90624ms step_avg:61.36ms
step:1478/2330 train_time:90688ms step_avg:61.36ms
step:1479/2330 train_time:90747ms step_avg:61.36ms
step:1480/2330 train_time:90810ms step_avg:61.36ms
step:1481/2330 train_time:90870ms step_avg:61.36ms
step:1482/2330 train_time:90932ms step_avg:61.36ms
step:1483/2330 train_time:90992ms step_avg:61.36ms
step:1484/2330 train_time:91055ms step_avg:61.36ms
step:1485/2330 train_time:91116ms step_avg:61.36ms
step:1486/2330 train_time:91179ms step_avg:61.36ms
step:1487/2330 train_time:91239ms step_avg:61.36ms
step:1488/2330 train_time:91302ms step_avg:61.36ms
step:1489/2330 train_time:91362ms step_avg:61.36ms
step:1490/2330 train_time:91425ms step_avg:61.36ms
step:1491/2330 train_time:91485ms step_avg:61.36ms
step:1492/2330 train_time:91548ms step_avg:61.36ms
step:1493/2330 train_time:91608ms step_avg:61.36ms
step:1494/2330 train_time:91671ms step_avg:61.36ms
step:1495/2330 train_time:91731ms step_avg:61.36ms
step:1496/2330 train_time:91796ms step_avg:61.36ms
step:1497/2330 train_time:91856ms step_avg:61.36ms
step:1498/2330 train_time:91920ms step_avg:61.36ms
step:1499/2330 train_time:91980ms step_avg:61.36ms
step:1500/2330 train_time:92044ms step_avg:61.36ms
step:1500/2330 val_loss:3.4835 train_time:92108ms step_avg:61.41ms
step:1501/2330 train_time:92131ms step_avg:61.38ms
step:1502/2330 train_time:92168ms step_avg:61.36ms
step:1503/2330 train_time:92230ms step_avg:61.36ms
step:1504/2330 train_time:92298ms step_avg:61.37ms
step:1505/2330 train_time:92358ms step_avg:61.37ms
step:1506/2330 train_time:92422ms step_avg:61.37ms
step:1507/2330 train_time:92481ms step_avg:61.37ms
step:1508/2330 train_time:92544ms step_avg:61.37ms
step:1509/2330 train_time:92604ms step_avg:61.37ms
step:1510/2330 train_time:92666ms step_avg:61.37ms
step:1511/2330 train_time:92725ms step_avg:61.37ms
step:1512/2330 train_time:92788ms step_avg:61.37ms
step:1513/2330 train_time:92846ms step_avg:61.37ms
step:1514/2330 train_time:92908ms step_avg:61.37ms
step:1515/2330 train_time:92967ms step_avg:61.36ms
step:1516/2330 train_time:93032ms step_avg:61.37ms
step:1517/2330 train_time:93094ms step_avg:61.37ms
step:1518/2330 train_time:93159ms step_avg:61.37ms
step:1519/2330 train_time:93221ms step_avg:61.37ms
step:1520/2330 train_time:93285ms step_avg:61.37ms
step:1521/2330 train_time:93346ms step_avg:61.37ms
step:1522/2330 train_time:93409ms step_avg:61.37ms
step:1523/2330 train_time:93469ms step_avg:61.37ms
step:1524/2330 train_time:93533ms step_avg:61.37ms
step:1525/2330 train_time:93593ms step_avg:61.37ms
step:1526/2330 train_time:93656ms step_avg:61.37ms
step:1527/2330 train_time:93717ms step_avg:61.37ms
step:1528/2330 train_time:93779ms step_avg:61.37ms
step:1529/2330 train_time:93839ms step_avg:61.37ms
step:1530/2330 train_time:93902ms step_avg:61.37ms
step:1531/2330 train_time:93962ms step_avg:61.37ms
step:1532/2330 train_time:94026ms step_avg:61.37ms
step:1533/2330 train_time:94086ms step_avg:61.37ms
step:1534/2330 train_time:94150ms step_avg:61.38ms
step:1535/2330 train_time:94212ms step_avg:61.38ms
step:1536/2330 train_time:94277ms step_avg:61.38ms
step:1537/2330 train_time:94338ms step_avg:61.38ms
step:1538/2330 train_time:94402ms step_avg:61.38ms
step:1539/2330 train_time:94463ms step_avg:61.38ms
step:1540/2330 train_time:94526ms step_avg:61.38ms
step:1541/2330 train_time:94587ms step_avg:61.38ms
step:1542/2330 train_time:94651ms step_avg:61.38ms
step:1543/2330 train_time:94711ms step_avg:61.38ms
step:1544/2330 train_time:94775ms step_avg:61.38ms
step:1545/2330 train_time:94835ms step_avg:61.38ms
step:1546/2330 train_time:94899ms step_avg:61.38ms
step:1547/2330 train_time:94959ms step_avg:61.38ms
step:1548/2330 train_time:95022ms step_avg:61.38ms
step:1549/2330 train_time:95083ms step_avg:61.38ms
step:1550/2330 train_time:95148ms step_avg:61.39ms
step:1551/2330 train_time:95209ms step_avg:61.39ms
step:1552/2330 train_time:95274ms step_avg:61.39ms
step:1553/2330 train_time:95335ms step_avg:61.39ms
step:1554/2330 train_time:95400ms step_avg:61.39ms
step:1555/2330 train_time:95460ms step_avg:61.39ms
step:1556/2330 train_time:95524ms step_avg:61.39ms
step:1557/2330 train_time:95585ms step_avg:61.39ms
step:1558/2330 train_time:95649ms step_avg:61.39ms
step:1559/2330 train_time:95709ms step_avg:61.39ms
step:1560/2330 train_time:95772ms step_avg:61.39ms
step:1561/2330 train_time:95832ms step_avg:61.39ms
step:1562/2330 train_time:95896ms step_avg:61.39ms
step:1563/2330 train_time:95957ms step_avg:61.39ms
step:1564/2330 train_time:96020ms step_avg:61.39ms
step:1565/2330 train_time:96081ms step_avg:61.39ms
step:1566/2330 train_time:96144ms step_avg:61.39ms
step:1567/2330 train_time:96205ms step_avg:61.39ms
step:1568/2330 train_time:96269ms step_avg:61.40ms
step:1569/2330 train_time:96331ms step_avg:61.40ms
step:1570/2330 train_time:96395ms step_avg:61.40ms
step:1571/2330 train_time:96456ms step_avg:61.40ms
step:1572/2330 train_time:96520ms step_avg:61.40ms
step:1573/2330 train_time:96581ms step_avg:61.40ms
step:1574/2330 train_time:96645ms step_avg:61.40ms
step:1575/2330 train_time:96705ms step_avg:61.40ms
step:1576/2330 train_time:96768ms step_avg:61.40ms
step:1577/2330 train_time:96829ms step_avg:61.40ms
step:1578/2330 train_time:96892ms step_avg:61.40ms
step:1579/2330 train_time:96953ms step_avg:61.40ms
step:1580/2330 train_time:97016ms step_avg:61.40ms
step:1581/2330 train_time:97077ms step_avg:61.40ms
step:1582/2330 train_time:97141ms step_avg:61.40ms
step:1583/2330 train_time:97201ms step_avg:61.40ms
step:1584/2330 train_time:97266ms step_avg:61.41ms
step:1585/2330 train_time:97327ms step_avg:61.41ms
step:1586/2330 train_time:97391ms step_avg:61.41ms
step:1587/2330 train_time:97451ms step_avg:61.41ms
step:1588/2330 train_time:97516ms step_avg:61.41ms
step:1589/2330 train_time:97577ms step_avg:61.41ms
step:1590/2330 train_time:97640ms step_avg:61.41ms
step:1591/2330 train_time:97700ms step_avg:61.41ms
step:1592/2330 train_time:97764ms step_avg:61.41ms
step:1593/2330 train_time:97824ms step_avg:61.41ms
step:1594/2330 train_time:97888ms step_avg:61.41ms
step:1595/2330 train_time:97948ms step_avg:61.41ms
step:1596/2330 train_time:98012ms step_avg:61.41ms
step:1597/2330 train_time:98072ms step_avg:61.41ms
step:1598/2330 train_time:98136ms step_avg:61.41ms
step:1599/2330 train_time:98198ms step_avg:61.41ms
step:1600/2330 train_time:98262ms step_avg:61.41ms
step:1601/2330 train_time:98322ms step_avg:61.41ms
step:1602/2330 train_time:98385ms step_avg:61.41ms
step:1603/2330 train_time:98445ms step_avg:61.41ms
step:1604/2330 train_time:98509ms step_avg:61.41ms
step:1605/2330 train_time:98571ms step_avg:61.41ms
step:1606/2330 train_time:98634ms step_avg:61.42ms
step:1607/2330 train_time:98695ms step_avg:61.42ms
step:1608/2330 train_time:98759ms step_avg:61.42ms
step:1609/2330 train_time:98819ms step_avg:61.42ms
step:1610/2330 train_time:98882ms step_avg:61.42ms
step:1611/2330 train_time:98943ms step_avg:61.42ms
step:1612/2330 train_time:99007ms step_avg:61.42ms
step:1613/2330 train_time:99067ms step_avg:61.42ms
step:1614/2330 train_time:99130ms step_avg:61.42ms
step:1615/2330 train_time:99191ms step_avg:61.42ms
step:1616/2330 train_time:99255ms step_avg:61.42ms
step:1617/2330 train_time:99316ms step_avg:61.42ms
step:1618/2330 train_time:99380ms step_avg:61.42ms
step:1619/2330 train_time:99441ms step_avg:61.42ms
step:1620/2330 train_time:99504ms step_avg:61.42ms
step:1621/2330 train_time:99564ms step_avg:61.42ms
step:1622/2330 train_time:99627ms step_avg:61.42ms
step:1623/2330 train_time:99688ms step_avg:61.42ms
step:1624/2330 train_time:99753ms step_avg:61.42ms
step:1625/2330 train_time:99813ms step_avg:61.42ms
step:1626/2330 train_time:99877ms step_avg:61.43ms
step:1627/2330 train_time:99938ms step_avg:61.42ms
step:1628/2330 train_time:100002ms step_avg:61.43ms
step:1629/2330 train_time:100062ms step_avg:61.43ms
step:1630/2330 train_time:100126ms step_avg:61.43ms
step:1631/2330 train_time:100187ms step_avg:61.43ms
step:1632/2330 train_time:100251ms step_avg:61.43ms
step:1633/2330 train_time:100312ms step_avg:61.43ms
step:1634/2330 train_time:100377ms step_avg:61.43ms
step:1635/2330 train_time:100439ms step_avg:61.43ms
step:1636/2330 train_time:100503ms step_avg:61.43ms
step:1637/2330 train_time:100562ms step_avg:61.43ms
step:1638/2330 train_time:100626ms step_avg:61.43ms
step:1639/2330 train_time:100686ms step_avg:61.43ms
step:1640/2330 train_time:100750ms step_avg:61.43ms
step:1641/2330 train_time:100811ms step_avg:61.43ms
step:1642/2330 train_time:100876ms step_avg:61.43ms
step:1643/2330 train_time:100937ms step_avg:61.43ms
step:1644/2330 train_time:101001ms step_avg:61.44ms
step:1645/2330 train_time:101061ms step_avg:61.44ms
step:1646/2330 train_time:101125ms step_avg:61.44ms
step:1647/2330 train_time:101185ms step_avg:61.44ms
step:1648/2330 train_time:101250ms step_avg:61.44ms
step:1649/2330 train_time:101311ms step_avg:61.44ms
step:1650/2330 train_time:101374ms step_avg:61.44ms
step:1651/2330 train_time:101436ms step_avg:61.44ms
step:1652/2330 train_time:101500ms step_avg:61.44ms
step:1653/2330 train_time:101561ms step_avg:61.44ms
step:1654/2330 train_time:101624ms step_avg:61.44ms
step:1655/2330 train_time:101684ms step_avg:61.44ms
step:1656/2330 train_time:101748ms step_avg:61.44ms
step:1657/2330 train_time:101808ms step_avg:61.44ms
step:1658/2330 train_time:101872ms step_avg:61.44ms
step:1659/2330 train_time:101932ms step_avg:61.44ms
step:1660/2330 train_time:101996ms step_avg:61.44ms
step:1661/2330 train_time:102057ms step_avg:61.44ms
step:1662/2330 train_time:102120ms step_avg:61.44ms
step:1663/2330 train_time:102181ms step_avg:61.44ms
step:1664/2330 train_time:102244ms step_avg:61.44ms
step:1665/2330 train_time:102304ms step_avg:61.44ms
step:1666/2330 train_time:102367ms step_avg:61.45ms
step:1667/2330 train_time:102428ms step_avg:61.44ms
step:1668/2330 train_time:102493ms step_avg:61.45ms
step:1669/2330 train_time:102554ms step_avg:61.45ms
step:1670/2330 train_time:102618ms step_avg:61.45ms
step:1671/2330 train_time:102678ms step_avg:61.45ms
step:1672/2330 train_time:102742ms step_avg:61.45ms
step:1673/2330 train_time:102802ms step_avg:61.45ms
step:1674/2330 train_time:102865ms step_avg:61.45ms
step:1675/2330 train_time:102926ms step_avg:61.45ms
step:1676/2330 train_time:102989ms step_avg:61.45ms
step:1677/2330 train_time:103050ms step_avg:61.45ms
step:1678/2330 train_time:103114ms step_avg:61.45ms
step:1679/2330 train_time:103175ms step_avg:61.45ms
step:1680/2330 train_time:103240ms step_avg:61.45ms
step:1681/2330 train_time:103301ms step_avg:61.45ms
step:1682/2330 train_time:103364ms step_avg:61.45ms
step:1683/2330 train_time:103425ms step_avg:61.45ms
step:1684/2330 train_time:103488ms step_avg:61.45ms
step:1685/2330 train_time:103549ms step_avg:61.45ms
step:1686/2330 train_time:103614ms step_avg:61.46ms
step:1687/2330 train_time:103675ms step_avg:61.46ms
step:1688/2330 train_time:103739ms step_avg:61.46ms
step:1689/2330 train_time:103799ms step_avg:61.46ms
step:1690/2330 train_time:103863ms step_avg:61.46ms
step:1691/2330 train_time:103923ms step_avg:61.46ms
step:1692/2330 train_time:103987ms step_avg:61.46ms
step:1693/2330 train_time:104048ms step_avg:61.46ms
step:1694/2330 train_time:104111ms step_avg:61.46ms
step:1695/2330 train_time:104171ms step_avg:61.46ms
step:1696/2330 train_time:104235ms step_avg:61.46ms
step:1697/2330 train_time:104296ms step_avg:61.46ms
step:1698/2330 train_time:104359ms step_avg:61.46ms
step:1699/2330 train_time:104420ms step_avg:61.46ms
step:1700/2330 train_time:104483ms step_avg:61.46ms
step:1701/2330 train_time:104544ms step_avg:61.46ms
step:1702/2330 train_time:104608ms step_avg:61.46ms
step:1703/2330 train_time:104669ms step_avg:61.46ms
step:1704/2330 train_time:104733ms step_avg:61.46ms
step:1705/2330 train_time:104794ms step_avg:61.46ms
step:1706/2330 train_time:104857ms step_avg:61.46ms
step:1707/2330 train_time:104918ms step_avg:61.46ms
step:1708/2330 train_time:104981ms step_avg:61.46ms
step:1709/2330 train_time:105041ms step_avg:61.46ms
step:1710/2330 train_time:105105ms step_avg:61.46ms
step:1711/2330 train_time:105165ms step_avg:61.46ms
step:1712/2330 train_time:105229ms step_avg:61.47ms
step:1713/2330 train_time:105289ms step_avg:61.46ms
step:1714/2330 train_time:105353ms step_avg:61.47ms
step:1715/2330 train_time:105413ms step_avg:61.47ms
step:1716/2330 train_time:105478ms step_avg:61.47ms
step:1717/2330 train_time:105539ms step_avg:61.47ms
step:1718/2330 train_time:105602ms step_avg:61.47ms
step:1719/2330 train_time:105663ms step_avg:61.47ms
step:1720/2330 train_time:105726ms step_avg:61.47ms
step:1721/2330 train_time:105787ms step_avg:61.47ms
step:1722/2330 train_time:105851ms step_avg:61.47ms
step:1723/2330 train_time:105911ms step_avg:61.47ms
step:1724/2330 train_time:105975ms step_avg:61.47ms
step:1725/2330 train_time:106036ms step_avg:61.47ms
step:1726/2330 train_time:106101ms step_avg:61.47ms
step:1727/2330 train_time:106161ms step_avg:61.47ms
step:1728/2330 train_time:106224ms step_avg:61.47ms
step:1729/2330 train_time:106285ms step_avg:61.47ms
step:1730/2330 train_time:106350ms step_avg:61.47ms
step:1731/2330 train_time:106410ms step_avg:61.47ms
step:1732/2330 train_time:106475ms step_avg:61.47ms
step:1733/2330 train_time:106536ms step_avg:61.48ms
step:1734/2330 train_time:106600ms step_avg:61.48ms
step:1735/2330 train_time:106660ms step_avg:61.48ms
step:1736/2330 train_time:106724ms step_avg:61.48ms
step:1737/2330 train_time:106784ms step_avg:61.48ms
step:1738/2330 train_time:106849ms step_avg:61.48ms
step:1739/2330 train_time:106909ms step_avg:61.48ms
step:1740/2330 train_time:106973ms step_avg:61.48ms
step:1741/2330 train_time:107034ms step_avg:61.48ms
step:1742/2330 train_time:107098ms step_avg:61.48ms
step:1743/2330 train_time:107160ms step_avg:61.48ms
step:1744/2330 train_time:107223ms step_avg:61.48ms
step:1745/2330 train_time:107283ms step_avg:61.48ms
step:1746/2330 train_time:107347ms step_avg:61.48ms
step:1747/2330 train_time:107408ms step_avg:61.48ms
step:1748/2330 train_time:107471ms step_avg:61.48ms
step:1749/2330 train_time:107532ms step_avg:61.48ms
step:1750/2330 train_time:107597ms step_avg:61.48ms
step:1750/2330 val_loss:3.4429 train_time:107663ms step_avg:61.52ms
step:1751/2330 train_time:107686ms step_avg:61.50ms
step:1752/2330 train_time:107724ms step_avg:61.49ms
step:1753/2330 train_time:107790ms step_avg:61.49ms
step:1754/2330 train_time:107858ms step_avg:61.49ms
step:1755/2330 train_time:107919ms step_avg:61.49ms
step:1756/2330 train_time:107982ms step_avg:61.49ms
step:1757/2330 train_time:108042ms step_avg:61.49ms
step:1758/2330 train_time:108105ms step_avg:61.49ms
step:1759/2330 train_time:108164ms step_avg:61.49ms
step:1760/2330 train_time:108227ms step_avg:61.49ms
step:1761/2330 train_time:108286ms step_avg:61.49ms
step:1762/2330 train_time:108349ms step_avg:61.49ms
step:1763/2330 train_time:108408ms step_avg:61.49ms
step:1764/2330 train_time:108471ms step_avg:61.49ms
step:1765/2330 train_time:108530ms step_avg:61.49ms
step:1766/2330 train_time:108594ms step_avg:61.49ms
step:1767/2330 train_time:108656ms step_avg:61.49ms
step:1768/2330 train_time:108721ms step_avg:61.49ms
step:1769/2330 train_time:108783ms step_avg:61.49ms
step:1770/2330 train_time:108847ms step_avg:61.50ms
step:1771/2330 train_time:108909ms step_avg:61.50ms
step:1772/2330 train_time:108972ms step_avg:61.50ms
step:1773/2330 train_time:109033ms step_avg:61.50ms
step:1774/2330 train_time:109096ms step_avg:61.50ms
step:1775/2330 train_time:109157ms step_avg:61.50ms
step:1776/2330 train_time:109221ms step_avg:61.50ms
step:1777/2330 train_time:109280ms step_avg:61.50ms
step:1778/2330 train_time:109343ms step_avg:61.50ms
step:1779/2330 train_time:109402ms step_avg:61.50ms
step:1780/2330 train_time:109465ms step_avg:61.50ms
step:1781/2330 train_time:109525ms step_avg:61.50ms
step:1782/2330 train_time:109589ms step_avg:61.50ms
step:1783/2330 train_time:109650ms step_avg:61.50ms
step:1784/2330 train_time:109715ms step_avg:61.50ms
step:1785/2330 train_time:109776ms step_avg:61.50ms
step:1786/2330 train_time:109840ms step_avg:61.50ms
step:1787/2330 train_time:109901ms step_avg:61.50ms
step:1788/2330 train_time:109965ms step_avg:61.50ms
step:1789/2330 train_time:110027ms step_avg:61.50ms
step:1790/2330 train_time:110090ms step_avg:61.50ms
step:1791/2330 train_time:110150ms step_avg:61.50ms
step:1792/2330 train_time:110213ms step_avg:61.50ms
step:1793/2330 train_time:110273ms step_avg:61.50ms
step:1794/2330 train_time:110337ms step_avg:61.50ms
step:1795/2330 train_time:110397ms step_avg:61.50ms
step:1796/2330 train_time:110461ms step_avg:61.50ms
step:1797/2330 train_time:110521ms step_avg:61.50ms
step:1798/2330 train_time:110584ms step_avg:61.50ms
step:1799/2330 train_time:110645ms step_avg:61.50ms
step:1800/2330 train_time:110709ms step_avg:61.50ms
step:1801/2330 train_time:110770ms step_avg:61.50ms
step:1802/2330 train_time:110834ms step_avg:61.51ms
step:1803/2330 train_time:110894ms step_avg:61.51ms
step:1804/2330 train_time:110958ms step_avg:61.51ms
step:1805/2330 train_time:111019ms step_avg:61.51ms
step:1806/2330 train_time:111083ms step_avg:61.51ms
step:1807/2330 train_time:111144ms step_avg:61.51ms
step:1808/2330 train_time:111208ms step_avg:61.51ms
step:1809/2330 train_time:111268ms step_avg:61.51ms
step:1810/2330 train_time:111331ms step_avg:61.51ms
step:1811/2330 train_time:111392ms step_avg:61.51ms
step:1812/2330 train_time:111456ms step_avg:61.51ms
step:1813/2330 train_time:111516ms step_avg:61.51ms
step:1814/2330 train_time:111580ms step_avg:61.51ms
step:1815/2330 train_time:111640ms step_avg:61.51ms
step:1816/2330 train_time:111704ms step_avg:61.51ms
step:1817/2330 train_time:111764ms step_avg:61.51ms
step:1818/2330 train_time:111828ms step_avg:61.51ms
step:1819/2330 train_time:111888ms step_avg:61.51ms
step:1820/2330 train_time:111952ms step_avg:61.51ms
step:1821/2330 train_time:112014ms step_avg:61.51ms
step:1822/2330 train_time:112078ms step_avg:61.51ms
step:1823/2330 train_time:112139ms step_avg:61.51ms
step:1824/2330 train_time:112202ms step_avg:61.51ms
step:1825/2330 train_time:112263ms step_avg:61.51ms
step:1826/2330 train_time:112326ms step_avg:61.51ms
step:1827/2330 train_time:112386ms step_avg:61.51ms
step:1828/2330 train_time:112450ms step_avg:61.52ms
step:1829/2330 train_time:112510ms step_avg:61.51ms
step:1830/2330 train_time:112574ms step_avg:61.52ms
step:1831/2330 train_time:112634ms step_avg:61.52ms
step:1832/2330 train_time:112698ms step_avg:61.52ms
step:1833/2330 train_time:112758ms step_avg:61.52ms
step:1834/2330 train_time:112820ms step_avg:61.52ms
step:1835/2330 train_time:112880ms step_avg:61.52ms
step:1836/2330 train_time:112944ms step_avg:61.52ms
step:1837/2330 train_time:113004ms step_avg:61.52ms
step:1838/2330 train_time:113068ms step_avg:61.52ms
step:1839/2330 train_time:113129ms step_avg:61.52ms
step:1840/2330 train_time:113193ms step_avg:61.52ms
step:1841/2330 train_time:113254ms step_avg:61.52ms
step:1842/2330 train_time:113317ms step_avg:61.52ms
step:1843/2330 train_time:113377ms step_avg:61.52ms
step:1844/2330 train_time:113440ms step_avg:61.52ms
step:1845/2330 train_time:113501ms step_avg:61.52ms
step:1846/2330 train_time:113566ms step_avg:61.52ms
step:1847/2330 train_time:113626ms step_avg:61.52ms
step:1848/2330 train_time:113690ms step_avg:61.52ms
step:1849/2330 train_time:113751ms step_avg:61.52ms
step:1850/2330 train_time:113814ms step_avg:61.52ms
step:1851/2330 train_time:113875ms step_avg:61.52ms
step:1852/2330 train_time:113939ms step_avg:61.52ms
step:1853/2330 train_time:114000ms step_avg:61.52ms
step:1854/2330 train_time:114063ms step_avg:61.52ms
step:1855/2330 train_time:114125ms step_avg:61.52ms
step:1856/2330 train_time:114189ms step_avg:61.52ms
step:1857/2330 train_time:114250ms step_avg:61.52ms
step:1858/2330 train_time:114313ms step_avg:61.52ms
step:1859/2330 train_time:114374ms step_avg:61.52ms
step:1860/2330 train_time:114436ms step_avg:61.52ms
step:1861/2330 train_time:114497ms step_avg:61.52ms
step:1862/2330 train_time:114561ms step_avg:61.53ms
step:1863/2330 train_time:114621ms step_avg:61.53ms
step:1864/2330 train_time:114686ms step_avg:61.53ms
step:1865/2330 train_time:114747ms step_avg:61.53ms
step:1866/2330 train_time:114810ms step_avg:61.53ms
step:1867/2330 train_time:114870ms step_avg:61.53ms
step:1868/2330 train_time:114934ms step_avg:61.53ms
step:1869/2330 train_time:114994ms step_avg:61.53ms
step:1870/2330 train_time:115058ms step_avg:61.53ms
step:1871/2330 train_time:115119ms step_avg:61.53ms
step:1872/2330 train_time:115183ms step_avg:61.53ms
step:1873/2330 train_time:115244ms step_avg:61.53ms
step:1874/2330 train_time:115307ms step_avg:61.53ms
step:1875/2330 train_time:115367ms step_avg:61.53ms
step:1876/2330 train_time:115431ms step_avg:61.53ms
step:1877/2330 train_time:115492ms step_avg:61.53ms
step:1878/2330 train_time:115556ms step_avg:61.53ms
step:1879/2330 train_time:115617ms step_avg:61.53ms
step:1880/2330 train_time:115680ms step_avg:61.53ms
step:1881/2330 train_time:115740ms step_avg:61.53ms
step:1882/2330 train_time:115803ms step_avg:61.53ms
step:1883/2330 train_time:115863ms step_avg:61.53ms
step:1884/2330 train_time:115927ms step_avg:61.53ms
step:1885/2330 train_time:115987ms step_avg:61.53ms
step:1886/2330 train_time:116051ms step_avg:61.53ms
step:1887/2330 train_time:116112ms step_avg:61.53ms
step:1888/2330 train_time:116176ms step_avg:61.53ms
step:1889/2330 train_time:116237ms step_avg:61.53ms
step:1890/2330 train_time:116300ms step_avg:61.53ms
step:1891/2330 train_time:116360ms step_avg:61.53ms
step:1892/2330 train_time:116424ms step_avg:61.53ms
step:1893/2330 train_time:116484ms step_avg:61.53ms
step:1894/2330 train_time:116548ms step_avg:61.54ms
step:1895/2330 train_time:116608ms step_avg:61.53ms
step:1896/2330 train_time:116672ms step_avg:61.54ms
step:1897/2330 train_time:116732ms step_avg:61.54ms
step:1898/2330 train_time:116796ms step_avg:61.54ms
step:1899/2330 train_time:116858ms step_avg:61.54ms
step:1900/2330 train_time:116921ms step_avg:61.54ms
step:1901/2330 train_time:116982ms step_avg:61.54ms
step:1902/2330 train_time:117045ms step_avg:61.54ms
step:1903/2330 train_time:117106ms step_avg:61.54ms
step:1904/2330 train_time:117168ms step_avg:61.54ms
step:1905/2330 train_time:117229ms step_avg:61.54ms
step:1906/2330 train_time:117292ms step_avg:61.54ms
step:1907/2330 train_time:117353ms step_avg:61.54ms
step:1908/2330 train_time:117416ms step_avg:61.54ms
step:1909/2330 train_time:117476ms step_avg:61.54ms
step:1910/2330 train_time:117541ms step_avg:61.54ms
step:1911/2330 train_time:117601ms step_avg:61.54ms
step:1912/2330 train_time:117666ms step_avg:61.54ms
step:1913/2330 train_time:117727ms step_avg:61.54ms
step:1914/2330 train_time:117791ms step_avg:61.54ms
step:1915/2330 train_time:117851ms step_avg:61.54ms
step:1916/2330 train_time:117914ms step_avg:61.54ms
step:1917/2330 train_time:117974ms step_avg:61.54ms
step:1918/2330 train_time:118038ms step_avg:61.54ms
step:1919/2330 train_time:118099ms step_avg:61.54ms
step:1920/2330 train_time:118162ms step_avg:61.54ms
step:1921/2330 train_time:118222ms step_avg:61.54ms
step:1922/2330 train_time:118286ms step_avg:61.54ms
step:1923/2330 train_time:118346ms step_avg:61.54ms
step:1924/2330 train_time:118410ms step_avg:61.54ms
step:1925/2330 train_time:118471ms step_avg:61.54ms
step:1926/2330 train_time:118534ms step_avg:61.54ms
step:1927/2330 train_time:118595ms step_avg:61.54ms
step:1928/2330 train_time:118660ms step_avg:61.55ms
step:1929/2330 train_time:118720ms step_avg:61.54ms
step:1930/2330 train_time:118784ms step_avg:61.55ms
step:1931/2330 train_time:118844ms step_avg:61.55ms
step:1932/2330 train_time:118908ms step_avg:61.55ms
step:1933/2330 train_time:118969ms step_avg:61.55ms
step:1934/2330 train_time:119032ms step_avg:61.55ms
step:1935/2330 train_time:119093ms step_avg:61.55ms
step:1936/2330 train_time:119156ms step_avg:61.55ms
step:1937/2330 train_time:119217ms step_avg:61.55ms
step:1938/2330 train_time:119280ms step_avg:61.55ms
step:1939/2330 train_time:119340ms step_avg:61.55ms
step:1940/2330 train_time:119405ms step_avg:61.55ms
step:1941/2330 train_time:119466ms step_avg:61.55ms
step:1942/2330 train_time:119529ms step_avg:61.55ms
step:1943/2330 train_time:119590ms step_avg:61.55ms
step:1944/2330 train_time:119654ms step_avg:61.55ms
step:1945/2330 train_time:119714ms step_avg:61.55ms
step:1946/2330 train_time:119777ms step_avg:61.55ms
step:1947/2330 train_time:119838ms step_avg:61.55ms
step:1948/2330 train_time:119901ms step_avg:61.55ms
step:1949/2330 train_time:119962ms step_avg:61.55ms
step:1950/2330 train_time:120025ms step_avg:61.55ms
step:1951/2330 train_time:120085ms step_avg:61.55ms
step:1952/2330 train_time:120149ms step_avg:61.55ms
step:1953/2330 train_time:120210ms step_avg:61.55ms
step:1954/2330 train_time:120273ms step_avg:61.55ms
step:1955/2330 train_time:120333ms step_avg:61.55ms
step:1956/2330 train_time:120398ms step_avg:61.55ms
step:1957/2330 train_time:120459ms step_avg:61.55ms
step:1958/2330 train_time:120522ms step_avg:61.55ms
step:1959/2330 train_time:120582ms step_avg:61.55ms
step:1960/2330 train_time:120646ms step_avg:61.55ms
step:1961/2330 train_time:120707ms step_avg:61.55ms
step:1962/2330 train_time:120771ms step_avg:61.55ms
step:1963/2330 train_time:120831ms step_avg:61.55ms
step:1964/2330 train_time:120894ms step_avg:61.56ms
step:1965/2330 train_time:120955ms step_avg:61.55ms
step:1966/2330 train_time:121019ms step_avg:61.56ms
step:1967/2330 train_time:121079ms step_avg:61.56ms
step:1968/2330 train_time:121142ms step_avg:61.56ms
step:1969/2330 train_time:121203ms step_avg:61.56ms
step:1970/2330 train_time:121266ms step_avg:61.56ms
step:1971/2330 train_time:121327ms step_avg:61.56ms
step:1972/2330 train_time:121390ms step_avg:61.56ms
step:1973/2330 train_time:121451ms step_avg:61.56ms
step:1974/2330 train_time:121515ms step_avg:61.56ms
step:1975/2330 train_time:121575ms step_avg:61.56ms
step:1976/2330 train_time:121638ms step_avg:61.56ms
step:1977/2330 train_time:121698ms step_avg:61.56ms
step:1978/2330 train_time:121762ms step_avg:61.56ms
step:1979/2330 train_time:121822ms step_avg:61.56ms
step:1980/2330 train_time:121887ms step_avg:61.56ms
step:1981/2330 train_time:121947ms step_avg:61.56ms
step:1982/2330 train_time:122011ms step_avg:61.56ms
step:1983/2330 train_time:122072ms step_avg:61.56ms
step:1984/2330 train_time:122135ms step_avg:61.56ms
step:1985/2330 train_time:122195ms step_avg:61.56ms
step:1986/2330 train_time:122260ms step_avg:61.56ms
step:1987/2330 train_time:122320ms step_avg:61.56ms
step:1988/2330 train_time:122384ms step_avg:61.56ms
step:1989/2330 train_time:122445ms step_avg:61.56ms
step:1990/2330 train_time:122509ms step_avg:61.56ms
step:1991/2330 train_time:122570ms step_avg:61.56ms
step:1992/2330 train_time:122633ms step_avg:61.56ms
step:1993/2330 train_time:122694ms step_avg:61.56ms
step:1994/2330 train_time:122758ms step_avg:61.56ms
step:1995/2330 train_time:122819ms step_avg:61.56ms
step:1996/2330 train_time:122882ms step_avg:61.56ms
step:1997/2330 train_time:122943ms step_avg:61.56ms
step:1998/2330 train_time:123007ms step_avg:61.56ms
step:1999/2330 train_time:123067ms step_avg:61.56ms
step:2000/2330 train_time:123130ms step_avg:61.57ms
step:2000/2330 val_loss:3.4183 train_time:123195ms step_avg:61.60ms
step:2001/2330 train_time:123218ms step_avg:61.58ms
step:2002/2330 train_time:123260ms step_avg:61.57ms
step:2003/2330 train_time:123326ms step_avg:61.57ms
step:2004/2330 train_time:123390ms step_avg:61.57ms
step:2005/2330 train_time:123450ms step_avg:61.57ms
step:2006/2330 train_time:123514ms step_avg:61.57ms
step:2007/2330 train_time:123574ms step_avg:61.57ms
step:2008/2330 train_time:123637ms step_avg:61.57ms
step:2009/2330 train_time:123698ms step_avg:61.57ms
step:2010/2330 train_time:123760ms step_avg:61.57ms
step:2011/2330 train_time:123820ms step_avg:61.57ms
step:2012/2330 train_time:123883ms step_avg:61.57ms
step:2013/2330 train_time:123942ms step_avg:61.57ms
step:2014/2330 train_time:124005ms step_avg:61.57ms
step:2015/2330 train_time:124064ms step_avg:61.57ms
step:2016/2330 train_time:124128ms step_avg:61.57ms
step:2017/2330 train_time:124189ms step_avg:61.57ms
step:2018/2330 train_time:124254ms step_avg:61.57ms
step:2019/2330 train_time:124317ms step_avg:61.57ms
step:2020/2330 train_time:124381ms step_avg:61.57ms
step:2021/2330 train_time:124442ms step_avg:61.57ms
step:2022/2330 train_time:124506ms step_avg:61.58ms
step:2023/2330 train_time:124567ms step_avg:61.58ms
step:2024/2330 train_time:124630ms step_avg:61.58ms
step:2025/2330 train_time:124691ms step_avg:61.58ms
step:2026/2330 train_time:124754ms step_avg:61.58ms
step:2027/2330 train_time:124814ms step_avg:61.58ms
step:2028/2330 train_time:124877ms step_avg:61.58ms
step:2029/2330 train_time:124937ms step_avg:61.58ms
step:2030/2330 train_time:124999ms step_avg:61.58ms
step:2031/2330 train_time:125059ms step_avg:61.58ms
step:2032/2330 train_time:125123ms step_avg:61.58ms
step:2033/2330 train_time:125183ms step_avg:61.58ms
step:2034/2330 train_time:125247ms step_avg:61.58ms
step:2035/2330 train_time:125308ms step_avg:61.58ms
step:2036/2330 train_time:125371ms step_avg:61.58ms
step:2037/2330 train_time:125433ms step_avg:61.58ms
step:2038/2330 train_time:125497ms step_avg:61.58ms
step:2039/2330 train_time:125557ms step_avg:61.58ms
step:2040/2330 train_time:125621ms step_avg:61.58ms
step:2041/2330 train_time:125683ms step_avg:61.58ms
step:2042/2330 train_time:125747ms step_avg:61.58ms
step:2043/2330 train_time:125807ms step_avg:61.58ms
step:2044/2330 train_time:125871ms step_avg:61.58ms
step:2045/2330 train_time:125932ms step_avg:61.58ms
step:2046/2330 train_time:125994ms step_avg:61.58ms
step:2047/2330 train_time:126055ms step_avg:61.58ms
step:2048/2330 train_time:126118ms step_avg:61.58ms
step:2049/2330 train_time:126179ms step_avg:61.58ms
step:2050/2330 train_time:126243ms step_avg:61.58ms
step:2051/2330 train_time:126305ms step_avg:61.58ms
step:2052/2330 train_time:126369ms step_avg:61.58ms
step:2053/2330 train_time:126429ms step_avg:61.58ms
step:2054/2330 train_time:126493ms step_avg:61.58ms
step:2055/2330 train_time:126554ms step_avg:61.58ms
step:2056/2330 train_time:126618ms step_avg:61.58ms
step:2057/2330 train_time:126678ms step_avg:61.58ms
step:2058/2330 train_time:126743ms step_avg:61.59ms
step:2059/2330 train_time:126803ms step_avg:61.58ms
step:2060/2330 train_time:126867ms step_avg:61.59ms
step:2061/2330 train_time:126927ms step_avg:61.59ms
step:2062/2330 train_time:126990ms step_avg:61.59ms
step:2063/2330 train_time:127051ms step_avg:61.59ms
step:2064/2330 train_time:127115ms step_avg:61.59ms
step:2065/2330 train_time:127176ms step_avg:61.59ms
step:2066/2330 train_time:127240ms step_avg:61.59ms
step:2067/2330 train_time:127301ms step_avg:61.59ms
step:2068/2330 train_time:127365ms step_avg:61.59ms
step:2069/2330 train_time:127426ms step_avg:61.59ms
step:2070/2330 train_time:127489ms step_avg:61.59ms
step:2071/2330 train_time:127550ms step_avg:61.59ms
step:2072/2330 train_time:127614ms step_avg:61.59ms
step:2073/2330 train_time:127675ms step_avg:61.59ms
step:2074/2330 train_time:127738ms step_avg:61.59ms
step:2075/2330 train_time:127798ms step_avg:61.59ms
step:2076/2330 train_time:127861ms step_avg:61.59ms
step:2077/2330 train_time:127922ms step_avg:61.59ms
step:2078/2330 train_time:127985ms step_avg:61.59ms
step:2079/2330 train_time:128045ms step_avg:61.59ms
step:2080/2330 train_time:128109ms step_avg:61.59ms
step:2081/2330 train_time:128169ms step_avg:61.59ms
step:2082/2330 train_time:128233ms step_avg:61.59ms
step:2083/2330 train_time:128294ms step_avg:61.59ms
step:2084/2330 train_time:128357ms step_avg:61.59ms
step:2085/2330 train_time:128418ms step_avg:61.59ms
step:2086/2330 train_time:128481ms step_avg:61.59ms
step:2087/2330 train_time:128542ms step_avg:61.59ms
step:2088/2330 train_time:128605ms step_avg:61.59ms
step:2089/2330 train_time:128666ms step_avg:61.59ms
step:2090/2330 train_time:128729ms step_avg:61.59ms
step:2091/2330 train_time:128790ms step_avg:61.59ms
step:2092/2330 train_time:128854ms step_avg:61.59ms
step:2093/2330 train_time:128915ms step_avg:61.59ms
step:2094/2330 train_time:128979ms step_avg:61.59ms
step:2095/2330 train_time:129039ms step_avg:61.59ms
step:2096/2330 train_time:129102ms step_avg:61.59ms
step:2097/2330 train_time:129164ms step_avg:61.59ms
step:2098/2330 train_time:129227ms step_avg:61.60ms
step:2099/2330 train_time:129287ms step_avg:61.59ms
step:2100/2330 train_time:129351ms step_avg:61.60ms
step:2101/2330 train_time:129412ms step_avg:61.60ms
step:2102/2330 train_time:129476ms step_avg:61.60ms
step:2103/2330 train_time:129537ms step_avg:61.60ms
step:2104/2330 train_time:129600ms step_avg:61.60ms
step:2105/2330 train_time:129660ms step_avg:61.60ms
step:2106/2330 train_time:129724ms step_avg:61.60ms
step:2107/2330 train_time:129785ms step_avg:61.60ms
step:2108/2330 train_time:129848ms step_avg:61.60ms
step:2109/2330 train_time:129909ms step_avg:61.60ms
step:2110/2330 train_time:129972ms step_avg:61.60ms
step:2111/2330 train_time:130033ms step_avg:61.60ms
step:2112/2330 train_time:130096ms step_avg:61.60ms
step:2113/2330 train_time:130157ms step_avg:61.60ms
step:2114/2330 train_time:130220ms step_avg:61.60ms
step:2115/2330 train_time:130280ms step_avg:61.60ms
step:2116/2330 train_time:130344ms step_avg:61.60ms
step:2117/2330 train_time:130405ms step_avg:61.60ms
step:2118/2330 train_time:130468ms step_avg:61.60ms
step:2119/2330 train_time:130529ms step_avg:61.60ms
step:2120/2330 train_time:130592ms step_avg:61.60ms
step:2121/2330 train_time:130653ms step_avg:61.60ms
step:2122/2330 train_time:130718ms step_avg:61.60ms
step:2123/2330 train_time:130779ms step_avg:61.60ms
step:2124/2330 train_time:130841ms step_avg:61.60ms
step:2125/2330 train_time:130902ms step_avg:61.60ms
step:2126/2330 train_time:130965ms step_avg:61.60ms
step:2127/2330 train_time:131026ms step_avg:61.60ms
step:2128/2330 train_time:131089ms step_avg:61.60ms
step:2129/2330 train_time:131150ms step_avg:61.60ms
step:2130/2330 train_time:131213ms step_avg:61.60ms
step:2131/2330 train_time:131274ms step_avg:61.60ms
step:2132/2330 train_time:131337ms step_avg:61.60ms
step:2133/2330 train_time:131398ms step_avg:61.60ms
step:2134/2330 train_time:131461ms step_avg:61.60ms
step:2135/2330 train_time:131521ms step_avg:61.60ms
step:2136/2330 train_time:131585ms step_avg:61.60ms
step:2137/2330 train_time:131646ms step_avg:61.60ms
step:2138/2330 train_time:131709ms step_avg:61.60ms
step:2139/2330 train_time:131770ms step_avg:61.60ms
step:2140/2330 train_time:131833ms step_avg:61.60ms
step:2141/2330 train_time:131893ms step_avg:61.60ms
step:2142/2330 train_time:131957ms step_avg:61.60ms
step:2143/2330 train_time:132018ms step_avg:61.60ms
step:2144/2330 train_time:132081ms step_avg:61.61ms
step:2145/2330 train_time:132142ms step_avg:61.60ms
step:2146/2330 train_time:132205ms step_avg:61.61ms
step:2147/2330 train_time:132265ms step_avg:61.60ms
step:2148/2330 train_time:132329ms step_avg:61.61ms
step:2149/2330 train_time:132390ms step_avg:61.61ms
step:2150/2330 train_time:132454ms step_avg:61.61ms
step:2151/2330 train_time:132515ms step_avg:61.61ms
step:2152/2330 train_time:132579ms step_avg:61.61ms
step:2153/2330 train_time:132639ms step_avg:61.61ms
step:2154/2330 train_time:132702ms step_avg:61.61ms
step:2155/2330 train_time:132764ms step_avg:61.61ms
step:2156/2330 train_time:132827ms step_avg:61.61ms
step:2157/2330 train_time:132888ms step_avg:61.61ms
step:2158/2330 train_time:132951ms step_avg:61.61ms
step:2159/2330 train_time:133012ms step_avg:61.61ms
step:2160/2330 train_time:133076ms step_avg:61.61ms
step:2161/2330 train_time:133137ms step_avg:61.61ms
step:2162/2330 train_time:133200ms step_avg:61.61ms
step:2163/2330 train_time:133261ms step_avg:61.61ms
step:2164/2330 train_time:133326ms step_avg:61.61ms
step:2165/2330 train_time:133386ms step_avg:61.61ms
step:2166/2330 train_time:133450ms step_avg:61.61ms
step:2167/2330 train_time:133511ms step_avg:61.61ms
step:2168/2330 train_time:133574ms step_avg:61.61ms
step:2169/2330 train_time:133635ms step_avg:61.61ms
step:2170/2330 train_time:133698ms step_avg:61.61ms
step:2171/2330 train_time:133758ms step_avg:61.61ms
step:2172/2330 train_time:133823ms step_avg:61.61ms
step:2173/2330 train_time:133884ms step_avg:61.61ms
step:2174/2330 train_time:133947ms step_avg:61.61ms
step:2175/2330 train_time:134008ms step_avg:61.61ms
step:2176/2330 train_time:134071ms step_avg:61.61ms
step:2177/2330 train_time:134132ms step_avg:61.61ms
step:2178/2330 train_time:134196ms step_avg:61.61ms
step:2179/2330 train_time:134256ms step_avg:61.61ms
step:2180/2330 train_time:134320ms step_avg:61.61ms
step:2181/2330 train_time:134381ms step_avg:61.61ms
step:2182/2330 train_time:134445ms step_avg:61.62ms
step:2183/2330 train_time:134505ms step_avg:61.61ms
step:2184/2330 train_time:134569ms step_avg:61.62ms
step:2185/2330 train_time:134629ms step_avg:61.62ms
step:2186/2330 train_time:134693ms step_avg:61.62ms
step:2187/2330 train_time:134754ms step_avg:61.62ms
step:2188/2330 train_time:134818ms step_avg:61.62ms
step:2189/2330 train_time:134878ms step_avg:61.62ms
step:2190/2330 train_time:134941ms step_avg:61.62ms
step:2191/2330 train_time:135001ms step_avg:61.62ms
step:2192/2330 train_time:135065ms step_avg:61.62ms
step:2193/2330 train_time:135126ms step_avg:61.62ms
step:2194/2330 train_time:135189ms step_avg:61.62ms
step:2195/2330 train_time:135249ms step_avg:61.62ms
step:2196/2330 train_time:135313ms step_avg:61.62ms
step:2197/2330 train_time:135374ms step_avg:61.62ms
step:2198/2330 train_time:135438ms step_avg:61.62ms
step:2199/2330 train_time:135498ms step_avg:61.62ms
step:2200/2330 train_time:135561ms step_avg:61.62ms
step:2201/2330 train_time:135622ms step_avg:61.62ms
step:2202/2330 train_time:135686ms step_avg:61.62ms
step:2203/2330 train_time:135746ms step_avg:61.62ms
step:2204/2330 train_time:135810ms step_avg:61.62ms
step:2205/2330 train_time:135870ms step_avg:61.62ms
step:2206/2330 train_time:135934ms step_avg:61.62ms
step:2207/2330 train_time:135994ms step_avg:61.62ms
step:2208/2330 train_time:136058ms step_avg:61.62ms
step:2209/2330 train_time:136119ms step_avg:61.62ms
step:2210/2330 train_time:136182ms step_avg:61.62ms
step:2211/2330 train_time:136243ms step_avg:61.62ms
step:2212/2330 train_time:136306ms step_avg:61.62ms
step:2213/2330 train_time:136367ms step_avg:61.62ms
step:2214/2330 train_time:136430ms step_avg:61.62ms
step:2215/2330 train_time:136492ms step_avg:61.62ms
step:2216/2330 train_time:136556ms step_avg:61.62ms
step:2217/2330 train_time:136616ms step_avg:61.62ms
step:2218/2330 train_time:136679ms step_avg:61.62ms
step:2219/2330 train_time:136739ms step_avg:61.62ms
step:2220/2330 train_time:136803ms step_avg:61.62ms
step:2221/2330 train_time:136864ms step_avg:61.62ms
step:2222/2330 train_time:136927ms step_avg:61.62ms
step:2223/2330 train_time:136987ms step_avg:61.62ms
step:2224/2330 train_time:137051ms step_avg:61.62ms
step:2225/2330 train_time:137111ms step_avg:61.62ms
step:2226/2330 train_time:137175ms step_avg:61.62ms
step:2227/2330 train_time:137235ms step_avg:61.62ms
step:2228/2330 train_time:137299ms step_avg:61.62ms
step:2229/2330 train_time:137359ms step_avg:61.62ms
step:2230/2330 train_time:137424ms step_avg:61.62ms
step:2231/2330 train_time:137484ms step_avg:61.62ms
step:2232/2330 train_time:137547ms step_avg:61.63ms
step:2233/2330 train_time:137608ms step_avg:61.62ms
step:2234/2330 train_time:137672ms step_avg:61.63ms
step:2235/2330 train_time:137733ms step_avg:61.63ms
step:2236/2330 train_time:137797ms step_avg:61.63ms
step:2237/2330 train_time:137858ms step_avg:61.63ms
step:2238/2330 train_time:137921ms step_avg:61.63ms
step:2239/2330 train_time:137982ms step_avg:61.63ms
step:2240/2330 train_time:138046ms step_avg:61.63ms
step:2241/2330 train_time:138106ms step_avg:61.63ms
step:2242/2330 train_time:138169ms step_avg:61.63ms
step:2243/2330 train_time:138230ms step_avg:61.63ms
step:2244/2330 train_time:138294ms step_avg:61.63ms
step:2245/2330 train_time:138354ms step_avg:61.63ms
step:2246/2330 train_time:138418ms step_avg:61.63ms
step:2247/2330 train_time:138478ms step_avg:61.63ms
step:2248/2330 train_time:138542ms step_avg:61.63ms
step:2249/2330 train_time:138602ms step_avg:61.63ms
step:2250/2330 train_time:138666ms step_avg:61.63ms
step:2250/2330 val_loss:3.3935 train_time:138731ms step_avg:61.66ms
step:2251/2330 train_time:138754ms step_avg:61.64ms
step:2252/2330 train_time:138795ms step_avg:61.63ms
step:2253/2330 train_time:138862ms step_avg:61.63ms
step:2254/2330 train_time:138928ms step_avg:61.64ms
step:2255/2330 train_time:138988ms step_avg:61.64ms
step:2256/2330 train_time:139052ms step_avg:61.64ms
step:2257/2330 train_time:139112ms step_avg:61.64ms
step:2258/2330 train_time:139176ms step_avg:61.64ms
step:2259/2330 train_time:139236ms step_avg:61.64ms
step:2260/2330 train_time:139298ms step_avg:61.64ms
step:2261/2330 train_time:139358ms step_avg:61.64ms
step:2262/2330 train_time:139420ms step_avg:61.64ms
step:2263/2330 train_time:139480ms step_avg:61.63ms
step:2264/2330 train_time:139543ms step_avg:61.64ms
step:2265/2330 train_time:139602ms step_avg:61.63ms
step:2266/2330 train_time:139666ms step_avg:61.64ms
step:2267/2330 train_time:139727ms step_avg:61.64ms
step:2268/2330 train_time:139793ms step_avg:61.64ms
step:2269/2330 train_time:139855ms step_avg:61.64ms
step:2270/2330 train_time:139920ms step_avg:61.64ms
step:2271/2330 train_time:139981ms step_avg:61.64ms
step:2272/2330 train_time:140045ms step_avg:61.64ms
step:2273/2330 train_time:140105ms step_avg:61.64ms
step:2274/2330 train_time:140168ms step_avg:61.64ms
step:2275/2330 train_time:140228ms step_avg:61.64ms
step:2276/2330 train_time:140292ms step_avg:61.64ms
step:2277/2330 train_time:140352ms step_avg:61.64ms
step:2278/2330 train_time:140414ms step_avg:61.64ms
step:2279/2330 train_time:140475ms step_avg:61.64ms
step:2280/2330 train_time:140538ms step_avg:61.64ms
step:2281/2330 train_time:140598ms step_avg:61.64ms
step:2282/2330 train_time:140661ms step_avg:61.64ms
step:2283/2330 train_time:140722ms step_avg:61.64ms
step:2284/2330 train_time:140787ms step_avg:61.64ms
step:2285/2330 train_time:140848ms step_avg:61.64ms
step:2286/2330 train_time:140912ms step_avg:61.64ms
step:2287/2330 train_time:140973ms step_avg:61.64ms
step:2288/2330 train_time:141037ms step_avg:61.64ms
step:2289/2330 train_time:141098ms step_avg:61.64ms
step:2290/2330 train_time:141161ms step_avg:61.64ms
step:2291/2330 train_time:141221ms step_avg:61.64ms
step:2292/2330 train_time:141286ms step_avg:61.64ms
step:2293/2330 train_time:141346ms step_avg:61.64ms
step:2294/2330 train_time:141409ms step_avg:61.64ms
step:2295/2330 train_time:141469ms step_avg:61.64ms
step:2296/2330 train_time:141533ms step_avg:61.64ms
step:2297/2330 train_time:141593ms step_avg:61.64ms
step:2298/2330 train_time:141657ms step_avg:61.64ms
step:2299/2330 train_time:141717ms step_avg:61.64ms
step:2300/2330 train_time:141780ms step_avg:61.64ms
step:2301/2330 train_time:141841ms step_avg:61.64ms
step:2302/2330 train_time:141904ms step_avg:61.64ms
step:2303/2330 train_time:141965ms step_avg:61.64ms
step:2304/2330 train_time:142029ms step_avg:61.64ms
step:2305/2330 train_time:142089ms step_avg:61.64ms
step:2306/2330 train_time:142153ms step_avg:61.64ms
step:2307/2330 train_time:142213ms step_avg:61.64ms
step:2308/2330 train_time:142277ms step_avg:61.65ms
step:2309/2330 train_time:142338ms step_avg:61.64ms
step:2310/2330 train_time:142402ms step_avg:61.65ms
step:2311/2330 train_time:142463ms step_avg:61.65ms
step:2312/2330 train_time:142525ms step_avg:61.65ms
step:2313/2330 train_time:142585ms step_avg:61.65ms
step:2314/2330 train_time:142648ms step_avg:61.65ms
step:2315/2330 train_time:142708ms step_avg:61.65ms
step:2316/2330 train_time:142773ms step_avg:61.65ms
step:2317/2330 train_time:142833ms step_avg:61.65ms
step:2318/2330 train_time:142897ms step_avg:61.65ms
step:2319/2330 train_time:142957ms step_avg:61.65ms
step:2320/2330 train_time:143021ms step_avg:61.65ms
step:2321/2330 train_time:143081ms step_avg:61.65ms
step:2322/2330 train_time:143145ms step_avg:61.65ms
step:2323/2330 train_time:143206ms step_avg:61.65ms
step:2324/2330 train_time:143269ms step_avg:61.65ms
step:2325/2330 train_time:143330ms step_avg:61.65ms
step:2326/2330 train_time:143393ms step_avg:61.65ms
step:2327/2330 train_time:143454ms step_avg:61.65ms
step:2328/2330 train_time:143517ms step_avg:61.65ms
step:2329/2330 train_time:143577ms step_avg:61.65ms
step:2330/2330 train_time:143641ms step_avg:61.65ms
step:2330/2330 val_loss:3.3838 train_time:143706ms step_avg:61.68ms
peak memory allocated: 29998 MiB reserved: 44076 MiB
