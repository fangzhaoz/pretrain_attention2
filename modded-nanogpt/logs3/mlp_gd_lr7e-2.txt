import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = divide_by_norm(batched)
                # v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = divide_by_norm(batched_update_grads)
                # v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        # self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "mlp_gd_lr7e-2"
    os.makedirs("logs3", exist_ok=True)
    logfile = f"logs3/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
# embed_params = [p for n, p in model.named_parameters() if "embed" in n]
embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=7e-2, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    return 1.0

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum_min + 0.5 * (momentum_max - momentum_min)

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs3/{run_id}", exist_ok=True)
            torch.save(log, f"logs3/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 08:17:12 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   33C    P0             112W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   32C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   32C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             116W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.05ms
step:1/2330 train_time:86ms step_avg:85.69ms
step:2/2330 train_time:162ms step_avg:80.88ms
step:3/2330 train_time:174ms step_avg:58.00ms
step:4/2330 train_time:186ms step_avg:46.62ms
step:5/2330 train_time:197ms step_avg:39.40ms
step:6/2330 train_time:222ms step_avg:36.97ms
step:7/2330 train_time:243ms step_avg:34.71ms
step:8/2330 train_time:298ms step_avg:37.21ms
step:9/2330 train_time:320ms step_avg:35.52ms
step:10/2330 train_time:374ms step_avg:37.43ms
step:11/2330 train_time:397ms step_avg:36.08ms
step:12/2330 train_time:453ms step_avg:37.73ms
step:13/2330 train_time:475ms step_avg:36.52ms
step:14/2330 train_time:531ms step_avg:37.90ms
step:15/2330 train_time:552ms step_avg:36.81ms
step:16/2330 train_time:608ms step_avg:38.01ms
step:17/2330 train_time:630ms step_avg:37.06ms
step:18/2330 train_time:686ms step_avg:38.10ms
step:19/2330 train_time:707ms step_avg:37.23ms
step:20/2330 train_time:763ms step_avg:38.13ms
step:21/2330 train_time:785ms step_avg:37.37ms
step:22/2330 train_time:840ms step_avg:38.19ms
step:23/2330 train_time:862ms step_avg:37.50ms
step:24/2330 train_time:918ms step_avg:38.25ms
step:25/2330 train_time:940ms step_avg:37.60ms
step:26/2330 train_time:997ms step_avg:38.36ms
step:27/2330 train_time:1023ms step_avg:37.88ms
step:28/2330 train_time:1083ms step_avg:38.68ms
step:29/2330 train_time:1108ms step_avg:38.21ms
step:30/2330 train_time:1168ms step_avg:38.94ms
step:31/2330 train_time:1192ms step_avg:38.44ms
step:32/2330 train_time:1249ms step_avg:39.02ms
step:33/2330 train_time:1271ms step_avg:38.52ms
step:34/2330 train_time:1327ms step_avg:39.03ms
step:35/2330 train_time:1349ms step_avg:38.55ms
step:36/2330 train_time:1405ms step_avg:39.02ms
step:37/2330 train_time:1427ms step_avg:38.56ms
step:38/2330 train_time:1482ms step_avg:39.01ms
step:39/2330 train_time:1505ms step_avg:38.58ms
step:40/2330 train_time:1560ms step_avg:38.99ms
step:41/2330 train_time:1582ms step_avg:38.59ms
step:42/2330 train_time:1638ms step_avg:38.99ms
step:43/2330 train_time:1660ms step_avg:38.60ms
step:44/2330 train_time:1715ms step_avg:38.98ms
step:45/2330 train_time:1737ms step_avg:38.60ms
step:46/2330 train_time:1793ms step_avg:38.98ms
step:47/2330 train_time:1815ms step_avg:38.62ms
step:48/2330 train_time:1871ms step_avg:38.97ms
step:49/2330 train_time:1893ms step_avg:38.64ms
step:50/2330 train_time:1950ms step_avg:39.00ms
step:51/2330 train_time:1972ms step_avg:38.68ms
step:52/2330 train_time:2031ms step_avg:39.06ms
step:53/2330 train_time:2054ms step_avg:38.75ms
step:54/2330 train_time:2111ms step_avg:39.10ms
step:55/2330 train_time:2134ms step_avg:38.80ms
step:56/2330 train_time:2191ms step_avg:39.13ms
step:57/2330 train_time:2214ms step_avg:38.85ms
step:58/2330 train_time:2271ms step_avg:39.15ms
step:59/2330 train_time:2293ms step_avg:38.87ms
step:60/2330 train_time:2351ms step_avg:39.18ms
step:61/2330 train_time:2373ms step_avg:38.90ms
step:62/2330 train_time:2429ms step_avg:39.18ms
step:63/2330 train_time:2451ms step_avg:38.90ms
step:64/2330 train_time:2507ms step_avg:39.17ms
step:65/2330 train_time:2529ms step_avg:38.90ms
step:66/2330 train_time:2585ms step_avg:39.17ms
step:67/2330 train_time:2607ms step_avg:38.91ms
step:68/2330 train_time:2662ms step_avg:39.15ms
step:69/2330 train_time:2685ms step_avg:38.91ms
step:70/2330 train_time:2740ms step_avg:39.14ms
step:71/2330 train_time:2762ms step_avg:38.91ms
step:72/2330 train_time:2818ms step_avg:39.14ms
step:73/2330 train_time:2840ms step_avg:38.91ms
step:74/2330 train_time:2896ms step_avg:39.14ms
step:75/2330 train_time:2919ms step_avg:38.92ms
step:76/2330 train_time:2975ms step_avg:39.14ms
step:77/2330 train_time:2997ms step_avg:38.93ms
step:78/2330 train_time:3055ms step_avg:39.16ms
step:79/2330 train_time:3077ms step_avg:38.95ms
step:80/2330 train_time:3134ms step_avg:39.17ms
step:81/2330 train_time:3157ms step_avg:38.98ms
step:82/2330 train_time:3215ms step_avg:39.20ms
step:83/2330 train_time:3237ms step_avg:39.00ms
step:84/2330 train_time:3294ms step_avg:39.21ms
step:85/2330 train_time:3317ms step_avg:39.02ms
step:86/2330 train_time:3373ms step_avg:39.22ms
step:87/2330 train_time:3396ms step_avg:39.04ms
step:88/2330 train_time:3453ms step_avg:39.24ms
step:89/2330 train_time:3476ms step_avg:39.06ms
step:90/2330 train_time:3533ms step_avg:39.26ms
step:91/2330 train_time:3556ms step_avg:39.07ms
step:92/2330 train_time:3612ms step_avg:39.26ms
step:93/2330 train_time:3634ms step_avg:39.08ms
step:94/2330 train_time:3690ms step_avg:39.26ms
step:95/2330 train_time:3712ms step_avg:39.07ms
step:96/2330 train_time:3768ms step_avg:39.25ms
step:97/2330 train_time:3790ms step_avg:39.07ms
step:98/2330 train_time:3846ms step_avg:39.25ms
step:99/2330 train_time:3868ms step_avg:39.07ms
step:100/2330 train_time:3924ms step_avg:39.24ms
step:101/2330 train_time:3947ms step_avg:39.08ms
step:102/2330 train_time:4003ms step_avg:39.25ms
step:103/2330 train_time:4026ms step_avg:39.09ms
step:104/2330 train_time:4083ms step_avg:39.26ms
step:105/2330 train_time:4106ms step_avg:39.11ms
step:106/2330 train_time:4163ms step_avg:39.27ms
step:107/2330 train_time:4187ms step_avg:39.13ms
step:108/2330 train_time:4244ms step_avg:39.29ms
step:109/2330 train_time:4267ms step_avg:39.15ms
step:110/2330 train_time:4324ms step_avg:39.31ms
step:111/2330 train_time:4347ms step_avg:39.17ms
step:112/2330 train_time:4404ms step_avg:39.32ms
step:113/2330 train_time:4427ms step_avg:39.18ms
step:114/2330 train_time:4483ms step_avg:39.33ms
step:115/2330 train_time:4506ms step_avg:39.18ms
step:116/2330 train_time:4562ms step_avg:39.33ms
step:117/2330 train_time:4585ms step_avg:39.19ms
step:118/2330 train_time:4641ms step_avg:39.33ms
step:119/2330 train_time:4663ms step_avg:39.19ms
step:120/2330 train_time:4719ms step_avg:39.33ms
step:121/2330 train_time:4742ms step_avg:39.19ms
step:122/2330 train_time:4798ms step_avg:39.33ms
step:123/2330 train_time:4820ms step_avg:39.19ms
step:124/2330 train_time:4876ms step_avg:39.32ms
step:125/2330 train_time:4898ms step_avg:39.18ms
step:126/2330 train_time:4955ms step_avg:39.32ms
step:127/2330 train_time:4977ms step_avg:39.19ms
step:128/2330 train_time:5034ms step_avg:39.32ms
step:129/2330 train_time:5056ms step_avg:39.19ms
step:130/2330 train_time:5113ms step_avg:39.33ms
step:131/2330 train_time:5135ms step_avg:39.20ms
step:132/2330 train_time:5192ms step_avg:39.33ms
step:133/2330 train_time:5214ms step_avg:39.21ms
step:134/2330 train_time:5272ms step_avg:39.34ms
step:135/2330 train_time:5293ms step_avg:39.21ms
step:136/2330 train_time:5351ms step_avg:39.34ms
step:137/2330 train_time:5373ms step_avg:39.22ms
step:138/2330 train_time:5430ms step_avg:39.35ms
step:139/2330 train_time:5452ms step_avg:39.22ms
step:140/2330 train_time:5509ms step_avg:39.35ms
step:141/2330 train_time:5531ms step_avg:39.23ms
step:142/2330 train_time:5588ms step_avg:39.35ms
step:143/2330 train_time:5609ms step_avg:39.23ms
step:144/2330 train_time:5666ms step_avg:39.35ms
step:145/2330 train_time:5688ms step_avg:39.23ms
step:146/2330 train_time:5745ms step_avg:39.35ms
step:147/2330 train_time:5767ms step_avg:39.23ms
step:148/2330 train_time:5823ms step_avg:39.34ms
step:149/2330 train_time:5846ms step_avg:39.24ms
step:150/2330 train_time:5903ms step_avg:39.35ms
step:151/2330 train_time:5926ms step_avg:39.25ms
step:152/2330 train_time:5982ms step_avg:39.36ms
step:153/2330 train_time:6006ms step_avg:39.25ms
step:154/2330 train_time:6061ms step_avg:39.36ms
step:155/2330 train_time:6084ms step_avg:39.25ms
step:156/2330 train_time:6141ms step_avg:39.37ms
step:157/2330 train_time:6164ms step_avg:39.26ms
step:158/2330 train_time:6221ms step_avg:39.37ms
step:159/2330 train_time:6244ms step_avg:39.27ms
step:160/2330 train_time:6300ms step_avg:39.37ms
step:161/2330 train_time:6322ms step_avg:39.27ms
step:162/2330 train_time:6379ms step_avg:39.37ms
step:163/2330 train_time:6402ms step_avg:39.27ms
step:164/2330 train_time:6458ms step_avg:39.38ms
step:165/2330 train_time:6482ms step_avg:39.29ms
step:166/2330 train_time:6538ms step_avg:39.38ms
step:167/2330 train_time:6560ms step_avg:39.28ms
step:168/2330 train_time:6616ms step_avg:39.38ms
step:169/2330 train_time:6639ms step_avg:39.28ms
step:170/2330 train_time:6694ms step_avg:39.38ms
step:171/2330 train_time:6716ms step_avg:39.28ms
step:172/2330 train_time:6773ms step_avg:39.38ms
step:173/2330 train_time:6796ms step_avg:39.28ms
step:174/2330 train_time:6853ms step_avg:39.38ms
step:175/2330 train_time:6875ms step_avg:39.28ms
step:176/2330 train_time:6931ms step_avg:39.38ms
step:177/2330 train_time:6953ms step_avg:39.28ms
step:178/2330 train_time:7010ms step_avg:39.38ms
step:179/2330 train_time:7032ms step_avg:39.29ms
step:180/2330 train_time:7089ms step_avg:39.38ms
step:181/2330 train_time:7111ms step_avg:39.29ms
step:182/2330 train_time:7168ms step_avg:39.38ms
step:183/2330 train_time:7190ms step_avg:39.29ms
step:184/2330 train_time:7247ms step_avg:39.39ms
step:185/2330 train_time:7269ms step_avg:39.29ms
step:186/2330 train_time:7325ms step_avg:39.38ms
step:187/2330 train_time:7348ms step_avg:39.29ms
step:188/2330 train_time:7404ms step_avg:39.38ms
step:189/2330 train_time:7427ms step_avg:39.30ms
step:190/2330 train_time:7484ms step_avg:39.39ms
step:191/2330 train_time:7507ms step_avg:39.31ms
step:192/2330 train_time:7564ms step_avg:39.39ms
step:193/2330 train_time:7587ms step_avg:39.31ms
step:194/2330 train_time:7643ms step_avg:39.40ms
step:195/2330 train_time:7666ms step_avg:39.31ms
step:196/2330 train_time:7722ms step_avg:39.40ms
step:197/2330 train_time:7745ms step_avg:39.32ms
step:198/2330 train_time:7802ms step_avg:39.40ms
step:199/2330 train_time:7824ms step_avg:39.32ms
step:200/2330 train_time:7881ms step_avg:39.40ms
step:201/2330 train_time:7904ms step_avg:39.32ms
step:202/2330 train_time:7960ms step_avg:39.40ms
step:203/2330 train_time:7982ms step_avg:39.32ms
step:204/2330 train_time:8038ms step_avg:39.40ms
step:205/2330 train_time:8061ms step_avg:39.32ms
step:206/2330 train_time:8118ms step_avg:39.41ms
step:207/2330 train_time:8140ms step_avg:39.33ms
step:208/2330 train_time:8196ms step_avg:39.41ms
step:209/2330 train_time:8219ms step_avg:39.33ms
step:210/2330 train_time:8275ms step_avg:39.40ms
step:211/2330 train_time:8298ms step_avg:39.33ms
step:212/2330 train_time:8354ms step_avg:39.41ms
step:213/2330 train_time:8377ms step_avg:39.33ms
step:214/2330 train_time:8434ms step_avg:39.41ms
step:215/2330 train_time:8458ms step_avg:39.34ms
step:216/2330 train_time:8514ms step_avg:39.42ms
step:217/2330 train_time:8537ms step_avg:39.34ms
step:218/2330 train_time:8594ms step_avg:39.42ms
step:219/2330 train_time:8616ms step_avg:39.34ms
step:220/2330 train_time:8673ms step_avg:39.42ms
step:221/2330 train_time:8695ms step_avg:39.35ms
step:222/2330 train_time:8753ms step_avg:39.43ms
step:223/2330 train_time:8775ms step_avg:39.35ms
step:224/2330 train_time:8831ms step_avg:39.42ms
step:225/2330 train_time:8853ms step_avg:39.35ms
step:226/2330 train_time:8909ms step_avg:39.42ms
step:227/2330 train_time:8932ms step_avg:39.35ms
step:228/2330 train_time:8989ms step_avg:39.42ms
step:229/2330 train_time:9011ms step_avg:39.35ms
step:230/2330 train_time:9067ms step_avg:39.42ms
step:231/2330 train_time:9089ms step_avg:39.35ms
step:232/2330 train_time:9146ms step_avg:39.42ms
step:233/2330 train_time:9168ms step_avg:39.35ms
step:234/2330 train_time:9224ms step_avg:39.42ms
step:235/2330 train_time:9247ms step_avg:39.35ms
step:236/2330 train_time:9303ms step_avg:39.42ms
step:237/2330 train_time:9326ms step_avg:39.35ms
step:238/2330 train_time:9383ms step_avg:39.42ms
step:239/2330 train_time:9406ms step_avg:39.36ms
step:240/2330 train_time:9463ms step_avg:39.43ms
step:241/2330 train_time:9487ms step_avg:39.36ms
step:242/2330 train_time:9544ms step_avg:39.44ms
step:243/2330 train_time:9567ms step_avg:39.37ms
step:244/2330 train_time:9624ms step_avg:39.44ms
step:245/2330 train_time:9647ms step_avg:39.37ms
step:246/2330 train_time:9703ms step_avg:39.44ms
step:247/2330 train_time:9726ms step_avg:39.38ms
step:248/2330 train_time:9782ms step_avg:39.44ms
step:249/2330 train_time:9805ms step_avg:39.38ms
step:250/2330 train_time:9861ms step_avg:39.44ms
step:250/2330 val_loss:5.5248 train_time:9957ms step_avg:39.83ms
step:251/2330 train_time:9971ms step_avg:39.73ms
step:252/2330 train_time:9984ms step_avg:39.62ms
step:253/2330 train_time:9995ms step_avg:39.50ms
step:254/2330 train_time:10020ms step_avg:39.45ms
step:255/2330 train_time:10041ms step_avg:39.38ms
step:256/2330 train_time:10096ms step_avg:39.44ms
step:257/2330 train_time:10118ms step_avg:39.37ms
step:258/2330 train_time:10174ms step_avg:39.43ms
step:259/2330 train_time:10196ms step_avg:39.37ms
step:260/2330 train_time:10252ms step_avg:39.43ms
step:261/2330 train_time:10278ms step_avg:39.38ms
step:262/2330 train_time:10338ms step_avg:39.46ms
step:263/2330 train_time:10363ms step_avg:39.40ms
step:264/2330 train_time:10421ms step_avg:39.47ms
step:265/2330 train_time:10444ms step_avg:39.41ms
step:266/2330 train_time:10500ms step_avg:39.47ms
step:267/2330 train_time:10523ms step_avg:39.41ms
step:268/2330 train_time:10578ms step_avg:39.47ms
step:269/2330 train_time:10601ms step_avg:39.41ms
step:270/2330 train_time:10657ms step_avg:39.47ms
step:271/2330 train_time:10680ms step_avg:39.41ms
step:272/2330 train_time:10736ms step_avg:39.47ms
step:273/2330 train_time:10758ms step_avg:39.41ms
step:274/2330 train_time:10813ms step_avg:39.46ms
step:275/2330 train_time:10835ms step_avg:39.40ms
step:276/2330 train_time:10891ms step_avg:39.46ms
step:277/2330 train_time:10916ms step_avg:39.41ms
step:278/2330 train_time:10973ms step_avg:39.47ms
step:279/2330 train_time:10994ms step_avg:39.41ms
step:280/2330 train_time:11050ms step_avg:39.46ms
step:281/2330 train_time:11072ms step_avg:39.40ms
step:282/2330 train_time:11128ms step_avg:39.46ms
step:283/2330 train_time:11150ms step_avg:39.40ms
step:284/2330 train_time:11207ms step_avg:39.46ms
step:285/2330 train_time:11231ms step_avg:39.41ms
step:286/2330 train_time:11290ms step_avg:39.48ms
step:287/2330 train_time:11313ms step_avg:39.42ms
step:288/2330 train_time:11371ms step_avg:39.48ms
step:289/2330 train_time:11395ms step_avg:39.43ms
step:290/2330 train_time:11453ms step_avg:39.49ms
step:291/2330 train_time:11476ms step_avg:39.43ms
step:292/2330 train_time:11532ms step_avg:39.49ms
step:293/2330 train_time:11555ms step_avg:39.44ms
step:294/2330 train_time:11611ms step_avg:39.49ms
step:295/2330 train_time:11634ms step_avg:39.44ms
step:296/2330 train_time:11690ms step_avg:39.49ms
step:297/2330 train_time:11712ms step_avg:39.43ms
step:298/2330 train_time:11768ms step_avg:39.49ms
step:299/2330 train_time:11790ms step_avg:39.43ms
step:300/2330 train_time:11847ms step_avg:39.49ms
step:301/2330 train_time:11869ms step_avg:39.43ms
step:302/2330 train_time:11925ms step_avg:39.49ms
step:303/2330 train_time:11947ms step_avg:39.43ms
step:304/2330 train_time:12004ms step_avg:39.49ms
step:305/2330 train_time:12025ms step_avg:39.43ms
step:306/2330 train_time:12081ms step_avg:39.48ms
step:307/2330 train_time:12104ms step_avg:39.43ms
step:308/2330 train_time:12159ms step_avg:39.48ms
step:309/2330 train_time:12183ms step_avg:39.43ms
step:310/2330 train_time:12239ms step_avg:39.48ms
step:311/2330 train_time:12263ms step_avg:39.43ms
step:312/2330 train_time:12320ms step_avg:39.49ms
step:313/2330 train_time:12343ms step_avg:39.43ms
step:314/2330 train_time:12399ms step_avg:39.49ms
step:315/2330 train_time:12423ms step_avg:39.44ms
step:316/2330 train_time:12480ms step_avg:39.49ms
step:317/2330 train_time:12503ms step_avg:39.44ms
step:318/2330 train_time:12560ms step_avg:39.50ms
step:319/2330 train_time:12583ms step_avg:39.45ms
step:320/2330 train_time:12639ms step_avg:39.50ms
step:321/2330 train_time:12662ms step_avg:39.44ms
step:322/2330 train_time:12717ms step_avg:39.50ms
step:323/2330 train_time:12740ms step_avg:39.44ms
step:324/2330 train_time:12796ms step_avg:39.49ms
step:325/2330 train_time:12819ms step_avg:39.44ms
step:326/2330 train_time:12875ms step_avg:39.49ms
step:327/2330 train_time:12898ms step_avg:39.44ms
step:328/2330 train_time:12953ms step_avg:39.49ms
step:329/2330 train_time:12976ms step_avg:39.44ms
step:330/2330 train_time:13032ms step_avg:39.49ms
step:331/2330 train_time:13054ms step_avg:39.44ms
step:332/2330 train_time:13110ms step_avg:39.49ms
step:333/2330 train_time:13133ms step_avg:39.44ms
step:334/2330 train_time:13191ms step_avg:39.49ms
step:335/2330 train_time:13215ms step_avg:39.45ms
step:336/2330 train_time:13272ms step_avg:39.50ms
step:337/2330 train_time:13294ms step_avg:39.45ms
step:338/2330 train_time:13351ms step_avg:39.50ms
step:339/2330 train_time:13374ms step_avg:39.45ms
step:340/2330 train_time:13431ms step_avg:39.50ms
step:341/2330 train_time:13454ms step_avg:39.45ms
step:342/2330 train_time:13511ms step_avg:39.51ms
step:343/2330 train_time:13533ms step_avg:39.46ms
step:344/2330 train_time:13590ms step_avg:39.51ms
step:345/2330 train_time:13613ms step_avg:39.46ms
step:346/2330 train_time:13669ms step_avg:39.51ms
step:347/2330 train_time:13692ms step_avg:39.46ms
step:348/2330 train_time:13748ms step_avg:39.51ms
step:349/2330 train_time:13770ms step_avg:39.46ms
step:350/2330 train_time:13827ms step_avg:39.51ms
step:351/2330 train_time:13849ms step_avg:39.45ms
step:352/2330 train_time:13905ms step_avg:39.50ms
step:353/2330 train_time:13927ms step_avg:39.45ms
step:354/2330 train_time:13983ms step_avg:39.50ms
step:355/2330 train_time:14005ms step_avg:39.45ms
step:356/2330 train_time:14060ms step_avg:39.50ms
step:357/2330 train_time:14083ms step_avg:39.45ms
step:358/2330 train_time:14139ms step_avg:39.50ms
step:359/2330 train_time:14163ms step_avg:39.45ms
step:360/2330 train_time:14219ms step_avg:39.50ms
step:361/2330 train_time:14243ms step_avg:39.46ms
step:362/2330 train_time:14301ms step_avg:39.50ms
step:363/2330 train_time:14324ms step_avg:39.46ms
step:364/2330 train_time:14381ms step_avg:39.51ms
step:365/2330 train_time:14404ms step_avg:39.46ms
step:366/2330 train_time:14461ms step_avg:39.51ms
step:367/2330 train_time:14484ms step_avg:39.47ms
step:368/2330 train_time:14540ms step_avg:39.51ms
step:369/2330 train_time:14563ms step_avg:39.47ms
step:370/2330 train_time:14619ms step_avg:39.51ms
step:371/2330 train_time:14642ms step_avg:39.47ms
step:372/2330 train_time:14698ms step_avg:39.51ms
step:373/2330 train_time:14721ms step_avg:39.47ms
step:374/2330 train_time:14778ms step_avg:39.51ms
step:375/2330 train_time:14801ms step_avg:39.47ms
step:376/2330 train_time:14856ms step_avg:39.51ms
step:377/2330 train_time:14879ms step_avg:39.47ms
step:378/2330 train_time:14935ms step_avg:39.51ms
step:379/2330 train_time:14957ms step_avg:39.47ms
step:380/2330 train_time:15014ms step_avg:39.51ms
step:381/2330 train_time:15036ms step_avg:39.46ms
step:382/2330 train_time:15092ms step_avg:39.51ms
step:383/2330 train_time:15114ms step_avg:39.46ms
step:384/2330 train_time:15171ms step_avg:39.51ms
step:385/2330 train_time:15194ms step_avg:39.46ms
step:386/2330 train_time:15251ms step_avg:39.51ms
step:387/2330 train_time:15273ms step_avg:39.47ms
step:388/2330 train_time:15331ms step_avg:39.51ms
step:389/2330 train_time:15354ms step_avg:39.47ms
step:390/2330 train_time:15410ms step_avg:39.51ms
step:391/2330 train_time:15433ms step_avg:39.47ms
step:392/2330 train_time:15489ms step_avg:39.51ms
step:393/2330 train_time:15511ms step_avg:39.47ms
step:394/2330 train_time:15569ms step_avg:39.51ms
step:395/2330 train_time:15590ms step_avg:39.47ms
step:396/2330 train_time:15647ms step_avg:39.51ms
step:397/2330 train_time:15669ms step_avg:39.47ms
step:398/2330 train_time:15726ms step_avg:39.51ms
step:399/2330 train_time:15748ms step_avg:39.47ms
step:400/2330 train_time:15805ms step_avg:39.51ms
step:401/2330 train_time:15827ms step_avg:39.47ms
step:402/2330 train_time:15883ms step_avg:39.51ms
step:403/2330 train_time:15906ms step_avg:39.47ms
step:404/2330 train_time:15963ms step_avg:39.51ms
step:405/2330 train_time:15985ms step_avg:39.47ms
step:406/2330 train_time:16041ms step_avg:39.51ms
step:407/2330 train_time:16063ms step_avg:39.47ms
step:408/2330 train_time:16119ms step_avg:39.51ms
step:409/2330 train_time:16143ms step_avg:39.47ms
step:410/2330 train_time:16199ms step_avg:39.51ms
step:411/2330 train_time:16223ms step_avg:39.47ms
step:412/2330 train_time:16280ms step_avg:39.51ms
step:413/2330 train_time:16304ms step_avg:39.48ms
step:414/2330 train_time:16361ms step_avg:39.52ms
step:415/2330 train_time:16384ms step_avg:39.48ms
step:416/2330 train_time:16441ms step_avg:39.52ms
step:417/2330 train_time:16464ms step_avg:39.48ms
step:418/2330 train_time:16521ms step_avg:39.52ms
step:419/2330 train_time:16544ms step_avg:39.48ms
step:420/2330 train_time:16600ms step_avg:39.52ms
step:421/2330 train_time:16623ms step_avg:39.49ms
step:422/2330 train_time:16680ms step_avg:39.53ms
step:423/2330 train_time:16703ms step_avg:39.49ms
step:424/2330 train_time:16759ms step_avg:39.53ms
step:425/2330 train_time:16782ms step_avg:39.49ms
step:426/2330 train_time:16838ms step_avg:39.53ms
step:427/2330 train_time:16861ms step_avg:39.49ms
step:428/2330 train_time:16917ms step_avg:39.53ms
step:429/2330 train_time:16940ms step_avg:39.49ms
step:430/2330 train_time:16997ms step_avg:39.53ms
step:431/2330 train_time:17019ms step_avg:39.49ms
step:432/2330 train_time:17076ms step_avg:39.53ms
step:433/2330 train_time:17098ms step_avg:39.49ms
step:434/2330 train_time:17155ms step_avg:39.53ms
step:435/2330 train_time:17178ms step_avg:39.49ms
step:436/2330 train_time:17234ms step_avg:39.53ms
step:437/2330 train_time:17257ms step_avg:39.49ms
step:438/2330 train_time:17314ms step_avg:39.53ms
step:439/2330 train_time:17337ms step_avg:39.49ms
step:440/2330 train_time:17393ms step_avg:39.53ms
step:441/2330 train_time:17416ms step_avg:39.49ms
step:442/2330 train_time:17473ms step_avg:39.53ms
step:443/2330 train_time:17496ms step_avg:39.49ms
step:444/2330 train_time:17552ms step_avg:39.53ms
step:445/2330 train_time:17575ms step_avg:39.49ms
step:446/2330 train_time:17631ms step_avg:39.53ms
step:447/2330 train_time:17653ms step_avg:39.49ms
step:448/2330 train_time:17710ms step_avg:39.53ms
step:449/2330 train_time:17733ms step_avg:39.49ms
step:450/2330 train_time:17790ms step_avg:39.53ms
step:451/2330 train_time:17813ms step_avg:39.50ms
step:452/2330 train_time:17870ms step_avg:39.54ms
step:453/2330 train_time:17892ms step_avg:39.50ms
step:454/2330 train_time:17949ms step_avg:39.53ms
step:455/2330 train_time:17971ms step_avg:39.50ms
step:456/2330 train_time:18029ms step_avg:39.54ms
step:457/2330 train_time:18051ms step_avg:39.50ms
step:458/2330 train_time:18109ms step_avg:39.54ms
step:459/2330 train_time:18131ms step_avg:39.50ms
step:460/2330 train_time:18189ms step_avg:39.54ms
step:461/2330 train_time:18211ms step_avg:39.50ms
step:462/2330 train_time:18267ms step_avg:39.54ms
step:463/2330 train_time:18289ms step_avg:39.50ms
step:464/2330 train_time:18346ms step_avg:39.54ms
step:465/2330 train_time:18369ms step_avg:39.50ms
step:466/2330 train_time:18426ms step_avg:39.54ms
step:467/2330 train_time:18448ms step_avg:39.50ms
step:468/2330 train_time:18505ms step_avg:39.54ms
step:469/2330 train_time:18527ms step_avg:39.50ms
step:470/2330 train_time:18583ms step_avg:39.54ms
step:471/2330 train_time:18606ms step_avg:39.50ms
step:472/2330 train_time:18663ms step_avg:39.54ms
step:473/2330 train_time:18685ms step_avg:39.50ms
step:474/2330 train_time:18742ms step_avg:39.54ms
step:475/2330 train_time:18765ms step_avg:39.50ms
step:476/2330 train_time:18821ms step_avg:39.54ms
step:477/2330 train_time:18844ms step_avg:39.51ms
step:478/2330 train_time:18900ms step_avg:39.54ms
step:479/2330 train_time:18923ms step_avg:39.51ms
step:480/2330 train_time:18979ms step_avg:39.54ms
step:481/2330 train_time:19002ms step_avg:39.51ms
step:482/2330 train_time:19059ms step_avg:39.54ms
step:483/2330 train_time:19082ms step_avg:39.51ms
step:484/2330 train_time:19138ms step_avg:39.54ms
step:485/2330 train_time:19161ms step_avg:39.51ms
step:486/2330 train_time:19217ms step_avg:39.54ms
step:487/2330 train_time:19240ms step_avg:39.51ms
step:488/2330 train_time:19297ms step_avg:39.54ms
step:489/2330 train_time:19319ms step_avg:39.51ms
step:490/2330 train_time:19376ms step_avg:39.54ms
step:491/2330 train_time:19399ms step_avg:39.51ms
step:492/2330 train_time:19455ms step_avg:39.54ms
step:493/2330 train_time:19478ms step_avg:39.51ms
step:494/2330 train_time:19535ms step_avg:39.54ms
step:495/2330 train_time:19557ms step_avg:39.51ms
step:496/2330 train_time:19614ms step_avg:39.54ms
step:497/2330 train_time:19637ms step_avg:39.51ms
step:498/2330 train_time:19694ms step_avg:39.55ms
step:499/2330 train_time:19717ms step_avg:39.51ms
step:500/2330 train_time:19773ms step_avg:39.55ms
step:500/2330 val_loss:5.3723 train_time:19870ms step_avg:39.74ms
step:501/2330 train_time:19883ms step_avg:39.69ms
step:502/2330 train_time:19895ms step_avg:39.63ms
step:503/2330 train_time:19905ms step_avg:39.57ms
step:504/2330 train_time:19936ms step_avg:39.56ms
step:505/2330 train_time:19958ms step_avg:39.52ms
step:506/2330 train_time:20013ms step_avg:39.55ms
step:507/2330 train_time:20035ms step_avg:39.52ms
step:508/2330 train_time:20090ms step_avg:39.55ms
step:509/2330 train_time:20112ms step_avg:39.51ms
step:510/2330 train_time:20169ms step_avg:39.55ms
step:511/2330 train_time:20195ms step_avg:39.52ms
step:512/2330 train_time:20256ms step_avg:39.56ms
step:513/2330 train_time:20280ms step_avg:39.53ms
step:514/2330 train_time:20338ms step_avg:39.57ms
step:515/2330 train_time:20362ms step_avg:39.54ms
step:516/2330 train_time:20418ms step_avg:39.57ms
step:517/2330 train_time:20440ms step_avg:39.54ms
step:518/2330 train_time:20496ms step_avg:39.57ms
step:519/2330 train_time:20519ms step_avg:39.53ms
step:520/2330 train_time:20575ms step_avg:39.57ms
step:521/2330 train_time:20598ms step_avg:39.53ms
step:522/2330 train_time:20653ms step_avg:39.57ms
step:523/2330 train_time:20675ms step_avg:39.53ms
step:524/2330 train_time:20731ms step_avg:39.56ms
step:525/2330 train_time:20754ms step_avg:39.53ms
step:526/2330 train_time:20809ms step_avg:39.56ms
step:527/2330 train_time:20832ms step_avg:39.53ms
step:528/2330 train_time:20888ms step_avg:39.56ms
step:529/2330 train_time:20911ms step_avg:39.53ms
step:530/2330 train_time:20967ms step_avg:39.56ms
step:531/2330 train_time:20989ms step_avg:39.53ms
step:532/2330 train_time:21045ms step_avg:39.56ms
step:533/2330 train_time:21067ms step_avg:39.53ms
step:534/2330 train_time:21124ms step_avg:39.56ms
step:535/2330 train_time:21148ms step_avg:39.53ms
step:536/2330 train_time:21205ms step_avg:39.56ms
step:537/2330 train_time:21230ms step_avg:39.54ms
step:538/2330 train_time:21288ms step_avg:39.57ms
step:539/2330 train_time:21312ms step_avg:39.54ms
step:540/2330 train_time:21369ms step_avg:39.57ms
step:541/2330 train_time:21393ms step_avg:39.54ms
step:542/2330 train_time:21449ms step_avg:39.57ms
step:543/2330 train_time:21472ms step_avg:39.54ms
step:544/2330 train_time:21528ms step_avg:39.57ms
step:545/2330 train_time:21551ms step_avg:39.54ms
step:546/2330 train_time:21607ms step_avg:39.57ms
step:547/2330 train_time:21630ms step_avg:39.54ms
step:548/2330 train_time:21686ms step_avg:39.57ms
step:549/2330 train_time:21708ms step_avg:39.54ms
step:550/2330 train_time:21764ms step_avg:39.57ms
step:551/2330 train_time:21786ms step_avg:39.54ms
step:552/2330 train_time:21842ms step_avg:39.57ms
step:553/2330 train_time:21865ms step_avg:39.54ms
step:554/2330 train_time:21921ms step_avg:39.57ms
step:555/2330 train_time:21943ms step_avg:39.54ms
step:556/2330 train_time:21999ms step_avg:39.57ms
step:557/2330 train_time:22022ms step_avg:39.54ms
step:558/2330 train_time:22078ms step_avg:39.57ms
step:559/2330 train_time:22101ms step_avg:39.54ms
step:560/2330 train_time:22159ms step_avg:39.57ms
step:561/2330 train_time:22183ms step_avg:39.54ms
step:562/2330 train_time:22240ms step_avg:39.57ms
step:563/2330 train_time:22264ms step_avg:39.54ms
step:564/2330 train_time:22321ms step_avg:39.58ms
step:565/2330 train_time:22344ms step_avg:39.55ms
step:566/2330 train_time:22402ms step_avg:39.58ms
step:567/2330 train_time:22425ms step_avg:39.55ms
step:568/2330 train_time:22481ms step_avg:39.58ms
step:569/2330 train_time:22504ms step_avg:39.55ms
step:570/2330 train_time:22560ms step_avg:39.58ms
step:571/2330 train_time:22583ms step_avg:39.55ms
step:572/2330 train_time:22639ms step_avg:39.58ms
step:573/2330 train_time:22661ms step_avg:39.55ms
step:574/2330 train_time:22718ms step_avg:39.58ms
step:575/2330 train_time:22740ms step_avg:39.55ms
step:576/2330 train_time:22796ms step_avg:39.58ms
step:577/2330 train_time:22819ms step_avg:39.55ms
step:578/2330 train_time:22875ms step_avg:39.58ms
step:579/2330 train_time:22897ms step_avg:39.55ms
step:580/2330 train_time:22953ms step_avg:39.57ms
step:581/2330 train_time:22975ms step_avg:39.54ms
step:582/2330 train_time:23032ms step_avg:39.57ms
step:583/2330 train_time:23055ms step_avg:39.55ms
step:584/2330 train_time:23112ms step_avg:39.58ms
step:585/2330 train_time:23134ms step_avg:39.55ms
step:586/2330 train_time:23191ms step_avg:39.58ms
step:587/2330 train_time:23215ms step_avg:39.55ms
step:588/2330 train_time:23271ms step_avg:39.58ms
step:589/2330 train_time:23294ms step_avg:39.55ms
step:590/2330 train_time:23352ms step_avg:39.58ms
step:591/2330 train_time:23375ms step_avg:39.55ms
step:592/2330 train_time:23432ms step_avg:39.58ms
step:593/2330 train_time:23455ms step_avg:39.55ms
step:594/2330 train_time:23513ms step_avg:39.58ms
step:595/2330 train_time:23535ms step_avg:39.55ms
step:596/2330 train_time:23591ms step_avg:39.58ms
step:597/2330 train_time:23614ms step_avg:39.55ms
step:598/2330 train_time:23670ms step_avg:39.58ms
step:599/2330 train_time:23693ms step_avg:39.55ms
step:600/2330 train_time:23749ms step_avg:39.58ms
step:601/2330 train_time:23772ms step_avg:39.55ms
step:602/2330 train_time:23827ms step_avg:39.58ms
step:603/2330 train_time:23850ms step_avg:39.55ms
step:604/2330 train_time:23906ms step_avg:39.58ms
step:605/2330 train_time:23929ms step_avg:39.55ms
step:606/2330 train_time:23985ms step_avg:39.58ms
step:607/2330 train_time:24008ms step_avg:39.55ms
step:608/2330 train_time:24064ms step_avg:39.58ms
step:609/2330 train_time:24087ms step_avg:39.55ms
step:610/2330 train_time:24144ms step_avg:39.58ms
step:611/2330 train_time:24166ms step_avg:39.55ms
step:612/2330 train_time:24223ms step_avg:39.58ms
step:613/2330 train_time:24245ms step_avg:39.55ms
step:614/2330 train_time:24302ms step_avg:39.58ms
step:615/2330 train_time:24325ms step_avg:39.55ms
step:616/2330 train_time:24383ms step_avg:39.58ms
step:617/2330 train_time:24406ms step_avg:39.56ms
step:618/2330 train_time:24463ms step_avg:39.58ms
step:619/2330 train_time:24486ms step_avg:39.56ms
step:620/2330 train_time:24543ms step_avg:39.59ms
step:621/2330 train_time:24566ms step_avg:39.56ms
step:622/2330 train_time:24622ms step_avg:39.59ms
step:623/2330 train_time:24645ms step_avg:39.56ms
step:624/2330 train_time:24701ms step_avg:39.59ms
step:625/2330 train_time:24724ms step_avg:39.56ms
step:626/2330 train_time:24781ms step_avg:39.59ms
step:627/2330 train_time:24803ms step_avg:39.56ms
step:628/2330 train_time:24860ms step_avg:39.59ms
step:629/2330 train_time:24882ms step_avg:39.56ms
step:630/2330 train_time:24939ms step_avg:39.59ms
step:631/2330 train_time:24961ms step_avg:39.56ms
step:632/2330 train_time:25018ms step_avg:39.59ms
step:633/2330 train_time:25041ms step_avg:39.56ms
step:634/2330 train_time:25098ms step_avg:39.59ms
step:635/2330 train_time:25120ms step_avg:39.56ms
step:636/2330 train_time:25177ms step_avg:39.59ms
step:637/2330 train_time:25199ms step_avg:39.56ms
step:638/2330 train_time:25256ms step_avg:39.59ms
step:639/2330 train_time:25279ms step_avg:39.56ms
step:640/2330 train_time:25336ms step_avg:39.59ms
step:641/2330 train_time:25358ms step_avg:39.56ms
step:642/2330 train_time:25416ms step_avg:39.59ms
step:643/2330 train_time:25438ms step_avg:39.56ms
step:644/2330 train_time:25495ms step_avg:39.59ms
step:645/2330 train_time:25518ms step_avg:39.56ms
step:646/2330 train_time:25575ms step_avg:39.59ms
step:647/2330 train_time:25597ms step_avg:39.56ms
step:648/2330 train_time:25653ms step_avg:39.59ms
step:649/2330 train_time:25675ms step_avg:39.56ms
step:650/2330 train_time:25732ms step_avg:39.59ms
step:651/2330 train_time:25755ms step_avg:39.56ms
step:652/2330 train_time:25811ms step_avg:39.59ms
step:653/2330 train_time:25834ms step_avg:39.56ms
step:654/2330 train_time:25890ms step_avg:39.59ms
step:655/2330 train_time:25913ms step_avg:39.56ms
step:656/2330 train_time:25969ms step_avg:39.59ms
step:657/2330 train_time:25992ms step_avg:39.56ms
step:658/2330 train_time:26049ms step_avg:39.59ms
step:659/2330 train_time:26072ms step_avg:39.56ms
step:660/2330 train_time:26129ms step_avg:39.59ms
step:661/2330 train_time:26151ms step_avg:39.56ms
step:662/2330 train_time:26207ms step_avg:39.59ms
step:663/2330 train_time:26230ms step_avg:39.56ms
step:664/2330 train_time:26287ms step_avg:39.59ms
step:665/2330 train_time:26310ms step_avg:39.56ms
step:666/2330 train_time:26367ms step_avg:39.59ms
step:667/2330 train_time:26390ms step_avg:39.57ms
step:668/2330 train_time:26446ms step_avg:39.59ms
step:669/2330 train_time:26470ms step_avg:39.57ms
step:670/2330 train_time:26526ms step_avg:39.59ms
step:671/2330 train_time:26550ms step_avg:39.57ms
step:672/2330 train_time:26606ms step_avg:39.59ms
step:673/2330 train_time:26628ms step_avg:39.57ms
step:674/2330 train_time:26685ms step_avg:39.59ms
step:675/2330 train_time:26708ms step_avg:39.57ms
step:676/2330 train_time:26764ms step_avg:39.59ms
step:677/2330 train_time:26787ms step_avg:39.57ms
step:678/2330 train_time:26844ms step_avg:39.59ms
step:679/2330 train_time:26867ms step_avg:39.57ms
step:680/2330 train_time:26923ms step_avg:39.59ms
step:681/2330 train_time:26945ms step_avg:39.57ms
step:682/2330 train_time:27002ms step_avg:39.59ms
step:683/2330 train_time:27024ms step_avg:39.57ms
step:684/2330 train_time:27082ms step_avg:39.59ms
step:685/2330 train_time:27105ms step_avg:39.57ms
step:686/2330 train_time:27161ms step_avg:39.59ms
step:687/2330 train_time:27184ms step_avg:39.57ms
step:688/2330 train_time:27241ms step_avg:39.59ms
step:689/2330 train_time:27263ms step_avg:39.57ms
step:690/2330 train_time:27320ms step_avg:39.59ms
step:691/2330 train_time:27343ms step_avg:39.57ms
step:692/2330 train_time:27400ms step_avg:39.60ms
step:693/2330 train_time:27423ms step_avg:39.57ms
step:694/2330 train_time:27480ms step_avg:39.60ms
step:695/2330 train_time:27503ms step_avg:39.57ms
step:696/2330 train_time:27559ms step_avg:39.60ms
step:697/2330 train_time:27581ms step_avg:39.57ms
step:698/2330 train_time:27638ms step_avg:39.60ms
step:699/2330 train_time:27660ms step_avg:39.57ms
step:700/2330 train_time:27717ms step_avg:39.60ms
step:701/2330 train_time:27739ms step_avg:39.57ms
step:702/2330 train_time:27796ms step_avg:39.59ms
step:703/2330 train_time:27819ms step_avg:39.57ms
step:704/2330 train_time:27875ms step_avg:39.60ms
step:705/2330 train_time:27897ms step_avg:39.57ms
step:706/2330 train_time:27954ms step_avg:39.59ms
step:707/2330 train_time:27977ms step_avg:39.57ms
step:708/2330 train_time:28034ms step_avg:39.60ms
step:709/2330 train_time:28057ms step_avg:39.57ms
step:710/2330 train_time:28113ms step_avg:39.60ms
step:711/2330 train_time:28136ms step_avg:39.57ms
step:712/2330 train_time:28193ms step_avg:39.60ms
step:713/2330 train_time:28215ms step_avg:39.57ms
step:714/2330 train_time:28272ms step_avg:39.60ms
step:715/2330 train_time:28295ms step_avg:39.57ms
step:716/2330 train_time:28352ms step_avg:39.60ms
step:717/2330 train_time:28375ms step_avg:39.57ms
step:718/2330 train_time:28432ms step_avg:39.60ms
step:719/2330 train_time:28455ms step_avg:39.58ms
step:720/2330 train_time:28511ms step_avg:39.60ms
step:721/2330 train_time:28533ms step_avg:39.57ms
step:722/2330 train_time:28589ms step_avg:39.60ms
step:723/2330 train_time:28613ms step_avg:39.58ms
step:724/2330 train_time:28669ms step_avg:39.60ms
step:725/2330 train_time:28692ms step_avg:39.58ms
step:726/2330 train_time:28748ms step_avg:39.60ms
step:727/2330 train_time:28772ms step_avg:39.58ms
step:728/2330 train_time:28828ms step_avg:39.60ms
step:729/2330 train_time:28852ms step_avg:39.58ms
step:730/2330 train_time:28908ms step_avg:39.60ms
step:731/2330 train_time:28931ms step_avg:39.58ms
step:732/2330 train_time:28988ms step_avg:39.60ms
step:733/2330 train_time:29011ms step_avg:39.58ms
step:734/2330 train_time:29068ms step_avg:39.60ms
step:735/2330 train_time:29091ms step_avg:39.58ms
step:736/2330 train_time:29147ms step_avg:39.60ms
step:737/2330 train_time:29170ms step_avg:39.58ms
step:738/2330 train_time:29226ms step_avg:39.60ms
step:739/2330 train_time:29249ms step_avg:39.58ms
step:740/2330 train_time:29305ms step_avg:39.60ms
step:741/2330 train_time:29328ms step_avg:39.58ms
step:742/2330 train_time:29385ms step_avg:39.60ms
step:743/2330 train_time:29408ms step_avg:39.58ms
step:744/2330 train_time:29464ms step_avg:39.60ms
step:745/2330 train_time:29487ms step_avg:39.58ms
step:746/2330 train_time:29543ms step_avg:39.60ms
step:747/2330 train_time:29565ms step_avg:39.58ms
step:748/2330 train_time:29622ms step_avg:39.60ms
step:749/2330 train_time:29644ms step_avg:39.58ms
step:750/2330 train_time:29701ms step_avg:39.60ms
step:750/2330 val_loss:5.3013 train_time:29799ms step_avg:39.73ms
step:751/2330 train_time:29812ms step_avg:39.70ms
step:752/2330 train_time:29823ms step_avg:39.66ms
step:753/2330 train_time:29834ms step_avg:39.62ms
step:754/2330 train_time:29863ms step_avg:39.61ms
step:755/2330 train_time:29885ms step_avg:39.58ms
step:756/2330 train_time:29941ms step_avg:39.60ms
step:757/2330 train_time:29962ms step_avg:39.58ms
step:758/2330 train_time:30019ms step_avg:39.60ms
step:759/2330 train_time:30040ms step_avg:39.58ms
step:760/2330 train_time:30099ms step_avg:39.60ms
step:761/2330 train_time:30123ms step_avg:39.58ms
step:762/2330 train_time:30184ms step_avg:39.61ms
step:763/2330 train_time:30207ms step_avg:39.59ms
step:764/2330 train_time:30264ms step_avg:39.61ms
step:765/2330 train_time:30289ms step_avg:39.59ms
step:766/2330 train_time:30346ms step_avg:39.62ms
step:767/2330 train_time:30368ms step_avg:39.59ms
step:768/2330 train_time:30424ms step_avg:39.61ms
step:769/2330 train_time:30447ms step_avg:39.59ms
step:770/2330 train_time:30503ms step_avg:39.61ms
step:771/2330 train_time:30526ms step_avg:39.59ms
step:772/2330 train_time:30582ms step_avg:39.61ms
step:773/2330 train_time:30605ms step_avg:39.59ms
step:774/2330 train_time:30662ms step_avg:39.62ms
step:775/2330 train_time:30685ms step_avg:39.59ms
step:776/2330 train_time:30742ms step_avg:39.62ms
step:777/2330 train_time:30764ms step_avg:39.59ms
step:778/2330 train_time:30821ms step_avg:39.62ms
step:779/2330 train_time:30843ms step_avg:39.59ms
step:780/2330 train_time:30899ms step_avg:39.61ms
step:781/2330 train_time:30922ms step_avg:39.59ms
step:782/2330 train_time:30979ms step_avg:39.61ms
step:783/2330 train_time:31001ms step_avg:39.59ms
step:784/2330 train_time:31058ms step_avg:39.61ms
step:785/2330 train_time:31081ms step_avg:39.59ms
step:786/2330 train_time:31139ms step_avg:39.62ms
step:787/2330 train_time:31163ms step_avg:39.60ms
step:788/2330 train_time:31220ms step_avg:39.62ms
step:789/2330 train_time:31242ms step_avg:39.60ms
step:790/2330 train_time:31300ms step_avg:39.62ms
step:791/2330 train_time:31323ms step_avg:39.60ms
step:792/2330 train_time:31380ms step_avg:39.62ms
step:793/2330 train_time:31402ms step_avg:39.60ms
step:794/2330 train_time:31459ms step_avg:39.62ms
step:795/2330 train_time:31481ms step_avg:39.60ms
step:796/2330 train_time:31538ms step_avg:39.62ms
step:797/2330 train_time:31560ms step_avg:39.60ms
step:798/2330 train_time:31616ms step_avg:39.62ms
step:799/2330 train_time:31639ms step_avg:39.60ms
step:800/2330 train_time:31695ms step_avg:39.62ms
step:801/2330 train_time:31718ms step_avg:39.60ms
step:802/2330 train_time:31774ms step_avg:39.62ms
step:803/2330 train_time:31797ms step_avg:39.60ms
step:804/2330 train_time:31853ms step_avg:39.62ms
step:805/2330 train_time:31876ms step_avg:39.60ms
step:806/2330 train_time:31932ms step_avg:39.62ms
step:807/2330 train_time:31955ms step_avg:39.60ms
step:808/2330 train_time:32011ms step_avg:39.62ms
step:809/2330 train_time:32035ms step_avg:39.60ms
step:810/2330 train_time:32091ms step_avg:39.62ms
step:811/2330 train_time:32115ms step_avg:39.60ms
step:812/2330 train_time:32172ms step_avg:39.62ms
step:813/2330 train_time:32196ms step_avg:39.60ms
step:814/2330 train_time:32252ms step_avg:39.62ms
step:815/2330 train_time:32276ms step_avg:39.60ms
step:816/2330 train_time:32333ms step_avg:39.62ms
step:817/2330 train_time:32357ms step_avg:39.60ms
step:818/2330 train_time:32413ms step_avg:39.62ms
step:819/2330 train_time:32436ms step_avg:39.60ms
step:820/2330 train_time:32493ms step_avg:39.63ms
step:821/2330 train_time:32517ms step_avg:39.61ms
step:822/2330 train_time:32573ms step_avg:39.63ms
step:823/2330 train_time:32595ms step_avg:39.61ms
step:824/2330 train_time:32652ms step_avg:39.63ms
step:825/2330 train_time:32674ms step_avg:39.61ms
step:826/2330 train_time:32730ms step_avg:39.63ms
step:827/2330 train_time:32753ms step_avg:39.61ms
step:828/2330 train_time:32810ms step_avg:39.63ms
step:829/2330 train_time:32833ms step_avg:39.61ms
step:830/2330 train_time:32888ms step_avg:39.62ms
step:831/2330 train_time:32911ms step_avg:39.60ms
step:832/2330 train_time:32967ms step_avg:39.62ms
step:833/2330 train_time:32990ms step_avg:39.60ms
step:834/2330 train_time:33046ms step_avg:39.62ms
step:835/2330 train_time:33070ms step_avg:39.60ms
step:836/2330 train_time:33126ms step_avg:39.62ms
step:837/2330 train_time:33150ms step_avg:39.61ms
step:838/2330 train_time:33207ms step_avg:39.63ms
step:839/2330 train_time:33230ms step_avg:39.61ms
step:840/2330 train_time:33287ms step_avg:39.63ms
step:841/2330 train_time:33309ms step_avg:39.61ms
step:842/2330 train_time:33366ms step_avg:39.63ms
step:843/2330 train_time:33389ms step_avg:39.61ms
step:844/2330 train_time:33446ms step_avg:39.63ms
step:845/2330 train_time:33469ms step_avg:39.61ms
step:846/2330 train_time:33526ms step_avg:39.63ms
step:847/2330 train_time:33549ms step_avg:39.61ms
step:848/2330 train_time:33606ms step_avg:39.63ms
step:849/2330 train_time:33628ms step_avg:39.61ms
step:850/2330 train_time:33685ms step_avg:39.63ms
step:851/2330 train_time:33708ms step_avg:39.61ms
step:852/2330 train_time:33765ms step_avg:39.63ms
step:853/2330 train_time:33787ms step_avg:39.61ms
step:854/2330 train_time:33844ms step_avg:39.63ms
step:855/2330 train_time:33867ms step_avg:39.61ms
step:856/2330 train_time:33924ms step_avg:39.63ms
step:857/2330 train_time:33947ms step_avg:39.61ms
step:858/2330 train_time:34003ms step_avg:39.63ms
step:859/2330 train_time:34026ms step_avg:39.61ms
step:860/2330 train_time:34083ms step_avg:39.63ms
step:861/2330 train_time:34105ms step_avg:39.61ms
step:862/2330 train_time:34162ms step_avg:39.63ms
step:863/2330 train_time:34184ms step_avg:39.61ms
step:864/2330 train_time:34242ms step_avg:39.63ms
step:865/2330 train_time:34264ms step_avg:39.61ms
step:866/2330 train_time:34321ms step_avg:39.63ms
step:867/2330 train_time:34343ms step_avg:39.61ms
step:868/2330 train_time:34401ms step_avg:39.63ms
step:869/2330 train_time:34424ms step_avg:39.61ms
step:870/2330 train_time:34481ms step_avg:39.63ms
step:871/2330 train_time:34503ms step_avg:39.61ms
step:872/2330 train_time:34560ms step_avg:39.63ms
step:873/2330 train_time:34582ms step_avg:39.61ms
step:874/2330 train_time:34639ms step_avg:39.63ms
step:875/2330 train_time:34661ms step_avg:39.61ms
step:876/2330 train_time:34718ms step_avg:39.63ms
step:877/2330 train_time:34740ms step_avg:39.61ms
step:878/2330 train_time:34797ms step_avg:39.63ms
step:879/2330 train_time:34820ms step_avg:39.61ms
step:880/2330 train_time:34876ms step_avg:39.63ms
step:881/2330 train_time:34899ms step_avg:39.61ms
step:882/2330 train_time:34956ms step_avg:39.63ms
step:883/2330 train_time:34978ms step_avg:39.61ms
step:884/2330 train_time:35034ms step_avg:39.63ms
step:885/2330 train_time:35057ms step_avg:39.61ms
step:886/2330 train_time:35113ms step_avg:39.63ms
step:887/2330 train_time:35136ms step_avg:39.61ms
step:888/2330 train_time:35192ms step_avg:39.63ms
step:889/2330 train_time:35216ms step_avg:39.61ms
step:890/2330 train_time:35273ms step_avg:39.63ms
step:891/2330 train_time:35296ms step_avg:39.61ms
step:892/2330 train_time:35353ms step_avg:39.63ms
step:893/2330 train_time:35376ms step_avg:39.61ms
step:894/2330 train_time:35432ms step_avg:39.63ms
step:895/2330 train_time:35455ms step_avg:39.61ms
step:896/2330 train_time:35511ms step_avg:39.63ms
step:897/2330 train_time:35534ms step_avg:39.61ms
step:898/2330 train_time:35591ms step_avg:39.63ms
step:899/2330 train_time:35614ms step_avg:39.61ms
step:900/2330 train_time:35670ms step_avg:39.63ms
step:901/2330 train_time:35693ms step_avg:39.61ms
step:902/2330 train_time:35749ms step_avg:39.63ms
step:903/2330 train_time:35771ms step_avg:39.61ms
step:904/2330 train_time:35828ms step_avg:39.63ms
step:905/2330 train_time:35851ms step_avg:39.61ms
step:906/2330 train_time:35907ms step_avg:39.63ms
step:907/2330 train_time:35930ms step_avg:39.61ms
step:908/2330 train_time:35987ms step_avg:39.63ms
step:909/2330 train_time:36009ms step_avg:39.61ms
step:910/2330 train_time:36066ms step_avg:39.63ms
step:911/2330 train_time:36089ms step_avg:39.61ms
step:912/2330 train_time:36146ms step_avg:39.63ms
step:913/2330 train_time:36169ms step_avg:39.62ms
step:914/2330 train_time:36226ms step_avg:39.64ms
step:915/2330 train_time:36250ms step_avg:39.62ms
step:916/2330 train_time:36307ms step_avg:39.64ms
step:917/2330 train_time:36330ms step_avg:39.62ms
step:918/2330 train_time:36387ms step_avg:39.64ms
step:919/2330 train_time:36409ms step_avg:39.62ms
step:920/2330 train_time:36466ms step_avg:39.64ms
step:921/2330 train_time:36489ms step_avg:39.62ms
step:922/2330 train_time:36547ms step_avg:39.64ms
step:923/2330 train_time:36569ms step_avg:39.62ms
step:924/2330 train_time:36626ms step_avg:39.64ms
step:925/2330 train_time:36649ms step_avg:39.62ms
step:926/2330 train_time:36705ms step_avg:39.64ms
step:927/2330 train_time:36729ms step_avg:39.62ms
step:928/2330 train_time:36785ms step_avg:39.64ms
step:929/2330 train_time:36808ms step_avg:39.62ms
step:930/2330 train_time:36865ms step_avg:39.64ms
step:931/2330 train_time:36888ms step_avg:39.62ms
step:932/2330 train_time:36945ms step_avg:39.64ms
step:933/2330 train_time:36968ms step_avg:39.62ms
step:934/2330 train_time:37025ms step_avg:39.64ms
step:935/2330 train_time:37048ms step_avg:39.62ms
step:936/2330 train_time:37105ms step_avg:39.64ms
step:937/2330 train_time:37128ms step_avg:39.62ms
step:938/2330 train_time:37185ms step_avg:39.64ms
step:939/2330 train_time:37209ms step_avg:39.63ms
step:940/2330 train_time:37265ms step_avg:39.64ms
step:941/2330 train_time:37288ms step_avg:39.63ms
step:942/2330 train_time:37346ms step_avg:39.64ms
step:943/2330 train_time:37368ms step_avg:39.63ms
step:944/2330 train_time:37425ms step_avg:39.64ms
step:945/2330 train_time:37448ms step_avg:39.63ms
step:946/2330 train_time:37504ms step_avg:39.65ms
step:947/2330 train_time:37527ms step_avg:39.63ms
step:948/2330 train_time:37584ms step_avg:39.65ms
step:949/2330 train_time:37607ms step_avg:39.63ms
step:950/2330 train_time:37664ms step_avg:39.65ms
step:951/2330 train_time:37686ms step_avg:39.63ms
step:952/2330 train_time:37743ms step_avg:39.65ms
step:953/2330 train_time:37766ms step_avg:39.63ms
step:954/2330 train_time:37822ms step_avg:39.65ms
step:955/2330 train_time:37844ms step_avg:39.63ms
step:956/2330 train_time:37900ms step_avg:39.64ms
step:957/2330 train_time:37923ms step_avg:39.63ms
step:958/2330 train_time:37980ms step_avg:39.64ms
step:959/2330 train_time:38002ms step_avg:39.63ms
step:960/2330 train_time:38059ms step_avg:39.64ms
step:961/2330 train_time:38081ms step_avg:39.63ms
step:962/2330 train_time:38138ms step_avg:39.64ms
step:963/2330 train_time:38161ms step_avg:39.63ms
step:964/2330 train_time:38217ms step_avg:39.64ms
step:965/2330 train_time:38240ms step_avg:39.63ms
step:966/2330 train_time:38297ms step_avg:39.64ms
step:967/2330 train_time:38319ms step_avg:39.63ms
step:968/2330 train_time:38376ms step_avg:39.65ms
step:969/2330 train_time:38399ms step_avg:39.63ms
step:970/2330 train_time:38456ms step_avg:39.65ms
step:971/2330 train_time:38479ms step_avg:39.63ms
step:972/2330 train_time:38536ms step_avg:39.65ms
step:973/2330 train_time:38558ms step_avg:39.63ms
step:974/2330 train_time:38615ms step_avg:39.65ms
step:975/2330 train_time:38638ms step_avg:39.63ms
step:976/2330 train_time:38695ms step_avg:39.65ms
step:977/2330 train_time:38718ms step_avg:39.63ms
step:978/2330 train_time:38774ms step_avg:39.65ms
step:979/2330 train_time:38797ms step_avg:39.63ms
step:980/2330 train_time:38853ms step_avg:39.65ms
step:981/2330 train_time:38876ms step_avg:39.63ms
step:982/2330 train_time:38932ms step_avg:39.65ms
step:983/2330 train_time:38955ms step_avg:39.63ms
step:984/2330 train_time:39011ms step_avg:39.65ms
step:985/2330 train_time:39034ms step_avg:39.63ms
step:986/2330 train_time:39090ms step_avg:39.65ms
step:987/2330 train_time:39114ms step_avg:39.63ms
step:988/2330 train_time:39170ms step_avg:39.65ms
step:989/2330 train_time:39193ms step_avg:39.63ms
step:990/2330 train_time:39250ms step_avg:39.65ms
step:991/2330 train_time:39273ms step_avg:39.63ms
step:992/2330 train_time:39329ms step_avg:39.65ms
step:993/2330 train_time:39352ms step_avg:39.63ms
step:994/2330 train_time:39409ms step_avg:39.65ms
step:995/2330 train_time:39431ms step_avg:39.63ms
step:996/2330 train_time:39488ms step_avg:39.65ms
step:997/2330 train_time:39511ms step_avg:39.63ms
step:998/2330 train_time:39568ms step_avg:39.65ms
step:999/2330 train_time:39591ms step_avg:39.63ms
step:1000/2330 train_time:39647ms step_avg:39.65ms
step:1000/2330 val_loss:5.2506 train_time:39744ms step_avg:39.74ms
step:1001/2330 train_time:39757ms step_avg:39.72ms
step:1002/2330 train_time:39769ms step_avg:39.69ms
step:1003/2330 train_time:39780ms step_avg:39.66ms
step:1004/2330 train_time:39806ms step_avg:39.65ms
step:1005/2330 train_time:39828ms step_avg:39.63ms
step:1006/2330 train_time:39883ms step_avg:39.64ms
step:1007/2330 train_time:39904ms step_avg:39.63ms
step:1008/2330 train_time:39960ms step_avg:39.64ms
step:1009/2330 train_time:39982ms step_avg:39.63ms
step:1010/2330 train_time:40038ms step_avg:39.64ms
step:1011/2330 train_time:40063ms step_avg:39.63ms
step:1012/2330 train_time:40126ms step_avg:39.65ms
step:1013/2330 train_time:40150ms step_avg:39.63ms
step:1014/2330 train_time:40208ms step_avg:39.65ms
step:1015/2330 train_time:40230ms step_avg:39.64ms
step:1016/2330 train_time:40288ms step_avg:39.65ms
step:1017/2330 train_time:40310ms step_avg:39.64ms
step:1018/2330 train_time:40367ms step_avg:39.65ms
step:1019/2330 train_time:40388ms step_avg:39.64ms
step:1020/2330 train_time:40445ms step_avg:39.65ms
step:1021/2330 train_time:40467ms step_avg:39.63ms
step:1022/2330 train_time:40522ms step_avg:39.65ms
step:1023/2330 train_time:40544ms step_avg:39.63ms
step:1024/2330 train_time:40601ms step_avg:39.65ms
step:1025/2330 train_time:40622ms step_avg:39.63ms
step:1026/2330 train_time:40679ms step_avg:39.65ms
step:1027/2330 train_time:40701ms step_avg:39.63ms
step:1028/2330 train_time:40757ms step_avg:39.65ms
step:1029/2330 train_time:40779ms step_avg:39.63ms
step:1030/2330 train_time:40835ms step_avg:39.65ms
step:1031/2330 train_time:40858ms step_avg:39.63ms
step:1032/2330 train_time:40914ms step_avg:39.65ms
step:1033/2330 train_time:40936ms step_avg:39.63ms
step:1034/2330 train_time:40993ms step_avg:39.65ms
step:1035/2330 train_time:41017ms step_avg:39.63ms
step:1036/2330 train_time:41075ms step_avg:39.65ms
step:1037/2330 train_time:41099ms step_avg:39.63ms
step:1038/2330 train_time:41157ms step_avg:39.65ms
step:1039/2330 train_time:41181ms step_avg:39.64ms
step:1040/2330 train_time:41239ms step_avg:39.65ms
step:1041/2330 train_time:41263ms step_avg:39.64ms
step:1042/2330 train_time:41320ms step_avg:39.65ms
step:1043/2330 train_time:41343ms step_avg:39.64ms
step:1044/2330 train_time:41400ms step_avg:39.66ms
step:1045/2330 train_time:41422ms step_avg:39.64ms
step:1046/2330 train_time:41478ms step_avg:39.65ms
step:1047/2330 train_time:41501ms step_avg:39.64ms
step:1048/2330 train_time:41557ms step_avg:39.65ms
step:1049/2330 train_time:41579ms step_avg:39.64ms
step:1050/2330 train_time:41635ms step_avg:39.65ms
step:1051/2330 train_time:41658ms step_avg:39.64ms
step:1052/2330 train_time:41714ms step_avg:39.65ms
step:1053/2330 train_time:41737ms step_avg:39.64ms
step:1054/2330 train_time:41792ms step_avg:39.65ms
step:1055/2330 train_time:41815ms step_avg:39.63ms
step:1056/2330 train_time:41871ms step_avg:39.65ms
step:1057/2330 train_time:41894ms step_avg:39.63ms
step:1058/2330 train_time:41950ms step_avg:39.65ms
step:1059/2330 train_time:41974ms step_avg:39.64ms
step:1060/2330 train_time:42032ms step_avg:39.65ms
step:1061/2330 train_time:42055ms step_avg:39.64ms
step:1062/2330 train_time:42112ms step_avg:39.65ms
step:1063/2330 train_time:42136ms step_avg:39.64ms
step:1064/2330 train_time:42192ms step_avg:39.65ms
step:1065/2330 train_time:42216ms step_avg:39.64ms
step:1066/2330 train_time:42273ms step_avg:39.66ms
step:1067/2330 train_time:42297ms step_avg:39.64ms
step:1068/2330 train_time:42353ms step_avg:39.66ms
step:1069/2330 train_time:42376ms step_avg:39.64ms
step:1070/2330 train_time:42432ms step_avg:39.66ms
step:1071/2330 train_time:42454ms step_avg:39.64ms
step:1072/2330 train_time:42511ms step_avg:39.66ms
step:1073/2330 train_time:42534ms step_avg:39.64ms
step:1074/2330 train_time:42590ms step_avg:39.66ms
step:1075/2330 train_time:42613ms step_avg:39.64ms
step:1076/2330 train_time:42671ms step_avg:39.66ms
step:1077/2330 train_time:42693ms step_avg:39.64ms
step:1078/2330 train_time:42749ms step_avg:39.66ms
step:1079/2330 train_time:42771ms step_avg:39.64ms
step:1080/2330 train_time:42828ms step_avg:39.66ms
step:1081/2330 train_time:42850ms step_avg:39.64ms
step:1082/2330 train_time:42908ms step_avg:39.66ms
step:1083/2330 train_time:42930ms step_avg:39.64ms
step:1084/2330 train_time:42988ms step_avg:39.66ms
step:1085/2330 train_time:43010ms step_avg:39.64ms
step:1086/2330 train_time:43068ms step_avg:39.66ms
step:1087/2330 train_time:43091ms step_avg:39.64ms
step:1088/2330 train_time:43149ms step_avg:39.66ms
step:1089/2330 train_time:43173ms step_avg:39.64ms
step:1090/2330 train_time:43231ms step_avg:39.66ms
step:1091/2330 train_time:43254ms step_avg:39.65ms
step:1092/2330 train_time:43311ms step_avg:39.66ms
step:1093/2330 train_time:43335ms step_avg:39.65ms
step:1094/2330 train_time:43391ms step_avg:39.66ms
step:1095/2330 train_time:43414ms step_avg:39.65ms
step:1096/2330 train_time:43471ms step_avg:39.66ms
step:1097/2330 train_time:43494ms step_avg:39.65ms
step:1098/2330 train_time:43550ms step_avg:39.66ms
step:1099/2330 train_time:43573ms step_avg:39.65ms
step:1100/2330 train_time:43629ms step_avg:39.66ms
step:1101/2330 train_time:43652ms step_avg:39.65ms
step:1102/2330 train_time:43708ms step_avg:39.66ms
step:1103/2330 train_time:43731ms step_avg:39.65ms
step:1104/2330 train_time:43787ms step_avg:39.66ms
step:1105/2330 train_time:43810ms step_avg:39.65ms
step:1106/2330 train_time:43868ms step_avg:39.66ms
step:1107/2330 train_time:43891ms step_avg:39.65ms
step:1108/2330 train_time:43949ms step_avg:39.66ms
step:1109/2330 train_time:43971ms step_avg:39.65ms
step:1110/2330 train_time:44028ms step_avg:39.66ms
step:1111/2330 train_time:44051ms step_avg:39.65ms
step:1112/2330 train_time:44110ms step_avg:39.67ms
step:1113/2330 train_time:44133ms step_avg:39.65ms
step:1114/2330 train_time:44191ms step_avg:39.67ms
step:1115/2330 train_time:44214ms step_avg:39.65ms
step:1116/2330 train_time:44270ms step_avg:39.67ms
step:1117/2330 train_time:44294ms step_avg:39.65ms
step:1118/2330 train_time:44350ms step_avg:39.67ms
step:1119/2330 train_time:44373ms step_avg:39.65ms
step:1120/2330 train_time:44430ms step_avg:39.67ms
step:1121/2330 train_time:44453ms step_avg:39.65ms
step:1122/2330 train_time:44509ms step_avg:39.67ms
step:1123/2330 train_time:44533ms step_avg:39.66ms
step:1124/2330 train_time:44589ms step_avg:39.67ms
step:1125/2330 train_time:44612ms step_avg:39.65ms
step:1126/2330 train_time:44669ms step_avg:39.67ms
step:1127/2330 train_time:44691ms step_avg:39.66ms
step:1128/2330 train_time:44748ms step_avg:39.67ms
step:1129/2330 train_time:44771ms step_avg:39.66ms
step:1130/2330 train_time:44827ms step_avg:39.67ms
step:1131/2330 train_time:44850ms step_avg:39.66ms
step:1132/2330 train_time:44907ms step_avg:39.67ms
step:1133/2330 train_time:44930ms step_avg:39.66ms
step:1134/2330 train_time:44987ms step_avg:39.67ms
step:1135/2330 train_time:45009ms step_avg:39.66ms
step:1136/2330 train_time:45067ms step_avg:39.67ms
step:1137/2330 train_time:45089ms step_avg:39.66ms
step:1138/2330 train_time:45148ms step_avg:39.67ms
step:1139/2330 train_time:45170ms step_avg:39.66ms
step:1140/2330 train_time:45228ms step_avg:39.67ms
step:1141/2330 train_time:45250ms step_avg:39.66ms
step:1142/2330 train_time:45307ms step_avg:39.67ms
step:1143/2330 train_time:45331ms step_avg:39.66ms
step:1144/2330 train_time:45388ms step_avg:39.67ms
step:1145/2330 train_time:45410ms step_avg:39.66ms
step:1146/2330 train_time:45467ms step_avg:39.67ms
step:1147/2330 train_time:45489ms step_avg:39.66ms
step:1148/2330 train_time:45546ms step_avg:39.67ms
step:1149/2330 train_time:45568ms step_avg:39.66ms
step:1150/2330 train_time:45624ms step_avg:39.67ms
step:1151/2330 train_time:45647ms step_avg:39.66ms
step:1152/2330 train_time:45705ms step_avg:39.67ms
step:1153/2330 train_time:45727ms step_avg:39.66ms
step:1154/2330 train_time:45784ms step_avg:39.67ms
step:1155/2330 train_time:45806ms step_avg:39.66ms
step:1156/2330 train_time:45863ms step_avg:39.67ms
step:1157/2330 train_time:45885ms step_avg:39.66ms
step:1158/2330 train_time:45942ms step_avg:39.67ms
step:1159/2330 train_time:45964ms step_avg:39.66ms
step:1160/2330 train_time:46020ms step_avg:39.67ms
step:1161/2330 train_time:46043ms step_avg:39.66ms
step:1162/2330 train_time:46100ms step_avg:39.67ms
step:1163/2330 train_time:46122ms step_avg:39.66ms
step:1164/2330 train_time:46179ms step_avg:39.67ms
step:1165/2330 train_time:46203ms step_avg:39.66ms
step:1166/2330 train_time:46260ms step_avg:39.67ms
step:1167/2330 train_time:46282ms step_avg:39.66ms
step:1168/2330 train_time:46339ms step_avg:39.67ms
step:1169/2330 train_time:46362ms step_avg:39.66ms
step:1170/2330 train_time:46418ms step_avg:39.67ms
step:1171/2330 train_time:46441ms step_avg:39.66ms
step:1172/2330 train_time:46498ms step_avg:39.67ms
step:1173/2330 train_time:46521ms step_avg:39.66ms
step:1174/2330 train_time:46577ms step_avg:39.67ms
step:1175/2330 train_time:46600ms step_avg:39.66ms
step:1176/2330 train_time:46657ms step_avg:39.67ms
step:1177/2330 train_time:46680ms step_avg:39.66ms
step:1178/2330 train_time:46736ms step_avg:39.67ms
step:1179/2330 train_time:46759ms step_avg:39.66ms
step:1180/2330 train_time:46816ms step_avg:39.67ms
step:1181/2330 train_time:46839ms step_avg:39.66ms
step:1182/2330 train_time:46895ms step_avg:39.67ms
step:1183/2330 train_time:46918ms step_avg:39.66ms
step:1184/2330 train_time:46975ms step_avg:39.67ms
step:1185/2330 train_time:46998ms step_avg:39.66ms
step:1186/2330 train_time:47054ms step_avg:39.67ms
step:1187/2330 train_time:47078ms step_avg:39.66ms
step:1188/2330 train_time:47134ms step_avg:39.67ms
step:1189/2330 train_time:47157ms step_avg:39.66ms
step:1190/2330 train_time:47214ms step_avg:39.68ms
step:1191/2330 train_time:47237ms step_avg:39.66ms
step:1192/2330 train_time:47293ms step_avg:39.68ms
step:1193/2330 train_time:47317ms step_avg:39.66ms
step:1194/2330 train_time:47373ms step_avg:39.68ms
step:1195/2330 train_time:47395ms step_avg:39.66ms
step:1196/2330 train_time:47452ms step_avg:39.68ms
step:1197/2330 train_time:47475ms step_avg:39.66ms
step:1198/2330 train_time:47531ms step_avg:39.68ms
step:1199/2330 train_time:47554ms step_avg:39.66ms
step:1200/2330 train_time:47612ms step_avg:39.68ms
step:1201/2330 train_time:47634ms step_avg:39.66ms
step:1202/2330 train_time:47690ms step_avg:39.68ms
step:1203/2330 train_time:47713ms step_avg:39.66ms
step:1204/2330 train_time:47769ms step_avg:39.68ms
step:1205/2330 train_time:47792ms step_avg:39.66ms
step:1206/2330 train_time:47849ms step_avg:39.68ms
step:1207/2330 train_time:47871ms step_avg:39.66ms
step:1208/2330 train_time:47929ms step_avg:39.68ms
step:1209/2330 train_time:47951ms step_avg:39.66ms
step:1210/2330 train_time:48009ms step_avg:39.68ms
step:1211/2330 train_time:48031ms step_avg:39.66ms
step:1212/2330 train_time:48088ms step_avg:39.68ms
step:1213/2330 train_time:48111ms step_avg:39.66ms
step:1214/2330 train_time:48169ms step_avg:39.68ms
step:1215/2330 train_time:48192ms step_avg:39.66ms
step:1216/2330 train_time:48249ms step_avg:39.68ms
step:1217/2330 train_time:48272ms step_avg:39.66ms
step:1218/2330 train_time:48329ms step_avg:39.68ms
step:1219/2330 train_time:48352ms step_avg:39.67ms
step:1220/2330 train_time:48410ms step_avg:39.68ms
step:1221/2330 train_time:48432ms step_avg:39.67ms
step:1222/2330 train_time:48489ms step_avg:39.68ms
step:1223/2330 train_time:48512ms step_avg:39.67ms
step:1224/2330 train_time:48570ms step_avg:39.68ms
step:1225/2330 train_time:48593ms step_avg:39.67ms
step:1226/2330 train_time:48651ms step_avg:39.68ms
step:1227/2330 train_time:48673ms step_avg:39.67ms
step:1228/2330 train_time:48730ms step_avg:39.68ms
step:1229/2330 train_time:48752ms step_avg:39.67ms
step:1230/2330 train_time:48809ms step_avg:39.68ms
step:1231/2330 train_time:48832ms step_avg:39.67ms
step:1232/2330 train_time:48888ms step_avg:39.68ms
step:1233/2330 train_time:48911ms step_avg:39.67ms
step:1234/2330 train_time:48968ms step_avg:39.68ms
step:1235/2330 train_time:48990ms step_avg:39.67ms
step:1236/2330 train_time:49048ms step_avg:39.68ms
step:1237/2330 train_time:49070ms step_avg:39.67ms
step:1238/2330 train_time:49128ms step_avg:39.68ms
step:1239/2330 train_time:49152ms step_avg:39.67ms
step:1240/2330 train_time:49210ms step_avg:39.69ms
step:1241/2330 train_time:49232ms step_avg:39.67ms
step:1242/2330 train_time:49289ms step_avg:39.69ms
step:1243/2330 train_time:49312ms step_avg:39.67ms
step:1244/2330 train_time:49369ms step_avg:39.69ms
step:1245/2330 train_time:49392ms step_avg:39.67ms
step:1246/2330 train_time:49448ms step_avg:39.69ms
step:1247/2330 train_time:49471ms step_avg:39.67ms
step:1248/2330 train_time:49529ms step_avg:39.69ms
step:1249/2330 train_time:49551ms step_avg:39.67ms
step:1250/2330 train_time:49608ms step_avg:39.69ms
step:1250/2330 val_loss:5.2360 train_time:49704ms step_avg:39.76ms
step:1251/2330 train_time:49718ms step_avg:39.74ms
step:1252/2330 train_time:49731ms step_avg:39.72ms
step:1253/2330 train_time:49742ms step_avg:39.70ms
step:1254/2330 train_time:49767ms step_avg:39.69ms
step:1255/2330 train_time:49788ms step_avg:39.67ms
step:1256/2330 train_time:49844ms step_avg:39.68ms
step:1257/2330 train_time:49866ms step_avg:39.67ms
step:1258/2330 train_time:49921ms step_avg:39.68ms
step:1259/2330 train_time:49944ms step_avg:39.67ms
step:1260/2330 train_time:50000ms step_avg:39.68ms
step:1261/2330 train_time:50026ms step_avg:39.67ms
step:1262/2330 train_time:50086ms step_avg:39.69ms
step:1263/2330 train_time:50111ms step_avg:39.68ms
step:1264/2330 train_time:50168ms step_avg:39.69ms
step:1265/2330 train_time:50190ms step_avg:39.68ms
step:1266/2330 train_time:50247ms step_avg:39.69ms
step:1267/2330 train_time:50270ms step_avg:39.68ms
step:1268/2330 train_time:50326ms step_avg:39.69ms
step:1269/2330 train_time:50349ms step_avg:39.68ms
step:1270/2330 train_time:50405ms step_avg:39.69ms
step:1271/2330 train_time:50428ms step_avg:39.68ms
step:1272/2330 train_time:50484ms step_avg:39.69ms
step:1273/2330 train_time:50507ms step_avg:39.68ms
step:1274/2330 train_time:50562ms step_avg:39.69ms
step:1275/2330 train_time:50585ms step_avg:39.67ms
step:1276/2330 train_time:50641ms step_avg:39.69ms
step:1277/2330 train_time:50664ms step_avg:39.67ms
step:1278/2330 train_time:50720ms step_avg:39.69ms
step:1279/2330 train_time:50743ms step_avg:39.67ms
step:1280/2330 train_time:50798ms step_avg:39.69ms
step:1281/2330 train_time:50820ms step_avg:39.67ms
step:1282/2330 train_time:50876ms step_avg:39.68ms
step:1283/2330 train_time:50899ms step_avg:39.67ms
step:1284/2330 train_time:50956ms step_avg:39.69ms
step:1285/2330 train_time:50979ms step_avg:39.67ms
step:1286/2330 train_time:51038ms step_avg:39.69ms
step:1287/2330 train_time:51062ms step_avg:39.67ms
step:1288/2330 train_time:51118ms step_avg:39.69ms
step:1289/2330 train_time:51141ms step_avg:39.68ms
step:1290/2330 train_time:51198ms step_avg:39.69ms
step:1291/2330 train_time:51222ms step_avg:39.68ms
step:1292/2330 train_time:51279ms step_avg:39.69ms
step:1293/2330 train_time:51302ms step_avg:39.68ms
step:1294/2330 train_time:51358ms step_avg:39.69ms
step:1295/2330 train_time:51381ms step_avg:39.68ms
step:1296/2330 train_time:51438ms step_avg:39.69ms
step:1297/2330 train_time:51460ms step_avg:39.68ms
step:1298/2330 train_time:51515ms step_avg:39.69ms
step:1299/2330 train_time:51538ms step_avg:39.68ms
step:1300/2330 train_time:51596ms step_avg:39.69ms
step:1301/2330 train_time:51618ms step_avg:39.68ms
step:1302/2330 train_time:51675ms step_avg:39.69ms
step:1303/2330 train_time:51697ms step_avg:39.68ms
step:1304/2330 train_time:51754ms step_avg:39.69ms
step:1305/2330 train_time:51776ms step_avg:39.68ms
step:1306/2330 train_time:51832ms step_avg:39.69ms
step:1307/2330 train_time:51854ms step_avg:39.67ms
step:1308/2330 train_time:51910ms step_avg:39.69ms
step:1309/2330 train_time:51933ms step_avg:39.67ms
step:1310/2330 train_time:51992ms step_avg:39.69ms
step:1311/2330 train_time:52014ms step_avg:39.68ms
step:1312/2330 train_time:52072ms step_avg:39.69ms
step:1313/2330 train_time:52094ms step_avg:39.68ms
step:1314/2330 train_time:52152ms step_avg:39.69ms
step:1315/2330 train_time:52174ms step_avg:39.68ms
step:1316/2330 train_time:52231ms step_avg:39.69ms
step:1317/2330 train_time:52255ms step_avg:39.68ms
step:1318/2330 train_time:52311ms step_avg:39.69ms
step:1319/2330 train_time:52333ms step_avg:39.68ms
step:1320/2330 train_time:52390ms step_avg:39.69ms
step:1321/2330 train_time:52412ms step_avg:39.68ms
step:1322/2330 train_time:52468ms step_avg:39.69ms
step:1323/2330 train_time:52490ms step_avg:39.68ms
step:1324/2330 train_time:52547ms step_avg:39.69ms
step:1325/2330 train_time:52569ms step_avg:39.68ms
step:1326/2330 train_time:52626ms step_avg:39.69ms
step:1327/2330 train_time:52649ms step_avg:39.68ms
step:1328/2330 train_time:52706ms step_avg:39.69ms
step:1329/2330 train_time:52728ms step_avg:39.67ms
step:1330/2330 train_time:52783ms step_avg:39.69ms
step:1331/2330 train_time:52806ms step_avg:39.67ms
step:1332/2330 train_time:52862ms step_avg:39.69ms
step:1333/2330 train_time:52886ms step_avg:39.67ms
step:1334/2330 train_time:52942ms step_avg:39.69ms
step:1335/2330 train_time:52966ms step_avg:39.68ms
step:1336/2330 train_time:53023ms step_avg:39.69ms
step:1337/2330 train_time:53047ms step_avg:39.68ms
step:1338/2330 train_time:53103ms step_avg:39.69ms
step:1339/2330 train_time:53127ms step_avg:39.68ms
step:1340/2330 train_time:53183ms step_avg:39.69ms
step:1341/2330 train_time:53207ms step_avg:39.68ms
step:1342/2330 train_time:53264ms step_avg:39.69ms
step:1343/2330 train_time:53287ms step_avg:39.68ms
step:1344/2330 train_time:53343ms step_avg:39.69ms
step:1345/2330 train_time:53366ms step_avg:39.68ms
step:1346/2330 train_time:53422ms step_avg:39.69ms
step:1347/2330 train_time:53445ms step_avg:39.68ms
step:1348/2330 train_time:53501ms step_avg:39.69ms
step:1349/2330 train_time:53524ms step_avg:39.68ms
step:1350/2330 train_time:53581ms step_avg:39.69ms
step:1351/2330 train_time:53605ms step_avg:39.68ms
step:1352/2330 train_time:53661ms step_avg:39.69ms
step:1353/2330 train_time:53683ms step_avg:39.68ms
step:1354/2330 train_time:53739ms step_avg:39.69ms
step:1355/2330 train_time:53762ms step_avg:39.68ms
step:1356/2330 train_time:53818ms step_avg:39.69ms
step:1357/2330 train_time:53841ms step_avg:39.68ms
step:1358/2330 train_time:53897ms step_avg:39.69ms
step:1359/2330 train_time:53920ms step_avg:39.68ms
step:1360/2330 train_time:53977ms step_avg:39.69ms
step:1361/2330 train_time:53999ms step_avg:39.68ms
step:1362/2330 train_time:54056ms step_avg:39.69ms
step:1363/2330 train_time:54078ms step_avg:39.68ms
step:1364/2330 train_time:54136ms step_avg:39.69ms
step:1365/2330 train_time:54159ms step_avg:39.68ms
step:1366/2330 train_time:54216ms step_avg:39.69ms
step:1367/2330 train_time:54239ms step_avg:39.68ms
step:1368/2330 train_time:54296ms step_avg:39.69ms
step:1369/2330 train_time:54319ms step_avg:39.68ms
step:1370/2330 train_time:54376ms step_avg:39.69ms
step:1371/2330 train_time:54399ms step_avg:39.68ms
step:1372/2330 train_time:54456ms step_avg:39.69ms
step:1373/2330 train_time:54478ms step_avg:39.68ms
step:1374/2330 train_time:54535ms step_avg:39.69ms
step:1375/2330 train_time:54558ms step_avg:39.68ms
step:1376/2330 train_time:54615ms step_avg:39.69ms
step:1377/2330 train_time:54638ms step_avg:39.68ms
step:1378/2330 train_time:54694ms step_avg:39.69ms
step:1379/2330 train_time:54716ms step_avg:39.68ms
step:1380/2330 train_time:54773ms step_avg:39.69ms
step:1381/2330 train_time:54795ms step_avg:39.68ms
step:1382/2330 train_time:54852ms step_avg:39.69ms
step:1383/2330 train_time:54874ms step_avg:39.68ms
step:1384/2330 train_time:54931ms step_avg:39.69ms
step:1385/2330 train_time:54954ms step_avg:39.68ms
step:1386/2330 train_time:55011ms step_avg:39.69ms
step:1387/2330 train_time:55032ms step_avg:39.68ms
step:1388/2330 train_time:55089ms step_avg:39.69ms
step:1389/2330 train_time:55112ms step_avg:39.68ms
step:1390/2330 train_time:55169ms step_avg:39.69ms
step:1391/2330 train_time:55192ms step_avg:39.68ms
step:1392/2330 train_time:55249ms step_avg:39.69ms
step:1393/2330 train_time:55271ms step_avg:39.68ms
step:1394/2330 train_time:55328ms step_avg:39.69ms
step:1395/2330 train_time:55351ms step_avg:39.68ms
step:1396/2330 train_time:55408ms step_avg:39.69ms
step:1397/2330 train_time:55430ms step_avg:39.68ms
step:1398/2330 train_time:55487ms step_avg:39.69ms
step:1399/2330 train_time:55509ms step_avg:39.68ms
step:1400/2330 train_time:55566ms step_avg:39.69ms
step:1401/2330 train_time:55589ms step_avg:39.68ms
step:1402/2330 train_time:55646ms step_avg:39.69ms
step:1403/2330 train_time:55668ms step_avg:39.68ms
step:1404/2330 train_time:55724ms step_avg:39.69ms
step:1405/2330 train_time:55747ms step_avg:39.68ms
step:1406/2330 train_time:55803ms step_avg:39.69ms
step:1407/2330 train_time:55826ms step_avg:39.68ms
step:1408/2330 train_time:55882ms step_avg:39.69ms
step:1409/2330 train_time:55905ms step_avg:39.68ms
step:1410/2330 train_time:55961ms step_avg:39.69ms
step:1411/2330 train_time:55985ms step_avg:39.68ms
step:1412/2330 train_time:56041ms step_avg:39.69ms
step:1413/2330 train_time:56064ms step_avg:39.68ms
step:1414/2330 train_time:56120ms step_avg:39.69ms
step:1415/2330 train_time:56144ms step_avg:39.68ms
step:1416/2330 train_time:56200ms step_avg:39.69ms
step:1417/2330 train_time:56223ms step_avg:39.68ms
step:1418/2330 train_time:56280ms step_avg:39.69ms
step:1419/2330 train_time:56302ms step_avg:39.68ms
step:1420/2330 train_time:56359ms step_avg:39.69ms
step:1421/2330 train_time:56382ms step_avg:39.68ms
step:1422/2330 train_time:56439ms step_avg:39.69ms
step:1423/2330 train_time:56462ms step_avg:39.68ms
step:1424/2330 train_time:56518ms step_avg:39.69ms
step:1425/2330 train_time:56540ms step_avg:39.68ms
step:1426/2330 train_time:56597ms step_avg:39.69ms
step:1427/2330 train_time:56620ms step_avg:39.68ms
step:1428/2330 train_time:56677ms step_avg:39.69ms
step:1429/2330 train_time:56700ms step_avg:39.68ms
step:1430/2330 train_time:56756ms step_avg:39.69ms
step:1431/2330 train_time:56779ms step_avg:39.68ms
step:1432/2330 train_time:56835ms step_avg:39.69ms
step:1433/2330 train_time:56859ms step_avg:39.68ms
step:1434/2330 train_time:56915ms step_avg:39.69ms
step:1435/2330 train_time:56938ms step_avg:39.68ms
step:1436/2330 train_time:56995ms step_avg:39.69ms
step:1437/2330 train_time:57018ms step_avg:39.68ms
step:1438/2330 train_time:57075ms step_avg:39.69ms
step:1439/2330 train_time:57098ms step_avg:39.68ms
step:1440/2330 train_time:57154ms step_avg:39.69ms
step:1441/2330 train_time:57177ms step_avg:39.68ms
step:1442/2330 train_time:57234ms step_avg:39.69ms
step:1443/2330 train_time:57256ms step_avg:39.68ms
step:1444/2330 train_time:57313ms step_avg:39.69ms
step:1445/2330 train_time:57335ms step_avg:39.68ms
step:1446/2330 train_time:57392ms step_avg:39.69ms
step:1447/2330 train_time:57414ms step_avg:39.68ms
step:1448/2330 train_time:57471ms step_avg:39.69ms
step:1449/2330 train_time:57494ms step_avg:39.68ms
step:1450/2330 train_time:57551ms step_avg:39.69ms
step:1451/2330 train_time:57573ms step_avg:39.68ms
step:1452/2330 train_time:57630ms step_avg:39.69ms
step:1453/2330 train_time:57653ms step_avg:39.68ms
step:1454/2330 train_time:57710ms step_avg:39.69ms
step:1455/2330 train_time:57732ms step_avg:39.68ms
step:1456/2330 train_time:57788ms step_avg:39.69ms
step:1457/2330 train_time:57810ms step_avg:39.68ms
step:1458/2330 train_time:57867ms step_avg:39.69ms
step:1459/2330 train_time:57889ms step_avg:39.68ms
step:1460/2330 train_time:57945ms step_avg:39.69ms
step:1461/2330 train_time:57968ms step_avg:39.68ms
step:1462/2330 train_time:58025ms step_avg:39.69ms
step:1463/2330 train_time:58048ms step_avg:39.68ms
step:1464/2330 train_time:58104ms step_avg:39.69ms
step:1465/2330 train_time:58127ms step_avg:39.68ms
step:1466/2330 train_time:58184ms step_avg:39.69ms
step:1467/2330 train_time:58207ms step_avg:39.68ms
step:1468/2330 train_time:58263ms step_avg:39.69ms
step:1469/2330 train_time:58287ms step_avg:39.68ms
step:1470/2330 train_time:58343ms step_avg:39.69ms
step:1471/2330 train_time:58366ms step_avg:39.68ms
step:1472/2330 train_time:58422ms step_avg:39.69ms
step:1473/2330 train_time:58446ms step_avg:39.68ms
step:1474/2330 train_time:58502ms step_avg:39.69ms
step:1475/2330 train_time:58526ms step_avg:39.68ms
step:1476/2330 train_time:58583ms step_avg:39.69ms
step:1477/2330 train_time:58606ms step_avg:39.68ms
step:1478/2330 train_time:58663ms step_avg:39.69ms
step:1479/2330 train_time:58686ms step_avg:39.68ms
step:1480/2330 train_time:58742ms step_avg:39.69ms
step:1481/2330 train_time:58765ms step_avg:39.68ms
step:1482/2330 train_time:58822ms step_avg:39.69ms
step:1483/2330 train_time:58845ms step_avg:39.68ms
step:1484/2330 train_time:58901ms step_avg:39.69ms
step:1485/2330 train_time:58924ms step_avg:39.68ms
step:1486/2330 train_time:58980ms step_avg:39.69ms
step:1487/2330 train_time:59003ms step_avg:39.68ms
step:1488/2330 train_time:59059ms step_avg:39.69ms
step:1489/2330 train_time:59082ms step_avg:39.68ms
step:1490/2330 train_time:59138ms step_avg:39.69ms
step:1491/2330 train_time:59161ms step_avg:39.68ms
step:1492/2330 train_time:59218ms step_avg:39.69ms
step:1493/2330 train_time:59241ms step_avg:39.68ms
step:1494/2330 train_time:59297ms step_avg:39.69ms
step:1495/2330 train_time:59320ms step_avg:39.68ms
step:1496/2330 train_time:59377ms step_avg:39.69ms
step:1497/2330 train_time:59399ms step_avg:39.68ms
step:1498/2330 train_time:59456ms step_avg:39.69ms
step:1499/2330 train_time:59479ms step_avg:39.68ms
step:1500/2330 train_time:59536ms step_avg:39.69ms
step:1500/2330 val_loss:5.1948 train_time:59633ms step_avg:39.76ms
step:1501/2330 train_time:59646ms step_avg:39.74ms
step:1502/2330 train_time:59659ms step_avg:39.72ms
step:1503/2330 train_time:59670ms step_avg:39.70ms
step:1504/2330 train_time:59696ms step_avg:39.69ms
step:1505/2330 train_time:59717ms step_avg:39.68ms
step:1506/2330 train_time:59773ms step_avg:39.69ms
step:1507/2330 train_time:59795ms step_avg:39.68ms
step:1508/2330 train_time:59851ms step_avg:39.69ms
step:1509/2330 train_time:59872ms step_avg:39.68ms
step:1510/2330 train_time:59929ms step_avg:39.69ms
step:1511/2330 train_time:59953ms step_avg:39.68ms
step:1512/2330 train_time:60013ms step_avg:39.69ms
step:1513/2330 train_time:60037ms step_avg:39.68ms
step:1514/2330 train_time:60095ms step_avg:39.69ms
step:1515/2330 train_time:60118ms step_avg:39.68ms
step:1516/2330 train_time:60175ms step_avg:39.69ms
step:1517/2330 train_time:60198ms step_avg:39.68ms
step:1518/2330 train_time:60254ms step_avg:39.69ms
step:1519/2330 train_time:60276ms step_avg:39.68ms
step:1520/2330 train_time:60334ms step_avg:39.69ms
step:1521/2330 train_time:60356ms step_avg:39.68ms
step:1522/2330 train_time:60412ms step_avg:39.69ms
step:1523/2330 train_time:60434ms step_avg:39.68ms
step:1524/2330 train_time:60490ms step_avg:39.69ms
step:1525/2330 train_time:60511ms step_avg:39.68ms
step:1526/2330 train_time:60569ms step_avg:39.69ms
step:1527/2330 train_time:60592ms step_avg:39.68ms
step:1528/2330 train_time:60649ms step_avg:39.69ms
step:1529/2330 train_time:60674ms step_avg:39.68ms
step:1530/2330 train_time:60730ms step_avg:39.69ms
step:1531/2330 train_time:60752ms step_avg:39.68ms
step:1532/2330 train_time:60807ms step_avg:39.69ms
step:1533/2330 train_time:60830ms step_avg:39.68ms
step:1534/2330 train_time:60887ms step_avg:39.69ms
step:1535/2330 train_time:60911ms step_avg:39.68ms
step:1536/2330 train_time:60967ms step_avg:39.69ms
step:1537/2330 train_time:60991ms step_avg:39.68ms
step:1538/2330 train_time:61048ms step_avg:39.69ms
step:1539/2330 train_time:61073ms step_avg:39.68ms
step:1540/2330 train_time:61130ms step_avg:39.69ms
step:1541/2330 train_time:61152ms step_avg:39.68ms
step:1542/2330 train_time:61209ms step_avg:39.69ms
step:1543/2330 train_time:61232ms step_avg:39.68ms
step:1544/2330 train_time:61288ms step_avg:39.69ms
step:1545/2330 train_time:61311ms step_avg:39.68ms
step:1546/2330 train_time:61366ms step_avg:39.69ms
step:1547/2330 train_time:61389ms step_avg:39.68ms
step:1548/2330 train_time:61445ms step_avg:39.69ms
step:1549/2330 train_time:61468ms step_avg:39.68ms
step:1550/2330 train_time:61524ms step_avg:39.69ms
step:1551/2330 train_time:61546ms step_avg:39.68ms
step:1552/2330 train_time:61603ms step_avg:39.69ms
step:1553/2330 train_time:61626ms step_avg:39.68ms
step:1554/2330 train_time:61682ms step_avg:39.69ms
step:1555/2330 train_time:61704ms step_avg:39.68ms
step:1556/2330 train_time:61761ms step_avg:39.69ms
step:1557/2330 train_time:61783ms step_avg:39.68ms
step:1558/2330 train_time:61840ms step_avg:39.69ms
step:1559/2330 train_time:61862ms step_avg:39.68ms
step:1560/2330 train_time:61920ms step_avg:39.69ms
step:1561/2330 train_time:61942ms step_avg:39.68ms
step:1562/2330 train_time:61999ms step_avg:39.69ms
step:1563/2330 train_time:62023ms step_avg:39.68ms
step:1564/2330 train_time:62080ms step_avg:39.69ms
step:1565/2330 train_time:62103ms step_avg:39.68ms
step:1566/2330 train_time:62160ms step_avg:39.69ms
step:1567/2330 train_time:62184ms step_avg:39.68ms
step:1568/2330 train_time:62240ms step_avg:39.69ms
step:1569/2330 train_time:62263ms step_avg:39.68ms
step:1570/2330 train_time:62319ms step_avg:39.69ms
step:1571/2330 train_time:62342ms step_avg:39.68ms
step:1572/2330 train_time:62399ms step_avg:39.69ms
step:1573/2330 train_time:62421ms step_avg:39.68ms
step:1574/2330 train_time:62478ms step_avg:39.69ms
step:1575/2330 train_time:62500ms step_avg:39.68ms
step:1576/2330 train_time:62557ms step_avg:39.69ms
step:1577/2330 train_time:62579ms step_avg:39.68ms
step:1578/2330 train_time:62635ms step_avg:39.69ms
step:1579/2330 train_time:62657ms step_avg:39.68ms
step:1580/2330 train_time:62714ms step_avg:39.69ms
step:1581/2330 train_time:62736ms step_avg:39.68ms
step:1582/2330 train_time:62793ms step_avg:39.69ms
step:1583/2330 train_time:62815ms step_avg:39.68ms
step:1584/2330 train_time:62873ms step_avg:39.69ms
step:1585/2330 train_time:62895ms step_avg:39.68ms
step:1586/2330 train_time:62952ms step_avg:39.69ms
step:1587/2330 train_time:62975ms step_avg:39.68ms
step:1588/2330 train_time:63032ms step_avg:39.69ms
step:1589/2330 train_time:63054ms step_avg:39.68ms
step:1590/2330 train_time:63111ms step_avg:39.69ms
step:1591/2330 train_time:63133ms step_avg:39.68ms
step:1592/2330 train_time:63190ms step_avg:39.69ms
step:1593/2330 train_time:63212ms step_avg:39.68ms
step:1594/2330 train_time:63269ms step_avg:39.69ms
step:1595/2330 train_time:63292ms step_avg:39.68ms
step:1596/2330 train_time:63348ms step_avg:39.69ms
step:1597/2330 train_time:63370ms step_avg:39.68ms
step:1598/2330 train_time:63426ms step_avg:39.69ms
step:1599/2330 train_time:63450ms step_avg:39.68ms
step:1600/2330 train_time:63506ms step_avg:39.69ms
step:1601/2330 train_time:63529ms step_avg:39.68ms
step:1602/2330 train_time:63585ms step_avg:39.69ms
step:1603/2330 train_time:63608ms step_avg:39.68ms
step:1604/2330 train_time:63664ms step_avg:39.69ms
step:1605/2330 train_time:63687ms step_avg:39.68ms
step:1606/2330 train_time:63744ms step_avg:39.69ms
step:1607/2330 train_time:63767ms step_avg:39.68ms
step:1608/2330 train_time:63823ms step_avg:39.69ms
step:1609/2330 train_time:63846ms step_avg:39.68ms
step:1610/2330 train_time:63903ms step_avg:39.69ms
step:1611/2330 train_time:63925ms step_avg:39.68ms
step:1612/2330 train_time:63982ms step_avg:39.69ms
step:1613/2330 train_time:64005ms step_avg:39.68ms
step:1614/2330 train_time:64061ms step_avg:39.69ms
step:1615/2330 train_time:64084ms step_avg:39.68ms
step:1616/2330 train_time:64141ms step_avg:39.69ms
step:1617/2330 train_time:64164ms step_avg:39.68ms
step:1618/2330 train_time:64220ms step_avg:39.69ms
step:1619/2330 train_time:64243ms step_avg:39.68ms
step:1620/2330 train_time:64300ms step_avg:39.69ms
step:1621/2330 train_time:64324ms step_avg:39.68ms
step:1622/2330 train_time:64381ms step_avg:39.69ms
step:1623/2330 train_time:64404ms step_avg:39.68ms
step:1624/2330 train_time:64460ms step_avg:39.69ms
step:1625/2330 train_time:64483ms step_avg:39.68ms
step:1626/2330 train_time:64540ms step_avg:39.69ms
step:1627/2330 train_time:64562ms step_avg:39.68ms
step:1628/2330 train_time:64618ms step_avg:39.69ms
step:1629/2330 train_time:64641ms step_avg:39.68ms
step:1630/2330 train_time:64698ms step_avg:39.69ms
step:1631/2330 train_time:64720ms step_avg:39.68ms
step:1632/2330 train_time:64776ms step_avg:39.69ms
step:1633/2330 train_time:64798ms step_avg:39.68ms
step:1634/2330 train_time:64855ms step_avg:39.69ms
step:1635/2330 train_time:64878ms step_avg:39.68ms
step:1636/2330 train_time:64935ms step_avg:39.69ms
step:1637/2330 train_time:64958ms step_avg:39.68ms
step:1638/2330 train_time:65015ms step_avg:39.69ms
step:1639/2330 train_time:65037ms step_avg:39.68ms
step:1640/2330 train_time:65095ms step_avg:39.69ms
step:1641/2330 train_time:65117ms step_avg:39.68ms
step:1642/2330 train_time:65174ms step_avg:39.69ms
step:1643/2330 train_time:65196ms step_avg:39.68ms
step:1644/2330 train_time:65254ms step_avg:39.69ms
step:1645/2330 train_time:65276ms step_avg:39.68ms
step:1646/2330 train_time:65333ms step_avg:39.69ms
step:1647/2330 train_time:65356ms step_avg:39.68ms
step:1648/2330 train_time:65413ms step_avg:39.69ms
step:1649/2330 train_time:65435ms step_avg:39.68ms
step:1650/2330 train_time:65492ms step_avg:39.69ms
step:1651/2330 train_time:65514ms step_avg:39.68ms
step:1652/2330 train_time:65571ms step_avg:39.69ms
step:1653/2330 train_time:65593ms step_avg:39.68ms
step:1654/2330 train_time:65649ms step_avg:39.69ms
step:1655/2330 train_time:65672ms step_avg:39.68ms
step:1656/2330 train_time:65728ms step_avg:39.69ms
step:1657/2330 train_time:65751ms step_avg:39.68ms
step:1658/2330 train_time:65807ms step_avg:39.69ms
step:1659/2330 train_time:65829ms step_avg:39.68ms
step:1660/2330 train_time:65886ms step_avg:39.69ms
step:1661/2330 train_time:65910ms step_avg:39.68ms
step:1662/2330 train_time:65966ms step_avg:39.69ms
step:1663/2330 train_time:65989ms step_avg:39.68ms
step:1664/2330 train_time:66045ms step_avg:39.69ms
step:1665/2330 train_time:66068ms step_avg:39.68ms
step:1666/2330 train_time:66124ms step_avg:39.69ms
step:1667/2330 train_time:66148ms step_avg:39.68ms
step:1668/2330 train_time:66204ms step_avg:39.69ms
step:1669/2330 train_time:66227ms step_avg:39.68ms
step:1670/2330 train_time:66283ms step_avg:39.69ms
step:1671/2330 train_time:66306ms step_avg:39.68ms
step:1672/2330 train_time:66363ms step_avg:39.69ms
step:1673/2330 train_time:66387ms step_avg:39.68ms
step:1674/2330 train_time:66443ms step_avg:39.69ms
step:1675/2330 train_time:66466ms step_avg:39.68ms
step:1676/2330 train_time:66522ms step_avg:39.69ms
step:1677/2330 train_time:66545ms step_avg:39.68ms
step:1678/2330 train_time:66601ms step_avg:39.69ms
step:1679/2330 train_time:66624ms step_avg:39.68ms
step:1680/2330 train_time:66680ms step_avg:39.69ms
step:1681/2330 train_time:66703ms step_avg:39.68ms
step:1682/2330 train_time:66760ms step_avg:39.69ms
step:1683/2330 train_time:66783ms step_avg:39.68ms
step:1684/2330 train_time:66839ms step_avg:39.69ms
step:1685/2330 train_time:66862ms step_avg:39.68ms
step:1686/2330 train_time:66918ms step_avg:39.69ms
step:1687/2330 train_time:66941ms step_avg:39.68ms
step:1688/2330 train_time:66998ms step_avg:39.69ms
step:1689/2330 train_time:67021ms step_avg:39.68ms
step:1690/2330 train_time:67079ms step_avg:39.69ms
step:1691/2330 train_time:67101ms step_avg:39.68ms
step:1692/2330 train_time:67159ms step_avg:39.69ms
step:1693/2330 train_time:67182ms step_avg:39.68ms
step:1694/2330 train_time:67239ms step_avg:39.69ms
step:1695/2330 train_time:67262ms step_avg:39.68ms
step:1696/2330 train_time:67319ms step_avg:39.69ms
step:1697/2330 train_time:67341ms step_avg:39.68ms
step:1698/2330 train_time:67398ms step_avg:39.69ms
step:1699/2330 train_time:67420ms step_avg:39.68ms
step:1700/2330 train_time:67476ms step_avg:39.69ms
step:1701/2330 train_time:67499ms step_avg:39.68ms
step:1702/2330 train_time:67556ms step_avg:39.69ms
step:1703/2330 train_time:67579ms step_avg:39.68ms
step:1704/2330 train_time:67636ms step_avg:39.69ms
step:1705/2330 train_time:67659ms step_avg:39.68ms
step:1706/2330 train_time:67716ms step_avg:39.69ms
step:1707/2330 train_time:67738ms step_avg:39.68ms
step:1708/2330 train_time:67795ms step_avg:39.69ms
step:1709/2330 train_time:67817ms step_avg:39.68ms
step:1710/2330 train_time:67875ms step_avg:39.69ms
step:1711/2330 train_time:67897ms step_avg:39.68ms
step:1712/2330 train_time:67954ms step_avg:39.69ms
step:1713/2330 train_time:67976ms step_avg:39.68ms
step:1714/2330 train_time:68033ms step_avg:39.69ms
step:1715/2330 train_time:68055ms step_avg:39.68ms
step:1716/2330 train_time:68112ms step_avg:39.69ms
step:1717/2330 train_time:68135ms step_avg:39.68ms
step:1718/2330 train_time:68192ms step_avg:39.69ms
step:1719/2330 train_time:68214ms step_avg:39.68ms
step:1720/2330 train_time:68271ms step_avg:39.69ms
step:1721/2330 train_time:68293ms step_avg:39.68ms
step:1722/2330 train_time:68350ms step_avg:39.69ms
step:1723/2330 train_time:68372ms step_avg:39.68ms
step:1724/2330 train_time:68429ms step_avg:39.69ms
step:1725/2330 train_time:68452ms step_avg:39.68ms
step:1726/2330 train_time:68508ms step_avg:39.69ms
step:1727/2330 train_time:68531ms step_avg:39.68ms
step:1728/2330 train_time:68587ms step_avg:39.69ms
step:1729/2330 train_time:68610ms step_avg:39.68ms
step:1730/2330 train_time:68666ms step_avg:39.69ms
step:1731/2330 train_time:68689ms step_avg:39.68ms
step:1732/2330 train_time:68746ms step_avg:39.69ms
step:1733/2330 train_time:68770ms step_avg:39.68ms
step:1734/2330 train_time:68826ms step_avg:39.69ms
step:1735/2330 train_time:68849ms step_avg:39.68ms
step:1736/2330 train_time:68905ms step_avg:39.69ms
step:1737/2330 train_time:68928ms step_avg:39.68ms
step:1738/2330 train_time:68985ms step_avg:39.69ms
step:1739/2330 train_time:69008ms step_avg:39.68ms
step:1740/2330 train_time:69064ms step_avg:39.69ms
step:1741/2330 train_time:69087ms step_avg:39.68ms
step:1742/2330 train_time:69144ms step_avg:39.69ms
step:1743/2330 train_time:69166ms step_avg:39.68ms
step:1744/2330 train_time:69223ms step_avg:39.69ms
step:1745/2330 train_time:69246ms step_avg:39.68ms
step:1746/2330 train_time:69302ms step_avg:39.69ms
step:1747/2330 train_time:69324ms step_avg:39.68ms
step:1748/2330 train_time:69381ms step_avg:39.69ms
step:1749/2330 train_time:69403ms step_avg:39.68ms
step:1750/2330 train_time:69460ms step_avg:39.69ms
step:1750/2330 val_loss:5.1820 train_time:69557ms step_avg:39.75ms
step:1751/2330 train_time:69569ms step_avg:39.73ms
step:1752/2330 train_time:69580ms step_avg:39.71ms
step:1753/2330 train_time:69590ms step_avg:39.70ms
step:1754/2330 train_time:69622ms step_avg:39.69ms
step:1755/2330 train_time:69644ms step_avg:39.68ms
step:1756/2330 train_time:69699ms step_avg:39.69ms
step:1757/2330 train_time:69721ms step_avg:39.68ms
step:1758/2330 train_time:69776ms step_avg:39.69ms
step:1759/2330 train_time:69798ms step_avg:39.68ms
step:1760/2330 train_time:69855ms step_avg:39.69ms
step:1761/2330 train_time:69880ms step_avg:39.68ms
step:1762/2330 train_time:69941ms step_avg:39.69ms
step:1763/2330 train_time:69964ms step_avg:39.68ms
step:1764/2330 train_time:70023ms step_avg:39.70ms
step:1765/2330 train_time:70046ms step_avg:39.69ms
step:1766/2330 train_time:70103ms step_avg:39.70ms
step:1767/2330 train_time:70125ms step_avg:39.69ms
step:1768/2330 train_time:70182ms step_avg:39.70ms
step:1769/2330 train_time:70204ms step_avg:39.69ms
step:1770/2330 train_time:70260ms step_avg:39.69ms
step:1771/2330 train_time:70282ms step_avg:39.68ms
step:1772/2330 train_time:70338ms step_avg:39.69ms
step:1773/2330 train_time:70360ms step_avg:39.68ms
step:1774/2330 train_time:70416ms step_avg:39.69ms
step:1775/2330 train_time:70438ms step_avg:39.68ms
step:1776/2330 train_time:70496ms step_avg:39.69ms
step:1777/2330 train_time:70519ms step_avg:39.68ms
step:1778/2330 train_time:70576ms step_avg:39.69ms
step:1779/2330 train_time:70598ms step_avg:39.68ms
step:1780/2330 train_time:70654ms step_avg:39.69ms
step:1781/2330 train_time:70676ms step_avg:39.68ms
step:1782/2330 train_time:70731ms step_avg:39.69ms
step:1783/2330 train_time:70754ms step_avg:39.68ms
step:1784/2330 train_time:70810ms step_avg:39.69ms
step:1785/2330 train_time:70834ms step_avg:39.68ms
step:1786/2330 train_time:70891ms step_avg:39.69ms
step:1787/2330 train_time:70916ms step_avg:39.68ms
step:1788/2330 train_time:70973ms step_avg:39.69ms
step:1789/2330 train_time:70997ms step_avg:39.69ms
step:1790/2330 train_time:71054ms step_avg:39.69ms
step:1791/2330 train_time:71077ms step_avg:39.69ms
step:1792/2330 train_time:71134ms step_avg:39.70ms
step:1793/2330 train_time:71157ms step_avg:39.69ms
step:1794/2330 train_time:71213ms step_avg:39.70ms
step:1795/2330 train_time:71236ms step_avg:39.69ms
step:1796/2330 train_time:71292ms step_avg:39.70ms
step:1797/2330 train_time:71316ms step_avg:39.69ms
step:1798/2330 train_time:71372ms step_avg:39.70ms
step:1799/2330 train_time:71395ms step_avg:39.69ms
step:1800/2330 train_time:71451ms step_avg:39.70ms
step:1801/2330 train_time:71474ms step_avg:39.69ms
step:1802/2330 train_time:71530ms step_avg:39.69ms
step:1803/2330 train_time:71552ms step_avg:39.69ms
step:1804/2330 train_time:71608ms step_avg:39.69ms
step:1805/2330 train_time:71631ms step_avg:39.68ms
step:1806/2330 train_time:71686ms step_avg:39.69ms
step:1807/2330 train_time:71709ms step_avg:39.68ms
step:1808/2330 train_time:71764ms step_avg:39.69ms
step:1809/2330 train_time:71788ms step_avg:39.68ms
step:1810/2330 train_time:71845ms step_avg:39.69ms
step:1811/2330 train_time:71868ms step_avg:39.68ms
step:1812/2330 train_time:71927ms step_avg:39.69ms
step:1813/2330 train_time:71951ms step_avg:39.69ms
step:1814/2330 train_time:72008ms step_avg:39.70ms
step:1815/2330 train_time:72031ms step_avg:39.69ms
step:1816/2330 train_time:72087ms step_avg:39.70ms
step:1817/2330 train_time:72110ms step_avg:39.69ms
step:1818/2330 train_time:72166ms step_avg:39.70ms
step:1819/2330 train_time:72189ms step_avg:39.69ms
step:1820/2330 train_time:72246ms step_avg:39.70ms
step:1821/2330 train_time:72269ms step_avg:39.69ms
step:1822/2330 train_time:72325ms step_avg:39.70ms
step:1823/2330 train_time:72347ms step_avg:39.69ms
step:1824/2330 train_time:72404ms step_avg:39.70ms
step:1825/2330 train_time:72426ms step_avg:39.69ms
step:1826/2330 train_time:72483ms step_avg:39.69ms
step:1827/2330 train_time:72504ms step_avg:39.68ms
step:1828/2330 train_time:72561ms step_avg:39.69ms
step:1829/2330 train_time:72583ms step_avg:39.68ms
step:1830/2330 train_time:72640ms step_avg:39.69ms
step:1831/2330 train_time:72662ms step_avg:39.68ms
step:1832/2330 train_time:72718ms step_avg:39.69ms
step:1833/2330 train_time:72740ms step_avg:39.68ms
step:1834/2330 train_time:72797ms step_avg:39.69ms
step:1835/2330 train_time:72821ms step_avg:39.68ms
step:1836/2330 train_time:72878ms step_avg:39.69ms
step:1837/2330 train_time:72901ms step_avg:39.68ms
step:1838/2330 train_time:72957ms step_avg:39.69ms
step:1839/2330 train_time:72980ms step_avg:39.68ms
step:1840/2330 train_time:73038ms step_avg:39.69ms
step:1841/2330 train_time:73060ms step_avg:39.69ms
step:1842/2330 train_time:73117ms step_avg:39.69ms
step:1843/2330 train_time:73139ms step_avg:39.68ms
step:1844/2330 train_time:73197ms step_avg:39.69ms
step:1845/2330 train_time:73219ms step_avg:39.69ms
step:1846/2330 train_time:73276ms step_avg:39.69ms
step:1847/2330 train_time:73299ms step_avg:39.69ms
step:1848/2330 train_time:73355ms step_avg:39.69ms
step:1849/2330 train_time:73377ms step_avg:39.68ms
step:1850/2330 train_time:73434ms step_avg:39.69ms
step:1851/2330 train_time:73456ms step_avg:39.68ms
step:1852/2330 train_time:73512ms step_avg:39.69ms
step:1853/2330 train_time:73535ms step_avg:39.68ms
step:1854/2330 train_time:73591ms step_avg:39.69ms
step:1855/2330 train_time:73615ms step_avg:39.68ms
step:1856/2330 train_time:73670ms step_avg:39.69ms
step:1857/2330 train_time:73693ms step_avg:39.68ms
step:1858/2330 train_time:73750ms step_avg:39.69ms
step:1859/2330 train_time:73773ms step_avg:39.68ms
step:1860/2330 train_time:73829ms step_avg:39.69ms
step:1861/2330 train_time:73852ms step_avg:39.68ms
step:1862/2330 train_time:73908ms step_avg:39.69ms
step:1863/2330 train_time:73931ms step_avg:39.68ms
step:1864/2330 train_time:73988ms step_avg:39.69ms
step:1865/2330 train_time:74011ms step_avg:39.68ms
step:1866/2330 train_time:74067ms step_avg:39.69ms
step:1867/2330 train_time:74090ms step_avg:39.68ms
step:1868/2330 train_time:74146ms step_avg:39.69ms
step:1869/2330 train_time:74169ms step_avg:39.68ms
step:1870/2330 train_time:74226ms step_avg:39.69ms
step:1871/2330 train_time:74250ms step_avg:39.68ms
step:1872/2330 train_time:74306ms step_avg:39.69ms
step:1873/2330 train_time:74328ms step_avg:39.68ms
step:1874/2330 train_time:74385ms step_avg:39.69ms
step:1875/2330 train_time:74407ms step_avg:39.68ms
step:1876/2330 train_time:74465ms step_avg:39.69ms
step:1877/2330 train_time:74487ms step_avg:39.68ms
step:1878/2330 train_time:74543ms step_avg:39.69ms
step:1879/2330 train_time:74566ms step_avg:39.68ms
step:1880/2330 train_time:74623ms step_avg:39.69ms
step:1881/2330 train_time:74645ms step_avg:39.68ms
step:1882/2330 train_time:74702ms step_avg:39.69ms
step:1883/2330 train_time:74724ms step_avg:39.68ms
step:1884/2330 train_time:74780ms step_avg:39.69ms
step:1885/2330 train_time:74803ms step_avg:39.68ms
step:1886/2330 train_time:74860ms step_avg:39.69ms
step:1887/2330 train_time:74882ms step_avg:39.68ms
step:1888/2330 train_time:74938ms step_avg:39.69ms
step:1889/2330 train_time:74962ms step_avg:39.68ms
step:1890/2330 train_time:75018ms step_avg:39.69ms
step:1891/2330 train_time:75040ms step_avg:39.68ms
step:1892/2330 train_time:75097ms step_avg:39.69ms
step:1893/2330 train_time:75121ms step_avg:39.68ms
step:1894/2330 train_time:75177ms step_avg:39.69ms
step:1895/2330 train_time:75200ms step_avg:39.68ms
step:1896/2330 train_time:75256ms step_avg:39.69ms
step:1897/2330 train_time:75279ms step_avg:39.68ms
step:1898/2330 train_time:75336ms step_avg:39.69ms
step:1899/2330 train_time:75359ms step_avg:39.68ms
step:1900/2330 train_time:75416ms step_avg:39.69ms
step:1901/2330 train_time:75438ms step_avg:39.68ms
step:1902/2330 train_time:75495ms step_avg:39.69ms
step:1903/2330 train_time:75517ms step_avg:39.68ms
step:1904/2330 train_time:75573ms step_avg:39.69ms
step:1905/2330 train_time:75596ms step_avg:39.68ms
step:1906/2330 train_time:75652ms step_avg:39.69ms
step:1907/2330 train_time:75675ms step_avg:39.68ms
step:1908/2330 train_time:75731ms step_avg:39.69ms
step:1909/2330 train_time:75755ms step_avg:39.68ms
step:1910/2330 train_time:75811ms step_avg:39.69ms
step:1911/2330 train_time:75834ms step_avg:39.68ms
step:1912/2330 train_time:75891ms step_avg:39.69ms
step:1913/2330 train_time:75914ms step_avg:39.68ms
step:1914/2330 train_time:75970ms step_avg:39.69ms
step:1915/2330 train_time:75993ms step_avg:39.68ms
step:1916/2330 train_time:76049ms step_avg:39.69ms
step:1917/2330 train_time:76072ms step_avg:39.68ms
step:1918/2330 train_time:76129ms step_avg:39.69ms
step:1919/2330 train_time:76152ms step_avg:39.68ms
step:1920/2330 train_time:76208ms step_avg:39.69ms
step:1921/2330 train_time:76230ms step_avg:39.68ms
step:1922/2330 train_time:76287ms step_avg:39.69ms
step:1923/2330 train_time:76310ms step_avg:39.68ms
step:1924/2330 train_time:76366ms step_avg:39.69ms
step:1925/2330 train_time:76390ms step_avg:39.68ms
step:1926/2330 train_time:76446ms step_avg:39.69ms
step:1927/2330 train_time:76470ms step_avg:39.68ms
step:1928/2330 train_time:76526ms step_avg:39.69ms
step:1929/2330 train_time:76549ms step_avg:39.68ms
step:1930/2330 train_time:76605ms step_avg:39.69ms
step:1931/2330 train_time:76627ms step_avg:39.68ms
step:1932/2330 train_time:76684ms step_avg:39.69ms
step:1933/2330 train_time:76707ms step_avg:39.68ms
step:1934/2330 train_time:76764ms step_avg:39.69ms
step:1935/2330 train_time:76786ms step_avg:39.68ms
step:1936/2330 train_time:76844ms step_avg:39.69ms
step:1937/2330 train_time:76866ms step_avg:39.68ms
step:1938/2330 train_time:76923ms step_avg:39.69ms
step:1939/2330 train_time:76945ms step_avg:39.68ms
step:1940/2330 train_time:77002ms step_avg:39.69ms
step:1941/2330 train_time:77024ms step_avg:39.68ms
step:1942/2330 train_time:77081ms step_avg:39.69ms
step:1943/2330 train_time:77104ms step_avg:39.68ms
step:1944/2330 train_time:77160ms step_avg:39.69ms
step:1945/2330 train_time:77182ms step_avg:39.68ms
step:1946/2330 train_time:77240ms step_avg:39.69ms
step:1947/2330 train_time:77262ms step_avg:39.68ms
step:1948/2330 train_time:77318ms step_avg:39.69ms
step:1949/2330 train_time:77340ms step_avg:39.68ms
step:1950/2330 train_time:77398ms step_avg:39.69ms
step:1951/2330 train_time:77420ms step_avg:39.68ms
step:1952/2330 train_time:77477ms step_avg:39.69ms
step:1953/2330 train_time:77499ms step_avg:39.68ms
step:1954/2330 train_time:77555ms step_avg:39.69ms
step:1955/2330 train_time:77578ms step_avg:39.68ms
step:1956/2330 train_time:77635ms step_avg:39.69ms
step:1957/2330 train_time:77657ms step_avg:39.68ms
step:1958/2330 train_time:77714ms step_avg:39.69ms
step:1959/2330 train_time:77737ms step_avg:39.68ms
step:1960/2330 train_time:77794ms step_avg:39.69ms
step:1961/2330 train_time:77816ms step_avg:39.68ms
step:1962/2330 train_time:77872ms step_avg:39.69ms
step:1963/2330 train_time:77896ms step_avg:39.68ms
step:1964/2330 train_time:77952ms step_avg:39.69ms
step:1965/2330 train_time:77975ms step_avg:39.68ms
step:1966/2330 train_time:78032ms step_avg:39.69ms
step:1967/2330 train_time:78055ms step_avg:39.68ms
step:1968/2330 train_time:78111ms step_avg:39.69ms
step:1969/2330 train_time:78134ms step_avg:39.68ms
step:1970/2330 train_time:78191ms step_avg:39.69ms
step:1971/2330 train_time:78215ms step_avg:39.68ms
step:1972/2330 train_time:78271ms step_avg:39.69ms
step:1973/2330 train_time:78294ms step_avg:39.68ms
step:1974/2330 train_time:78350ms step_avg:39.69ms
step:1975/2330 train_time:78373ms step_avg:39.68ms
step:1976/2330 train_time:78429ms step_avg:39.69ms
step:1977/2330 train_time:78452ms step_avg:39.68ms
step:1978/2330 train_time:78508ms step_avg:39.69ms
step:1979/2330 train_time:78531ms step_avg:39.68ms
step:1980/2330 train_time:78588ms step_avg:39.69ms
step:1981/2330 train_time:78611ms step_avg:39.68ms
step:1982/2330 train_time:78667ms step_avg:39.69ms
step:1983/2330 train_time:78690ms step_avg:39.68ms
step:1984/2330 train_time:78747ms step_avg:39.69ms
step:1985/2330 train_time:78769ms step_avg:39.68ms
step:1986/2330 train_time:78826ms step_avg:39.69ms
step:1987/2330 train_time:78848ms step_avg:39.68ms
step:1988/2330 train_time:78905ms step_avg:39.69ms
step:1989/2330 train_time:78928ms step_avg:39.68ms
step:1990/2330 train_time:78985ms step_avg:39.69ms
step:1991/2330 train_time:79008ms step_avg:39.68ms
step:1992/2330 train_time:79064ms step_avg:39.69ms
step:1993/2330 train_time:79087ms step_avg:39.68ms
step:1994/2330 train_time:79146ms step_avg:39.69ms
step:1995/2330 train_time:79169ms step_avg:39.68ms
step:1996/2330 train_time:79227ms step_avg:39.69ms
step:1997/2330 train_time:79249ms step_avg:39.68ms
step:1998/2330 train_time:79305ms step_avg:39.69ms
step:1999/2330 train_time:79327ms step_avg:39.68ms
step:2000/2330 train_time:79384ms step_avg:39.69ms
step:2000/2330 val_loss:5.1597 train_time:79481ms step_avg:39.74ms
step:2001/2330 train_time:79493ms step_avg:39.73ms
step:2002/2330 train_time:79503ms step_avg:39.71ms
step:2003/2330 train_time:79513ms step_avg:39.70ms
step:2004/2330 train_time:79545ms step_avg:39.69ms
step:2005/2330 train_time:79566ms step_avg:39.68ms
step:2006/2330 train_time:79622ms step_avg:39.69ms
step:2007/2330 train_time:79644ms step_avg:39.68ms
step:2008/2330 train_time:79699ms step_avg:39.69ms
step:2009/2330 train_time:79722ms step_avg:39.68ms
step:2010/2330 train_time:79778ms step_avg:39.69ms
step:2011/2330 train_time:79803ms step_avg:39.68ms
step:2012/2330 train_time:79863ms step_avg:39.69ms
step:2013/2330 train_time:79888ms step_avg:39.69ms
step:2014/2330 train_time:79947ms step_avg:39.70ms
step:2015/2330 train_time:79970ms step_avg:39.69ms
step:2016/2330 train_time:80026ms step_avg:39.70ms
step:2017/2330 train_time:80049ms step_avg:39.69ms
step:2018/2330 train_time:80106ms step_avg:39.70ms
step:2019/2330 train_time:80128ms step_avg:39.69ms
step:2020/2330 train_time:80184ms step_avg:39.70ms
step:2021/2330 train_time:80206ms step_avg:39.69ms
step:2022/2330 train_time:80262ms step_avg:39.69ms
step:2023/2330 train_time:80285ms step_avg:39.69ms
step:2024/2330 train_time:80340ms step_avg:39.69ms
step:2025/2330 train_time:80362ms step_avg:39.69ms
step:2026/2330 train_time:80420ms step_avg:39.69ms
step:2027/2330 train_time:80443ms step_avg:39.69ms
step:2028/2330 train_time:80500ms step_avg:39.69ms
step:2029/2330 train_time:80523ms step_avg:39.69ms
step:2030/2330 train_time:80579ms step_avg:39.69ms
step:2031/2330 train_time:80601ms step_avg:39.69ms
step:2032/2330 train_time:80657ms step_avg:39.69ms
step:2033/2330 train_time:80679ms step_avg:39.68ms
step:2034/2330 train_time:80736ms step_avg:39.69ms
step:2035/2330 train_time:80760ms step_avg:39.69ms
step:2036/2330 train_time:80818ms step_avg:39.69ms
step:2037/2330 train_time:80842ms step_avg:39.69ms
step:2038/2330 train_time:80899ms step_avg:39.70ms
step:2039/2330 train_time:80923ms step_avg:39.69ms
step:2040/2330 train_time:80979ms step_avg:39.70ms
step:2041/2330 train_time:81003ms step_avg:39.69ms
step:2042/2330 train_time:81059ms step_avg:39.70ms
step:2043/2330 train_time:81082ms step_avg:39.69ms
step:2044/2330 train_time:81139ms step_avg:39.70ms
step:2045/2330 train_time:81162ms step_avg:39.69ms
step:2046/2330 train_time:81218ms step_avg:39.70ms
step:2047/2330 train_time:81240ms step_avg:39.69ms
step:2048/2330 train_time:81296ms step_avg:39.70ms
step:2049/2330 train_time:81319ms step_avg:39.69ms
step:2050/2330 train_time:81375ms step_avg:39.70ms
step:2051/2330 train_time:81397ms step_avg:39.69ms
step:2052/2330 train_time:81454ms step_avg:39.69ms
step:2053/2330 train_time:81476ms step_avg:39.69ms
step:2054/2330 train_time:81532ms step_avg:39.69ms
step:2055/2330 train_time:81554ms step_avg:39.69ms
step:2056/2330 train_time:81611ms step_avg:39.69ms
step:2057/2330 train_time:81633ms step_avg:39.69ms
step:2058/2330 train_time:81691ms step_avg:39.69ms
step:2059/2330 train_time:81713ms step_avg:39.69ms
step:2060/2330 train_time:81772ms step_avg:39.70ms
step:2061/2330 train_time:81795ms step_avg:39.69ms
step:2062/2330 train_time:81853ms step_avg:39.70ms
step:2063/2330 train_time:81876ms step_avg:39.69ms
step:2064/2330 train_time:81933ms step_avg:39.70ms
step:2065/2330 train_time:81956ms step_avg:39.69ms
step:2066/2330 train_time:82014ms step_avg:39.70ms
step:2067/2330 train_time:82038ms step_avg:39.69ms
step:2068/2330 train_time:82095ms step_avg:39.70ms
step:2069/2330 train_time:82117ms step_avg:39.69ms
step:2070/2330 train_time:82174ms step_avg:39.70ms
step:2071/2330 train_time:82197ms step_avg:39.69ms
step:2072/2330 train_time:82253ms step_avg:39.70ms
step:2073/2330 train_time:82276ms step_avg:39.69ms
step:2074/2330 train_time:82332ms step_avg:39.70ms
step:2075/2330 train_time:82355ms step_avg:39.69ms
step:2076/2330 train_time:82411ms step_avg:39.70ms
step:2077/2330 train_time:82434ms step_avg:39.69ms
step:2078/2330 train_time:82490ms step_avg:39.70ms
step:2079/2330 train_time:82512ms step_avg:39.69ms
step:2080/2330 train_time:82569ms step_avg:39.70ms
step:2081/2330 train_time:82591ms step_avg:39.69ms
step:2082/2330 train_time:82648ms step_avg:39.70ms
step:2083/2330 train_time:82671ms step_avg:39.69ms
step:2084/2330 train_time:82728ms step_avg:39.70ms
step:2085/2330 train_time:82751ms step_avg:39.69ms
step:2086/2330 train_time:82808ms step_avg:39.70ms
step:2087/2330 train_time:82830ms step_avg:39.69ms
step:2088/2330 train_time:82888ms step_avg:39.70ms
step:2089/2330 train_time:82910ms step_avg:39.69ms
step:2090/2330 train_time:82968ms step_avg:39.70ms
step:2091/2330 train_time:82990ms step_avg:39.69ms
step:2092/2330 train_time:83048ms step_avg:39.70ms
step:2093/2330 train_time:83070ms step_avg:39.69ms
step:2094/2330 train_time:83127ms step_avg:39.70ms
step:2095/2330 train_time:83149ms step_avg:39.69ms
step:2096/2330 train_time:83206ms step_avg:39.70ms
step:2097/2330 train_time:83228ms step_avg:39.69ms
step:2098/2330 train_time:83284ms step_avg:39.70ms
step:2099/2330 train_time:83307ms step_avg:39.69ms
step:2100/2330 train_time:83363ms step_avg:39.70ms
step:2101/2330 train_time:83385ms step_avg:39.69ms
step:2102/2330 train_time:83442ms step_avg:39.70ms
step:2103/2330 train_time:83464ms step_avg:39.69ms
step:2104/2330 train_time:83521ms step_avg:39.70ms
step:2105/2330 train_time:83543ms step_avg:39.69ms
step:2106/2330 train_time:83599ms step_avg:39.70ms
step:2107/2330 train_time:83623ms step_avg:39.69ms
step:2108/2330 train_time:83680ms step_avg:39.70ms
step:2109/2330 train_time:83703ms step_avg:39.69ms
step:2110/2330 train_time:83760ms step_avg:39.70ms
step:2111/2330 train_time:83783ms step_avg:39.69ms
step:2112/2330 train_time:83840ms step_avg:39.70ms
step:2113/2330 train_time:83863ms step_avg:39.69ms
step:2114/2330 train_time:83920ms step_avg:39.70ms
step:2115/2330 train_time:83943ms step_avg:39.69ms
step:2116/2330 train_time:84000ms step_avg:39.70ms
step:2117/2330 train_time:84023ms step_avg:39.69ms
step:2118/2330 train_time:84080ms step_avg:39.70ms
step:2119/2330 train_time:84103ms step_avg:39.69ms
step:2120/2330 train_time:84160ms step_avg:39.70ms
step:2121/2330 train_time:84182ms step_avg:39.69ms
step:2122/2330 train_time:84239ms step_avg:39.70ms
step:2123/2330 train_time:84261ms step_avg:39.69ms
step:2124/2330 train_time:84318ms step_avg:39.70ms
step:2125/2330 train_time:84340ms step_avg:39.69ms
step:2126/2330 train_time:84397ms step_avg:39.70ms
step:2127/2330 train_time:84420ms step_avg:39.69ms
step:2128/2330 train_time:84476ms step_avg:39.70ms
step:2129/2330 train_time:84499ms step_avg:39.69ms
step:2130/2330 train_time:84555ms step_avg:39.70ms
step:2131/2330 train_time:84578ms step_avg:39.69ms
step:2132/2330 train_time:84634ms step_avg:39.70ms
step:2133/2330 train_time:84656ms step_avg:39.69ms
step:2134/2330 train_time:84714ms step_avg:39.70ms
step:2135/2330 train_time:84736ms step_avg:39.69ms
step:2136/2330 train_time:84793ms step_avg:39.70ms
step:2137/2330 train_time:84816ms step_avg:39.69ms
step:2138/2330 train_time:84874ms step_avg:39.70ms
step:2139/2330 train_time:84897ms step_avg:39.69ms
step:2140/2330 train_time:84953ms step_avg:39.70ms
step:2141/2330 train_time:84976ms step_avg:39.69ms
step:2142/2330 train_time:85033ms step_avg:39.70ms
step:2143/2330 train_time:85056ms step_avg:39.69ms
step:2144/2330 train_time:85113ms step_avg:39.70ms
step:2145/2330 train_time:85135ms step_avg:39.69ms
step:2146/2330 train_time:85192ms step_avg:39.70ms
step:2147/2330 train_time:85214ms step_avg:39.69ms
step:2148/2330 train_time:85270ms step_avg:39.70ms
step:2149/2330 train_time:85293ms step_avg:39.69ms
step:2150/2330 train_time:85350ms step_avg:39.70ms
step:2151/2330 train_time:85372ms step_avg:39.69ms
step:2152/2330 train_time:85429ms step_avg:39.70ms
step:2153/2330 train_time:85452ms step_avg:39.69ms
step:2154/2330 train_time:85509ms step_avg:39.70ms
step:2155/2330 train_time:85532ms step_avg:39.69ms
step:2156/2330 train_time:85589ms step_avg:39.70ms
step:2157/2330 train_time:85611ms step_avg:39.69ms
step:2158/2330 train_time:85668ms step_avg:39.70ms
step:2159/2330 train_time:85690ms step_avg:39.69ms
step:2160/2330 train_time:85747ms step_avg:39.70ms
step:2161/2330 train_time:85769ms step_avg:39.69ms
step:2162/2330 train_time:85826ms step_avg:39.70ms
step:2163/2330 train_time:85848ms step_avg:39.69ms
step:2164/2330 train_time:85905ms step_avg:39.70ms
step:2165/2330 train_time:85927ms step_avg:39.69ms
step:2166/2330 train_time:85985ms step_avg:39.70ms
step:2167/2330 train_time:86007ms step_avg:39.69ms
step:2168/2330 train_time:86064ms step_avg:39.70ms
step:2169/2330 train_time:86086ms step_avg:39.69ms
step:2170/2330 train_time:86143ms step_avg:39.70ms
step:2171/2330 train_time:86166ms step_avg:39.69ms
step:2172/2330 train_time:86223ms step_avg:39.70ms
step:2173/2330 train_time:86246ms step_avg:39.69ms
step:2174/2330 train_time:86302ms step_avg:39.70ms
step:2175/2330 train_time:86325ms step_avg:39.69ms
step:2176/2330 train_time:86381ms step_avg:39.70ms
step:2177/2330 train_time:86404ms step_avg:39.69ms
step:2178/2330 train_time:86460ms step_avg:39.70ms
step:2179/2330 train_time:86483ms step_avg:39.69ms
step:2180/2330 train_time:86540ms step_avg:39.70ms
step:2181/2330 train_time:86563ms step_avg:39.69ms
step:2182/2330 train_time:86620ms step_avg:39.70ms
step:2183/2330 train_time:86643ms step_avg:39.69ms
step:2184/2330 train_time:86699ms step_avg:39.70ms
step:2185/2330 train_time:86722ms step_avg:39.69ms
step:2186/2330 train_time:86778ms step_avg:39.70ms
step:2187/2330 train_time:86801ms step_avg:39.69ms
step:2188/2330 train_time:86857ms step_avg:39.70ms
step:2189/2330 train_time:86881ms step_avg:39.69ms
step:2190/2330 train_time:86937ms step_avg:39.70ms
step:2191/2330 train_time:86961ms step_avg:39.69ms
step:2192/2330 train_time:87017ms step_avg:39.70ms
step:2193/2330 train_time:87040ms step_avg:39.69ms
step:2194/2330 train_time:87096ms step_avg:39.70ms
step:2195/2330 train_time:87119ms step_avg:39.69ms
step:2196/2330 train_time:87175ms step_avg:39.70ms
step:2197/2330 train_time:87198ms step_avg:39.69ms
step:2198/2330 train_time:87255ms step_avg:39.70ms
step:2199/2330 train_time:87277ms step_avg:39.69ms
step:2200/2330 train_time:87334ms step_avg:39.70ms
step:2201/2330 train_time:87356ms step_avg:39.69ms
step:2202/2330 train_time:87413ms step_avg:39.70ms
step:2203/2330 train_time:87436ms step_avg:39.69ms
step:2204/2330 train_time:87493ms step_avg:39.70ms
step:2205/2330 train_time:87515ms step_avg:39.69ms
step:2206/2330 train_time:87573ms step_avg:39.70ms
step:2207/2330 train_time:87596ms step_avg:39.69ms
step:2208/2330 train_time:87653ms step_avg:39.70ms
step:2209/2330 train_time:87675ms step_avg:39.69ms
step:2210/2330 train_time:87732ms step_avg:39.70ms
step:2211/2330 train_time:87754ms step_avg:39.69ms
step:2212/2330 train_time:87811ms step_avg:39.70ms
step:2213/2330 train_time:87833ms step_avg:39.69ms
step:2214/2330 train_time:87890ms step_avg:39.70ms
step:2215/2330 train_time:87912ms step_avg:39.69ms
step:2216/2330 train_time:87970ms step_avg:39.70ms
step:2217/2330 train_time:87992ms step_avg:39.69ms
step:2218/2330 train_time:88049ms step_avg:39.70ms
step:2219/2330 train_time:88071ms step_avg:39.69ms
step:2220/2330 train_time:88128ms step_avg:39.70ms
step:2221/2330 train_time:88150ms step_avg:39.69ms
step:2222/2330 train_time:88207ms step_avg:39.70ms
step:2223/2330 train_time:88229ms step_avg:39.69ms
step:2224/2330 train_time:88286ms step_avg:39.70ms
step:2225/2330 train_time:88308ms step_avg:39.69ms
step:2226/2330 train_time:88365ms step_avg:39.70ms
step:2227/2330 train_time:88387ms step_avg:39.69ms
step:2228/2330 train_time:88445ms step_avg:39.70ms
step:2229/2330 train_time:88467ms step_avg:39.69ms
step:2230/2330 train_time:88524ms step_avg:39.70ms
step:2231/2330 train_time:88546ms step_avg:39.69ms
step:2232/2330 train_time:88603ms step_avg:39.70ms
step:2233/2330 train_time:88626ms step_avg:39.69ms
step:2234/2330 train_time:88682ms step_avg:39.70ms
step:2235/2330 train_time:88705ms step_avg:39.69ms
step:2236/2330 train_time:88761ms step_avg:39.70ms
step:2237/2330 train_time:88784ms step_avg:39.69ms
step:2238/2330 train_time:88840ms step_avg:39.70ms
step:2239/2330 train_time:88863ms step_avg:39.69ms
step:2240/2330 train_time:88919ms step_avg:39.70ms
step:2241/2330 train_time:88943ms step_avg:39.69ms
step:2242/2330 train_time:88999ms step_avg:39.70ms
step:2243/2330 train_time:89023ms step_avg:39.69ms
step:2244/2330 train_time:89079ms step_avg:39.70ms
step:2245/2330 train_time:89102ms step_avg:39.69ms
step:2246/2330 train_time:89158ms step_avg:39.70ms
step:2247/2330 train_time:89181ms step_avg:39.69ms
step:2248/2330 train_time:89238ms step_avg:39.70ms
step:2249/2330 train_time:89261ms step_avg:39.69ms
step:2250/2330 train_time:89318ms step_avg:39.70ms
step:2250/2330 val_loss:5.1487 train_time:89415ms step_avg:39.74ms
step:2251/2330 train_time:89426ms step_avg:39.73ms
step:2252/2330 train_time:89438ms step_avg:39.71ms
step:2253/2330 train_time:89447ms step_avg:39.70ms
step:2254/2330 train_time:89477ms step_avg:39.70ms
step:2255/2330 train_time:89499ms step_avg:39.69ms
step:2256/2330 train_time:89555ms step_avg:39.70ms
step:2257/2330 train_time:89576ms step_avg:39.69ms
step:2258/2330 train_time:89632ms step_avg:39.70ms
step:2259/2330 train_time:89654ms step_avg:39.69ms
step:2260/2330 train_time:89713ms step_avg:39.70ms
step:2261/2330 train_time:89738ms step_avg:39.69ms
step:2262/2330 train_time:89799ms step_avg:39.70ms
step:2263/2330 train_time:89823ms step_avg:39.69ms
step:2264/2330 train_time:89881ms step_avg:39.70ms
step:2265/2330 train_time:89904ms step_avg:39.69ms
step:2266/2330 train_time:89959ms step_avg:39.70ms
step:2267/2330 train_time:89982ms step_avg:39.69ms
step:2268/2330 train_time:90038ms step_avg:39.70ms
step:2269/2330 train_time:90060ms step_avg:39.69ms
step:2270/2330 train_time:90118ms step_avg:39.70ms
step:2271/2330 train_time:90140ms step_avg:39.69ms
step:2272/2330 train_time:90196ms step_avg:39.70ms
step:2273/2330 train_time:90219ms step_avg:39.69ms
step:2274/2330 train_time:90274ms step_avg:39.70ms
step:2275/2330 train_time:90296ms step_avg:39.69ms
step:2276/2330 train_time:90353ms step_avg:39.70ms
step:2277/2330 train_time:90376ms step_avg:39.69ms
step:2278/2330 train_time:90433ms step_avg:39.70ms
step:2279/2330 train_time:90455ms step_avg:39.69ms
step:2280/2330 train_time:90511ms step_avg:39.70ms
step:2281/2330 train_time:90533ms step_avg:39.69ms
step:2282/2330 train_time:90589ms step_avg:39.70ms
step:2283/2330 train_time:90611ms step_avg:39.69ms
step:2284/2330 train_time:90669ms step_avg:39.70ms
step:2285/2330 train_time:90691ms step_avg:39.69ms
step:2286/2330 train_time:90749ms step_avg:39.70ms
step:2287/2330 train_time:90771ms step_avg:39.69ms
step:2288/2330 train_time:90829ms step_avg:39.70ms
step:2289/2330 train_time:90852ms step_avg:39.69ms
step:2290/2330 train_time:90909ms step_avg:39.70ms
step:2291/2330 train_time:90931ms step_avg:39.69ms
step:2292/2330 train_time:90988ms step_avg:39.70ms
step:2293/2330 train_time:91010ms step_avg:39.69ms
step:2294/2330 train_time:91067ms step_avg:39.70ms
step:2295/2330 train_time:91089ms step_avg:39.69ms
step:2296/2330 train_time:91145ms step_avg:39.70ms
step:2297/2330 train_time:91168ms step_avg:39.69ms
step:2298/2330 train_time:91224ms step_avg:39.70ms
step:2299/2330 train_time:91247ms step_avg:39.69ms
step:2300/2330 train_time:91304ms step_avg:39.70ms
step:2301/2330 train_time:91327ms step_avg:39.69ms
step:2302/2330 train_time:91383ms step_avg:39.70ms
step:2303/2330 train_time:91406ms step_avg:39.69ms
step:2304/2330 train_time:91463ms step_avg:39.70ms
step:2305/2330 train_time:91486ms step_avg:39.69ms
step:2306/2330 train_time:91542ms step_avg:39.70ms
step:2307/2330 train_time:91564ms step_avg:39.69ms
step:2308/2330 train_time:91621ms step_avg:39.70ms
step:2309/2330 train_time:91643ms step_avg:39.69ms
step:2310/2330 train_time:91700ms step_avg:39.70ms
step:2311/2330 train_time:91723ms step_avg:39.69ms
step:2312/2330 train_time:91780ms step_avg:39.70ms
step:2313/2330 train_time:91804ms step_avg:39.69ms
step:2314/2330 train_time:91861ms step_avg:39.70ms
step:2315/2330 train_time:91883ms step_avg:39.69ms
step:2316/2330 train_time:91940ms step_avg:39.70ms
step:2317/2330 train_time:91963ms step_avg:39.69ms
step:2318/2330 train_time:92020ms step_avg:39.70ms
step:2319/2330 train_time:92043ms step_avg:39.69ms
step:2320/2330 train_time:92099ms step_avg:39.70ms
step:2321/2330 train_time:92121ms step_avg:39.69ms
step:2322/2330 train_time:92178ms step_avg:39.70ms
step:2323/2330 train_time:92200ms step_avg:39.69ms
step:2324/2330 train_time:92257ms step_avg:39.70ms
step:2325/2330 train_time:92280ms step_avg:39.69ms
step:2326/2330 train_time:92337ms step_avg:39.70ms
step:2327/2330 train_time:92360ms step_avg:39.69ms
step:2328/2330 train_time:92416ms step_avg:39.70ms
step:2329/2330 train_time:92438ms step_avg:39.69ms
step:2330/2330 train_time:92495ms step_avg:39.70ms
step:2330/2330 val_loss:5.1474 train_time:92591ms step_avg:39.74ms
peak memory allocated: 29226 MiB reserved: 38888 MiB
