import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = divide_by_norm(batched)
                # v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = divide_by_norm(batched_update_grads)
                # v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "gd_lr2e-1"
    os.makedirs("logs3", exist_ok=True)
    logfile = f"logs3/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
# embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=2e-1, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    return 1.0

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum_min + 0.5 * (momentum_max - momentum_min)

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs3/{run_id}", exist_ok=True)
            torch.save(log, f"logs3/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 07:45:22 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   31C    P0             111W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   28C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   28C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   29C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   28C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   31C    P0             114W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:83ms step_avg:83.13ms
step:2/2330 train_time:177ms step_avg:88.65ms
step:3/2330 train_time:198ms step_avg:65.91ms
step:4/2330 train_time:227ms step_avg:56.87ms
step:5/2330 train_time:284ms step_avg:56.87ms
step:6/2330 train_time:345ms step_avg:57.50ms
step:7/2330 train_time:403ms step_avg:57.52ms
step:8/2330 train_time:464ms step_avg:58.04ms
step:9/2330 train_time:522ms step_avg:57.97ms
step:10/2330 train_time:583ms step_avg:58.27ms
step:11/2330 train_time:640ms step_avg:58.21ms
step:12/2330 train_time:702ms step_avg:58.51ms
step:13/2330 train_time:759ms step_avg:58.42ms
step:14/2330 train_time:821ms step_avg:58.62ms
step:15/2330 train_time:878ms step_avg:58.52ms
step:16/2330 train_time:939ms step_avg:58.69ms
step:17/2330 train_time:997ms step_avg:58.63ms
step:18/2330 train_time:1061ms step_avg:58.94ms
step:19/2330 train_time:1122ms step_avg:59.07ms
step:20/2330 train_time:1187ms step_avg:59.37ms
step:21/2330 train_time:1246ms step_avg:59.34ms
step:22/2330 train_time:1309ms step_avg:59.49ms
step:23/2330 train_time:1367ms step_avg:59.44ms
step:24/2330 train_time:1428ms step_avg:59.50ms
step:25/2330 train_time:1486ms step_avg:59.43ms
step:26/2330 train_time:1548ms step_avg:59.52ms
step:27/2330 train_time:1605ms step_avg:59.46ms
step:28/2330 train_time:1668ms step_avg:59.56ms
step:29/2330 train_time:1725ms step_avg:59.49ms
step:30/2330 train_time:1786ms step_avg:59.54ms
step:31/2330 train_time:1844ms step_avg:59.48ms
step:32/2330 train_time:1905ms step_avg:59.53ms
step:33/2330 train_time:1963ms step_avg:59.49ms
step:34/2330 train_time:2025ms step_avg:59.57ms
step:35/2330 train_time:2085ms step_avg:59.56ms
step:36/2330 train_time:2148ms step_avg:59.66ms
step:37/2330 train_time:2207ms step_avg:59.66ms
step:38/2330 train_time:2269ms step_avg:59.72ms
step:39/2330 train_time:2328ms step_avg:59.69ms
step:40/2330 train_time:2389ms step_avg:59.72ms
step:41/2330 train_time:2446ms step_avg:59.67ms
step:42/2330 train_time:2508ms step_avg:59.72ms
step:43/2330 train_time:2566ms step_avg:59.67ms
step:44/2330 train_time:2627ms step_avg:59.71ms
step:45/2330 train_time:2684ms step_avg:59.65ms
step:46/2330 train_time:2745ms step_avg:59.68ms
step:47/2330 train_time:2803ms step_avg:59.64ms
step:48/2330 train_time:2864ms step_avg:59.67ms
step:49/2330 train_time:2921ms step_avg:59.62ms
step:50/2330 train_time:2983ms step_avg:59.66ms
step:51/2330 train_time:3041ms step_avg:59.63ms
step:52/2330 train_time:3103ms step_avg:59.68ms
step:53/2330 train_time:3162ms step_avg:59.66ms
step:54/2330 train_time:3224ms step_avg:59.70ms
step:55/2330 train_time:3282ms step_avg:59.68ms
step:56/2330 train_time:3344ms step_avg:59.72ms
step:57/2330 train_time:3403ms step_avg:59.70ms
step:58/2330 train_time:3465ms step_avg:59.74ms
step:59/2330 train_time:3522ms step_avg:59.70ms
step:60/2330 train_time:3585ms step_avg:59.74ms
step:61/2330 train_time:3642ms step_avg:59.71ms
step:62/2330 train_time:3703ms step_avg:59.73ms
step:63/2330 train_time:3761ms step_avg:59.71ms
step:64/2330 train_time:3823ms step_avg:59.73ms
step:65/2330 train_time:3881ms step_avg:59.70ms
step:66/2330 train_time:3942ms step_avg:59.73ms
step:67/2330 train_time:4000ms step_avg:59.70ms
step:68/2330 train_time:4062ms step_avg:59.73ms
step:69/2330 train_time:4120ms step_avg:59.71ms
step:70/2330 train_time:4182ms step_avg:59.74ms
step:71/2330 train_time:4240ms step_avg:59.72ms
step:72/2330 train_time:4303ms step_avg:59.76ms
step:73/2330 train_time:4361ms step_avg:59.74ms
step:74/2330 train_time:4424ms step_avg:59.78ms
step:75/2330 train_time:4482ms step_avg:59.76ms
step:76/2330 train_time:4543ms step_avg:59.78ms
step:77/2330 train_time:4602ms step_avg:59.76ms
step:78/2330 train_time:4663ms step_avg:59.78ms
step:79/2330 train_time:4721ms step_avg:59.76ms
step:80/2330 train_time:4782ms step_avg:59.78ms
step:81/2330 train_time:4840ms step_avg:59.75ms
step:82/2330 train_time:4902ms step_avg:59.78ms
step:83/2330 train_time:4960ms step_avg:59.75ms
step:84/2330 train_time:5021ms step_avg:59.78ms
step:85/2330 train_time:5079ms step_avg:59.76ms
step:86/2330 train_time:5141ms step_avg:59.78ms
step:87/2330 train_time:5199ms step_avg:59.76ms
step:88/2330 train_time:5261ms step_avg:59.78ms
step:89/2330 train_time:5319ms step_avg:59.76ms
step:90/2330 train_time:5381ms step_avg:59.79ms
step:91/2330 train_time:5439ms step_avg:59.77ms
step:92/2330 train_time:5502ms step_avg:59.80ms
step:93/2330 train_time:5559ms step_avg:59.78ms
step:94/2330 train_time:5621ms step_avg:59.79ms
step:95/2330 train_time:5678ms step_avg:59.77ms
step:96/2330 train_time:5739ms step_avg:59.79ms
step:97/2330 train_time:5798ms step_avg:59.77ms
step:98/2330 train_time:5859ms step_avg:59.79ms
step:99/2330 train_time:5917ms step_avg:59.76ms
step:100/2330 train_time:5978ms step_avg:59.78ms
step:101/2330 train_time:6035ms step_avg:59.76ms
step:102/2330 train_time:6097ms step_avg:59.77ms
step:103/2330 train_time:6156ms step_avg:59.76ms
step:104/2330 train_time:6217ms step_avg:59.78ms
step:105/2330 train_time:6275ms step_avg:59.76ms
step:106/2330 train_time:6337ms step_avg:59.78ms
step:107/2330 train_time:6395ms step_avg:59.77ms
step:108/2330 train_time:6457ms step_avg:59.79ms
step:109/2330 train_time:6515ms step_avg:59.77ms
step:110/2330 train_time:6577ms step_avg:59.79ms
step:111/2330 train_time:6634ms step_avg:59.77ms
step:112/2330 train_time:6696ms step_avg:59.79ms
step:113/2330 train_time:6755ms step_avg:59.78ms
step:114/2330 train_time:6817ms step_avg:59.79ms
step:115/2330 train_time:6874ms step_avg:59.77ms
step:116/2330 train_time:6935ms step_avg:59.79ms
step:117/2330 train_time:6995ms step_avg:59.78ms
step:118/2330 train_time:7056ms step_avg:59.80ms
step:119/2330 train_time:7114ms step_avg:59.78ms
step:120/2330 train_time:7175ms step_avg:59.79ms
step:121/2330 train_time:7233ms step_avg:59.77ms
step:122/2330 train_time:7294ms step_avg:59.79ms
step:123/2330 train_time:7353ms step_avg:59.78ms
step:124/2330 train_time:7414ms step_avg:59.79ms
step:125/2330 train_time:7472ms step_avg:59.77ms
step:126/2330 train_time:7533ms step_avg:59.78ms
step:127/2330 train_time:7591ms step_avg:59.78ms
step:128/2330 train_time:7654ms step_avg:59.80ms
step:129/2330 train_time:7713ms step_avg:59.79ms
step:130/2330 train_time:7774ms step_avg:59.80ms
step:131/2330 train_time:7832ms step_avg:59.78ms
step:132/2330 train_time:7893ms step_avg:59.80ms
step:133/2330 train_time:7951ms step_avg:59.78ms
step:134/2330 train_time:8013ms step_avg:59.80ms
step:135/2330 train_time:8070ms step_avg:59.78ms
step:136/2330 train_time:8132ms step_avg:59.80ms
step:137/2330 train_time:8190ms step_avg:59.78ms
step:138/2330 train_time:8252ms step_avg:59.80ms
step:139/2330 train_time:8311ms step_avg:59.79ms
step:140/2330 train_time:8372ms step_avg:59.80ms
step:141/2330 train_time:8430ms step_avg:59.79ms
step:142/2330 train_time:8491ms step_avg:59.80ms
step:143/2330 train_time:8550ms step_avg:59.79ms
step:144/2330 train_time:8611ms step_avg:59.80ms
step:145/2330 train_time:8669ms step_avg:59.79ms
step:146/2330 train_time:8731ms step_avg:59.80ms
step:147/2330 train_time:8789ms step_avg:59.79ms
step:148/2330 train_time:8851ms step_avg:59.80ms
step:149/2330 train_time:8909ms step_avg:59.79ms
step:150/2330 train_time:8971ms step_avg:59.81ms
step:151/2330 train_time:9029ms step_avg:59.79ms
step:152/2330 train_time:9091ms step_avg:59.81ms
step:153/2330 train_time:9149ms step_avg:59.79ms
step:154/2330 train_time:9211ms step_avg:59.81ms
step:155/2330 train_time:9269ms step_avg:59.80ms
step:156/2330 train_time:9330ms step_avg:59.81ms
step:157/2330 train_time:9388ms step_avg:59.80ms
step:158/2330 train_time:9450ms step_avg:59.81ms
step:159/2330 train_time:9508ms step_avg:59.80ms
step:160/2330 train_time:9570ms step_avg:59.81ms
step:161/2330 train_time:9627ms step_avg:59.80ms
step:162/2330 train_time:9689ms step_avg:59.81ms
step:163/2330 train_time:9747ms step_avg:59.80ms
step:164/2330 train_time:9809ms step_avg:59.81ms
step:165/2330 train_time:9867ms step_avg:59.80ms
step:166/2330 train_time:9930ms step_avg:59.82ms
step:167/2330 train_time:9988ms step_avg:59.81ms
step:168/2330 train_time:10050ms step_avg:59.82ms
step:169/2330 train_time:10108ms step_avg:59.81ms
step:170/2330 train_time:10170ms step_avg:59.82ms
step:171/2330 train_time:10228ms step_avg:59.82ms
step:172/2330 train_time:10291ms step_avg:59.83ms
step:173/2330 train_time:10349ms step_avg:59.82ms
step:174/2330 train_time:10411ms step_avg:59.83ms
step:175/2330 train_time:10468ms step_avg:59.82ms
step:176/2330 train_time:10531ms step_avg:59.84ms
step:177/2330 train_time:10589ms step_avg:59.82ms
step:178/2330 train_time:10651ms step_avg:59.83ms
step:179/2330 train_time:10708ms step_avg:59.82ms
step:180/2330 train_time:10770ms step_avg:59.83ms
step:181/2330 train_time:10828ms step_avg:59.82ms
step:182/2330 train_time:10890ms step_avg:59.84ms
step:183/2330 train_time:10948ms step_avg:59.83ms
step:184/2330 train_time:11010ms step_avg:59.84ms
step:185/2330 train_time:11068ms step_avg:59.83ms
step:186/2330 train_time:11130ms step_avg:59.84ms
step:187/2330 train_time:11188ms step_avg:59.83ms
step:188/2330 train_time:11251ms step_avg:59.84ms
step:189/2330 train_time:11309ms step_avg:59.84ms
step:190/2330 train_time:11371ms step_avg:59.85ms
step:191/2330 train_time:11429ms step_avg:59.84ms
step:192/2330 train_time:11491ms step_avg:59.85ms
step:193/2330 train_time:11548ms step_avg:59.84ms
step:194/2330 train_time:11610ms step_avg:59.85ms
step:195/2330 train_time:11668ms step_avg:59.84ms
step:196/2330 train_time:11730ms step_avg:59.85ms
step:197/2330 train_time:11789ms step_avg:59.84ms
step:198/2330 train_time:11851ms step_avg:59.85ms
step:199/2330 train_time:11909ms step_avg:59.85ms
step:200/2330 train_time:11971ms step_avg:59.85ms
step:201/2330 train_time:12028ms step_avg:59.84ms
step:202/2330 train_time:12090ms step_avg:59.85ms
step:203/2330 train_time:12149ms step_avg:59.84ms
step:204/2330 train_time:12211ms step_avg:59.86ms
step:205/2330 train_time:12269ms step_avg:59.85ms
step:206/2330 train_time:12331ms step_avg:59.86ms
step:207/2330 train_time:12389ms step_avg:59.85ms
step:208/2330 train_time:12451ms step_avg:59.86ms
step:209/2330 train_time:12509ms step_avg:59.85ms
step:210/2330 train_time:12571ms step_avg:59.86ms
step:211/2330 train_time:12629ms step_avg:59.85ms
step:212/2330 train_time:12691ms step_avg:59.86ms
step:213/2330 train_time:12749ms step_avg:59.85ms
step:214/2330 train_time:12811ms step_avg:59.87ms
step:215/2330 train_time:12869ms step_avg:59.86ms
step:216/2330 train_time:12931ms step_avg:59.86ms
step:217/2330 train_time:12988ms step_avg:59.85ms
step:218/2330 train_time:13050ms step_avg:59.86ms
step:219/2330 train_time:13108ms step_avg:59.86ms
step:220/2330 train_time:13170ms step_avg:59.86ms
step:221/2330 train_time:13228ms step_avg:59.86ms
step:222/2330 train_time:13290ms step_avg:59.86ms
step:223/2330 train_time:13348ms step_avg:59.86ms
step:224/2330 train_time:13410ms step_avg:59.87ms
step:225/2330 train_time:13469ms step_avg:59.86ms
step:226/2330 train_time:13531ms step_avg:59.87ms
step:227/2330 train_time:13589ms step_avg:59.86ms
step:228/2330 train_time:13651ms step_avg:59.87ms
step:229/2330 train_time:13709ms step_avg:59.87ms
step:230/2330 train_time:13771ms step_avg:59.88ms
step:231/2330 train_time:13830ms step_avg:59.87ms
step:232/2330 train_time:13891ms step_avg:59.88ms
step:233/2330 train_time:13950ms step_avg:59.87ms
step:234/2330 train_time:14012ms step_avg:59.88ms
step:235/2330 train_time:14070ms step_avg:59.87ms
step:236/2330 train_time:14132ms step_avg:59.88ms
step:237/2330 train_time:14190ms step_avg:59.87ms
step:238/2330 train_time:14252ms step_avg:59.88ms
step:239/2330 train_time:14310ms step_avg:59.87ms
step:240/2330 train_time:14371ms step_avg:59.88ms
step:241/2330 train_time:14430ms step_avg:59.87ms
step:242/2330 train_time:14491ms step_avg:59.88ms
step:243/2330 train_time:14550ms step_avg:59.87ms
step:244/2330 train_time:14612ms step_avg:59.88ms
step:245/2330 train_time:14670ms step_avg:59.88ms
step:246/2330 train_time:14732ms step_avg:59.88ms
step:247/2330 train_time:14790ms step_avg:59.88ms
step:248/2330 train_time:14852ms step_avg:59.89ms
step:249/2330 train_time:14911ms step_avg:59.88ms
step:250/2330 train_time:14972ms step_avg:59.89ms
step:250/2330 val_loss:4.8750 train_time:15042ms step_avg:60.17ms
step:251/2330 train_time:15063ms step_avg:60.01ms
step:252/2330 train_time:15093ms step_avg:59.89ms
step:253/2330 train_time:15152ms step_avg:59.89ms
step:254/2330 train_time:15220ms step_avg:59.92ms
step:255/2330 train_time:15282ms step_avg:59.93ms
step:256/2330 train_time:15345ms step_avg:59.94ms
step:257/2330 train_time:15403ms step_avg:59.93ms
step:258/2330 train_time:15464ms step_avg:59.94ms
step:259/2330 train_time:15522ms step_avg:59.93ms
step:260/2330 train_time:15583ms step_avg:59.94ms
step:261/2330 train_time:15640ms step_avg:59.92ms
step:262/2330 train_time:15701ms step_avg:59.93ms
step:263/2330 train_time:15758ms step_avg:59.92ms
step:264/2330 train_time:15820ms step_avg:59.92ms
step:265/2330 train_time:15877ms step_avg:59.91ms
step:266/2330 train_time:15938ms step_avg:59.92ms
step:267/2330 train_time:15997ms step_avg:59.92ms
step:268/2330 train_time:16062ms step_avg:59.93ms
step:269/2330 train_time:16122ms step_avg:59.93ms
step:270/2330 train_time:16186ms step_avg:59.95ms
step:271/2330 train_time:16245ms step_avg:59.94ms
step:272/2330 train_time:16307ms step_avg:59.95ms
step:273/2330 train_time:16365ms step_avg:59.95ms
step:274/2330 train_time:16426ms step_avg:59.95ms
step:275/2330 train_time:16485ms step_avg:59.95ms
step:276/2330 train_time:16546ms step_avg:59.95ms
step:277/2330 train_time:16604ms step_avg:59.94ms
step:278/2330 train_time:16665ms step_avg:59.95ms
step:279/2330 train_time:16724ms step_avg:59.94ms
step:280/2330 train_time:16785ms step_avg:59.95ms
step:281/2330 train_time:16843ms step_avg:59.94ms
step:282/2330 train_time:16904ms step_avg:59.94ms
step:283/2330 train_time:16962ms step_avg:59.94ms
step:284/2330 train_time:17025ms step_avg:59.95ms
step:285/2330 train_time:17086ms step_avg:59.95ms
step:286/2330 train_time:17149ms step_avg:59.96ms
step:287/2330 train_time:17208ms step_avg:59.96ms
step:288/2330 train_time:17271ms step_avg:59.97ms
step:289/2330 train_time:17329ms step_avg:59.96ms
step:290/2330 train_time:17391ms step_avg:59.97ms
step:291/2330 train_time:17449ms step_avg:59.96ms
step:292/2330 train_time:17511ms step_avg:59.97ms
step:293/2330 train_time:17569ms step_avg:59.96ms
step:294/2330 train_time:17631ms step_avg:59.97ms
step:295/2330 train_time:17689ms step_avg:59.96ms
step:296/2330 train_time:17751ms step_avg:59.97ms
step:297/2330 train_time:17809ms step_avg:59.96ms
step:298/2330 train_time:17870ms step_avg:59.97ms
step:299/2330 train_time:17928ms step_avg:59.96ms
step:300/2330 train_time:17990ms step_avg:59.97ms
step:301/2330 train_time:18048ms step_avg:59.96ms
step:302/2330 train_time:18111ms step_avg:59.97ms
step:303/2330 train_time:18169ms step_avg:59.96ms
step:304/2330 train_time:18232ms step_avg:59.97ms
step:305/2330 train_time:18290ms step_avg:59.97ms
step:306/2330 train_time:18352ms step_avg:59.97ms
step:307/2330 train_time:18410ms step_avg:59.97ms
step:308/2330 train_time:18473ms step_avg:59.98ms
step:309/2330 train_time:18531ms step_avg:59.97ms
step:310/2330 train_time:18593ms step_avg:59.98ms
step:311/2330 train_time:18651ms step_avg:59.97ms
step:312/2330 train_time:18713ms step_avg:59.98ms
step:313/2330 train_time:18771ms step_avg:59.97ms
step:314/2330 train_time:18833ms step_avg:59.98ms
step:315/2330 train_time:18891ms step_avg:59.97ms
step:316/2330 train_time:18953ms step_avg:59.98ms
step:317/2330 train_time:19012ms step_avg:59.97ms
step:318/2330 train_time:19075ms step_avg:59.98ms
step:319/2330 train_time:19133ms step_avg:59.98ms
step:320/2330 train_time:19195ms step_avg:59.98ms
step:321/2330 train_time:19253ms step_avg:59.98ms
step:322/2330 train_time:19315ms step_avg:59.98ms
step:323/2330 train_time:19373ms step_avg:59.98ms
step:324/2330 train_time:19435ms step_avg:59.98ms
step:325/2330 train_time:19493ms step_avg:59.98ms
step:326/2330 train_time:19554ms step_avg:59.98ms
step:327/2330 train_time:19613ms step_avg:59.98ms
step:328/2330 train_time:19674ms step_avg:59.98ms
step:329/2330 train_time:19732ms step_avg:59.98ms
step:330/2330 train_time:19795ms step_avg:59.98ms
step:331/2330 train_time:19853ms step_avg:59.98ms
step:332/2330 train_time:19914ms step_avg:59.98ms
step:333/2330 train_time:19972ms step_avg:59.98ms
step:334/2330 train_time:20035ms step_avg:59.98ms
step:335/2330 train_time:20094ms step_avg:59.98ms
step:336/2330 train_time:20155ms step_avg:59.99ms
step:337/2330 train_time:20215ms step_avg:59.98ms
step:338/2330 train_time:20277ms step_avg:59.99ms
step:339/2330 train_time:20335ms step_avg:59.99ms
step:340/2330 train_time:20397ms step_avg:59.99ms
step:341/2330 train_time:20455ms step_avg:59.98ms
step:342/2330 train_time:20516ms step_avg:59.99ms
step:343/2330 train_time:20574ms step_avg:59.98ms
step:344/2330 train_time:20636ms step_avg:59.99ms
step:345/2330 train_time:20695ms step_avg:59.98ms
step:346/2330 train_time:20756ms step_avg:59.99ms
step:347/2330 train_time:20815ms step_avg:59.99ms
step:348/2330 train_time:20876ms step_avg:59.99ms
step:349/2330 train_time:20935ms step_avg:59.99ms
step:350/2330 train_time:20998ms step_avg:59.99ms
step:351/2330 train_time:21057ms step_avg:59.99ms
step:352/2330 train_time:21120ms step_avg:60.00ms
step:353/2330 train_time:21178ms step_avg:59.99ms
step:354/2330 train_time:21241ms step_avg:60.00ms
step:355/2330 train_time:21299ms step_avg:60.00ms
step:356/2330 train_time:21360ms step_avg:60.00ms
step:357/2330 train_time:21419ms step_avg:60.00ms
step:358/2330 train_time:21481ms step_avg:60.00ms
step:359/2330 train_time:21539ms step_avg:60.00ms
step:360/2330 train_time:21600ms step_avg:60.00ms
step:361/2330 train_time:21658ms step_avg:59.99ms
step:362/2330 train_time:21720ms step_avg:60.00ms
step:363/2330 train_time:21779ms step_avg:60.00ms
step:364/2330 train_time:21841ms step_avg:60.00ms
step:365/2330 train_time:21899ms step_avg:60.00ms
step:366/2330 train_time:21961ms step_avg:60.00ms
step:367/2330 train_time:22019ms step_avg:60.00ms
step:368/2330 train_time:22083ms step_avg:60.01ms
step:369/2330 train_time:22140ms step_avg:60.00ms
step:370/2330 train_time:22202ms step_avg:60.00ms
step:371/2330 train_time:22259ms step_avg:60.00ms
step:372/2330 train_time:22322ms step_avg:60.01ms
step:373/2330 train_time:22380ms step_avg:60.00ms
step:374/2330 train_time:22442ms step_avg:60.01ms
step:375/2330 train_time:22500ms step_avg:60.00ms
step:376/2330 train_time:22561ms step_avg:60.00ms
step:377/2330 train_time:22620ms step_avg:60.00ms
step:378/2330 train_time:22681ms step_avg:60.00ms
step:379/2330 train_time:22739ms step_avg:60.00ms
step:380/2330 train_time:22801ms step_avg:60.00ms
step:381/2330 train_time:22859ms step_avg:60.00ms
step:382/2330 train_time:22920ms step_avg:60.00ms
step:383/2330 train_time:22979ms step_avg:60.00ms
step:384/2330 train_time:23041ms step_avg:60.00ms
step:385/2330 train_time:23100ms step_avg:60.00ms
step:386/2330 train_time:23162ms step_avg:60.00ms
step:387/2330 train_time:23219ms step_avg:60.00ms
step:388/2330 train_time:23281ms step_avg:60.00ms
step:389/2330 train_time:23339ms step_avg:60.00ms
step:390/2330 train_time:23402ms step_avg:60.00ms
step:391/2330 train_time:23460ms step_avg:60.00ms
step:392/2330 train_time:23523ms step_avg:60.01ms
step:393/2330 train_time:23581ms step_avg:60.00ms
step:394/2330 train_time:23642ms step_avg:60.00ms
step:395/2330 train_time:23700ms step_avg:60.00ms
step:396/2330 train_time:23761ms step_avg:60.00ms
step:397/2330 train_time:23820ms step_avg:60.00ms
step:398/2330 train_time:23881ms step_avg:60.00ms
step:399/2330 train_time:23940ms step_avg:60.00ms
step:400/2330 train_time:24002ms step_avg:60.00ms
step:401/2330 train_time:24061ms step_avg:60.00ms
step:402/2330 train_time:24122ms step_avg:60.00ms
step:403/2330 train_time:24180ms step_avg:60.00ms
step:404/2330 train_time:24242ms step_avg:60.00ms
step:405/2330 train_time:24300ms step_avg:60.00ms
step:406/2330 train_time:24362ms step_avg:60.01ms
step:407/2330 train_time:24420ms step_avg:60.00ms
step:408/2330 train_time:24484ms step_avg:60.01ms
step:409/2330 train_time:24542ms step_avg:60.00ms
step:410/2330 train_time:24604ms step_avg:60.01ms
step:411/2330 train_time:24661ms step_avg:60.00ms
step:412/2330 train_time:24723ms step_avg:60.01ms
step:413/2330 train_time:24782ms step_avg:60.01ms
step:414/2330 train_time:24844ms step_avg:60.01ms
step:415/2330 train_time:24903ms step_avg:60.01ms
step:416/2330 train_time:24965ms step_avg:60.01ms
step:417/2330 train_time:25023ms step_avg:60.01ms
step:418/2330 train_time:25086ms step_avg:60.01ms
step:419/2330 train_time:25144ms step_avg:60.01ms
step:420/2330 train_time:25206ms step_avg:60.01ms
step:421/2330 train_time:25264ms step_avg:60.01ms
step:422/2330 train_time:25326ms step_avg:60.01ms
step:423/2330 train_time:25384ms step_avg:60.01ms
step:424/2330 train_time:25446ms step_avg:60.01ms
step:425/2330 train_time:25504ms step_avg:60.01ms
step:426/2330 train_time:25566ms step_avg:60.01ms
step:427/2330 train_time:25624ms step_avg:60.01ms
step:428/2330 train_time:25686ms step_avg:60.01ms
step:429/2330 train_time:25745ms step_avg:60.01ms
step:430/2330 train_time:25807ms step_avg:60.02ms
step:431/2330 train_time:25866ms step_avg:60.01ms
step:432/2330 train_time:25928ms step_avg:60.02ms
step:433/2330 train_time:25987ms step_avg:60.02ms
step:434/2330 train_time:26050ms step_avg:60.02ms
step:435/2330 train_time:26108ms step_avg:60.02ms
step:436/2330 train_time:26170ms step_avg:60.02ms
step:437/2330 train_time:26227ms step_avg:60.02ms
step:438/2330 train_time:26290ms step_avg:60.02ms
step:439/2330 train_time:26348ms step_avg:60.02ms
step:440/2330 train_time:26410ms step_avg:60.02ms
step:441/2330 train_time:26468ms step_avg:60.02ms
step:442/2330 train_time:26530ms step_avg:60.02ms
step:443/2330 train_time:26589ms step_avg:60.02ms
step:444/2330 train_time:26651ms step_avg:60.02ms
step:445/2330 train_time:26709ms step_avg:60.02ms
step:446/2330 train_time:26771ms step_avg:60.02ms
step:447/2330 train_time:26829ms step_avg:60.02ms
step:448/2330 train_time:26891ms step_avg:60.03ms
step:449/2330 train_time:26949ms step_avg:60.02ms
step:450/2330 train_time:27012ms step_avg:60.03ms
step:451/2330 train_time:27071ms step_avg:60.02ms
step:452/2330 train_time:27133ms step_avg:60.03ms
step:453/2330 train_time:27191ms step_avg:60.02ms
step:454/2330 train_time:27253ms step_avg:60.03ms
step:455/2330 train_time:27311ms step_avg:60.02ms
step:456/2330 train_time:27374ms step_avg:60.03ms
step:457/2330 train_time:27431ms step_avg:60.03ms
step:458/2330 train_time:27494ms step_avg:60.03ms
step:459/2330 train_time:27552ms step_avg:60.03ms
step:460/2330 train_time:27614ms step_avg:60.03ms
step:461/2330 train_time:27672ms step_avg:60.03ms
step:462/2330 train_time:27735ms step_avg:60.03ms
step:463/2330 train_time:27794ms step_avg:60.03ms
step:464/2330 train_time:27855ms step_avg:60.03ms
step:465/2330 train_time:27914ms step_avg:60.03ms
step:466/2330 train_time:27976ms step_avg:60.03ms
step:467/2330 train_time:28035ms step_avg:60.03ms
step:468/2330 train_time:28097ms step_avg:60.04ms
step:469/2330 train_time:28156ms step_avg:60.03ms
step:470/2330 train_time:28218ms step_avg:60.04ms
step:471/2330 train_time:28276ms step_avg:60.03ms
step:472/2330 train_time:28339ms step_avg:60.04ms
step:473/2330 train_time:28397ms step_avg:60.04ms
step:474/2330 train_time:28460ms step_avg:60.04ms
step:475/2330 train_time:28518ms step_avg:60.04ms
step:476/2330 train_time:28579ms step_avg:60.04ms
step:477/2330 train_time:28638ms step_avg:60.04ms
step:478/2330 train_time:28700ms step_avg:60.04ms
step:479/2330 train_time:28759ms step_avg:60.04ms
step:480/2330 train_time:28821ms step_avg:60.04ms
step:481/2330 train_time:28879ms step_avg:60.04ms
step:482/2330 train_time:28941ms step_avg:60.04ms
step:483/2330 train_time:29000ms step_avg:60.04ms
step:484/2330 train_time:29062ms step_avg:60.05ms
step:485/2330 train_time:29119ms step_avg:60.04ms
step:486/2330 train_time:29181ms step_avg:60.04ms
step:487/2330 train_time:29241ms step_avg:60.04ms
step:488/2330 train_time:29302ms step_avg:60.04ms
step:489/2330 train_time:29359ms step_avg:60.04ms
step:490/2330 train_time:29422ms step_avg:60.04ms
step:491/2330 train_time:29481ms step_avg:60.04ms
step:492/2330 train_time:29543ms step_avg:60.05ms
step:493/2330 train_time:29601ms step_avg:60.04ms
step:494/2330 train_time:29663ms step_avg:60.05ms
step:495/2330 train_time:29721ms step_avg:60.04ms
step:496/2330 train_time:29783ms step_avg:60.05ms
step:497/2330 train_time:29841ms step_avg:60.04ms
step:498/2330 train_time:29903ms step_avg:60.05ms
step:499/2330 train_time:29962ms step_avg:60.04ms
step:500/2330 train_time:30024ms step_avg:60.05ms
step:500/2330 val_loss:4.3695 train_time:30094ms step_avg:60.19ms
step:501/2330 train_time:30117ms step_avg:60.11ms
step:502/2330 train_time:30146ms step_avg:60.05ms
step:503/2330 train_time:30204ms step_avg:60.05ms
step:504/2330 train_time:30274ms step_avg:60.07ms
step:505/2330 train_time:30334ms step_avg:60.07ms
step:506/2330 train_time:30395ms step_avg:60.07ms
step:507/2330 train_time:30455ms step_avg:60.07ms
step:508/2330 train_time:30518ms step_avg:60.08ms
step:509/2330 train_time:30576ms step_avg:60.07ms
step:510/2330 train_time:30637ms step_avg:60.07ms
step:511/2330 train_time:30694ms step_avg:60.07ms
step:512/2330 train_time:30756ms step_avg:60.07ms
step:513/2330 train_time:30813ms step_avg:60.06ms
step:514/2330 train_time:30874ms step_avg:60.07ms
step:515/2330 train_time:30932ms step_avg:60.06ms
step:516/2330 train_time:30993ms step_avg:60.06ms
step:517/2330 train_time:31052ms step_avg:60.06ms
step:518/2330 train_time:31115ms step_avg:60.07ms
step:519/2330 train_time:31176ms step_avg:60.07ms
step:520/2330 train_time:31239ms step_avg:60.08ms
step:521/2330 train_time:31298ms step_avg:60.07ms
step:522/2330 train_time:31360ms step_avg:60.08ms
step:523/2330 train_time:31420ms step_avg:60.08ms
step:524/2330 train_time:31482ms step_avg:60.08ms
step:525/2330 train_time:31539ms step_avg:60.07ms
step:526/2330 train_time:31600ms step_avg:60.08ms
step:527/2330 train_time:31658ms step_avg:60.07ms
step:528/2330 train_time:31719ms step_avg:60.07ms
step:529/2330 train_time:31777ms step_avg:60.07ms
step:530/2330 train_time:31839ms step_avg:60.07ms
step:531/2330 train_time:31896ms step_avg:60.07ms
step:532/2330 train_time:31957ms step_avg:60.07ms
step:533/2330 train_time:32016ms step_avg:60.07ms
step:534/2330 train_time:32078ms step_avg:60.07ms
step:535/2330 train_time:32137ms step_avg:60.07ms
step:536/2330 train_time:32200ms step_avg:60.08ms
step:537/2330 train_time:32259ms step_avg:60.07ms
step:538/2330 train_time:32322ms step_avg:60.08ms
step:539/2330 train_time:32381ms step_avg:60.08ms
step:540/2330 train_time:32443ms step_avg:60.08ms
step:541/2330 train_time:32501ms step_avg:60.08ms
step:542/2330 train_time:32564ms step_avg:60.08ms
step:543/2330 train_time:32621ms step_avg:60.08ms
step:544/2330 train_time:32682ms step_avg:60.08ms
step:545/2330 train_time:32740ms step_avg:60.07ms
step:546/2330 train_time:32802ms step_avg:60.08ms
step:547/2330 train_time:32860ms step_avg:60.07ms
step:548/2330 train_time:32922ms step_avg:60.08ms
step:549/2330 train_time:32981ms step_avg:60.07ms
step:550/2330 train_time:33043ms step_avg:60.08ms
step:551/2330 train_time:33101ms step_avg:60.07ms
step:552/2330 train_time:33165ms step_avg:60.08ms
step:553/2330 train_time:33224ms step_avg:60.08ms
step:554/2330 train_time:33286ms step_avg:60.08ms
step:555/2330 train_time:33344ms step_avg:60.08ms
step:556/2330 train_time:33406ms step_avg:60.08ms
step:557/2330 train_time:33464ms step_avg:60.08ms
step:558/2330 train_time:33526ms step_avg:60.08ms
step:559/2330 train_time:33584ms step_avg:60.08ms
step:560/2330 train_time:33645ms step_avg:60.08ms
step:561/2330 train_time:33703ms step_avg:60.08ms
step:562/2330 train_time:33765ms step_avg:60.08ms
step:563/2330 train_time:33824ms step_avg:60.08ms
step:564/2330 train_time:33885ms step_avg:60.08ms
step:565/2330 train_time:33944ms step_avg:60.08ms
step:566/2330 train_time:34006ms step_avg:60.08ms
step:567/2330 train_time:34065ms step_avg:60.08ms
step:568/2330 train_time:34127ms step_avg:60.08ms
step:569/2330 train_time:34185ms step_avg:60.08ms
step:570/2330 train_time:34247ms step_avg:60.08ms
step:571/2330 train_time:34305ms step_avg:60.08ms
step:572/2330 train_time:34368ms step_avg:60.08ms
step:573/2330 train_time:34427ms step_avg:60.08ms
step:574/2330 train_time:34488ms step_avg:60.08ms
step:575/2330 train_time:34547ms step_avg:60.08ms
step:576/2330 train_time:34609ms step_avg:60.08ms
step:577/2330 train_time:34666ms step_avg:60.08ms
step:578/2330 train_time:34728ms step_avg:60.08ms
step:579/2330 train_time:34787ms step_avg:60.08ms
step:580/2330 train_time:34849ms step_avg:60.08ms
step:581/2330 train_time:34907ms step_avg:60.08ms
step:582/2330 train_time:34969ms step_avg:60.08ms
step:583/2330 train_time:35027ms step_avg:60.08ms
step:584/2330 train_time:35090ms step_avg:60.09ms
step:585/2330 train_time:35148ms step_avg:60.08ms
step:586/2330 train_time:35211ms step_avg:60.09ms
step:587/2330 train_time:35270ms step_avg:60.08ms
step:588/2330 train_time:35332ms step_avg:60.09ms
step:589/2330 train_time:35390ms step_avg:60.09ms
step:590/2330 train_time:35453ms step_avg:60.09ms
step:591/2330 train_time:35511ms step_avg:60.09ms
step:592/2330 train_time:35573ms step_avg:60.09ms
step:593/2330 train_time:35632ms step_avg:60.09ms
step:594/2330 train_time:35693ms step_avg:60.09ms
step:595/2330 train_time:35752ms step_avg:60.09ms
step:596/2330 train_time:35815ms step_avg:60.09ms
step:597/2330 train_time:35874ms step_avg:60.09ms
step:598/2330 train_time:35936ms step_avg:60.09ms
step:599/2330 train_time:35995ms step_avg:60.09ms
step:600/2330 train_time:36057ms step_avg:60.10ms
step:601/2330 train_time:36116ms step_avg:60.09ms
step:602/2330 train_time:36177ms step_avg:60.10ms
step:603/2330 train_time:36235ms step_avg:60.09ms
step:604/2330 train_time:36297ms step_avg:60.09ms
step:605/2330 train_time:36355ms step_avg:60.09ms
step:606/2330 train_time:36416ms step_avg:60.09ms
step:607/2330 train_time:36475ms step_avg:60.09ms
step:608/2330 train_time:36537ms step_avg:60.09ms
step:609/2330 train_time:36596ms step_avg:60.09ms
step:610/2330 train_time:36658ms step_avg:60.09ms
step:611/2330 train_time:36716ms step_avg:60.09ms
step:612/2330 train_time:36778ms step_avg:60.10ms
step:613/2330 train_time:36837ms step_avg:60.09ms
step:614/2330 train_time:36898ms step_avg:60.09ms
step:615/2330 train_time:36957ms step_avg:60.09ms
step:616/2330 train_time:37019ms step_avg:60.10ms
step:617/2330 train_time:37077ms step_avg:60.09ms
step:618/2330 train_time:37139ms step_avg:60.09ms
step:619/2330 train_time:37197ms step_avg:60.09ms
step:620/2330 train_time:37259ms step_avg:60.10ms
step:621/2330 train_time:37318ms step_avg:60.09ms
step:622/2330 train_time:37379ms step_avg:60.09ms
step:623/2330 train_time:37438ms step_avg:60.09ms
step:624/2330 train_time:37500ms step_avg:60.10ms
step:625/2330 train_time:37558ms step_avg:60.09ms
step:626/2330 train_time:37619ms step_avg:60.09ms
step:627/2330 train_time:37678ms step_avg:60.09ms
step:628/2330 train_time:37740ms step_avg:60.09ms
step:629/2330 train_time:37798ms step_avg:60.09ms
step:630/2330 train_time:37860ms step_avg:60.10ms
step:631/2330 train_time:37919ms step_avg:60.09ms
step:632/2330 train_time:37981ms step_avg:60.10ms
step:633/2330 train_time:38039ms step_avg:60.09ms
step:634/2330 train_time:38108ms step_avg:60.11ms
step:635/2330 train_time:38159ms step_avg:60.09ms
step:636/2330 train_time:38221ms step_avg:60.10ms
step:637/2330 train_time:38279ms step_avg:60.09ms
step:638/2330 train_time:38341ms step_avg:60.09ms
step:639/2330 train_time:38399ms step_avg:60.09ms
step:640/2330 train_time:38461ms step_avg:60.10ms
step:641/2330 train_time:38520ms step_avg:60.09ms
step:642/2330 train_time:38582ms step_avg:60.10ms
step:643/2330 train_time:38640ms step_avg:60.09ms
step:644/2330 train_time:38701ms step_avg:60.10ms
step:645/2330 train_time:38760ms step_avg:60.09ms
step:646/2330 train_time:38822ms step_avg:60.10ms
step:647/2330 train_time:38881ms step_avg:60.09ms
step:648/2330 train_time:38943ms step_avg:60.10ms
step:649/2330 train_time:39000ms step_avg:60.09ms
step:650/2330 train_time:39062ms step_avg:60.10ms
step:651/2330 train_time:39122ms step_avg:60.09ms
step:652/2330 train_time:39183ms step_avg:60.10ms
step:653/2330 train_time:39241ms step_avg:60.09ms
step:654/2330 train_time:39303ms step_avg:60.10ms
step:655/2330 train_time:39362ms step_avg:60.10ms
step:656/2330 train_time:39425ms step_avg:60.10ms
step:657/2330 train_time:39483ms step_avg:60.10ms
step:658/2330 train_time:39545ms step_avg:60.10ms
step:659/2330 train_time:39603ms step_avg:60.10ms
step:660/2330 train_time:39665ms step_avg:60.10ms
step:661/2330 train_time:39724ms step_avg:60.10ms
step:662/2330 train_time:39786ms step_avg:60.10ms
step:663/2330 train_time:39844ms step_avg:60.10ms
step:664/2330 train_time:39906ms step_avg:60.10ms
step:665/2330 train_time:39964ms step_avg:60.10ms
step:666/2330 train_time:40027ms step_avg:60.10ms
step:667/2330 train_time:40085ms step_avg:60.10ms
step:668/2330 train_time:40146ms step_avg:60.10ms
step:669/2330 train_time:40204ms step_avg:60.10ms
step:670/2330 train_time:40266ms step_avg:60.10ms
step:671/2330 train_time:40325ms step_avg:60.10ms
step:672/2330 train_time:40387ms step_avg:60.10ms
step:673/2330 train_time:40445ms step_avg:60.10ms
step:674/2330 train_time:40508ms step_avg:60.10ms
step:675/2330 train_time:40566ms step_avg:60.10ms
step:676/2330 train_time:40629ms step_avg:60.10ms
step:677/2330 train_time:40687ms step_avg:60.10ms
step:678/2330 train_time:40749ms step_avg:60.10ms
step:679/2330 train_time:40807ms step_avg:60.10ms
step:680/2330 train_time:40870ms step_avg:60.10ms
step:681/2330 train_time:40928ms step_avg:60.10ms
step:682/2330 train_time:40990ms step_avg:60.10ms
step:683/2330 train_time:41049ms step_avg:60.10ms
step:684/2330 train_time:41111ms step_avg:60.10ms
step:685/2330 train_time:41170ms step_avg:60.10ms
step:686/2330 train_time:41232ms step_avg:60.11ms
step:687/2330 train_time:41291ms step_avg:60.10ms
step:688/2330 train_time:41355ms step_avg:60.11ms
step:689/2330 train_time:41414ms step_avg:60.11ms
step:690/2330 train_time:41475ms step_avg:60.11ms
step:691/2330 train_time:41535ms step_avg:60.11ms
step:692/2330 train_time:41597ms step_avg:60.11ms
step:693/2330 train_time:41655ms step_avg:60.11ms
step:694/2330 train_time:41717ms step_avg:60.11ms
step:695/2330 train_time:41776ms step_avg:60.11ms
step:696/2330 train_time:41837ms step_avg:60.11ms
step:697/2330 train_time:41896ms step_avg:60.11ms
step:698/2330 train_time:41957ms step_avg:60.11ms
step:699/2330 train_time:42016ms step_avg:60.11ms
step:700/2330 train_time:42078ms step_avg:60.11ms
step:701/2330 train_time:42136ms step_avg:60.11ms
step:702/2330 train_time:42198ms step_avg:60.11ms
step:703/2330 train_time:42256ms step_avg:60.11ms
step:704/2330 train_time:42320ms step_avg:60.11ms
step:705/2330 train_time:42378ms step_avg:60.11ms
step:706/2330 train_time:42439ms step_avg:60.11ms
step:707/2330 train_time:42497ms step_avg:60.11ms
step:708/2330 train_time:42559ms step_avg:60.11ms
step:709/2330 train_time:42617ms step_avg:60.11ms
step:710/2330 train_time:42680ms step_avg:60.11ms
step:711/2330 train_time:42738ms step_avg:60.11ms
step:712/2330 train_time:42799ms step_avg:60.11ms
step:713/2330 train_time:42858ms step_avg:60.11ms
step:714/2330 train_time:42920ms step_avg:60.11ms
step:715/2330 train_time:42978ms step_avg:60.11ms
step:716/2330 train_time:43040ms step_avg:60.11ms
step:717/2330 train_time:43099ms step_avg:60.11ms
step:718/2330 train_time:43160ms step_avg:60.11ms
step:719/2330 train_time:43218ms step_avg:60.11ms
step:720/2330 train_time:43280ms step_avg:60.11ms
step:721/2330 train_time:43338ms step_avg:60.11ms
step:722/2330 train_time:43400ms step_avg:60.11ms
step:723/2330 train_time:43459ms step_avg:60.11ms
step:724/2330 train_time:43521ms step_avg:60.11ms
step:725/2330 train_time:43578ms step_avg:60.11ms
step:726/2330 train_time:43640ms step_avg:60.11ms
step:727/2330 train_time:43699ms step_avg:60.11ms
step:728/2330 train_time:43760ms step_avg:60.11ms
step:729/2330 train_time:43819ms step_avg:60.11ms
step:730/2330 train_time:43881ms step_avg:60.11ms
step:731/2330 train_time:43939ms step_avg:60.11ms
step:732/2330 train_time:44002ms step_avg:60.11ms
step:733/2330 train_time:44061ms step_avg:60.11ms
step:734/2330 train_time:44122ms step_avg:60.11ms
step:735/2330 train_time:44181ms step_avg:60.11ms
step:736/2330 train_time:44243ms step_avg:60.11ms
step:737/2330 train_time:44301ms step_avg:60.11ms
step:738/2330 train_time:44363ms step_avg:60.11ms
step:739/2330 train_time:44422ms step_avg:60.11ms
step:740/2330 train_time:44483ms step_avg:60.11ms
step:741/2330 train_time:44541ms step_avg:60.11ms
step:742/2330 train_time:44603ms step_avg:60.11ms
step:743/2330 train_time:44661ms step_avg:60.11ms
step:744/2330 train_time:44723ms step_avg:60.11ms
step:745/2330 train_time:44782ms step_avg:60.11ms
step:746/2330 train_time:44843ms step_avg:60.11ms
step:747/2330 train_time:44902ms step_avg:60.11ms
step:748/2330 train_time:44964ms step_avg:60.11ms
step:749/2330 train_time:45023ms step_avg:60.11ms
step:750/2330 train_time:45085ms step_avg:60.11ms
step:750/2330 val_loss:4.1629 train_time:45156ms step_avg:60.21ms
step:751/2330 train_time:45178ms step_avg:60.16ms
step:752/2330 train_time:45208ms step_avg:60.12ms
step:753/2330 train_time:45268ms step_avg:60.12ms
step:754/2330 train_time:45333ms step_avg:60.12ms
step:755/2330 train_time:45393ms step_avg:60.12ms
step:756/2330 train_time:45454ms step_avg:60.12ms
step:757/2330 train_time:45512ms step_avg:60.12ms
step:758/2330 train_time:45574ms step_avg:60.12ms
step:759/2330 train_time:45632ms step_avg:60.12ms
step:760/2330 train_time:45693ms step_avg:60.12ms
step:761/2330 train_time:45750ms step_avg:60.12ms
step:762/2330 train_time:45811ms step_avg:60.12ms
step:763/2330 train_time:45869ms step_avg:60.12ms
step:764/2330 train_time:45930ms step_avg:60.12ms
step:765/2330 train_time:45989ms step_avg:60.12ms
step:766/2330 train_time:46051ms step_avg:60.12ms
step:767/2330 train_time:46109ms step_avg:60.12ms
step:768/2330 train_time:46173ms step_avg:60.12ms
step:769/2330 train_time:46233ms step_avg:60.12ms
step:770/2330 train_time:46297ms step_avg:60.13ms
step:771/2330 train_time:46356ms step_avg:60.12ms
step:772/2330 train_time:46420ms step_avg:60.13ms
step:773/2330 train_time:46479ms step_avg:60.13ms
step:774/2330 train_time:46541ms step_avg:60.13ms
step:775/2330 train_time:46601ms step_avg:60.13ms
step:776/2330 train_time:46663ms step_avg:60.13ms
step:777/2330 train_time:46722ms step_avg:60.13ms
step:778/2330 train_time:46785ms step_avg:60.14ms
step:779/2330 train_time:46845ms step_avg:60.13ms
step:780/2330 train_time:46906ms step_avg:60.14ms
step:781/2330 train_time:46965ms step_avg:60.13ms
step:782/2330 train_time:47028ms step_avg:60.14ms
step:783/2330 train_time:47087ms step_avg:60.14ms
step:784/2330 train_time:47150ms step_avg:60.14ms
step:785/2330 train_time:47210ms step_avg:60.14ms
step:786/2330 train_time:47273ms step_avg:60.14ms
step:787/2330 train_time:47332ms step_avg:60.14ms
step:788/2330 train_time:47394ms step_avg:60.15ms
step:789/2330 train_time:47453ms step_avg:60.14ms
step:790/2330 train_time:47515ms step_avg:60.15ms
step:791/2330 train_time:47575ms step_avg:60.15ms
step:792/2330 train_time:47637ms step_avg:60.15ms
step:793/2330 train_time:47696ms step_avg:60.15ms
step:794/2330 train_time:47759ms step_avg:60.15ms
step:795/2330 train_time:47817ms step_avg:60.15ms
step:796/2330 train_time:47881ms step_avg:60.15ms
step:797/2330 train_time:47939ms step_avg:60.15ms
step:798/2330 train_time:48001ms step_avg:60.15ms
step:799/2330 train_time:48060ms step_avg:60.15ms
step:800/2330 train_time:48123ms step_avg:60.15ms
step:801/2330 train_time:48183ms step_avg:60.15ms
step:802/2330 train_time:48246ms step_avg:60.16ms
step:803/2330 train_time:48306ms step_avg:60.16ms
step:804/2330 train_time:48369ms step_avg:60.16ms
step:805/2330 train_time:48428ms step_avg:60.16ms
step:806/2330 train_time:48491ms step_avg:60.16ms
step:807/2330 train_time:48550ms step_avg:60.16ms
step:808/2330 train_time:48612ms step_avg:60.16ms
step:809/2330 train_time:48671ms step_avg:60.16ms
step:810/2330 train_time:48734ms step_avg:60.17ms
step:811/2330 train_time:48793ms step_avg:60.16ms
step:812/2330 train_time:48855ms step_avg:60.17ms
step:813/2330 train_time:48914ms step_avg:60.17ms
step:814/2330 train_time:48978ms step_avg:60.17ms
step:815/2330 train_time:49036ms step_avg:60.17ms
step:816/2330 train_time:49099ms step_avg:60.17ms
step:817/2330 train_time:49158ms step_avg:60.17ms
step:818/2330 train_time:49221ms step_avg:60.17ms
step:819/2330 train_time:49281ms step_avg:60.17ms
step:820/2330 train_time:49344ms step_avg:60.18ms
step:821/2330 train_time:49404ms step_avg:60.18ms
step:822/2330 train_time:49468ms step_avg:60.18ms
step:823/2330 train_time:49527ms step_avg:60.18ms
step:824/2330 train_time:49590ms step_avg:60.18ms
step:825/2330 train_time:49650ms step_avg:60.18ms
step:826/2330 train_time:49711ms step_avg:60.18ms
step:827/2330 train_time:49771ms step_avg:60.18ms
step:828/2330 train_time:49834ms step_avg:60.19ms
step:829/2330 train_time:49893ms step_avg:60.18ms
step:830/2330 train_time:49956ms step_avg:60.19ms
step:831/2330 train_time:50014ms step_avg:60.19ms
step:832/2330 train_time:50078ms step_avg:60.19ms
step:833/2330 train_time:50137ms step_avg:60.19ms
step:834/2330 train_time:50200ms step_avg:60.19ms
step:835/2330 train_time:50259ms step_avg:60.19ms
step:836/2330 train_time:50322ms step_avg:60.19ms
step:837/2330 train_time:50382ms step_avg:60.19ms
step:838/2330 train_time:50445ms step_avg:60.20ms
step:839/2330 train_time:50505ms step_avg:60.20ms
step:840/2330 train_time:50569ms step_avg:60.20ms
step:841/2330 train_time:50629ms step_avg:60.20ms
step:842/2330 train_time:50691ms step_avg:60.20ms
step:843/2330 train_time:50750ms step_avg:60.20ms
step:844/2330 train_time:50812ms step_avg:60.20ms
step:845/2330 train_time:50871ms step_avg:60.20ms
step:846/2330 train_time:50933ms step_avg:60.20ms
step:847/2330 train_time:50993ms step_avg:60.20ms
step:848/2330 train_time:51055ms step_avg:60.21ms
step:849/2330 train_time:51115ms step_avg:60.21ms
step:850/2330 train_time:51178ms step_avg:60.21ms
step:851/2330 train_time:51236ms step_avg:60.21ms
step:852/2330 train_time:51300ms step_avg:60.21ms
step:853/2330 train_time:51359ms step_avg:60.21ms
step:854/2330 train_time:51422ms step_avg:60.21ms
step:855/2330 train_time:51481ms step_avg:60.21ms
step:856/2330 train_time:51543ms step_avg:60.21ms
step:857/2330 train_time:51604ms step_avg:60.21ms
step:858/2330 train_time:51667ms step_avg:60.22ms
step:859/2330 train_time:51727ms step_avg:60.22ms
step:860/2330 train_time:51789ms step_avg:60.22ms
step:861/2330 train_time:51848ms step_avg:60.22ms
step:862/2330 train_time:51911ms step_avg:60.22ms
step:863/2330 train_time:51970ms step_avg:60.22ms
step:864/2330 train_time:52032ms step_avg:60.22ms
step:865/2330 train_time:52091ms step_avg:60.22ms
step:866/2330 train_time:52154ms step_avg:60.22ms
step:867/2330 train_time:52214ms step_avg:60.22ms
step:868/2330 train_time:52277ms step_avg:60.23ms
step:869/2330 train_time:52336ms step_avg:60.23ms
step:870/2330 train_time:52398ms step_avg:60.23ms
step:871/2330 train_time:52457ms step_avg:60.23ms
step:872/2330 train_time:52520ms step_avg:60.23ms
step:873/2330 train_time:52580ms step_avg:60.23ms
step:874/2330 train_time:52642ms step_avg:60.23ms
step:875/2330 train_time:52702ms step_avg:60.23ms
step:876/2330 train_time:52765ms step_avg:60.23ms
step:877/2330 train_time:52826ms step_avg:60.23ms
step:878/2330 train_time:52888ms step_avg:60.24ms
step:879/2330 train_time:52948ms step_avg:60.24ms
step:880/2330 train_time:53010ms step_avg:60.24ms
step:881/2330 train_time:53070ms step_avg:60.24ms
step:882/2330 train_time:53132ms step_avg:60.24ms
step:883/2330 train_time:53191ms step_avg:60.24ms
step:884/2330 train_time:53254ms step_avg:60.24ms
step:885/2330 train_time:53313ms step_avg:60.24ms
step:886/2330 train_time:53377ms step_avg:60.24ms
step:887/2330 train_time:53435ms step_avg:60.24ms
step:888/2330 train_time:53499ms step_avg:60.25ms
step:889/2330 train_time:53558ms step_avg:60.25ms
step:890/2330 train_time:53621ms step_avg:60.25ms
step:891/2330 train_time:53680ms step_avg:60.25ms
step:892/2330 train_time:53742ms step_avg:60.25ms
step:893/2330 train_time:53802ms step_avg:60.25ms
step:894/2330 train_time:53865ms step_avg:60.25ms
step:895/2330 train_time:53925ms step_avg:60.25ms
step:896/2330 train_time:53988ms step_avg:60.25ms
step:897/2330 train_time:54047ms step_avg:60.25ms
step:898/2330 train_time:54110ms step_avg:60.26ms
step:899/2330 train_time:54169ms step_avg:60.25ms
step:900/2330 train_time:54231ms step_avg:60.26ms
step:901/2330 train_time:54291ms step_avg:60.26ms
step:902/2330 train_time:54353ms step_avg:60.26ms
step:903/2330 train_time:54413ms step_avg:60.26ms
step:904/2330 train_time:54476ms step_avg:60.26ms
step:905/2330 train_time:54535ms step_avg:60.26ms
step:906/2330 train_time:54599ms step_avg:60.26ms
step:907/2330 train_time:54658ms step_avg:60.26ms
step:908/2330 train_time:54721ms step_avg:60.27ms
step:909/2330 train_time:54780ms step_avg:60.26ms
step:910/2330 train_time:54841ms step_avg:60.27ms
step:911/2330 train_time:54901ms step_avg:60.26ms
step:912/2330 train_time:54964ms step_avg:60.27ms
step:913/2330 train_time:55024ms step_avg:60.27ms
step:914/2330 train_time:55088ms step_avg:60.27ms
step:915/2330 train_time:55148ms step_avg:60.27ms
step:916/2330 train_time:55210ms step_avg:60.27ms
step:917/2330 train_time:55270ms step_avg:60.27ms
step:918/2330 train_time:55333ms step_avg:60.28ms
step:919/2330 train_time:55391ms step_avg:60.27ms
step:920/2330 train_time:55454ms step_avg:60.28ms
step:921/2330 train_time:55514ms step_avg:60.28ms
step:922/2330 train_time:55577ms step_avg:60.28ms
step:923/2330 train_time:55636ms step_avg:60.28ms
step:924/2330 train_time:55699ms step_avg:60.28ms
step:925/2330 train_time:55758ms step_avg:60.28ms
step:926/2330 train_time:55820ms step_avg:60.28ms
step:927/2330 train_time:55879ms step_avg:60.28ms
step:928/2330 train_time:55942ms step_avg:60.28ms
step:929/2330 train_time:56001ms step_avg:60.28ms
step:930/2330 train_time:56064ms step_avg:60.28ms
step:931/2330 train_time:56124ms step_avg:60.28ms
step:932/2330 train_time:56187ms step_avg:60.29ms
step:933/2330 train_time:56247ms step_avg:60.29ms
step:934/2330 train_time:56311ms step_avg:60.29ms
step:935/2330 train_time:56371ms step_avg:60.29ms
step:936/2330 train_time:56433ms step_avg:60.29ms
step:937/2330 train_time:56492ms step_avg:60.29ms
step:938/2330 train_time:56554ms step_avg:60.29ms
step:939/2330 train_time:56613ms step_avg:60.29ms
step:940/2330 train_time:56676ms step_avg:60.29ms
step:941/2330 train_time:56735ms step_avg:60.29ms
step:942/2330 train_time:56797ms step_avg:60.29ms
step:943/2330 train_time:56856ms step_avg:60.29ms
step:944/2330 train_time:56919ms step_avg:60.30ms
step:945/2330 train_time:56978ms step_avg:60.29ms
step:946/2330 train_time:57040ms step_avg:60.30ms
step:947/2330 train_time:57098ms step_avg:60.29ms
step:948/2330 train_time:57161ms step_avg:60.30ms
step:949/2330 train_time:57220ms step_avg:60.30ms
step:950/2330 train_time:57284ms step_avg:60.30ms
step:951/2330 train_time:57344ms step_avg:60.30ms
step:952/2330 train_time:57407ms step_avg:60.30ms
step:953/2330 train_time:57466ms step_avg:60.30ms
step:954/2330 train_time:57529ms step_avg:60.30ms
step:955/2330 train_time:57588ms step_avg:60.30ms
step:956/2330 train_time:57651ms step_avg:60.30ms
step:957/2330 train_time:57710ms step_avg:60.30ms
step:958/2330 train_time:57773ms step_avg:60.31ms
step:959/2330 train_time:57831ms step_avg:60.30ms
step:960/2330 train_time:57894ms step_avg:60.31ms
step:961/2330 train_time:57953ms step_avg:60.31ms
step:962/2330 train_time:58016ms step_avg:60.31ms
step:963/2330 train_time:58076ms step_avg:60.31ms
step:964/2330 train_time:58138ms step_avg:60.31ms
step:965/2330 train_time:58197ms step_avg:60.31ms
step:966/2330 train_time:58259ms step_avg:60.31ms
step:967/2330 train_time:58318ms step_avg:60.31ms
step:968/2330 train_time:58381ms step_avg:60.31ms
step:969/2330 train_time:58440ms step_avg:60.31ms
step:970/2330 train_time:58503ms step_avg:60.31ms
step:971/2330 train_time:58564ms step_avg:60.31ms
step:972/2330 train_time:58628ms step_avg:60.32ms
step:973/2330 train_time:58688ms step_avg:60.32ms
step:974/2330 train_time:58750ms step_avg:60.32ms
step:975/2330 train_time:58809ms step_avg:60.32ms
step:976/2330 train_time:58872ms step_avg:60.32ms
step:977/2330 train_time:58931ms step_avg:60.32ms
step:978/2330 train_time:58993ms step_avg:60.32ms
step:979/2330 train_time:59053ms step_avg:60.32ms
step:980/2330 train_time:59115ms step_avg:60.32ms
step:981/2330 train_time:59173ms step_avg:60.32ms
step:982/2330 train_time:59237ms step_avg:60.32ms
step:983/2330 train_time:59296ms step_avg:60.32ms
step:984/2330 train_time:59358ms step_avg:60.32ms
step:985/2330 train_time:59417ms step_avg:60.32ms
step:986/2330 train_time:59480ms step_avg:60.32ms
step:987/2330 train_time:59539ms step_avg:60.32ms
step:988/2330 train_time:59602ms step_avg:60.33ms
step:989/2330 train_time:59663ms step_avg:60.33ms
step:990/2330 train_time:59726ms step_avg:60.33ms
step:991/2330 train_time:59785ms step_avg:60.33ms
step:992/2330 train_time:59849ms step_avg:60.33ms
step:993/2330 train_time:59908ms step_avg:60.33ms
step:994/2330 train_time:59972ms step_avg:60.33ms
step:995/2330 train_time:60031ms step_avg:60.33ms
step:996/2330 train_time:60092ms step_avg:60.33ms
step:997/2330 train_time:60151ms step_avg:60.33ms
step:998/2330 train_time:60214ms step_avg:60.33ms
step:999/2330 train_time:60273ms step_avg:60.33ms
step:1000/2330 train_time:60336ms step_avg:60.34ms
step:1000/2330 val_loss:3.9837 train_time:60409ms step_avg:60.41ms
step:1001/2330 train_time:60432ms step_avg:60.37ms
step:1002/2330 train_time:60463ms step_avg:60.34ms
step:1003/2330 train_time:60526ms step_avg:60.34ms
step:1004/2330 train_time:60592ms step_avg:60.35ms
step:1005/2330 train_time:60651ms step_avg:60.35ms
step:1006/2330 train_time:60714ms step_avg:60.35ms
step:1007/2330 train_time:60772ms step_avg:60.35ms
step:1008/2330 train_time:60834ms step_avg:60.35ms
step:1009/2330 train_time:60892ms step_avg:60.35ms
step:1010/2330 train_time:60954ms step_avg:60.35ms
step:1011/2330 train_time:61012ms step_avg:60.35ms
step:1012/2330 train_time:61074ms step_avg:60.35ms
step:1013/2330 train_time:61132ms step_avg:60.35ms
step:1014/2330 train_time:61194ms step_avg:60.35ms
step:1015/2330 train_time:61252ms step_avg:60.35ms
step:1016/2330 train_time:61315ms step_avg:60.35ms
step:1017/2330 train_time:61380ms step_avg:60.35ms
step:1018/2330 train_time:61448ms step_avg:60.36ms
step:1019/2330 train_time:61508ms step_avg:60.36ms
step:1020/2330 train_time:61571ms step_avg:60.36ms
step:1021/2330 train_time:61630ms step_avg:60.36ms
step:1022/2330 train_time:61693ms step_avg:60.36ms
step:1023/2330 train_time:61751ms step_avg:60.36ms
step:1024/2330 train_time:61813ms step_avg:60.36ms
step:1025/2330 train_time:61872ms step_avg:60.36ms
step:1026/2330 train_time:61933ms step_avg:60.36ms
step:1027/2330 train_time:61991ms step_avg:60.36ms
step:1028/2330 train_time:62053ms step_avg:60.36ms
step:1029/2330 train_time:62112ms step_avg:60.36ms
step:1030/2330 train_time:62173ms step_avg:60.36ms
step:1031/2330 train_time:62232ms step_avg:60.36ms
step:1032/2330 train_time:62295ms step_avg:60.36ms
step:1033/2330 train_time:62357ms step_avg:60.36ms
step:1034/2330 train_time:62422ms step_avg:60.37ms
step:1035/2330 train_time:62481ms step_avg:60.37ms
step:1036/2330 train_time:62545ms step_avg:60.37ms
step:1037/2330 train_time:62604ms step_avg:60.37ms
step:1038/2330 train_time:62667ms step_avg:60.37ms
step:1039/2330 train_time:62726ms step_avg:60.37ms
step:1040/2330 train_time:62789ms step_avg:60.37ms
step:1041/2330 train_time:62847ms step_avg:60.37ms
step:1042/2330 train_time:62910ms step_avg:60.37ms
step:1043/2330 train_time:62969ms step_avg:60.37ms
step:1044/2330 train_time:63031ms step_avg:60.37ms
step:1045/2330 train_time:63090ms step_avg:60.37ms
step:1046/2330 train_time:63152ms step_avg:60.38ms
step:1047/2330 train_time:63211ms step_avg:60.37ms
step:1048/2330 train_time:63275ms step_avg:60.38ms
step:1049/2330 train_time:63334ms step_avg:60.38ms
step:1050/2330 train_time:63399ms step_avg:60.38ms
step:1051/2330 train_time:63460ms step_avg:60.38ms
step:1052/2330 train_time:63523ms step_avg:60.38ms
step:1053/2330 train_time:63582ms step_avg:60.38ms
step:1054/2330 train_time:63645ms step_avg:60.38ms
step:1055/2330 train_time:63705ms step_avg:60.38ms
step:1056/2330 train_time:63767ms step_avg:60.39ms
step:1057/2330 train_time:63825ms step_avg:60.38ms
step:1058/2330 train_time:63888ms step_avg:60.39ms
step:1059/2330 train_time:63947ms step_avg:60.38ms
step:1060/2330 train_time:64010ms step_avg:60.39ms
step:1061/2330 train_time:64068ms step_avg:60.38ms
step:1062/2330 train_time:64132ms step_avg:60.39ms
step:1063/2330 train_time:64190ms step_avg:60.39ms
step:1064/2330 train_time:64252ms step_avg:60.39ms
step:1065/2330 train_time:64312ms step_avg:60.39ms
step:1066/2330 train_time:64376ms step_avg:60.39ms
step:1067/2330 train_time:64436ms step_avg:60.39ms
step:1068/2330 train_time:64501ms step_avg:60.39ms
step:1069/2330 train_time:64562ms step_avg:60.39ms
step:1070/2330 train_time:64624ms step_avg:60.40ms
step:1071/2330 train_time:64683ms step_avg:60.39ms
step:1072/2330 train_time:64745ms step_avg:60.40ms
step:1073/2330 train_time:64804ms step_avg:60.40ms
step:1074/2330 train_time:64867ms step_avg:60.40ms
step:1075/2330 train_time:64925ms step_avg:60.40ms
step:1076/2330 train_time:64988ms step_avg:60.40ms
step:1077/2330 train_time:65047ms step_avg:60.40ms
step:1078/2330 train_time:65110ms step_avg:60.40ms
step:1079/2330 train_time:65169ms step_avg:60.40ms
step:1080/2330 train_time:65231ms step_avg:60.40ms
step:1081/2330 train_time:65290ms step_avg:60.40ms
step:1082/2330 train_time:65353ms step_avg:60.40ms
step:1083/2330 train_time:65413ms step_avg:60.40ms
step:1084/2330 train_time:65477ms step_avg:60.40ms
step:1085/2330 train_time:65536ms step_avg:60.40ms
step:1086/2330 train_time:65600ms step_avg:60.40ms
step:1087/2330 train_time:65660ms step_avg:60.40ms
step:1088/2330 train_time:65722ms step_avg:60.41ms
step:1089/2330 train_time:65782ms step_avg:60.41ms
step:1090/2330 train_time:65844ms step_avg:60.41ms
step:1091/2330 train_time:65903ms step_avg:60.41ms
step:1092/2330 train_time:65967ms step_avg:60.41ms
step:1093/2330 train_time:66025ms step_avg:60.41ms
step:1094/2330 train_time:66088ms step_avg:60.41ms
step:1095/2330 train_time:66147ms step_avg:60.41ms
step:1096/2330 train_time:66211ms step_avg:60.41ms
step:1097/2330 train_time:66270ms step_avg:60.41ms
step:1098/2330 train_time:66333ms step_avg:60.41ms
step:1099/2330 train_time:66392ms step_avg:60.41ms
step:1100/2330 train_time:66456ms step_avg:60.41ms
step:1101/2330 train_time:66516ms step_avg:60.41ms
step:1102/2330 train_time:66579ms step_avg:60.42ms
step:1103/2330 train_time:66638ms step_avg:60.41ms
step:1104/2330 train_time:66701ms step_avg:60.42ms
step:1105/2330 train_time:66761ms step_avg:60.42ms
step:1106/2330 train_time:66824ms step_avg:60.42ms
step:1107/2330 train_time:66883ms step_avg:60.42ms
step:1108/2330 train_time:66945ms step_avg:60.42ms
step:1109/2330 train_time:67004ms step_avg:60.42ms
step:1110/2330 train_time:67068ms step_avg:60.42ms
step:1111/2330 train_time:67127ms step_avg:60.42ms
step:1112/2330 train_time:67190ms step_avg:60.42ms
step:1113/2330 train_time:67249ms step_avg:60.42ms
step:1114/2330 train_time:67312ms step_avg:60.42ms
step:1115/2330 train_time:67371ms step_avg:60.42ms
step:1116/2330 train_time:67434ms step_avg:60.42ms
step:1117/2330 train_time:67493ms step_avg:60.42ms
step:1118/2330 train_time:67556ms step_avg:60.43ms
step:1119/2330 train_time:67615ms step_avg:60.42ms
step:1120/2330 train_time:67679ms step_avg:60.43ms
step:1121/2330 train_time:67738ms step_avg:60.43ms
step:1122/2330 train_time:67801ms step_avg:60.43ms
step:1123/2330 train_time:67860ms step_avg:60.43ms
step:1124/2330 train_time:67923ms step_avg:60.43ms
step:1125/2330 train_time:67982ms step_avg:60.43ms
step:1126/2330 train_time:68045ms step_avg:60.43ms
step:1127/2330 train_time:68104ms step_avg:60.43ms
step:1128/2330 train_time:68167ms step_avg:60.43ms
step:1129/2330 train_time:68226ms step_avg:60.43ms
step:1130/2330 train_time:68289ms step_avg:60.43ms
step:1131/2330 train_time:68348ms step_avg:60.43ms
step:1132/2330 train_time:68412ms step_avg:60.43ms
step:1133/2330 train_time:68472ms step_avg:60.43ms
step:1134/2330 train_time:68535ms step_avg:60.44ms
step:1135/2330 train_time:68593ms step_avg:60.43ms
step:1136/2330 train_time:68656ms step_avg:60.44ms
step:1137/2330 train_time:68717ms step_avg:60.44ms
step:1138/2330 train_time:68781ms step_avg:60.44ms
step:1139/2330 train_time:68840ms step_avg:60.44ms
step:1140/2330 train_time:68903ms step_avg:60.44ms
step:1141/2330 train_time:68963ms step_avg:60.44ms
step:1142/2330 train_time:69025ms step_avg:60.44ms
step:1143/2330 train_time:69084ms step_avg:60.44ms
step:1144/2330 train_time:69147ms step_avg:60.44ms
step:1145/2330 train_time:69206ms step_avg:60.44ms
step:1146/2330 train_time:69268ms step_avg:60.44ms
step:1147/2330 train_time:69327ms step_avg:60.44ms
step:1148/2330 train_time:69391ms step_avg:60.44ms
step:1149/2330 train_time:69450ms step_avg:60.44ms
step:1150/2330 train_time:69512ms step_avg:60.45ms
step:1151/2330 train_time:69571ms step_avg:60.44ms
step:1152/2330 train_time:69634ms step_avg:60.45ms
step:1153/2330 train_time:69693ms step_avg:60.45ms
step:1154/2330 train_time:69757ms step_avg:60.45ms
step:1155/2330 train_time:69817ms step_avg:60.45ms
step:1156/2330 train_time:69880ms step_avg:60.45ms
step:1157/2330 train_time:69940ms step_avg:60.45ms
step:1158/2330 train_time:70003ms step_avg:60.45ms
step:1159/2330 train_time:70062ms step_avg:60.45ms
step:1160/2330 train_time:70124ms step_avg:60.45ms
step:1161/2330 train_time:70183ms step_avg:60.45ms
step:1162/2330 train_time:70246ms step_avg:60.45ms
step:1163/2330 train_time:70305ms step_avg:60.45ms
step:1164/2330 train_time:70369ms step_avg:60.45ms
step:1165/2330 train_time:70427ms step_avg:60.45ms
step:1166/2330 train_time:70491ms step_avg:60.46ms
step:1167/2330 train_time:70550ms step_avg:60.45ms
step:1168/2330 train_time:70614ms step_avg:60.46ms
step:1169/2330 train_time:70674ms step_avg:60.46ms
step:1170/2330 train_time:70737ms step_avg:60.46ms
step:1171/2330 train_time:70795ms step_avg:60.46ms
step:1172/2330 train_time:70859ms step_avg:60.46ms
step:1173/2330 train_time:70919ms step_avg:60.46ms
step:1174/2330 train_time:70982ms step_avg:60.46ms
step:1175/2330 train_time:71042ms step_avg:60.46ms
step:1176/2330 train_time:71105ms step_avg:60.46ms
step:1177/2330 train_time:71164ms step_avg:60.46ms
step:1178/2330 train_time:71227ms step_avg:60.46ms
step:1179/2330 train_time:71285ms step_avg:60.46ms
step:1180/2330 train_time:71349ms step_avg:60.47ms
step:1181/2330 train_time:71407ms step_avg:60.46ms
step:1182/2330 train_time:71471ms step_avg:60.47ms
step:1183/2330 train_time:71530ms step_avg:60.46ms
step:1184/2330 train_time:71594ms step_avg:60.47ms
step:1185/2330 train_time:71653ms step_avg:60.47ms
step:1186/2330 train_time:71716ms step_avg:60.47ms
step:1187/2330 train_time:71775ms step_avg:60.47ms
step:1188/2330 train_time:71839ms step_avg:60.47ms
step:1189/2330 train_time:71898ms step_avg:60.47ms
step:1190/2330 train_time:71961ms step_avg:60.47ms
step:1191/2330 train_time:72021ms step_avg:60.47ms
step:1192/2330 train_time:72084ms step_avg:60.47ms
step:1193/2330 train_time:72143ms step_avg:60.47ms
step:1194/2330 train_time:72205ms step_avg:60.47ms
step:1195/2330 train_time:72265ms step_avg:60.47ms
step:1196/2330 train_time:72326ms step_avg:60.47ms
step:1197/2330 train_time:72386ms step_avg:60.47ms
step:1198/2330 train_time:72448ms step_avg:60.47ms
step:1199/2330 train_time:72508ms step_avg:60.47ms
step:1200/2330 train_time:72571ms step_avg:60.48ms
step:1201/2330 train_time:72630ms step_avg:60.47ms
step:1202/2330 train_time:72694ms step_avg:60.48ms
step:1203/2330 train_time:72753ms step_avg:60.48ms
step:1204/2330 train_time:72815ms step_avg:60.48ms
step:1205/2330 train_time:72875ms step_avg:60.48ms
step:1206/2330 train_time:72938ms step_avg:60.48ms
step:1207/2330 train_time:72997ms step_avg:60.48ms
step:1208/2330 train_time:73061ms step_avg:60.48ms
step:1209/2330 train_time:73120ms step_avg:60.48ms
step:1210/2330 train_time:73183ms step_avg:60.48ms
step:1211/2330 train_time:73242ms step_avg:60.48ms
step:1212/2330 train_time:73304ms step_avg:60.48ms
step:1213/2330 train_time:73366ms step_avg:60.48ms
step:1214/2330 train_time:73428ms step_avg:60.48ms
step:1215/2330 train_time:73487ms step_avg:60.48ms
step:1216/2330 train_time:73550ms step_avg:60.48ms
step:1217/2330 train_time:73609ms step_avg:60.48ms
step:1218/2330 train_time:73673ms step_avg:60.49ms
step:1219/2330 train_time:73732ms step_avg:60.49ms
step:1220/2330 train_time:73796ms step_avg:60.49ms
step:1221/2330 train_time:73855ms step_avg:60.49ms
step:1222/2330 train_time:73918ms step_avg:60.49ms
step:1223/2330 train_time:73977ms step_avg:60.49ms
step:1224/2330 train_time:74041ms step_avg:60.49ms
step:1225/2330 train_time:74100ms step_avg:60.49ms
step:1226/2330 train_time:74165ms step_avg:60.49ms
step:1227/2330 train_time:74224ms step_avg:60.49ms
step:1228/2330 train_time:74287ms step_avg:60.49ms
step:1229/2330 train_time:74346ms step_avg:60.49ms
step:1230/2330 train_time:74409ms step_avg:60.49ms
step:1231/2330 train_time:74468ms step_avg:60.49ms
step:1232/2330 train_time:74531ms step_avg:60.50ms
step:1233/2330 train_time:74590ms step_avg:60.49ms
step:1234/2330 train_time:74653ms step_avg:60.50ms
step:1235/2330 train_time:74712ms step_avg:60.50ms
step:1236/2330 train_time:74775ms step_avg:60.50ms
step:1237/2330 train_time:74835ms step_avg:60.50ms
step:1238/2330 train_time:74898ms step_avg:60.50ms
step:1239/2330 train_time:74958ms step_avg:60.50ms
step:1240/2330 train_time:75021ms step_avg:60.50ms
step:1241/2330 train_time:75080ms step_avg:60.50ms
step:1242/2330 train_time:75143ms step_avg:60.50ms
step:1243/2330 train_time:75202ms step_avg:60.50ms
step:1244/2330 train_time:75264ms step_avg:60.50ms
step:1245/2330 train_time:75323ms step_avg:60.50ms
step:1246/2330 train_time:75387ms step_avg:60.50ms
step:1247/2330 train_time:75446ms step_avg:60.50ms
step:1248/2330 train_time:75508ms step_avg:60.50ms
step:1249/2330 train_time:75568ms step_avg:60.50ms
step:1250/2330 train_time:75631ms step_avg:60.50ms
step:1250/2330 val_loss:3.8971 train_time:75704ms step_avg:60.56ms
step:1251/2330 train_time:75726ms step_avg:60.53ms
step:1252/2330 train_time:75757ms step_avg:60.51ms
step:1253/2330 train_time:75819ms step_avg:60.51ms
step:1254/2330 train_time:75885ms step_avg:60.51ms
step:1255/2330 train_time:75946ms step_avg:60.51ms
step:1256/2330 train_time:76011ms step_avg:60.52ms
step:1257/2330 train_time:76069ms step_avg:60.52ms
step:1258/2330 train_time:76131ms step_avg:60.52ms
step:1259/2330 train_time:76190ms step_avg:60.52ms
step:1260/2330 train_time:76253ms step_avg:60.52ms
step:1261/2330 train_time:76311ms step_avg:60.52ms
step:1262/2330 train_time:76373ms step_avg:60.52ms
step:1263/2330 train_time:76431ms step_avg:60.52ms
step:1264/2330 train_time:76494ms step_avg:60.52ms
step:1265/2330 train_time:76551ms step_avg:60.51ms
step:1266/2330 train_time:76613ms step_avg:60.52ms
step:1267/2330 train_time:76672ms step_avg:60.51ms
step:1268/2330 train_time:76736ms step_avg:60.52ms
step:1269/2330 train_time:76796ms step_avg:60.52ms
step:1270/2330 train_time:76860ms step_avg:60.52ms
step:1271/2330 train_time:76920ms step_avg:60.52ms
step:1272/2330 train_time:76985ms step_avg:60.52ms
step:1273/2330 train_time:77046ms step_avg:60.52ms
step:1274/2330 train_time:77108ms step_avg:60.52ms
step:1275/2330 train_time:77167ms step_avg:60.52ms
step:1276/2330 train_time:77230ms step_avg:60.53ms
step:1277/2330 train_time:77289ms step_avg:60.52ms
step:1278/2330 train_time:77352ms step_avg:60.53ms
step:1279/2330 train_time:77410ms step_avg:60.52ms
step:1280/2330 train_time:77472ms step_avg:60.52ms
step:1281/2330 train_time:77530ms step_avg:60.52ms
step:1282/2330 train_time:77593ms step_avg:60.52ms
step:1283/2330 train_time:77651ms step_avg:60.52ms
step:1284/2330 train_time:77714ms step_avg:60.52ms
step:1285/2330 train_time:77774ms step_avg:60.52ms
step:1286/2330 train_time:77839ms step_avg:60.53ms
step:1287/2330 train_time:77899ms step_avg:60.53ms
step:1288/2330 train_time:77963ms step_avg:60.53ms
step:1289/2330 train_time:78023ms step_avg:60.53ms
step:1290/2330 train_time:78086ms step_avg:60.53ms
step:1291/2330 train_time:78146ms step_avg:60.53ms
step:1292/2330 train_time:78209ms step_avg:60.53ms
step:1293/2330 train_time:78267ms step_avg:60.53ms
step:1294/2330 train_time:78330ms step_avg:60.53ms
step:1295/2330 train_time:78388ms step_avg:60.53ms
step:1296/2330 train_time:78451ms step_avg:60.53ms
step:1297/2330 train_time:78509ms step_avg:60.53ms
step:1298/2330 train_time:78572ms step_avg:60.53ms
step:1299/2330 train_time:78631ms step_avg:60.53ms
step:1300/2330 train_time:78695ms step_avg:60.53ms
step:1301/2330 train_time:78754ms step_avg:60.53ms
step:1302/2330 train_time:78817ms step_avg:60.54ms
step:1303/2330 train_time:78877ms step_avg:60.54ms
step:1304/2330 train_time:78942ms step_avg:60.54ms
step:1305/2330 train_time:79002ms step_avg:60.54ms
step:1306/2330 train_time:79065ms step_avg:60.54ms
step:1307/2330 train_time:79124ms step_avg:60.54ms
step:1308/2330 train_time:79187ms step_avg:60.54ms
step:1309/2330 train_time:79246ms step_avg:60.54ms
step:1310/2330 train_time:79308ms step_avg:60.54ms
step:1311/2330 train_time:79367ms step_avg:60.54ms
step:1312/2330 train_time:79429ms step_avg:60.54ms
step:1313/2330 train_time:79488ms step_avg:60.54ms
step:1314/2330 train_time:79550ms step_avg:60.54ms
step:1315/2330 train_time:79609ms step_avg:60.54ms
step:1316/2330 train_time:79673ms step_avg:60.54ms
step:1317/2330 train_time:79732ms step_avg:60.54ms
step:1318/2330 train_time:79796ms step_avg:60.54ms
step:1319/2330 train_time:79855ms step_avg:60.54ms
step:1320/2330 train_time:79918ms step_avg:60.54ms
step:1321/2330 train_time:79978ms step_avg:60.54ms
step:1322/2330 train_time:80041ms step_avg:60.55ms
step:1323/2330 train_time:80102ms step_avg:60.55ms
step:1324/2330 train_time:80165ms step_avg:60.55ms
step:1325/2330 train_time:80225ms step_avg:60.55ms
step:1326/2330 train_time:80288ms step_avg:60.55ms
step:1327/2330 train_time:80347ms step_avg:60.55ms
step:1328/2330 train_time:80409ms step_avg:60.55ms
step:1329/2330 train_time:80469ms step_avg:60.55ms
step:1330/2330 train_time:80531ms step_avg:60.55ms
step:1331/2330 train_time:80590ms step_avg:60.55ms
step:1332/2330 train_time:80653ms step_avg:60.55ms
step:1333/2330 train_time:80712ms step_avg:60.55ms
step:1334/2330 train_time:80775ms step_avg:60.55ms
step:1335/2330 train_time:80834ms step_avg:60.55ms
step:1336/2330 train_time:80898ms step_avg:60.55ms
step:1337/2330 train_time:80957ms step_avg:60.55ms
step:1338/2330 train_time:81019ms step_avg:60.55ms
step:1339/2330 train_time:81080ms step_avg:60.55ms
step:1340/2330 train_time:81143ms step_avg:60.55ms
step:1341/2330 train_time:81203ms step_avg:60.55ms
step:1342/2330 train_time:81267ms step_avg:60.56ms
step:1343/2330 train_time:81326ms step_avg:60.56ms
step:1344/2330 train_time:81389ms step_avg:60.56ms
step:1345/2330 train_time:81447ms step_avg:60.56ms
step:1346/2330 train_time:81510ms step_avg:60.56ms
step:1347/2330 train_time:81570ms step_avg:60.56ms
step:1348/2330 train_time:81632ms step_avg:60.56ms
step:1349/2330 train_time:81692ms step_avg:60.56ms
step:1350/2330 train_time:81754ms step_avg:60.56ms
step:1351/2330 train_time:81814ms step_avg:60.56ms
step:1352/2330 train_time:81877ms step_avg:60.56ms
step:1353/2330 train_time:81936ms step_avg:60.56ms
step:1354/2330 train_time:81999ms step_avg:60.56ms
step:1355/2330 train_time:82058ms step_avg:60.56ms
step:1356/2330 train_time:82122ms step_avg:60.56ms
step:1357/2330 train_time:82182ms step_avg:60.56ms
step:1358/2330 train_time:82245ms step_avg:60.56ms
step:1359/2330 train_time:82304ms step_avg:60.56ms
step:1360/2330 train_time:82368ms step_avg:60.56ms
step:1361/2330 train_time:82427ms step_avg:60.56ms
step:1362/2330 train_time:82491ms step_avg:60.57ms
step:1363/2330 train_time:82549ms step_avg:60.56ms
step:1364/2330 train_time:82612ms step_avg:60.57ms
step:1365/2330 train_time:82671ms step_avg:60.56ms
step:1366/2330 train_time:82734ms step_avg:60.57ms
step:1367/2330 train_time:82793ms step_avg:60.57ms
step:1368/2330 train_time:82857ms step_avg:60.57ms
step:1369/2330 train_time:82916ms step_avg:60.57ms
step:1370/2330 train_time:82980ms step_avg:60.57ms
step:1371/2330 train_time:83039ms step_avg:60.57ms
step:1372/2330 train_time:83102ms step_avg:60.57ms
step:1373/2330 train_time:83162ms step_avg:60.57ms
step:1374/2330 train_time:83225ms step_avg:60.57ms
step:1375/2330 train_time:83284ms step_avg:60.57ms
step:1376/2330 train_time:83348ms step_avg:60.57ms
step:1377/2330 train_time:83407ms step_avg:60.57ms
step:1378/2330 train_time:83469ms step_avg:60.57ms
step:1379/2330 train_time:83528ms step_avg:60.57ms
step:1380/2330 train_time:83591ms step_avg:60.57ms
step:1381/2330 train_time:83650ms step_avg:60.57ms
step:1382/2330 train_time:83713ms step_avg:60.57ms
step:1383/2330 train_time:83772ms step_avg:60.57ms
step:1384/2330 train_time:83835ms step_avg:60.57ms
step:1385/2330 train_time:83894ms step_avg:60.57ms
step:1386/2330 train_time:83958ms step_avg:60.58ms
step:1387/2330 train_time:84017ms step_avg:60.57ms
step:1388/2330 train_time:84081ms step_avg:60.58ms
step:1389/2330 train_time:84140ms step_avg:60.58ms
step:1390/2330 train_time:84204ms step_avg:60.58ms
step:1391/2330 train_time:84263ms step_avg:60.58ms
step:1392/2330 train_time:84326ms step_avg:60.58ms
step:1393/2330 train_time:84386ms step_avg:60.58ms
step:1394/2330 train_time:84448ms step_avg:60.58ms
step:1395/2330 train_time:84507ms step_avg:60.58ms
step:1396/2330 train_time:84571ms step_avg:60.58ms
step:1397/2330 train_time:84630ms step_avg:60.58ms
step:1398/2330 train_time:84693ms step_avg:60.58ms
step:1399/2330 train_time:84752ms step_avg:60.58ms
step:1400/2330 train_time:84815ms step_avg:60.58ms
step:1401/2330 train_time:84874ms step_avg:60.58ms
step:1402/2330 train_time:84937ms step_avg:60.58ms
step:1403/2330 train_time:84997ms step_avg:60.58ms
step:1404/2330 train_time:85059ms step_avg:60.58ms
step:1405/2330 train_time:85119ms step_avg:60.58ms
step:1406/2330 train_time:85182ms step_avg:60.58ms
step:1407/2330 train_time:85242ms step_avg:60.58ms
step:1408/2330 train_time:85305ms step_avg:60.59ms
step:1409/2330 train_time:85365ms step_avg:60.59ms
step:1410/2330 train_time:85428ms step_avg:60.59ms
step:1411/2330 train_time:85487ms step_avg:60.59ms
step:1412/2330 train_time:85551ms step_avg:60.59ms
step:1413/2330 train_time:85610ms step_avg:60.59ms
step:1414/2330 train_time:85673ms step_avg:60.59ms
step:1415/2330 train_time:85732ms step_avg:60.59ms
step:1416/2330 train_time:85795ms step_avg:60.59ms
step:1417/2330 train_time:85854ms step_avg:60.59ms
step:1418/2330 train_time:85917ms step_avg:60.59ms
step:1419/2330 train_time:85976ms step_avg:60.59ms
step:1420/2330 train_time:86039ms step_avg:60.59ms
step:1421/2330 train_time:86099ms step_avg:60.59ms
step:1422/2330 train_time:86162ms step_avg:60.59ms
step:1423/2330 train_time:86221ms step_avg:60.59ms
step:1424/2330 train_time:86284ms step_avg:60.59ms
step:1425/2330 train_time:86344ms step_avg:60.59ms
step:1426/2330 train_time:86407ms step_avg:60.59ms
step:1427/2330 train_time:86466ms step_avg:60.59ms
step:1428/2330 train_time:86529ms step_avg:60.59ms
step:1429/2330 train_time:86589ms step_avg:60.59ms
step:1430/2330 train_time:86651ms step_avg:60.60ms
step:1431/2330 train_time:86710ms step_avg:60.59ms
step:1432/2330 train_time:86773ms step_avg:60.60ms
step:1433/2330 train_time:86832ms step_avg:60.59ms
step:1434/2330 train_time:86896ms step_avg:60.60ms
step:1435/2330 train_time:86955ms step_avg:60.60ms
step:1436/2330 train_time:87018ms step_avg:60.60ms
step:1437/2330 train_time:87077ms step_avg:60.60ms
step:1438/2330 train_time:87141ms step_avg:60.60ms
step:1439/2330 train_time:87200ms step_avg:60.60ms
step:1440/2330 train_time:87265ms step_avg:60.60ms
step:1441/2330 train_time:87324ms step_avg:60.60ms
step:1442/2330 train_time:87387ms step_avg:60.60ms
step:1443/2330 train_time:87446ms step_avg:60.60ms
step:1444/2330 train_time:87509ms step_avg:60.60ms
step:1445/2330 train_time:87569ms step_avg:60.60ms
step:1446/2330 train_time:87631ms step_avg:60.60ms
step:1447/2330 train_time:87691ms step_avg:60.60ms
step:1448/2330 train_time:87753ms step_avg:60.60ms
step:1449/2330 train_time:87813ms step_avg:60.60ms
step:1450/2330 train_time:87875ms step_avg:60.60ms
step:1451/2330 train_time:87933ms step_avg:60.60ms
step:1452/2330 train_time:87996ms step_avg:60.60ms
step:1453/2330 train_time:88055ms step_avg:60.60ms
step:1454/2330 train_time:88118ms step_avg:60.60ms
step:1455/2330 train_time:88177ms step_avg:60.60ms
step:1456/2330 train_time:88241ms step_avg:60.60ms
step:1457/2330 train_time:88300ms step_avg:60.60ms
step:1458/2330 train_time:88364ms step_avg:60.61ms
step:1459/2330 train_time:88424ms step_avg:60.61ms
step:1460/2330 train_time:88488ms step_avg:60.61ms
step:1461/2330 train_time:88547ms step_avg:60.61ms
step:1462/2330 train_time:88610ms step_avg:60.61ms
step:1463/2330 train_time:88669ms step_avg:60.61ms
step:1464/2330 train_time:88732ms step_avg:60.61ms
step:1465/2330 train_time:88791ms step_avg:60.61ms
step:1466/2330 train_time:88854ms step_avg:60.61ms
step:1467/2330 train_time:88912ms step_avg:60.61ms
step:1468/2330 train_time:88975ms step_avg:60.61ms
step:1469/2330 train_time:89034ms step_avg:60.61ms
step:1470/2330 train_time:89097ms step_avg:60.61ms
step:1471/2330 train_time:89157ms step_avg:60.61ms
step:1472/2330 train_time:89220ms step_avg:60.61ms
step:1473/2330 train_time:89279ms step_avg:60.61ms
step:1474/2330 train_time:89343ms step_avg:60.61ms
step:1475/2330 train_time:89403ms step_avg:60.61ms
step:1476/2330 train_time:89467ms step_avg:60.61ms
step:1477/2330 train_time:89526ms step_avg:60.61ms
step:1478/2330 train_time:89590ms step_avg:60.62ms
step:1479/2330 train_time:89649ms step_avg:60.61ms
step:1480/2330 train_time:89712ms step_avg:60.62ms
step:1481/2330 train_time:89771ms step_avg:60.62ms
step:1482/2330 train_time:89834ms step_avg:60.62ms
step:1483/2330 train_time:89892ms step_avg:60.62ms
step:1484/2330 train_time:89955ms step_avg:60.62ms
step:1485/2330 train_time:90013ms step_avg:60.62ms
step:1486/2330 train_time:90076ms step_avg:60.62ms
step:1487/2330 train_time:90135ms step_avg:60.62ms
step:1488/2330 train_time:90199ms step_avg:60.62ms
step:1489/2330 train_time:90259ms step_avg:60.62ms
step:1490/2330 train_time:90323ms step_avg:60.62ms
step:1491/2330 train_time:90383ms step_avg:60.62ms
step:1492/2330 train_time:90445ms step_avg:60.62ms
step:1493/2330 train_time:90504ms step_avg:60.62ms
step:1494/2330 train_time:90567ms step_avg:60.62ms
step:1495/2330 train_time:90627ms step_avg:60.62ms
step:1496/2330 train_time:90690ms step_avg:60.62ms
step:1497/2330 train_time:90750ms step_avg:60.62ms
step:1498/2330 train_time:90812ms step_avg:60.62ms
step:1499/2330 train_time:90870ms step_avg:60.62ms
step:1500/2330 train_time:90933ms step_avg:60.62ms
step:1500/2330 val_loss:3.8275 train_time:91005ms step_avg:60.67ms
step:1501/2330 train_time:91026ms step_avg:60.64ms
step:1502/2330 train_time:91059ms step_avg:60.62ms
step:1503/2330 train_time:91122ms step_avg:60.63ms
step:1504/2330 train_time:91185ms step_avg:60.63ms
step:1505/2330 train_time:91246ms step_avg:60.63ms
step:1506/2330 train_time:91310ms step_avg:60.63ms
step:1507/2330 train_time:91368ms step_avg:60.63ms
step:1508/2330 train_time:91430ms step_avg:60.63ms
step:1509/2330 train_time:91488ms step_avg:60.63ms
step:1510/2330 train_time:91550ms step_avg:60.63ms
step:1511/2330 train_time:91608ms step_avg:60.63ms
step:1512/2330 train_time:91670ms step_avg:60.63ms
step:1513/2330 train_time:91729ms step_avg:60.63ms
step:1514/2330 train_time:91791ms step_avg:60.63ms
step:1515/2330 train_time:91849ms step_avg:60.63ms
step:1516/2330 train_time:91912ms step_avg:60.63ms
step:1517/2330 train_time:91972ms step_avg:60.63ms
step:1518/2330 train_time:92037ms step_avg:60.63ms
step:1519/2330 train_time:92099ms step_avg:60.63ms
step:1520/2330 train_time:92161ms step_avg:60.63ms
step:1521/2330 train_time:92221ms step_avg:60.63ms
step:1522/2330 train_time:92283ms step_avg:60.63ms
step:1523/2330 train_time:92342ms step_avg:60.63ms
step:1524/2330 train_time:92406ms step_avg:60.63ms
step:1525/2330 train_time:92464ms step_avg:60.63ms
step:1526/2330 train_time:92527ms step_avg:60.63ms
step:1527/2330 train_time:92586ms step_avg:60.63ms
step:1528/2330 train_time:92648ms step_avg:60.63ms
step:1529/2330 train_time:92707ms step_avg:60.63ms
step:1530/2330 train_time:92769ms step_avg:60.63ms
step:1531/2330 train_time:92828ms step_avg:60.63ms
step:1532/2330 train_time:92892ms step_avg:60.63ms
step:1533/2330 train_time:92952ms step_avg:60.63ms
step:1534/2330 train_time:93016ms step_avg:60.64ms
step:1535/2330 train_time:93076ms step_avg:60.64ms
step:1536/2330 train_time:93141ms step_avg:60.64ms
step:1537/2330 train_time:93200ms step_avg:60.64ms
step:1538/2330 train_time:93263ms step_avg:60.64ms
step:1539/2330 train_time:93323ms step_avg:60.64ms
step:1540/2330 train_time:93387ms step_avg:60.64ms
step:1541/2330 train_time:93446ms step_avg:60.64ms
step:1542/2330 train_time:93509ms step_avg:60.64ms
step:1543/2330 train_time:93568ms step_avg:60.64ms
step:1544/2330 train_time:93631ms step_avg:60.64ms
step:1545/2330 train_time:93690ms step_avg:60.64ms
step:1546/2330 train_time:93753ms step_avg:60.64ms
step:1547/2330 train_time:93812ms step_avg:60.64ms
step:1548/2330 train_time:93875ms step_avg:60.64ms
step:1549/2330 train_time:93935ms step_avg:60.64ms
step:1550/2330 train_time:93999ms step_avg:60.64ms
step:1551/2330 train_time:94060ms step_avg:60.64ms
step:1552/2330 train_time:94123ms step_avg:60.65ms
step:1553/2330 train_time:94182ms step_avg:60.65ms
step:1554/2330 train_time:94245ms step_avg:60.65ms
step:1555/2330 train_time:94306ms step_avg:60.65ms
step:1556/2330 train_time:94369ms step_avg:60.65ms
step:1557/2330 train_time:94429ms step_avg:60.65ms
step:1558/2330 train_time:94492ms step_avg:60.65ms
step:1559/2330 train_time:94551ms step_avg:60.65ms
step:1560/2330 train_time:94613ms step_avg:60.65ms
step:1561/2330 train_time:94673ms step_avg:60.65ms
step:1562/2330 train_time:94735ms step_avg:60.65ms
step:1563/2330 train_time:94794ms step_avg:60.65ms
step:1564/2330 train_time:94856ms step_avg:60.65ms
step:1565/2330 train_time:94916ms step_avg:60.65ms
step:1566/2330 train_time:94980ms step_avg:60.65ms
step:1567/2330 train_time:95039ms step_avg:60.65ms
step:1568/2330 train_time:95102ms step_avg:60.65ms
step:1569/2330 train_time:95162ms step_avg:60.65ms
step:1570/2330 train_time:95226ms step_avg:60.65ms
step:1571/2330 train_time:95286ms step_avg:60.65ms
step:1572/2330 train_time:95349ms step_avg:60.65ms
step:1573/2330 train_time:95408ms step_avg:60.65ms
step:1574/2330 train_time:95471ms step_avg:60.65ms
step:1575/2330 train_time:95531ms step_avg:60.65ms
step:1576/2330 train_time:95594ms step_avg:60.66ms
step:1577/2330 train_time:95653ms step_avg:60.66ms
step:1578/2330 train_time:95716ms step_avg:60.66ms
step:1579/2330 train_time:95776ms step_avg:60.66ms
step:1580/2330 train_time:95838ms step_avg:60.66ms
step:1581/2330 train_time:95897ms step_avg:60.66ms
step:1582/2330 train_time:95961ms step_avg:60.66ms
step:1583/2330 train_time:96021ms step_avg:60.66ms
step:1584/2330 train_time:96089ms step_avg:60.66ms
step:1585/2330 train_time:96143ms step_avg:60.66ms
step:1586/2330 train_time:96207ms step_avg:60.66ms
step:1587/2330 train_time:96266ms step_avg:60.66ms
step:1588/2330 train_time:96330ms step_avg:60.66ms
step:1589/2330 train_time:96389ms step_avg:60.66ms
step:1590/2330 train_time:96453ms step_avg:60.66ms
step:1591/2330 train_time:96513ms step_avg:60.66ms
step:1592/2330 train_time:96577ms step_avg:60.66ms
step:1593/2330 train_time:96637ms step_avg:60.66ms
step:1594/2330 train_time:96700ms step_avg:60.66ms
step:1595/2330 train_time:96759ms step_avg:60.66ms
step:1596/2330 train_time:96821ms step_avg:60.67ms
step:1597/2330 train_time:96881ms step_avg:60.66ms
step:1598/2330 train_time:96944ms step_avg:60.67ms
step:1599/2330 train_time:97004ms step_avg:60.67ms
step:1600/2330 train_time:97067ms step_avg:60.67ms
step:1601/2330 train_time:97126ms step_avg:60.67ms
step:1602/2330 train_time:97190ms step_avg:60.67ms
step:1603/2330 train_time:97249ms step_avg:60.67ms
step:1604/2330 train_time:97312ms step_avg:60.67ms
step:1605/2330 train_time:97372ms step_avg:60.67ms
step:1606/2330 train_time:97434ms step_avg:60.67ms
step:1607/2330 train_time:97494ms step_avg:60.67ms
step:1608/2330 train_time:97558ms step_avg:60.67ms
step:1609/2330 train_time:97617ms step_avg:60.67ms
step:1610/2330 train_time:97680ms step_avg:60.67ms
step:1611/2330 train_time:97739ms step_avg:60.67ms
step:1612/2330 train_time:97802ms step_avg:60.67ms
step:1613/2330 train_time:97861ms step_avg:60.67ms
step:1614/2330 train_time:97926ms step_avg:60.67ms
step:1615/2330 train_time:97986ms step_avg:60.67ms
step:1616/2330 train_time:98050ms step_avg:60.67ms
step:1617/2330 train_time:98109ms step_avg:60.67ms
step:1618/2330 train_time:98173ms step_avg:60.68ms
step:1619/2330 train_time:98233ms step_avg:60.67ms
step:1620/2330 train_time:98296ms step_avg:60.68ms
step:1621/2330 train_time:98355ms step_avg:60.68ms
step:1622/2330 train_time:98418ms step_avg:60.68ms
step:1623/2330 train_time:98478ms step_avg:60.68ms
step:1624/2330 train_time:98540ms step_avg:60.68ms
step:1625/2330 train_time:98599ms step_avg:60.68ms
step:1626/2330 train_time:98663ms step_avg:60.68ms
step:1627/2330 train_time:98723ms step_avg:60.68ms
step:1628/2330 train_time:98785ms step_avg:60.68ms
step:1629/2330 train_time:98845ms step_avg:60.68ms
step:1630/2330 train_time:98908ms step_avg:60.68ms
step:1631/2330 train_time:98967ms step_avg:60.68ms
step:1632/2330 train_time:99031ms step_avg:60.68ms
step:1633/2330 train_time:99091ms step_avg:60.68ms
step:1634/2330 train_time:99153ms step_avg:60.68ms
step:1635/2330 train_time:99213ms step_avg:60.68ms
step:1636/2330 train_time:99277ms step_avg:60.68ms
step:1637/2330 train_time:99337ms step_avg:60.68ms
step:1638/2330 train_time:99399ms step_avg:60.68ms
step:1639/2330 train_time:99458ms step_avg:60.68ms
step:1640/2330 train_time:99522ms step_avg:60.68ms
step:1641/2330 train_time:99581ms step_avg:60.68ms
step:1642/2330 train_time:99644ms step_avg:60.68ms
step:1643/2330 train_time:99703ms step_avg:60.68ms
step:1644/2330 train_time:99767ms step_avg:60.69ms
step:1645/2330 train_time:99825ms step_avg:60.68ms
step:1646/2330 train_time:99889ms step_avg:60.69ms
step:1647/2330 train_time:99948ms step_avg:60.68ms
step:1648/2330 train_time:100011ms step_avg:60.69ms
step:1649/2330 train_time:100071ms step_avg:60.69ms
step:1650/2330 train_time:100134ms step_avg:60.69ms
step:1651/2330 train_time:100193ms step_avg:60.69ms
step:1652/2330 train_time:100257ms step_avg:60.69ms
step:1653/2330 train_time:100317ms step_avg:60.69ms
step:1654/2330 train_time:100380ms step_avg:60.69ms
step:1655/2330 train_time:100439ms step_avg:60.69ms
step:1656/2330 train_time:100501ms step_avg:60.69ms
step:1657/2330 train_time:100561ms step_avg:60.69ms
step:1658/2330 train_time:100626ms step_avg:60.69ms
step:1659/2330 train_time:100685ms step_avg:60.69ms
step:1660/2330 train_time:100747ms step_avg:60.69ms
step:1661/2330 train_time:100806ms step_avg:60.69ms
step:1662/2330 train_time:100869ms step_avg:60.69ms
step:1663/2330 train_time:100928ms step_avg:60.69ms
step:1664/2330 train_time:100992ms step_avg:60.69ms
step:1665/2330 train_time:101052ms step_avg:60.69ms
step:1666/2330 train_time:101116ms step_avg:60.69ms
step:1667/2330 train_time:101175ms step_avg:60.69ms
step:1668/2330 train_time:101238ms step_avg:60.69ms
step:1669/2330 train_time:101297ms step_avg:60.69ms
step:1670/2330 train_time:101361ms step_avg:60.70ms
step:1671/2330 train_time:101421ms step_avg:60.69ms
step:1672/2330 train_time:101484ms step_avg:60.70ms
step:1673/2330 train_time:101544ms step_avg:60.70ms
step:1674/2330 train_time:101607ms step_avg:60.70ms
step:1675/2330 train_time:101666ms step_avg:60.70ms
step:1676/2330 train_time:101729ms step_avg:60.70ms
step:1677/2330 train_time:101788ms step_avg:60.70ms
step:1678/2330 train_time:101851ms step_avg:60.70ms
step:1679/2330 train_time:101910ms step_avg:60.70ms
step:1680/2330 train_time:101973ms step_avg:60.70ms
step:1681/2330 train_time:102033ms step_avg:60.70ms
step:1682/2330 train_time:102096ms step_avg:60.70ms
step:1683/2330 train_time:102156ms step_avg:60.70ms
step:1684/2330 train_time:102219ms step_avg:60.70ms
step:1685/2330 train_time:102278ms step_avg:60.70ms
step:1686/2330 train_time:102341ms step_avg:60.70ms
step:1687/2330 train_time:102400ms step_avg:60.70ms
step:1688/2330 train_time:102463ms step_avg:60.70ms
step:1689/2330 train_time:102523ms step_avg:60.70ms
step:1690/2330 train_time:102586ms step_avg:60.70ms
step:1691/2330 train_time:102646ms step_avg:60.70ms
step:1692/2330 train_time:102709ms step_avg:60.70ms
step:1693/2330 train_time:102769ms step_avg:60.70ms
step:1694/2330 train_time:102833ms step_avg:60.70ms
step:1695/2330 train_time:102892ms step_avg:60.70ms
step:1696/2330 train_time:102954ms step_avg:60.70ms
step:1697/2330 train_time:103014ms step_avg:60.70ms
step:1698/2330 train_time:103078ms step_avg:60.71ms
step:1699/2330 train_time:103137ms step_avg:60.70ms
step:1700/2330 train_time:103200ms step_avg:60.71ms
step:1701/2330 train_time:103259ms step_avg:60.71ms
step:1702/2330 train_time:103323ms step_avg:60.71ms
step:1703/2330 train_time:103383ms step_avg:60.71ms
step:1704/2330 train_time:103446ms step_avg:60.71ms
step:1705/2330 train_time:103506ms step_avg:60.71ms
step:1706/2330 train_time:103569ms step_avg:60.71ms
step:1707/2330 train_time:103629ms step_avg:60.71ms
step:1708/2330 train_time:103692ms step_avg:60.71ms
step:1709/2330 train_time:103752ms step_avg:60.71ms
step:1710/2330 train_time:103815ms step_avg:60.71ms
step:1711/2330 train_time:103875ms step_avg:60.71ms
step:1712/2330 train_time:103938ms step_avg:60.71ms
step:1713/2330 train_time:103997ms step_avg:60.71ms
step:1714/2330 train_time:104060ms step_avg:60.71ms
step:1715/2330 train_time:104120ms step_avg:60.71ms
step:1716/2330 train_time:104183ms step_avg:60.71ms
step:1717/2330 train_time:104242ms step_avg:60.71ms
step:1718/2330 train_time:104306ms step_avg:60.71ms
step:1719/2330 train_time:104365ms step_avg:60.71ms
step:1720/2330 train_time:104428ms step_avg:60.71ms
step:1721/2330 train_time:104486ms step_avg:60.71ms
step:1722/2330 train_time:104550ms step_avg:60.71ms
step:1723/2330 train_time:104609ms step_avg:60.71ms
step:1724/2330 train_time:104673ms step_avg:60.72ms
step:1725/2330 train_time:104733ms step_avg:60.71ms
step:1726/2330 train_time:104796ms step_avg:60.72ms
step:1727/2330 train_time:104855ms step_avg:60.72ms
step:1728/2330 train_time:104919ms step_avg:60.72ms
step:1729/2330 train_time:104978ms step_avg:60.72ms
step:1730/2330 train_time:105041ms step_avg:60.72ms
step:1731/2330 train_time:105100ms step_avg:60.72ms
step:1732/2330 train_time:105163ms step_avg:60.72ms
step:1733/2330 train_time:105223ms step_avg:60.72ms
step:1734/2330 train_time:105285ms step_avg:60.72ms
step:1735/2330 train_time:105345ms step_avg:60.72ms
step:1736/2330 train_time:105408ms step_avg:60.72ms
step:1737/2330 train_time:105467ms step_avg:60.72ms
step:1738/2330 train_time:105531ms step_avg:60.72ms
step:1739/2330 train_time:105590ms step_avg:60.72ms
step:1740/2330 train_time:105653ms step_avg:60.72ms
step:1741/2330 train_time:105713ms step_avg:60.72ms
step:1742/2330 train_time:105777ms step_avg:60.72ms
step:1743/2330 train_time:105836ms step_avg:60.72ms
step:1744/2330 train_time:105899ms step_avg:60.72ms
step:1745/2330 train_time:105959ms step_avg:60.72ms
step:1746/2330 train_time:106022ms step_avg:60.72ms
step:1747/2330 train_time:106081ms step_avg:60.72ms
step:1748/2330 train_time:106145ms step_avg:60.72ms
step:1749/2330 train_time:106204ms step_avg:60.72ms
step:1750/2330 train_time:106267ms step_avg:60.72ms
step:1750/2330 val_loss:3.7572 train_time:106338ms step_avg:60.76ms
step:1751/2330 train_time:106361ms step_avg:60.74ms
step:1752/2330 train_time:106390ms step_avg:60.72ms
step:1753/2330 train_time:106450ms step_avg:60.72ms
step:1754/2330 train_time:106519ms step_avg:60.73ms
step:1755/2330 train_time:106581ms step_avg:60.73ms
step:1756/2330 train_time:106645ms step_avg:60.73ms
step:1757/2330 train_time:106704ms step_avg:60.73ms
step:1758/2330 train_time:106766ms step_avg:60.73ms
step:1759/2330 train_time:106825ms step_avg:60.73ms
step:1760/2330 train_time:106887ms step_avg:60.73ms
step:1761/2330 train_time:106946ms step_avg:60.73ms
step:1762/2330 train_time:107008ms step_avg:60.73ms
step:1763/2330 train_time:107068ms step_avg:60.73ms
step:1764/2330 train_time:107130ms step_avg:60.73ms
step:1765/2330 train_time:107189ms step_avg:60.73ms
step:1766/2330 train_time:107252ms step_avg:60.73ms
step:1767/2330 train_time:107317ms step_avg:60.73ms
step:1768/2330 train_time:107383ms step_avg:60.74ms
step:1769/2330 train_time:107444ms step_avg:60.74ms
step:1770/2330 train_time:107509ms step_avg:60.74ms
step:1771/2330 train_time:107569ms step_avg:60.74ms
step:1772/2330 train_time:107632ms step_avg:60.74ms
step:1773/2330 train_time:107692ms step_avg:60.74ms
step:1774/2330 train_time:107755ms step_avg:60.74ms
step:1775/2330 train_time:107813ms step_avg:60.74ms
step:1776/2330 train_time:107877ms step_avg:60.74ms
step:1777/2330 train_time:107937ms step_avg:60.74ms
step:1778/2330 train_time:108000ms step_avg:60.74ms
step:1779/2330 train_time:108059ms step_avg:60.74ms
step:1780/2330 train_time:108122ms step_avg:60.74ms
step:1781/2330 train_time:108181ms step_avg:60.74ms
step:1782/2330 train_time:108245ms step_avg:60.74ms
step:1783/2330 train_time:108305ms step_avg:60.74ms
step:1784/2330 train_time:108370ms step_avg:60.75ms
step:1785/2330 train_time:108431ms step_avg:60.75ms
step:1786/2330 train_time:108495ms step_avg:60.75ms
step:1787/2330 train_time:108556ms step_avg:60.75ms
step:1788/2330 train_time:108619ms step_avg:60.75ms
step:1789/2330 train_time:108677ms step_avg:60.75ms
step:1790/2330 train_time:108740ms step_avg:60.75ms
step:1791/2330 train_time:108800ms step_avg:60.75ms
step:1792/2330 train_time:108863ms step_avg:60.75ms
step:1793/2330 train_time:108922ms step_avg:60.75ms
step:1794/2330 train_time:108985ms step_avg:60.75ms
step:1795/2330 train_time:109044ms step_avg:60.75ms
step:1796/2330 train_time:109107ms step_avg:60.75ms
step:1797/2330 train_time:109167ms step_avg:60.75ms
step:1798/2330 train_time:109230ms step_avg:60.75ms
step:1799/2330 train_time:109290ms step_avg:60.75ms
step:1800/2330 train_time:109353ms step_avg:60.75ms
step:1801/2330 train_time:109415ms step_avg:60.75ms
step:1802/2330 train_time:109478ms step_avg:60.75ms
step:1803/2330 train_time:109539ms step_avg:60.75ms
step:1804/2330 train_time:109602ms step_avg:60.76ms
step:1805/2330 train_time:109661ms step_avg:60.75ms
step:1806/2330 train_time:109725ms step_avg:60.76ms
step:1807/2330 train_time:109784ms step_avg:60.75ms
step:1808/2330 train_time:109848ms step_avg:60.76ms
step:1809/2330 train_time:109907ms step_avg:60.76ms
step:1810/2330 train_time:109970ms step_avg:60.76ms
step:1811/2330 train_time:110030ms step_avg:60.76ms
step:1812/2330 train_time:110093ms step_avg:60.76ms
step:1813/2330 train_time:110152ms step_avg:60.76ms
step:1814/2330 train_time:110216ms step_avg:60.76ms
step:1815/2330 train_time:110276ms step_avg:60.76ms
step:1816/2330 train_time:110339ms step_avg:60.76ms
step:1817/2330 train_time:110398ms step_avg:60.76ms
step:1818/2330 train_time:110461ms step_avg:60.76ms
step:1819/2330 train_time:110520ms step_avg:60.76ms
step:1820/2330 train_time:110584ms step_avg:60.76ms
step:1821/2330 train_time:110644ms step_avg:60.76ms
step:1822/2330 train_time:110708ms step_avg:60.76ms
step:1823/2330 train_time:110766ms step_avg:60.76ms
step:1824/2330 train_time:110829ms step_avg:60.76ms
step:1825/2330 train_time:110889ms step_avg:60.76ms
step:1826/2330 train_time:110951ms step_avg:60.76ms
step:1827/2330 train_time:111011ms step_avg:60.76ms
step:1828/2330 train_time:111074ms step_avg:60.76ms
step:1829/2330 train_time:111133ms step_avg:60.76ms
step:1830/2330 train_time:111196ms step_avg:60.76ms
step:1831/2330 train_time:111256ms step_avg:60.76ms
step:1832/2330 train_time:111319ms step_avg:60.76ms
step:1833/2330 train_time:111379ms step_avg:60.76ms
step:1834/2330 train_time:111441ms step_avg:60.76ms
step:1835/2330 train_time:111501ms step_avg:60.76ms
step:1836/2330 train_time:111564ms step_avg:60.76ms
step:1837/2330 train_time:111624ms step_avg:60.76ms
step:1838/2330 train_time:111687ms step_avg:60.77ms
step:1839/2330 train_time:111746ms step_avg:60.76ms
step:1840/2330 train_time:111809ms step_avg:60.77ms
step:1841/2330 train_time:111869ms step_avg:60.77ms
step:1842/2330 train_time:111933ms step_avg:60.77ms
step:1843/2330 train_time:111992ms step_avg:60.77ms
step:1844/2330 train_time:112055ms step_avg:60.77ms
step:1845/2330 train_time:112115ms step_avg:60.77ms
step:1846/2330 train_time:112178ms step_avg:60.77ms
step:1847/2330 train_time:112238ms step_avg:60.77ms
step:1848/2330 train_time:112300ms step_avg:60.77ms
step:1849/2330 train_time:112360ms step_avg:60.77ms
step:1850/2330 train_time:112423ms step_avg:60.77ms
step:1851/2330 train_time:112483ms step_avg:60.77ms
step:1852/2330 train_time:112547ms step_avg:60.77ms
step:1853/2330 train_time:112606ms step_avg:60.77ms
step:1854/2330 train_time:112669ms step_avg:60.77ms
step:1855/2330 train_time:112729ms step_avg:60.77ms
step:1856/2330 train_time:112792ms step_avg:60.77ms
step:1857/2330 train_time:112852ms step_avg:60.77ms
step:1858/2330 train_time:112915ms step_avg:60.77ms
step:1859/2330 train_time:112975ms step_avg:60.77ms
step:1860/2330 train_time:113038ms step_avg:60.77ms
step:1861/2330 train_time:113097ms step_avg:60.77ms
step:1862/2330 train_time:113160ms step_avg:60.77ms
step:1863/2330 train_time:113220ms step_avg:60.77ms
step:1864/2330 train_time:113283ms step_avg:60.77ms
step:1865/2330 train_time:113342ms step_avg:60.77ms
step:1866/2330 train_time:113406ms step_avg:60.77ms
step:1867/2330 train_time:113465ms step_avg:60.77ms
step:1868/2330 train_time:113529ms step_avg:60.78ms
step:1869/2330 train_time:113589ms step_avg:60.78ms
step:1870/2330 train_time:113652ms step_avg:60.78ms
step:1871/2330 train_time:113712ms step_avg:60.78ms
step:1872/2330 train_time:113776ms step_avg:60.78ms
step:1873/2330 train_time:113835ms step_avg:60.78ms
step:1874/2330 train_time:113898ms step_avg:60.78ms
step:1875/2330 train_time:113957ms step_avg:60.78ms
step:1876/2330 train_time:114020ms step_avg:60.78ms
step:1877/2330 train_time:114079ms step_avg:60.78ms
step:1878/2330 train_time:114142ms step_avg:60.78ms
step:1879/2330 train_time:114202ms step_avg:60.78ms
step:1880/2330 train_time:114265ms step_avg:60.78ms
step:1881/2330 train_time:114324ms step_avg:60.78ms
step:1882/2330 train_time:114387ms step_avg:60.78ms
step:1883/2330 train_time:114447ms step_avg:60.78ms
step:1884/2330 train_time:114510ms step_avg:60.78ms
step:1885/2330 train_time:114572ms step_avg:60.78ms
step:1886/2330 train_time:114635ms step_avg:60.78ms
step:1887/2330 train_time:114695ms step_avg:60.78ms
step:1888/2330 train_time:114757ms step_avg:60.78ms
step:1889/2330 train_time:114817ms step_avg:60.78ms
step:1890/2330 train_time:114880ms step_avg:60.78ms
step:1891/2330 train_time:114940ms step_avg:60.78ms
step:1892/2330 train_time:115003ms step_avg:60.78ms
step:1893/2330 train_time:115062ms step_avg:60.78ms
step:1894/2330 train_time:115126ms step_avg:60.78ms
step:1895/2330 train_time:115185ms step_avg:60.78ms
step:1896/2330 train_time:115249ms step_avg:60.79ms
step:1897/2330 train_time:115309ms step_avg:60.78ms
step:1898/2330 train_time:115372ms step_avg:60.79ms
step:1899/2330 train_time:115432ms step_avg:60.79ms
step:1900/2330 train_time:115494ms step_avg:60.79ms
step:1901/2330 train_time:115555ms step_avg:60.79ms
step:1902/2330 train_time:115619ms step_avg:60.79ms
step:1903/2330 train_time:115677ms step_avg:60.79ms
step:1904/2330 train_time:115740ms step_avg:60.79ms
step:1905/2330 train_time:115799ms step_avg:60.79ms
step:1906/2330 train_time:115862ms step_avg:60.79ms
step:1907/2330 train_time:115921ms step_avg:60.79ms
step:1908/2330 train_time:115985ms step_avg:60.79ms
step:1909/2330 train_time:116043ms step_avg:60.79ms
step:1910/2330 train_time:116108ms step_avg:60.79ms
step:1911/2330 train_time:116167ms step_avg:60.79ms
step:1912/2330 train_time:116230ms step_avg:60.79ms
step:1913/2330 train_time:116291ms step_avg:60.79ms
step:1914/2330 train_time:116355ms step_avg:60.79ms
step:1915/2330 train_time:116414ms step_avg:60.79ms
step:1916/2330 train_time:116478ms step_avg:60.79ms
step:1917/2330 train_time:116537ms step_avg:60.79ms
step:1918/2330 train_time:116600ms step_avg:60.79ms
step:1919/2330 train_time:116659ms step_avg:60.79ms
step:1920/2330 train_time:116723ms step_avg:60.79ms
step:1921/2330 train_time:116781ms step_avg:60.79ms
step:1922/2330 train_time:116845ms step_avg:60.79ms
step:1923/2330 train_time:116904ms step_avg:60.79ms
step:1924/2330 train_time:116968ms step_avg:60.79ms
step:1925/2330 train_time:117028ms step_avg:60.79ms
step:1926/2330 train_time:117090ms step_avg:60.79ms
step:1927/2330 train_time:117149ms step_avg:60.79ms
step:1928/2330 train_time:117213ms step_avg:60.79ms
step:1929/2330 train_time:117274ms step_avg:60.80ms
step:1930/2330 train_time:117337ms step_avg:60.80ms
step:1931/2330 train_time:117397ms step_avg:60.80ms
step:1932/2330 train_time:117459ms step_avg:60.80ms
step:1933/2330 train_time:117520ms step_avg:60.80ms
step:1934/2330 train_time:117583ms step_avg:60.80ms
step:1935/2330 train_time:117643ms step_avg:60.80ms
step:1936/2330 train_time:117706ms step_avg:60.80ms
step:1937/2330 train_time:117765ms step_avg:60.80ms
step:1938/2330 train_time:117829ms step_avg:60.80ms
step:1939/2330 train_time:117888ms step_avg:60.80ms
step:1940/2330 train_time:117952ms step_avg:60.80ms
step:1941/2330 train_time:118012ms step_avg:60.80ms
step:1942/2330 train_time:118075ms step_avg:60.80ms
step:1943/2330 train_time:118134ms step_avg:60.80ms
step:1944/2330 train_time:118197ms step_avg:60.80ms
step:1945/2330 train_time:118256ms step_avg:60.80ms
step:1946/2330 train_time:118320ms step_avg:60.80ms
step:1947/2330 train_time:118378ms step_avg:60.80ms
step:1948/2330 train_time:118442ms step_avg:60.80ms
step:1949/2330 train_time:118501ms step_avg:60.80ms
step:1950/2330 train_time:118564ms step_avg:60.80ms
step:1951/2330 train_time:118624ms step_avg:60.80ms
step:1952/2330 train_time:118687ms step_avg:60.80ms
step:1953/2330 train_time:118746ms step_avg:60.80ms
step:1954/2330 train_time:118809ms step_avg:60.80ms
step:1955/2330 train_time:118869ms step_avg:60.80ms
step:1956/2330 train_time:118932ms step_avg:60.80ms
step:1957/2330 train_time:118993ms step_avg:60.80ms
step:1958/2330 train_time:119057ms step_avg:60.81ms
step:1959/2330 train_time:119116ms step_avg:60.80ms
step:1960/2330 train_time:119178ms step_avg:60.81ms
step:1961/2330 train_time:119237ms step_avg:60.80ms
step:1962/2330 train_time:119301ms step_avg:60.81ms
step:1963/2330 train_time:119360ms step_avg:60.80ms
step:1964/2330 train_time:119423ms step_avg:60.81ms
step:1965/2330 train_time:119483ms step_avg:60.81ms
step:1966/2330 train_time:119546ms step_avg:60.81ms
step:1967/2330 train_time:119606ms step_avg:60.81ms
step:1968/2330 train_time:119669ms step_avg:60.81ms
step:1969/2330 train_time:119729ms step_avg:60.81ms
step:1970/2330 train_time:119792ms step_avg:60.81ms
step:1971/2330 train_time:119852ms step_avg:60.81ms
step:1972/2330 train_time:119915ms step_avg:60.81ms
step:1973/2330 train_time:119975ms step_avg:60.81ms
step:1974/2330 train_time:120038ms step_avg:60.81ms
step:1975/2330 train_time:120097ms step_avg:60.81ms
step:1976/2330 train_time:120160ms step_avg:60.81ms
step:1977/2330 train_time:120219ms step_avg:60.81ms
step:1978/2330 train_time:120282ms step_avg:60.81ms
step:1979/2330 train_time:120342ms step_avg:60.81ms
step:1980/2330 train_time:120405ms step_avg:60.81ms
step:1981/2330 train_time:120464ms step_avg:60.81ms
step:1982/2330 train_time:120528ms step_avg:60.81ms
step:1983/2330 train_time:120587ms step_avg:60.81ms
step:1984/2330 train_time:120650ms step_avg:60.81ms
step:1985/2330 train_time:120711ms step_avg:60.81ms
step:1986/2330 train_time:120775ms step_avg:60.81ms
step:1987/2330 train_time:120833ms step_avg:60.81ms
step:1988/2330 train_time:120897ms step_avg:60.81ms
step:1989/2330 train_time:120955ms step_avg:60.81ms
step:1990/2330 train_time:121019ms step_avg:60.81ms
step:1991/2330 train_time:121079ms step_avg:60.81ms
step:1992/2330 train_time:121142ms step_avg:60.81ms
step:1993/2330 train_time:121202ms step_avg:60.81ms
step:1994/2330 train_time:121264ms step_avg:60.81ms
step:1995/2330 train_time:121325ms step_avg:60.81ms
step:1996/2330 train_time:121389ms step_avg:60.82ms
step:1997/2330 train_time:121449ms step_avg:60.82ms
step:1998/2330 train_time:121512ms step_avg:60.82ms
step:1999/2330 train_time:121570ms step_avg:60.82ms
step:2000/2330 train_time:121634ms step_avg:60.82ms
step:2000/2330 val_loss:3.7114 train_time:121705ms step_avg:60.85ms
step:2001/2330 train_time:121727ms step_avg:60.83ms
step:2002/2330 train_time:121760ms step_avg:60.82ms
step:2003/2330 train_time:121823ms step_avg:60.82ms
step:2004/2330 train_time:121890ms step_avg:60.82ms
step:2005/2330 train_time:121951ms step_avg:60.82ms
step:2006/2330 train_time:122013ms step_avg:60.82ms
step:2007/2330 train_time:122072ms step_avg:60.82ms
step:2008/2330 train_time:122135ms step_avg:60.82ms
step:2009/2330 train_time:122194ms step_avg:60.82ms
step:2010/2330 train_time:122257ms step_avg:60.82ms
step:2011/2330 train_time:122315ms step_avg:60.82ms
step:2012/2330 train_time:122377ms step_avg:60.82ms
step:2013/2330 train_time:122436ms step_avg:60.82ms
step:2014/2330 train_time:122499ms step_avg:60.82ms
step:2015/2330 train_time:122558ms step_avg:60.82ms
step:2016/2330 train_time:122620ms step_avg:60.82ms
step:2017/2330 train_time:122681ms step_avg:60.82ms
step:2018/2330 train_time:122747ms step_avg:60.83ms
step:2019/2330 train_time:122808ms step_avg:60.83ms
step:2020/2330 train_time:122871ms step_avg:60.83ms
step:2021/2330 train_time:122932ms step_avg:60.83ms
step:2022/2330 train_time:122995ms step_avg:60.83ms
step:2023/2330 train_time:123054ms step_avg:60.83ms
step:2024/2330 train_time:123117ms step_avg:60.83ms
step:2025/2330 train_time:123176ms step_avg:60.83ms
step:2026/2330 train_time:123239ms step_avg:60.83ms
step:2027/2330 train_time:123298ms step_avg:60.83ms
step:2028/2330 train_time:123361ms step_avg:60.83ms
step:2029/2330 train_time:123419ms step_avg:60.83ms
step:2030/2330 train_time:123482ms step_avg:60.83ms
step:2031/2330 train_time:123541ms step_avg:60.83ms
step:2032/2330 train_time:123604ms step_avg:60.83ms
step:2033/2330 train_time:123664ms step_avg:60.83ms
step:2034/2330 train_time:123728ms step_avg:60.83ms
step:2035/2330 train_time:123789ms step_avg:60.83ms
step:2036/2330 train_time:123853ms step_avg:60.83ms
step:2037/2330 train_time:123913ms step_avg:60.83ms
step:2038/2330 train_time:123976ms step_avg:60.83ms
step:2039/2330 train_time:124035ms step_avg:60.83ms
step:2040/2330 train_time:124098ms step_avg:60.83ms
step:2041/2330 train_time:124158ms step_avg:60.83ms
step:2042/2330 train_time:124220ms step_avg:60.83ms
step:2043/2330 train_time:124280ms step_avg:60.83ms
step:2044/2330 train_time:124342ms step_avg:60.83ms
step:2045/2330 train_time:124401ms step_avg:60.83ms
step:2046/2330 train_time:124464ms step_avg:60.83ms
step:2047/2330 train_time:124524ms step_avg:60.83ms
step:2048/2330 train_time:124586ms step_avg:60.83ms
step:2049/2330 train_time:124646ms step_avg:60.83ms
step:2050/2330 train_time:124709ms step_avg:60.83ms
step:2051/2330 train_time:124769ms step_avg:60.83ms
step:2052/2330 train_time:124834ms step_avg:60.84ms
step:2053/2330 train_time:124893ms step_avg:60.83ms
step:2054/2330 train_time:124956ms step_avg:60.84ms
step:2055/2330 train_time:125015ms step_avg:60.83ms
step:2056/2330 train_time:125079ms step_avg:60.84ms
step:2057/2330 train_time:125138ms step_avg:60.84ms
step:2058/2330 train_time:125200ms step_avg:60.84ms
step:2059/2330 train_time:125260ms step_avg:60.84ms
step:2060/2330 train_time:125323ms step_avg:60.84ms
step:2061/2330 train_time:125382ms step_avg:60.84ms
step:2062/2330 train_time:125446ms step_avg:60.84ms
step:2063/2330 train_time:125505ms step_avg:60.84ms
step:2064/2330 train_time:125568ms step_avg:60.84ms
step:2065/2330 train_time:125627ms step_avg:60.84ms
step:2066/2330 train_time:125689ms step_avg:60.84ms
step:2067/2330 train_time:125749ms step_avg:60.84ms
step:2068/2330 train_time:125813ms step_avg:60.84ms
step:2069/2330 train_time:125873ms step_avg:60.84ms
step:2070/2330 train_time:125936ms step_avg:60.84ms
step:2071/2330 train_time:125995ms step_avg:60.84ms
step:2072/2330 train_time:126059ms step_avg:60.84ms
step:2073/2330 train_time:126118ms step_avg:60.84ms
step:2074/2330 train_time:126180ms step_avg:60.84ms
step:2075/2330 train_time:126240ms step_avg:60.84ms
step:2076/2330 train_time:126303ms step_avg:60.84ms
step:2077/2330 train_time:126363ms step_avg:60.84ms
step:2078/2330 train_time:126425ms step_avg:60.84ms
step:2079/2330 train_time:126485ms step_avg:60.84ms
step:2080/2330 train_time:126549ms step_avg:60.84ms
step:2081/2330 train_time:126608ms step_avg:60.84ms
step:2082/2330 train_time:126671ms step_avg:60.84ms
step:2083/2330 train_time:126730ms step_avg:60.84ms
step:2084/2330 train_time:126793ms step_avg:60.84ms
step:2085/2330 train_time:126853ms step_avg:60.84ms
step:2086/2330 train_time:126916ms step_avg:60.84ms
step:2087/2330 train_time:126975ms step_avg:60.84ms
step:2088/2330 train_time:127039ms step_avg:60.84ms
step:2089/2330 train_time:127099ms step_avg:60.84ms
step:2090/2330 train_time:127162ms step_avg:60.84ms
step:2091/2330 train_time:127221ms step_avg:60.84ms
step:2092/2330 train_time:127284ms step_avg:60.84ms
step:2093/2330 train_time:127344ms step_avg:60.84ms
step:2094/2330 train_time:127407ms step_avg:60.84ms
step:2095/2330 train_time:127466ms step_avg:60.84ms
step:2096/2330 train_time:127529ms step_avg:60.84ms
step:2097/2330 train_time:127589ms step_avg:60.84ms
step:2098/2330 train_time:127653ms step_avg:60.84ms
step:2099/2330 train_time:127712ms step_avg:60.84ms
step:2100/2330 train_time:127775ms step_avg:60.85ms
step:2101/2330 train_time:127834ms step_avg:60.84ms
step:2102/2330 train_time:127898ms step_avg:60.85ms
step:2103/2330 train_time:127958ms step_avg:60.85ms
step:2104/2330 train_time:128021ms step_avg:60.85ms
step:2105/2330 train_time:128080ms step_avg:60.85ms
step:2106/2330 train_time:128144ms step_avg:60.85ms
step:2107/2330 train_time:128203ms step_avg:60.85ms
step:2108/2330 train_time:128266ms step_avg:60.85ms
step:2109/2330 train_time:128325ms step_avg:60.85ms
step:2110/2330 train_time:128389ms step_avg:60.85ms
step:2111/2330 train_time:128448ms step_avg:60.85ms
step:2112/2330 train_time:128510ms step_avg:60.85ms
step:2113/2330 train_time:128569ms step_avg:60.85ms
step:2114/2330 train_time:128633ms step_avg:60.85ms
step:2115/2330 train_time:128693ms step_avg:60.85ms
step:2116/2330 train_time:128756ms step_avg:60.85ms
step:2117/2330 train_time:128814ms step_avg:60.85ms
step:2118/2330 train_time:128878ms step_avg:60.85ms
step:2119/2330 train_time:128937ms step_avg:60.85ms
step:2120/2330 train_time:129001ms step_avg:60.85ms
step:2121/2330 train_time:129060ms step_avg:60.85ms
step:2122/2330 train_time:129123ms step_avg:60.85ms
step:2123/2330 train_time:129183ms step_avg:60.85ms
step:2124/2330 train_time:129246ms step_avg:60.85ms
step:2125/2330 train_time:129305ms step_avg:60.85ms
step:2126/2330 train_time:129368ms step_avg:60.85ms
step:2127/2330 train_time:129427ms step_avg:60.85ms
step:2128/2330 train_time:129490ms step_avg:60.85ms
step:2129/2330 train_time:129550ms step_avg:60.85ms
step:2130/2330 train_time:129613ms step_avg:60.85ms
step:2131/2330 train_time:129672ms step_avg:60.85ms
step:2132/2330 train_time:129735ms step_avg:60.85ms
step:2133/2330 train_time:129794ms step_avg:60.85ms
step:2134/2330 train_time:129858ms step_avg:60.85ms
step:2135/2330 train_time:129917ms step_avg:60.85ms
step:2136/2330 train_time:129981ms step_avg:60.85ms
step:2137/2330 train_time:130041ms step_avg:60.85ms
step:2138/2330 train_time:130103ms step_avg:60.85ms
step:2139/2330 train_time:130163ms step_avg:60.85ms
step:2140/2330 train_time:130226ms step_avg:60.85ms
step:2141/2330 train_time:130286ms step_avg:60.85ms
step:2142/2330 train_time:130349ms step_avg:60.85ms
step:2143/2330 train_time:130409ms step_avg:60.85ms
step:2144/2330 train_time:130471ms step_avg:60.85ms
step:2145/2330 train_time:130531ms step_avg:60.85ms
step:2146/2330 train_time:130593ms step_avg:60.85ms
step:2147/2330 train_time:130652ms step_avg:60.85ms
step:2148/2330 train_time:130715ms step_avg:60.85ms
step:2149/2330 train_time:130774ms step_avg:60.85ms
step:2150/2330 train_time:130838ms step_avg:60.85ms
step:2151/2330 train_time:130897ms step_avg:60.85ms
step:2152/2330 train_time:130961ms step_avg:60.86ms
step:2153/2330 train_time:131021ms step_avg:60.85ms
step:2154/2330 train_time:131084ms step_avg:60.86ms
step:2155/2330 train_time:131144ms step_avg:60.86ms
step:2156/2330 train_time:131207ms step_avg:60.86ms
step:2157/2330 train_time:131267ms step_avg:60.86ms
step:2158/2330 train_time:131329ms step_avg:60.86ms
step:2159/2330 train_time:131389ms step_avg:60.86ms
step:2160/2330 train_time:131452ms step_avg:60.86ms
step:2161/2330 train_time:131512ms step_avg:60.86ms
step:2162/2330 train_time:131575ms step_avg:60.86ms
step:2163/2330 train_time:131634ms step_avg:60.86ms
step:2164/2330 train_time:131698ms step_avg:60.86ms
step:2165/2330 train_time:131757ms step_avg:60.86ms
step:2166/2330 train_time:131820ms step_avg:60.86ms
step:2167/2330 train_time:131879ms step_avg:60.86ms
step:2168/2330 train_time:131941ms step_avg:60.86ms
step:2169/2330 train_time:132001ms step_avg:60.86ms
step:2170/2330 train_time:132064ms step_avg:60.86ms
step:2171/2330 train_time:132124ms step_avg:60.86ms
step:2172/2330 train_time:132187ms step_avg:60.86ms
step:2173/2330 train_time:132246ms step_avg:60.86ms
step:2174/2330 train_time:132309ms step_avg:60.86ms
step:2175/2330 train_time:132369ms step_avg:60.86ms
step:2176/2330 train_time:132433ms step_avg:60.86ms
step:2177/2330 train_time:132491ms step_avg:60.86ms
step:2178/2330 train_time:132555ms step_avg:60.86ms
step:2179/2330 train_time:132615ms step_avg:60.86ms
step:2180/2330 train_time:132678ms step_avg:60.86ms
step:2181/2330 train_time:132738ms step_avg:60.86ms
step:2182/2330 train_time:132801ms step_avg:60.86ms
step:2183/2330 train_time:132860ms step_avg:60.86ms
step:2184/2330 train_time:132923ms step_avg:60.86ms
step:2185/2330 train_time:132984ms step_avg:60.86ms
step:2186/2330 train_time:133047ms step_avg:60.86ms
step:2187/2330 train_time:133106ms step_avg:60.86ms
step:2188/2330 train_time:133170ms step_avg:60.86ms
step:2189/2330 train_time:133229ms step_avg:60.86ms
step:2190/2330 train_time:133293ms step_avg:60.86ms
step:2191/2330 train_time:133353ms step_avg:60.86ms
step:2192/2330 train_time:133416ms step_avg:60.86ms
step:2193/2330 train_time:133475ms step_avg:60.86ms
step:2194/2330 train_time:133539ms step_avg:60.87ms
step:2195/2330 train_time:133598ms step_avg:60.86ms
step:2196/2330 train_time:133661ms step_avg:60.87ms
step:2197/2330 train_time:133720ms step_avg:60.86ms
step:2198/2330 train_time:133783ms step_avg:60.87ms
step:2199/2330 train_time:133843ms step_avg:60.87ms
step:2200/2330 train_time:133906ms step_avg:60.87ms
step:2201/2330 train_time:133965ms step_avg:60.87ms
step:2202/2330 train_time:134028ms step_avg:60.87ms
step:2203/2330 train_time:134087ms step_avg:60.87ms
step:2204/2330 train_time:134151ms step_avg:60.87ms
step:2205/2330 train_time:134210ms step_avg:60.87ms
step:2206/2330 train_time:134273ms step_avg:60.87ms
step:2207/2330 train_time:134333ms step_avg:60.87ms
step:2208/2330 train_time:134396ms step_avg:60.87ms
step:2209/2330 train_time:134455ms step_avg:60.87ms
step:2210/2330 train_time:134519ms step_avg:60.87ms
step:2211/2330 train_time:134579ms step_avg:60.87ms
step:2212/2330 train_time:134642ms step_avg:60.87ms
step:2213/2330 train_time:134701ms step_avg:60.87ms
step:2214/2330 train_time:134765ms step_avg:60.87ms
step:2215/2330 train_time:134825ms step_avg:60.87ms
step:2216/2330 train_time:134888ms step_avg:60.87ms
step:2217/2330 train_time:134948ms step_avg:60.87ms
step:2218/2330 train_time:135010ms step_avg:60.87ms
step:2219/2330 train_time:135069ms step_avg:60.87ms
step:2220/2330 train_time:135133ms step_avg:60.87ms
step:2221/2330 train_time:135193ms step_avg:60.87ms
step:2222/2330 train_time:135256ms step_avg:60.87ms
step:2223/2330 train_time:135315ms step_avg:60.87ms
step:2224/2330 train_time:135378ms step_avg:60.87ms
step:2225/2330 train_time:135438ms step_avg:60.87ms
step:2226/2330 train_time:135501ms step_avg:60.87ms
step:2227/2330 train_time:135561ms step_avg:60.87ms
step:2228/2330 train_time:135623ms step_avg:60.87ms
step:2229/2330 train_time:135682ms step_avg:60.87ms
step:2230/2330 train_time:135746ms step_avg:60.87ms
step:2231/2330 train_time:135807ms step_avg:60.87ms
step:2232/2330 train_time:135869ms step_avg:60.87ms
step:2233/2330 train_time:135928ms step_avg:60.87ms
step:2234/2330 train_time:135991ms step_avg:60.87ms
step:2235/2330 train_time:136051ms step_avg:60.87ms
step:2236/2330 train_time:136114ms step_avg:60.87ms
step:2237/2330 train_time:136174ms step_avg:60.87ms
step:2238/2330 train_time:136237ms step_avg:60.87ms
step:2239/2330 train_time:136296ms step_avg:60.87ms
step:2240/2330 train_time:136359ms step_avg:60.87ms
step:2241/2330 train_time:136418ms step_avg:60.87ms
step:2242/2330 train_time:136481ms step_avg:60.87ms
step:2243/2330 train_time:136541ms step_avg:60.87ms
step:2244/2330 train_time:136604ms step_avg:60.88ms
step:2245/2330 train_time:136663ms step_avg:60.87ms
step:2246/2330 train_time:136727ms step_avg:60.88ms
step:2247/2330 train_time:136787ms step_avg:60.88ms
step:2248/2330 train_time:136849ms step_avg:60.88ms
step:2249/2330 train_time:136908ms step_avg:60.88ms
step:2250/2330 train_time:136972ms step_avg:60.88ms
step:2250/2330 val_loss:3.6709 train_time:137043ms step_avg:60.91ms
step:2251/2330 train_time:137065ms step_avg:60.89ms
step:2252/2330 train_time:137097ms step_avg:60.88ms
step:2253/2330 train_time:137158ms step_avg:60.88ms
step:2254/2330 train_time:137225ms step_avg:60.88ms
step:2255/2330 train_time:137286ms step_avg:60.88ms
step:2256/2330 train_time:137348ms step_avg:60.88ms
step:2257/2330 train_time:137408ms step_avg:60.88ms
step:2258/2330 train_time:137471ms step_avg:60.88ms
step:2259/2330 train_time:137530ms step_avg:60.88ms
step:2260/2330 train_time:137593ms step_avg:60.88ms
step:2261/2330 train_time:137652ms step_avg:60.88ms
step:2262/2330 train_time:137714ms step_avg:60.88ms
step:2263/2330 train_time:137773ms step_avg:60.88ms
step:2264/2330 train_time:137835ms step_avg:60.88ms
step:2265/2330 train_time:137894ms step_avg:60.88ms
step:2266/2330 train_time:137956ms step_avg:60.88ms
step:2267/2330 train_time:138017ms step_avg:60.88ms
step:2268/2330 train_time:138083ms step_avg:60.88ms
step:2269/2330 train_time:138143ms step_avg:60.88ms
step:2270/2330 train_time:138206ms step_avg:60.88ms
step:2271/2330 train_time:138267ms step_avg:60.88ms
step:2272/2330 train_time:138331ms step_avg:60.89ms
step:2273/2330 train_time:138390ms step_avg:60.88ms
step:2274/2330 train_time:138453ms step_avg:60.89ms
step:2275/2330 train_time:138512ms step_avg:60.88ms
step:2276/2330 train_time:138574ms step_avg:60.89ms
step:2277/2330 train_time:138634ms step_avg:60.88ms
step:2278/2330 train_time:138696ms step_avg:60.89ms
step:2279/2330 train_time:138755ms step_avg:60.88ms
step:2280/2330 train_time:138818ms step_avg:60.88ms
step:2281/2330 train_time:138877ms step_avg:60.88ms
step:2282/2330 train_time:138939ms step_avg:60.88ms
step:2283/2330 train_time:138999ms step_avg:60.88ms
step:2284/2330 train_time:139063ms step_avg:60.89ms
step:2285/2330 train_time:139123ms step_avg:60.89ms
step:2286/2330 train_time:139187ms step_avg:60.89ms
step:2287/2330 train_time:139247ms step_avg:60.89ms
step:2288/2330 train_time:139310ms step_avg:60.89ms
step:2289/2330 train_time:139369ms step_avg:60.89ms
step:2290/2330 train_time:139434ms step_avg:60.89ms
step:2291/2330 train_time:139493ms step_avg:60.89ms
step:2292/2330 train_time:139555ms step_avg:60.89ms
step:2293/2330 train_time:139615ms step_avg:60.89ms
step:2294/2330 train_time:139678ms step_avg:60.89ms
step:2295/2330 train_time:139736ms step_avg:60.89ms
step:2296/2330 train_time:139799ms step_avg:60.89ms
step:2297/2330 train_time:139857ms step_avg:60.89ms
step:2298/2330 train_time:139921ms step_avg:60.89ms
step:2299/2330 train_time:139980ms step_avg:60.89ms
step:2300/2330 train_time:140043ms step_avg:60.89ms
step:2301/2330 train_time:140102ms step_avg:60.89ms
step:2302/2330 train_time:140166ms step_avg:60.89ms
step:2303/2330 train_time:140226ms step_avg:60.89ms
step:2304/2330 train_time:140290ms step_avg:60.89ms
step:2305/2330 train_time:140349ms step_avg:60.89ms
step:2306/2330 train_time:140413ms step_avg:60.89ms
step:2307/2330 train_time:140472ms step_avg:60.89ms
step:2308/2330 train_time:140535ms step_avg:60.89ms
step:2309/2330 train_time:140594ms step_avg:60.89ms
step:2310/2330 train_time:140657ms step_avg:60.89ms
step:2311/2330 train_time:140716ms step_avg:60.89ms
step:2312/2330 train_time:140778ms step_avg:60.89ms
step:2313/2330 train_time:140837ms step_avg:60.89ms
step:2314/2330 train_time:140900ms step_avg:60.89ms
step:2315/2330 train_time:140959ms step_avg:60.89ms
step:2316/2330 train_time:141022ms step_avg:60.89ms
step:2317/2330 train_time:141082ms step_avg:60.89ms
step:2318/2330 train_time:141144ms step_avg:60.89ms
step:2319/2330 train_time:141205ms step_avg:60.89ms
step:2320/2330 train_time:141269ms step_avg:60.89ms
step:2321/2330 train_time:141328ms step_avg:60.89ms
step:2322/2330 train_time:141392ms step_avg:60.89ms
step:2323/2330 train_time:141452ms step_avg:60.89ms
step:2324/2330 train_time:141515ms step_avg:60.89ms
step:2325/2330 train_time:141574ms step_avg:60.89ms
step:2326/2330 train_time:141637ms step_avg:60.89ms
step:2327/2330 train_time:141696ms step_avg:60.89ms
step:2328/2330 train_time:141759ms step_avg:60.89ms
step:2329/2330 train_time:141819ms step_avg:60.89ms
step:2330/2330 train_time:141881ms step_avg:60.89ms
step:2330/2330 val_loss:3.6597 train_time:141954ms step_avg:60.92ms
peak memory allocated: 29998 MiB reserved: 44076 MiB
