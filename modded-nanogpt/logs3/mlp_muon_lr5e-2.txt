import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                # v_chunk = divide_by_norm(batched)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                # v_chunk = divide_by_norm(batched_update_grads)
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        # self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "mlp_muon_lr5e-2"
    os.makedirs("logs3", exist_ok=True)
    logfile = f"logs3/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
# embed_params = [p for n, p in model.named_parameters() if "embed" in n]
embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=5e-2, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    return 1.0

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum_min + 0.5 * (momentum_max - momentum_min)

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs3/{run_id}", exist_ok=True)
            torch.save(log, f"logs3/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 08:03:44 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   32C    P0             112W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   31C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   28C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   31C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   32C    P0             115W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:76ms step_avg:76.25ms
step:2/2330 train_time:163ms step_avg:81.39ms
step:3/2330 train_time:175ms step_avg:58.39ms
step:4/2330 train_time:188ms step_avg:46.93ms
step:5/2330 train_time:198ms step_avg:39.69ms
step:6/2330 train_time:320ms step_avg:53.28ms
step:7/2330 train_time:353ms step_avg:50.41ms
step:8/2330 train_time:397ms step_avg:49.56ms
step:9/2330 train_time:431ms step_avg:47.94ms
step:10/2330 train_time:475ms step_avg:47.48ms
step:11/2330 train_time:509ms step_avg:46.30ms
step:12/2330 train_time:554ms step_avg:46.13ms
step:13/2330 train_time:588ms step_avg:45.27ms
step:14/2330 train_time:632ms step_avg:45.17ms
step:15/2330 train_time:667ms step_avg:44.47ms
step:16/2330 train_time:711ms step_avg:44.46ms
step:17/2330 train_time:746ms step_avg:43.90ms
step:18/2330 train_time:790ms step_avg:43.91ms
step:19/2330 train_time:825ms step_avg:43.42ms
step:20/2330 train_time:869ms step_avg:43.47ms
step:21/2330 train_time:904ms step_avg:43.06ms
step:22/2330 train_time:949ms step_avg:43.12ms
step:23/2330 train_time:983ms step_avg:42.74ms
step:24/2330 train_time:1027ms step_avg:42.80ms
step:25/2330 train_time:1062ms step_avg:42.47ms
step:26/2330 train_time:1107ms step_avg:42.58ms
step:27/2330 train_time:1141ms step_avg:42.27ms
step:28/2330 train_time:1190ms step_avg:42.51ms
step:29/2330 train_time:1230ms step_avg:42.43ms
step:30/2330 train_time:1279ms step_avg:42.63ms
step:31/2330 train_time:1316ms step_avg:42.44ms
step:32/2330 train_time:1361ms step_avg:42.52ms
step:33/2330 train_time:1396ms step_avg:42.29ms
step:34/2330 train_time:1441ms step_avg:42.39ms
step:35/2330 train_time:1477ms step_avg:42.21ms
step:36/2330 train_time:1522ms step_avg:42.28ms
step:37/2330 train_time:1558ms step_avg:42.10ms
step:38/2330 train_time:1602ms step_avg:42.16ms
step:39/2330 train_time:1637ms step_avg:41.98ms
step:40/2330 train_time:1682ms step_avg:42.04ms
step:41/2330 train_time:1716ms step_avg:41.86ms
step:42/2330 train_time:1760ms step_avg:41.91ms
step:43/2330 train_time:1795ms step_avg:41.75ms
step:44/2330 train_time:1840ms step_avg:41.81ms
step:45/2330 train_time:1874ms step_avg:41.65ms
step:46/2330 train_time:1919ms step_avg:41.71ms
step:47/2330 train_time:1954ms step_avg:41.57ms
step:48/2330 train_time:1998ms step_avg:41.63ms
step:49/2330 train_time:2032ms step_avg:41.48ms
step:50/2330 train_time:2077ms step_avg:41.53ms
step:51/2330 train_time:2112ms step_avg:41.41ms
step:52/2330 train_time:2158ms step_avg:41.50ms
step:53/2330 train_time:2195ms step_avg:41.41ms
step:54/2330 train_time:2241ms step_avg:41.49ms
step:55/2330 train_time:2277ms step_avg:41.40ms
step:56/2330 train_time:2322ms step_avg:41.47ms
step:57/2330 train_time:2359ms step_avg:41.38ms
step:58/2330 train_time:2404ms step_avg:41.45ms
step:59/2330 train_time:2440ms step_avg:41.36ms
step:60/2330 train_time:2485ms step_avg:41.42ms
step:61/2330 train_time:2520ms step_avg:41.32ms
step:62/2330 train_time:2565ms step_avg:41.37ms
step:63/2330 train_time:2600ms step_avg:41.28ms
step:64/2330 train_time:2646ms step_avg:41.34ms
step:65/2330 train_time:2680ms step_avg:41.24ms
step:66/2330 train_time:2724ms step_avg:41.28ms
step:67/2330 train_time:2759ms step_avg:41.17ms
step:68/2330 train_time:2802ms step_avg:41.21ms
step:69/2330 train_time:2838ms step_avg:41.12ms
step:70/2330 train_time:2882ms step_avg:41.17ms
step:71/2330 train_time:2917ms step_avg:41.09ms
step:72/2330 train_time:2961ms step_avg:41.12ms
step:73/2330 train_time:2995ms step_avg:41.03ms
step:74/2330 train_time:3039ms step_avg:41.07ms
step:75/2330 train_time:3075ms step_avg:40.99ms
step:76/2330 train_time:3120ms step_avg:41.05ms
step:77/2330 train_time:3155ms step_avg:40.98ms
step:78/2330 train_time:3201ms step_avg:41.03ms
step:79/2330 train_time:3236ms step_avg:40.96ms
step:80/2330 train_time:3282ms step_avg:41.02ms
step:81/2330 train_time:3318ms step_avg:40.96ms
step:82/2330 train_time:3362ms step_avg:41.00ms
step:83/2330 train_time:3398ms step_avg:40.93ms
step:84/2330 train_time:3443ms step_avg:40.99ms
step:85/2330 train_time:3479ms step_avg:40.93ms
step:86/2330 train_time:3524ms step_avg:40.97ms
step:87/2330 train_time:3559ms step_avg:40.91ms
step:88/2330 train_time:3603ms step_avg:40.95ms
step:89/2330 train_time:3638ms step_avg:40.88ms
step:90/2330 train_time:3682ms step_avg:40.91ms
step:91/2330 train_time:3717ms step_avg:40.85ms
step:92/2330 train_time:3762ms step_avg:40.89ms
step:93/2330 train_time:3797ms step_avg:40.82ms
step:94/2330 train_time:3840ms step_avg:40.86ms
step:95/2330 train_time:3875ms step_avg:40.79ms
step:96/2330 train_time:3919ms step_avg:40.82ms
step:97/2330 train_time:3954ms step_avg:40.76ms
step:98/2330 train_time:3998ms step_avg:40.80ms
step:99/2330 train_time:4033ms step_avg:40.74ms
step:100/2330 train_time:4078ms step_avg:40.78ms
step:101/2330 train_time:4114ms step_avg:40.74ms
step:102/2330 train_time:4159ms step_avg:40.78ms
step:103/2330 train_time:4195ms step_avg:40.73ms
step:104/2330 train_time:4240ms step_avg:40.77ms
step:105/2330 train_time:4276ms step_avg:40.72ms
step:106/2330 train_time:4321ms step_avg:40.76ms
step:107/2330 train_time:4357ms step_avg:40.72ms
step:108/2330 train_time:4402ms step_avg:40.76ms
step:109/2330 train_time:4438ms step_avg:40.71ms
step:110/2330 train_time:4482ms step_avg:40.75ms
step:111/2330 train_time:4518ms step_avg:40.70ms
step:112/2330 train_time:4562ms step_avg:40.74ms
step:113/2330 train_time:4597ms step_avg:40.69ms
step:114/2330 train_time:4642ms step_avg:40.72ms
step:115/2330 train_time:4677ms step_avg:40.67ms
step:116/2330 train_time:4721ms step_avg:40.70ms
step:117/2330 train_time:4756ms step_avg:40.65ms
step:118/2330 train_time:4800ms step_avg:40.68ms
step:119/2330 train_time:4835ms step_avg:40.63ms
step:120/2330 train_time:4879ms step_avg:40.66ms
step:121/2330 train_time:4914ms step_avg:40.61ms
step:122/2330 train_time:4958ms step_avg:40.64ms
step:123/2330 train_time:4994ms step_avg:40.60ms
step:124/2330 train_time:5038ms step_avg:40.63ms
step:125/2330 train_time:5074ms step_avg:40.59ms
step:126/2330 train_time:5118ms step_avg:40.62ms
step:127/2330 train_time:5154ms step_avg:40.58ms
step:128/2330 train_time:5199ms step_avg:40.61ms
step:129/2330 train_time:5234ms step_avg:40.57ms
step:130/2330 train_time:5279ms step_avg:40.60ms
step:131/2330 train_time:5315ms step_avg:40.57ms
step:132/2330 train_time:5359ms step_avg:40.60ms
step:133/2330 train_time:5395ms step_avg:40.56ms
step:134/2330 train_time:5440ms step_avg:40.60ms
step:135/2330 train_time:5475ms step_avg:40.56ms
step:136/2330 train_time:5521ms step_avg:40.59ms
step:137/2330 train_time:5556ms step_avg:40.56ms
step:138/2330 train_time:5601ms step_avg:40.58ms
step:139/2330 train_time:5636ms step_avg:40.54ms
step:140/2330 train_time:5680ms step_avg:40.57ms
step:141/2330 train_time:5715ms step_avg:40.53ms
step:142/2330 train_time:5759ms step_avg:40.56ms
step:143/2330 train_time:5794ms step_avg:40.52ms
step:144/2330 train_time:5838ms step_avg:40.54ms
step:145/2330 train_time:5873ms step_avg:40.50ms
step:146/2330 train_time:5918ms step_avg:40.53ms
step:147/2330 train_time:5952ms step_avg:40.49ms
step:148/2330 train_time:5997ms step_avg:40.52ms
step:149/2330 train_time:6032ms step_avg:40.48ms
step:150/2330 train_time:6077ms step_avg:40.51ms
step:151/2330 train_time:6112ms step_avg:40.48ms
step:152/2330 train_time:6157ms step_avg:40.51ms
step:153/2330 train_time:6192ms step_avg:40.47ms
step:154/2330 train_time:6236ms step_avg:40.49ms
step:155/2330 train_time:6271ms step_avg:40.46ms
step:156/2330 train_time:6316ms step_avg:40.49ms
step:157/2330 train_time:6352ms step_avg:40.46ms
step:158/2330 train_time:6397ms step_avg:40.49ms
step:159/2330 train_time:6432ms step_avg:40.45ms
step:160/2330 train_time:6478ms step_avg:40.49ms
step:161/2330 train_time:6513ms step_avg:40.45ms
step:162/2330 train_time:6558ms step_avg:40.48ms
step:163/2330 train_time:6593ms step_avg:40.45ms
step:164/2330 train_time:6637ms step_avg:40.47ms
step:165/2330 train_time:6672ms step_avg:40.43ms
step:166/2330 train_time:6716ms step_avg:40.46ms
step:167/2330 train_time:6751ms step_avg:40.43ms
step:168/2330 train_time:6796ms step_avg:40.45ms
step:169/2330 train_time:6830ms step_avg:40.41ms
step:170/2330 train_time:6874ms step_avg:40.44ms
step:171/2330 train_time:6909ms step_avg:40.40ms
step:172/2330 train_time:6953ms step_avg:40.43ms
step:173/2330 train_time:6988ms step_avg:40.39ms
step:174/2330 train_time:7033ms step_avg:40.42ms
step:175/2330 train_time:7068ms step_avg:40.39ms
step:176/2330 train_time:7113ms step_avg:40.42ms
step:177/2330 train_time:7148ms step_avg:40.39ms
step:178/2330 train_time:7193ms step_avg:40.41ms
step:179/2330 train_time:7228ms step_avg:40.38ms
step:180/2330 train_time:7273ms step_avg:40.40ms
step:181/2330 train_time:7308ms step_avg:40.38ms
step:182/2330 train_time:7353ms step_avg:40.40ms
step:183/2330 train_time:7389ms step_avg:40.38ms
step:184/2330 train_time:7434ms step_avg:40.40ms
step:185/2330 train_time:7469ms step_avg:40.37ms
step:186/2330 train_time:7514ms step_avg:40.40ms
step:187/2330 train_time:7549ms step_avg:40.37ms
step:188/2330 train_time:7594ms step_avg:40.39ms
step:189/2330 train_time:7629ms step_avg:40.36ms
step:190/2330 train_time:7674ms step_avg:40.39ms
step:191/2330 train_time:7708ms step_avg:40.36ms
step:192/2330 train_time:7753ms step_avg:40.38ms
step:193/2330 train_time:7787ms step_avg:40.35ms
step:194/2330 train_time:7832ms step_avg:40.37ms
step:195/2330 train_time:7868ms step_avg:40.35ms
step:196/2330 train_time:7912ms step_avg:40.37ms
step:197/2330 train_time:7947ms step_avg:40.34ms
step:198/2330 train_time:7991ms step_avg:40.36ms
step:199/2330 train_time:8026ms step_avg:40.33ms
step:200/2330 train_time:8070ms step_avg:40.35ms
step:201/2330 train_time:8106ms step_avg:40.33ms
step:202/2330 train_time:8151ms step_avg:40.35ms
step:203/2330 train_time:8187ms step_avg:40.33ms
step:204/2330 train_time:8233ms step_avg:40.36ms
step:205/2330 train_time:8268ms step_avg:40.33ms
step:206/2330 train_time:8314ms step_avg:40.36ms
step:207/2330 train_time:8349ms step_avg:40.33ms
step:208/2330 train_time:8394ms step_avg:40.36ms
step:209/2330 train_time:8429ms step_avg:40.33ms
step:210/2330 train_time:8474ms step_avg:40.35ms
step:211/2330 train_time:8509ms step_avg:40.33ms
step:212/2330 train_time:8554ms step_avg:40.35ms
step:213/2330 train_time:8589ms step_avg:40.32ms
step:214/2330 train_time:8634ms step_avg:40.35ms
step:215/2330 train_time:8669ms step_avg:40.32ms
step:216/2330 train_time:8714ms step_avg:40.34ms
step:217/2330 train_time:8749ms step_avg:40.32ms
step:218/2330 train_time:8793ms step_avg:40.34ms
step:219/2330 train_time:8828ms step_avg:40.31ms
step:220/2330 train_time:8872ms step_avg:40.33ms
step:221/2330 train_time:8907ms step_avg:40.31ms
step:222/2330 train_time:8952ms step_avg:40.33ms
step:223/2330 train_time:8987ms step_avg:40.30ms
step:224/2330 train_time:9032ms step_avg:40.32ms
step:225/2330 train_time:9067ms step_avg:40.30ms
step:226/2330 train_time:9112ms step_avg:40.32ms
step:227/2330 train_time:9147ms step_avg:40.29ms
step:228/2330 train_time:9192ms step_avg:40.31ms
step:229/2330 train_time:9227ms step_avg:40.29ms
step:230/2330 train_time:9272ms step_avg:40.31ms
step:231/2330 train_time:9308ms step_avg:40.29ms
step:232/2330 train_time:9353ms step_avg:40.32ms
step:233/2330 train_time:9388ms step_avg:40.29ms
step:234/2330 train_time:9433ms step_avg:40.31ms
step:235/2330 train_time:9469ms step_avg:40.29ms
step:236/2330 train_time:9513ms step_avg:40.31ms
step:237/2330 train_time:9548ms step_avg:40.29ms
step:238/2330 train_time:9593ms step_avg:40.31ms
step:239/2330 train_time:9629ms step_avg:40.29ms
step:240/2330 train_time:9673ms step_avg:40.31ms
step:241/2330 train_time:9708ms step_avg:40.28ms
step:242/2330 train_time:9753ms step_avg:40.30ms
step:243/2330 train_time:9788ms step_avg:40.28ms
step:244/2330 train_time:9832ms step_avg:40.30ms
step:245/2330 train_time:9867ms step_avg:40.28ms
step:246/2330 train_time:9912ms step_avg:40.29ms
step:247/2330 train_time:9947ms step_avg:40.27ms
step:248/2330 train_time:9991ms step_avg:40.29ms
step:249/2330 train_time:10026ms step_avg:40.27ms
step:250/2330 train_time:10071ms step_avg:40.28ms
step:250/2330 val_loss:5.4137 train_time:10157ms step_avg:40.63ms
step:251/2330 train_time:10168ms step_avg:40.51ms
step:252/2330 train_time:10180ms step_avg:40.40ms
step:253/2330 train_time:10191ms step_avg:40.28ms
step:254/2330 train_time:10229ms step_avg:40.27ms
step:255/2330 train_time:10263ms step_avg:40.25ms
step:256/2330 train_time:10307ms step_avg:40.26ms
step:257/2330 train_time:10341ms step_avg:40.24ms
step:258/2330 train_time:10385ms step_avg:40.25ms
step:259/2330 train_time:10419ms step_avg:40.23ms
step:260/2330 train_time:10463ms step_avg:40.24ms
step:261/2330 train_time:10503ms step_avg:40.24ms
step:262/2330 train_time:10549ms step_avg:40.26ms
step:263/2330 train_time:10585ms step_avg:40.25ms
step:264/2330 train_time:10632ms step_avg:40.27ms
step:265/2330 train_time:10667ms step_avg:40.25ms
step:266/2330 train_time:10711ms step_avg:40.27ms
step:267/2330 train_time:10746ms step_avg:40.25ms
step:268/2330 train_time:10791ms step_avg:40.26ms
step:269/2330 train_time:10825ms step_avg:40.24ms
step:270/2330 train_time:10869ms step_avg:40.26ms
step:271/2330 train_time:10904ms step_avg:40.24ms
step:272/2330 train_time:10948ms step_avg:40.25ms
step:273/2330 train_time:10983ms step_avg:40.23ms
step:274/2330 train_time:11026ms step_avg:40.24ms
step:275/2330 train_time:11061ms step_avg:40.22ms
step:276/2330 train_time:11108ms step_avg:40.25ms
step:277/2330 train_time:11144ms step_avg:40.23ms
step:278/2330 train_time:11190ms step_avg:40.25ms
step:279/2330 train_time:11225ms step_avg:40.23ms
step:280/2330 train_time:11270ms step_avg:40.25ms
step:281/2330 train_time:11305ms step_avg:40.23ms
step:282/2330 train_time:11348ms step_avg:40.24ms
step:283/2330 train_time:11383ms step_avg:40.22ms
step:284/2330 train_time:11428ms step_avg:40.24ms
step:285/2330 train_time:11463ms step_avg:40.22ms
step:286/2330 train_time:11509ms step_avg:40.24ms
step:287/2330 train_time:11544ms step_avg:40.22ms
step:288/2330 train_time:11589ms step_avg:40.24ms
step:289/2330 train_time:11625ms step_avg:40.22ms
step:290/2330 train_time:11670ms step_avg:40.24ms
step:291/2330 train_time:11705ms step_avg:40.22ms
step:292/2330 train_time:11750ms step_avg:40.24ms
step:293/2330 train_time:11785ms step_avg:40.22ms
step:294/2330 train_time:11829ms step_avg:40.24ms
step:295/2330 train_time:11864ms step_avg:40.22ms
step:296/2330 train_time:11908ms step_avg:40.23ms
step:297/2330 train_time:11943ms step_avg:40.21ms
step:298/2330 train_time:11987ms step_avg:40.22ms
step:299/2330 train_time:12022ms step_avg:40.21ms
step:300/2330 train_time:12067ms step_avg:40.22ms
step:301/2330 train_time:12102ms step_avg:40.21ms
step:302/2330 train_time:12147ms step_avg:40.22ms
step:303/2330 train_time:12182ms step_avg:40.21ms
step:304/2330 train_time:12227ms step_avg:40.22ms
step:305/2330 train_time:12262ms step_avg:40.20ms
step:306/2330 train_time:12307ms step_avg:40.22ms
step:307/2330 train_time:12342ms step_avg:40.20ms
step:308/2330 train_time:12387ms step_avg:40.22ms
step:309/2330 train_time:12423ms step_avg:40.20ms
step:310/2330 train_time:12468ms step_avg:40.22ms
step:311/2330 train_time:12503ms step_avg:40.20ms
step:312/2330 train_time:12548ms step_avg:40.22ms
step:313/2330 train_time:12583ms step_avg:40.20ms
step:314/2330 train_time:12628ms step_avg:40.22ms
step:315/2330 train_time:12664ms step_avg:40.20ms
step:316/2330 train_time:12709ms step_avg:40.22ms
step:317/2330 train_time:12744ms step_avg:40.20ms
step:318/2330 train_time:12788ms step_avg:40.22ms
step:319/2330 train_time:12824ms step_avg:40.20ms
step:320/2330 train_time:12868ms step_avg:40.21ms
step:321/2330 train_time:12903ms step_avg:40.20ms
step:322/2330 train_time:12947ms step_avg:40.21ms
step:323/2330 train_time:12982ms step_avg:40.19ms
step:324/2330 train_time:13027ms step_avg:40.21ms
step:325/2330 train_time:13062ms step_avg:40.19ms
step:326/2330 train_time:13107ms step_avg:40.20ms
step:327/2330 train_time:13142ms step_avg:40.19ms
step:328/2330 train_time:13188ms step_avg:40.21ms
step:329/2330 train_time:13223ms step_avg:40.19ms
step:330/2330 train_time:13267ms step_avg:40.20ms
step:331/2330 train_time:13302ms step_avg:40.19ms
step:332/2330 train_time:13347ms step_avg:40.20ms
step:333/2330 train_time:13382ms step_avg:40.19ms
step:334/2330 train_time:13427ms step_avg:40.20ms
step:335/2330 train_time:13463ms step_avg:40.19ms
step:336/2330 train_time:13508ms step_avg:40.20ms
step:337/2330 train_time:13543ms step_avg:40.19ms
step:338/2330 train_time:13589ms step_avg:40.20ms
step:339/2330 train_time:13624ms step_avg:40.19ms
step:340/2330 train_time:13669ms step_avg:40.20ms
step:341/2330 train_time:13705ms step_avg:40.19ms
step:342/2330 train_time:13749ms step_avg:40.20ms
step:343/2330 train_time:13785ms step_avg:40.19ms
step:344/2330 train_time:13829ms step_avg:40.20ms
step:345/2330 train_time:13864ms step_avg:40.18ms
step:346/2330 train_time:13908ms step_avg:40.20ms
step:347/2330 train_time:13943ms step_avg:40.18ms
step:348/2330 train_time:13987ms step_avg:40.19ms
step:349/2330 train_time:14022ms step_avg:40.18ms
step:350/2330 train_time:14067ms step_avg:40.19ms
step:351/2330 train_time:14102ms step_avg:40.18ms
step:352/2330 train_time:14147ms step_avg:40.19ms
step:353/2330 train_time:14182ms step_avg:40.17ms
step:354/2330 train_time:14227ms step_avg:40.19ms
step:355/2330 train_time:14262ms step_avg:40.17ms
step:356/2330 train_time:14307ms step_avg:40.19ms
step:357/2330 train_time:14342ms step_avg:40.17ms
step:358/2330 train_time:14387ms step_avg:40.19ms
step:359/2330 train_time:14422ms step_avg:40.17ms
step:360/2330 train_time:14467ms step_avg:40.19ms
step:361/2330 train_time:14503ms step_avg:40.17ms
step:362/2330 train_time:14548ms step_avg:40.19ms
step:363/2330 train_time:14584ms step_avg:40.18ms
step:364/2330 train_time:14629ms step_avg:40.19ms
step:365/2330 train_time:14665ms step_avg:40.18ms
step:366/2330 train_time:14710ms step_avg:40.19ms
step:367/2330 train_time:14745ms step_avg:40.18ms
step:368/2330 train_time:14789ms step_avg:40.19ms
step:369/2330 train_time:14824ms step_avg:40.17ms
step:370/2330 train_time:14869ms step_avg:40.19ms
step:371/2330 train_time:14904ms step_avg:40.17ms
step:372/2330 train_time:14948ms step_avg:40.18ms
step:373/2330 train_time:14983ms step_avg:40.17ms
step:374/2330 train_time:15028ms step_avg:40.18ms
step:375/2330 train_time:15063ms step_avg:40.17ms
step:376/2330 train_time:15107ms step_avg:40.18ms
step:377/2330 train_time:15142ms step_avg:40.16ms
step:378/2330 train_time:15186ms step_avg:40.17ms
step:379/2330 train_time:15221ms step_avg:40.16ms
step:380/2330 train_time:15266ms step_avg:40.17ms
step:381/2330 train_time:15301ms step_avg:40.16ms
step:382/2330 train_time:15346ms step_avg:40.17ms
step:383/2330 train_time:15381ms step_avg:40.16ms
step:384/2330 train_time:15426ms step_avg:40.17ms
step:385/2330 train_time:15462ms step_avg:40.16ms
step:386/2330 train_time:15507ms step_avg:40.17ms
step:387/2330 train_time:15543ms step_avg:40.16ms
step:388/2330 train_time:15588ms step_avg:40.18ms
step:389/2330 train_time:15623ms step_avg:40.16ms
step:390/2330 train_time:15668ms step_avg:40.17ms
step:391/2330 train_time:15704ms step_avg:40.16ms
step:392/2330 train_time:15748ms step_avg:40.17ms
step:393/2330 train_time:15784ms step_avg:40.16ms
step:394/2330 train_time:15828ms step_avg:40.17ms
step:395/2330 train_time:15863ms step_avg:40.16ms
step:396/2330 train_time:15908ms step_avg:40.17ms
step:397/2330 train_time:15943ms step_avg:40.16ms
step:398/2330 train_time:15986ms step_avg:40.17ms
step:399/2330 train_time:16022ms step_avg:40.15ms
step:400/2330 train_time:16066ms step_avg:40.16ms
step:401/2330 train_time:16101ms step_avg:40.15ms
step:402/2330 train_time:16145ms step_avg:40.16ms
step:403/2330 train_time:16180ms step_avg:40.15ms
step:404/2330 train_time:16224ms step_avg:40.16ms
step:405/2330 train_time:16260ms step_avg:40.15ms
step:406/2330 train_time:16304ms step_avg:40.16ms
step:407/2330 train_time:16338ms step_avg:40.14ms
step:408/2330 train_time:16382ms step_avg:40.15ms
step:409/2330 train_time:16418ms step_avg:40.14ms
step:410/2330 train_time:16463ms step_avg:40.15ms
step:411/2330 train_time:16498ms step_avg:40.14ms
step:412/2330 train_time:16543ms step_avg:40.15ms
step:413/2330 train_time:16579ms step_avg:40.14ms
step:414/2330 train_time:16624ms step_avg:40.15ms
step:415/2330 train_time:16659ms step_avg:40.14ms
step:416/2330 train_time:16704ms step_avg:40.15ms
step:417/2330 train_time:16739ms step_avg:40.14ms
step:418/2330 train_time:16784ms step_avg:40.15ms
step:419/2330 train_time:16819ms step_avg:40.14ms
step:420/2330 train_time:16863ms step_avg:40.15ms
step:421/2330 train_time:16898ms step_avg:40.14ms
step:422/2330 train_time:16943ms step_avg:40.15ms
step:423/2330 train_time:16977ms step_avg:40.14ms
step:424/2330 train_time:17021ms step_avg:40.14ms
step:425/2330 train_time:17057ms step_avg:40.13ms
step:426/2330 train_time:17101ms step_avg:40.14ms
step:427/2330 train_time:17135ms step_avg:40.13ms
step:428/2330 train_time:17180ms step_avg:40.14ms
step:429/2330 train_time:17214ms step_avg:40.13ms
step:430/2330 train_time:17258ms step_avg:40.14ms
step:431/2330 train_time:17293ms step_avg:40.12ms
step:432/2330 train_time:17337ms step_avg:40.13ms
step:433/2330 train_time:17372ms step_avg:40.12ms
step:434/2330 train_time:17417ms step_avg:40.13ms
step:435/2330 train_time:17452ms step_avg:40.12ms
step:436/2330 train_time:17497ms step_avg:40.13ms
step:437/2330 train_time:17533ms step_avg:40.12ms
step:438/2330 train_time:17577ms step_avg:40.13ms
step:439/2330 train_time:17613ms step_avg:40.12ms
step:440/2330 train_time:17657ms step_avg:40.13ms
step:441/2330 train_time:17693ms step_avg:40.12ms
step:442/2330 train_time:17737ms step_avg:40.13ms
step:443/2330 train_time:17772ms step_avg:40.12ms
step:444/2330 train_time:17818ms step_avg:40.13ms
step:445/2330 train_time:17853ms step_avg:40.12ms
step:446/2330 train_time:17897ms step_avg:40.13ms
step:447/2330 train_time:17932ms step_avg:40.12ms
step:448/2330 train_time:17977ms step_avg:40.13ms
step:449/2330 train_time:18012ms step_avg:40.12ms
step:450/2330 train_time:18056ms step_avg:40.12ms
step:451/2330 train_time:18092ms step_avg:40.11ms
step:452/2330 train_time:18136ms step_avg:40.12ms
step:453/2330 train_time:18171ms step_avg:40.11ms
step:454/2330 train_time:18215ms step_avg:40.12ms
step:455/2330 train_time:18250ms step_avg:40.11ms
step:456/2330 train_time:18295ms step_avg:40.12ms
step:457/2330 train_time:18330ms step_avg:40.11ms
step:458/2330 train_time:18374ms step_avg:40.12ms
step:459/2330 train_time:18408ms step_avg:40.11ms
step:460/2330 train_time:18453ms step_avg:40.12ms
step:461/2330 train_time:18488ms step_avg:40.10ms
step:462/2330 train_time:18534ms step_avg:40.12ms
step:463/2330 train_time:18569ms step_avg:40.11ms
step:464/2330 train_time:18613ms step_avg:40.11ms
step:465/2330 train_time:18648ms step_avg:40.10ms
step:466/2330 train_time:18693ms step_avg:40.11ms
step:467/2330 train_time:18729ms step_avg:40.10ms
step:468/2330 train_time:18773ms step_avg:40.11ms
step:469/2330 train_time:18808ms step_avg:40.10ms
step:470/2330 train_time:18853ms step_avg:40.11ms
step:471/2330 train_time:18888ms step_avg:40.10ms
step:472/2330 train_time:18933ms step_avg:40.11ms
step:473/2330 train_time:18968ms step_avg:40.10ms
step:474/2330 train_time:19012ms step_avg:40.11ms
step:475/2330 train_time:19047ms step_avg:40.10ms
step:476/2330 train_time:19091ms step_avg:40.11ms
step:477/2330 train_time:19127ms step_avg:40.10ms
step:478/2330 train_time:19171ms step_avg:40.11ms
step:479/2330 train_time:19206ms step_avg:40.10ms
step:480/2330 train_time:19250ms step_avg:40.11ms
step:481/2330 train_time:19285ms step_avg:40.09ms
step:482/2330 train_time:19330ms step_avg:40.10ms
step:483/2330 train_time:19365ms step_avg:40.09ms
step:484/2330 train_time:19410ms step_avg:40.10ms
step:485/2330 train_time:19445ms step_avg:40.09ms
step:486/2330 train_time:19489ms step_avg:40.10ms
step:487/2330 train_time:19524ms step_avg:40.09ms
step:488/2330 train_time:19569ms step_avg:40.10ms
step:489/2330 train_time:19604ms step_avg:40.09ms
step:490/2330 train_time:19649ms step_avg:40.10ms
step:491/2330 train_time:19684ms step_avg:40.09ms
step:492/2330 train_time:19729ms step_avg:40.10ms
step:493/2330 train_time:19763ms step_avg:40.09ms
step:494/2330 train_time:19808ms step_avg:40.10ms
step:495/2330 train_time:19842ms step_avg:40.09ms
step:496/2330 train_time:19887ms step_avg:40.10ms
step:497/2330 train_time:19923ms step_avg:40.09ms
step:498/2330 train_time:19968ms step_avg:40.10ms
step:499/2330 train_time:20003ms step_avg:40.09ms
step:500/2330 train_time:20047ms step_avg:40.09ms
step:500/2330 val_loss:5.2914 train_time:20134ms step_avg:40.27ms
step:501/2330 train_time:20147ms step_avg:40.21ms
step:502/2330 train_time:20159ms step_avg:40.16ms
step:503/2330 train_time:20169ms step_avg:40.10ms
step:504/2330 train_time:20207ms step_avg:40.09ms
step:505/2330 train_time:20241ms step_avg:40.08ms
step:506/2330 train_time:20286ms step_avg:40.09ms
step:507/2330 train_time:20320ms step_avg:40.08ms
step:508/2330 train_time:20364ms step_avg:40.09ms
step:509/2330 train_time:20398ms step_avg:40.08ms
step:510/2330 train_time:20444ms step_avg:40.09ms
step:511/2330 train_time:20483ms step_avg:40.08ms
step:512/2330 train_time:20531ms step_avg:40.10ms
step:513/2330 train_time:20568ms step_avg:40.09ms
step:514/2330 train_time:20614ms step_avg:40.10ms
step:515/2330 train_time:20649ms step_avg:40.09ms
step:516/2330 train_time:20694ms step_avg:40.10ms
step:517/2330 train_time:20729ms step_avg:40.09ms
step:518/2330 train_time:20773ms step_avg:40.10ms
step:519/2330 train_time:20808ms step_avg:40.09ms
step:520/2330 train_time:20852ms step_avg:40.10ms
step:521/2330 train_time:20886ms step_avg:40.09ms
step:522/2330 train_time:20930ms step_avg:40.10ms
step:523/2330 train_time:20965ms step_avg:40.09ms
step:524/2330 train_time:21009ms step_avg:40.09ms
step:525/2330 train_time:21044ms step_avg:40.08ms
step:526/2330 train_time:21089ms step_avg:40.09ms
step:527/2330 train_time:21124ms step_avg:40.08ms
step:528/2330 train_time:21168ms step_avg:40.09ms
step:529/2330 train_time:21203ms step_avg:40.08ms
step:530/2330 train_time:21247ms step_avg:40.09ms
step:531/2330 train_time:21281ms step_avg:40.08ms
step:532/2330 train_time:21325ms step_avg:40.09ms
step:533/2330 train_time:21361ms step_avg:40.08ms
step:534/2330 train_time:21406ms step_avg:40.09ms
step:535/2330 train_time:21443ms step_avg:40.08ms
step:536/2330 train_time:21488ms step_avg:40.09ms
step:537/2330 train_time:21524ms step_avg:40.08ms
step:538/2330 train_time:21570ms step_avg:40.09ms
step:539/2330 train_time:21604ms step_avg:40.08ms
step:540/2330 train_time:21649ms step_avg:40.09ms
step:541/2330 train_time:21685ms step_avg:40.08ms
step:542/2330 train_time:21729ms step_avg:40.09ms
step:543/2330 train_time:21765ms step_avg:40.08ms
step:544/2330 train_time:21809ms step_avg:40.09ms
step:545/2330 train_time:21843ms step_avg:40.08ms
step:546/2330 train_time:21888ms step_avg:40.09ms
step:547/2330 train_time:21922ms step_avg:40.08ms
step:548/2330 train_time:21966ms step_avg:40.08ms
step:549/2330 train_time:22000ms step_avg:40.07ms
step:550/2330 train_time:22044ms step_avg:40.08ms
step:551/2330 train_time:22079ms step_avg:40.07ms
step:552/2330 train_time:22123ms step_avg:40.08ms
step:553/2330 train_time:22157ms step_avg:40.07ms
step:554/2330 train_time:22201ms step_avg:40.07ms
step:555/2330 train_time:22237ms step_avg:40.07ms
step:556/2330 train_time:22280ms step_avg:40.07ms
step:557/2330 train_time:22315ms step_avg:40.06ms
step:558/2330 train_time:22360ms step_avg:40.07ms
step:559/2330 train_time:22396ms step_avg:40.06ms
step:560/2330 train_time:22440ms step_avg:40.07ms
step:561/2330 train_time:22476ms step_avg:40.06ms
step:562/2330 train_time:22521ms step_avg:40.07ms
step:563/2330 train_time:22557ms step_avg:40.07ms
step:564/2330 train_time:22602ms step_avg:40.07ms
step:565/2330 train_time:22638ms step_avg:40.07ms
step:566/2330 train_time:22682ms step_avg:40.07ms
step:567/2330 train_time:22718ms step_avg:40.07ms
step:568/2330 train_time:22763ms step_avg:40.08ms
step:569/2330 train_time:22798ms step_avg:40.07ms
step:570/2330 train_time:22843ms step_avg:40.07ms
step:571/2330 train_time:22877ms step_avg:40.07ms
step:572/2330 train_time:22922ms step_avg:40.07ms
step:573/2330 train_time:22957ms step_avg:40.06ms
step:574/2330 train_time:23002ms step_avg:40.07ms
step:575/2330 train_time:23037ms step_avg:40.06ms
step:576/2330 train_time:23081ms step_avg:40.07ms
step:577/2330 train_time:23116ms step_avg:40.06ms
step:578/2330 train_time:23160ms step_avg:40.07ms
step:579/2330 train_time:23195ms step_avg:40.06ms
step:580/2330 train_time:23239ms step_avg:40.07ms
step:581/2330 train_time:23274ms step_avg:40.06ms
step:582/2330 train_time:23318ms step_avg:40.07ms
step:583/2330 train_time:23353ms step_avg:40.06ms
step:584/2330 train_time:23399ms step_avg:40.07ms
step:585/2330 train_time:23434ms step_avg:40.06ms
step:586/2330 train_time:23480ms step_avg:40.07ms
step:587/2330 train_time:23515ms step_avg:40.06ms
step:588/2330 train_time:23560ms step_avg:40.07ms
step:589/2330 train_time:23596ms step_avg:40.06ms
step:590/2330 train_time:23641ms step_avg:40.07ms
step:591/2330 train_time:23677ms step_avg:40.06ms
step:592/2330 train_time:23722ms step_avg:40.07ms
step:593/2330 train_time:23757ms step_avg:40.06ms
step:594/2330 train_time:23802ms step_avg:40.07ms
step:595/2330 train_time:23837ms step_avg:40.06ms
step:596/2330 train_time:23882ms step_avg:40.07ms
step:597/2330 train_time:23917ms step_avg:40.06ms
step:598/2330 train_time:23961ms step_avg:40.07ms
step:599/2330 train_time:23996ms step_avg:40.06ms
step:600/2330 train_time:24040ms step_avg:40.07ms
step:601/2330 train_time:24075ms step_avg:40.06ms
step:602/2330 train_time:24120ms step_avg:40.07ms
step:603/2330 train_time:24154ms step_avg:40.06ms
step:604/2330 train_time:24199ms step_avg:40.06ms
step:605/2330 train_time:24234ms step_avg:40.06ms
step:606/2330 train_time:24278ms step_avg:40.06ms
step:607/2330 train_time:24313ms step_avg:40.05ms
step:608/2330 train_time:24358ms step_avg:40.06ms
step:609/2330 train_time:24393ms step_avg:40.05ms
step:610/2330 train_time:24439ms step_avg:40.06ms
step:611/2330 train_time:24474ms step_avg:40.05ms
step:612/2330 train_time:24519ms step_avg:40.06ms
step:613/2330 train_time:24554ms step_avg:40.06ms
step:614/2330 train_time:24599ms step_avg:40.06ms
step:615/2330 train_time:24634ms step_avg:40.06ms
step:616/2330 train_time:24680ms step_avg:40.06ms
step:617/2330 train_time:24715ms step_avg:40.06ms
step:618/2330 train_time:24760ms step_avg:40.06ms
step:619/2330 train_time:24796ms step_avg:40.06ms
step:620/2330 train_time:24840ms step_avg:40.06ms
step:621/2330 train_time:24875ms step_avg:40.06ms
step:622/2330 train_time:24919ms step_avg:40.06ms
step:623/2330 train_time:24955ms step_avg:40.06ms
step:624/2330 train_time:24999ms step_avg:40.06ms
step:625/2330 train_time:25035ms step_avg:40.06ms
step:626/2330 train_time:25079ms step_avg:40.06ms
step:627/2330 train_time:25114ms step_avg:40.05ms
step:628/2330 train_time:25158ms step_avg:40.06ms
step:629/2330 train_time:25193ms step_avg:40.05ms
step:630/2330 train_time:25237ms step_avg:40.06ms
step:631/2330 train_time:25272ms step_avg:40.05ms
step:632/2330 train_time:25317ms step_avg:40.06ms
step:633/2330 train_time:25352ms step_avg:40.05ms
step:634/2330 train_time:25396ms step_avg:40.06ms
step:635/2330 train_time:25432ms step_avg:40.05ms
step:636/2330 train_time:25476ms step_avg:40.06ms
step:637/2330 train_time:25512ms step_avg:40.05ms
step:638/2330 train_time:25557ms step_avg:40.06ms
step:639/2330 train_time:25594ms step_avg:40.05ms
step:640/2330 train_time:25638ms step_avg:40.06ms
step:641/2330 train_time:25674ms step_avg:40.05ms
step:642/2330 train_time:25718ms step_avg:40.06ms
step:643/2330 train_time:25754ms step_avg:40.05ms
step:644/2330 train_time:25799ms step_avg:40.06ms
step:645/2330 train_time:25834ms step_avg:40.05ms
step:646/2330 train_time:25878ms step_avg:40.06ms
step:647/2330 train_time:25914ms step_avg:40.05ms
step:648/2330 train_time:25958ms step_avg:40.06ms
step:649/2330 train_time:25993ms step_avg:40.05ms
step:650/2330 train_time:26038ms step_avg:40.06ms
step:651/2330 train_time:26073ms step_avg:40.05ms
step:652/2330 train_time:26117ms step_avg:40.06ms
step:653/2330 train_time:26152ms step_avg:40.05ms
step:654/2330 train_time:26197ms step_avg:40.06ms
step:655/2330 train_time:26232ms step_avg:40.05ms
step:656/2330 train_time:26276ms step_avg:40.05ms
step:657/2330 train_time:26311ms step_avg:40.05ms
step:658/2330 train_time:26356ms step_avg:40.05ms
step:659/2330 train_time:26390ms step_avg:40.05ms
step:660/2330 train_time:26435ms step_avg:40.05ms
step:661/2330 train_time:26470ms step_avg:40.05ms
step:662/2330 train_time:26515ms step_avg:40.05ms
step:663/2330 train_time:26550ms step_avg:40.05ms
step:664/2330 train_time:26594ms step_avg:40.05ms
step:665/2330 train_time:26630ms step_avg:40.04ms
step:666/2330 train_time:26674ms step_avg:40.05ms
step:667/2330 train_time:26709ms step_avg:40.04ms
step:668/2330 train_time:26753ms step_avg:40.05ms
step:669/2330 train_time:26789ms step_avg:40.04ms
step:670/2330 train_time:26834ms step_avg:40.05ms
step:671/2330 train_time:26869ms step_avg:40.04ms
step:672/2330 train_time:26914ms step_avg:40.05ms
step:673/2330 train_time:26949ms step_avg:40.04ms
step:674/2330 train_time:26994ms step_avg:40.05ms
step:675/2330 train_time:27029ms step_avg:40.04ms
step:676/2330 train_time:27074ms step_avg:40.05ms
step:677/2330 train_time:27108ms step_avg:40.04ms
step:678/2330 train_time:27152ms step_avg:40.05ms
step:679/2330 train_time:27187ms step_avg:40.04ms
step:680/2330 train_time:27232ms step_avg:40.05ms
step:681/2330 train_time:27267ms step_avg:40.04ms
step:682/2330 train_time:27311ms step_avg:40.05ms
step:683/2330 train_time:27347ms step_avg:40.04ms
step:684/2330 train_time:27391ms step_avg:40.05ms
step:685/2330 train_time:27427ms step_avg:40.04ms
step:686/2330 train_time:27472ms step_avg:40.05ms
step:687/2330 train_time:27507ms step_avg:40.04ms
step:688/2330 train_time:27552ms step_avg:40.05ms
step:689/2330 train_time:27587ms step_avg:40.04ms
step:690/2330 train_time:27632ms step_avg:40.05ms
step:691/2330 train_time:27667ms step_avg:40.04ms
step:692/2330 train_time:27712ms step_avg:40.05ms
step:693/2330 train_time:27748ms step_avg:40.04ms
step:694/2330 train_time:27792ms step_avg:40.05ms
step:695/2330 train_time:27827ms step_avg:40.04ms
step:696/2330 train_time:27872ms step_avg:40.05ms
step:697/2330 train_time:27907ms step_avg:40.04ms
step:698/2330 train_time:27952ms step_avg:40.05ms
step:699/2330 train_time:27987ms step_avg:40.04ms
step:700/2330 train_time:28032ms step_avg:40.05ms
step:701/2330 train_time:28067ms step_avg:40.04ms
step:702/2330 train_time:28111ms step_avg:40.04ms
step:703/2330 train_time:28146ms step_avg:40.04ms
step:704/2330 train_time:28191ms step_avg:40.04ms
step:705/2330 train_time:28226ms step_avg:40.04ms
step:706/2330 train_time:28270ms step_avg:40.04ms
step:707/2330 train_time:28305ms step_avg:40.04ms
step:708/2330 train_time:28350ms step_avg:40.04ms
step:709/2330 train_time:28386ms step_avg:40.04ms
step:710/2330 train_time:28430ms step_avg:40.04ms
step:711/2330 train_time:28466ms step_avg:40.04ms
step:712/2330 train_time:28511ms step_avg:40.04ms
step:713/2330 train_time:28545ms step_avg:40.04ms
step:714/2330 train_time:28591ms step_avg:40.04ms
step:715/2330 train_time:28627ms step_avg:40.04ms
step:716/2330 train_time:28671ms step_avg:40.04ms
step:717/2330 train_time:28707ms step_avg:40.04ms
step:718/2330 train_time:28752ms step_avg:40.04ms
step:719/2330 train_time:28787ms step_avg:40.04ms
step:720/2330 train_time:28832ms step_avg:40.04ms
step:721/2330 train_time:28867ms step_avg:40.04ms
step:722/2330 train_time:28912ms step_avg:40.04ms
step:723/2330 train_time:28947ms step_avg:40.04ms
step:724/2330 train_time:28992ms step_avg:40.04ms
step:725/2330 train_time:29027ms step_avg:40.04ms
step:726/2330 train_time:29072ms step_avg:40.04ms
step:727/2330 train_time:29107ms step_avg:40.04ms
step:728/2330 train_time:29151ms step_avg:40.04ms
step:729/2330 train_time:29186ms step_avg:40.04ms
step:730/2330 train_time:29231ms step_avg:40.04ms
step:731/2330 train_time:29265ms step_avg:40.03ms
step:732/2330 train_time:29310ms step_avg:40.04ms
step:733/2330 train_time:29345ms step_avg:40.03ms
step:734/2330 train_time:29390ms step_avg:40.04ms
step:735/2330 train_time:29426ms step_avg:40.03ms
step:736/2330 train_time:29470ms step_avg:40.04ms
step:737/2330 train_time:29506ms step_avg:40.03ms
step:738/2330 train_time:29550ms step_avg:40.04ms
step:739/2330 train_time:29586ms step_avg:40.04ms
step:740/2330 train_time:29631ms step_avg:40.04ms
step:741/2330 train_time:29667ms step_avg:40.04ms
step:742/2330 train_time:29712ms step_avg:40.04ms
step:743/2330 train_time:29748ms step_avg:40.04ms
step:744/2330 train_time:29793ms step_avg:40.04ms
step:745/2330 train_time:29827ms step_avg:40.04ms
step:746/2330 train_time:29872ms step_avg:40.04ms
step:747/2330 train_time:29907ms step_avg:40.04ms
step:748/2330 train_time:29952ms step_avg:40.04ms
step:749/2330 train_time:29987ms step_avg:40.04ms
step:750/2330 train_time:30032ms step_avg:40.04ms
step:750/2330 val_loss:5.2283 train_time:30118ms step_avg:40.16ms
step:751/2330 train_time:30131ms step_avg:40.12ms
step:752/2330 train_time:30143ms step_avg:40.08ms
step:753/2330 train_time:30153ms step_avg:40.04ms
step:754/2330 train_time:30190ms step_avg:40.04ms
step:755/2330 train_time:30225ms step_avg:40.03ms
step:756/2330 train_time:30269ms step_avg:40.04ms
step:757/2330 train_time:30303ms step_avg:40.03ms
step:758/2330 train_time:30346ms step_avg:40.03ms
step:759/2330 train_time:30380ms step_avg:40.03ms
step:760/2330 train_time:30428ms step_avg:40.04ms
step:761/2330 train_time:30466ms step_avg:40.03ms
step:762/2330 train_time:30513ms step_avg:40.04ms
step:763/2330 train_time:30549ms step_avg:40.04ms
step:764/2330 train_time:30594ms step_avg:40.04ms
step:765/2330 train_time:30631ms step_avg:40.04ms
step:766/2330 train_time:30674ms step_avg:40.04ms
step:767/2330 train_time:30708ms step_avg:40.04ms
step:768/2330 train_time:30752ms step_avg:40.04ms
step:769/2330 train_time:30787ms step_avg:40.03ms
step:770/2330 train_time:30830ms step_avg:40.04ms
step:771/2330 train_time:30865ms step_avg:40.03ms
step:772/2330 train_time:30908ms step_avg:40.04ms
step:773/2330 train_time:30943ms step_avg:40.03ms
step:774/2330 train_time:30987ms step_avg:40.03ms
step:775/2330 train_time:31023ms step_avg:40.03ms
step:776/2330 train_time:31068ms step_avg:40.04ms
step:777/2330 train_time:31103ms step_avg:40.03ms
step:778/2330 train_time:31148ms step_avg:40.04ms
step:779/2330 train_time:31183ms step_avg:40.03ms
step:780/2330 train_time:31227ms step_avg:40.03ms
step:781/2330 train_time:31261ms step_avg:40.03ms
step:782/2330 train_time:31306ms step_avg:40.03ms
step:783/2330 train_time:31340ms step_avg:40.03ms
step:784/2330 train_time:31385ms step_avg:40.03ms
step:785/2330 train_time:31421ms step_avg:40.03ms
step:786/2330 train_time:31468ms step_avg:40.04ms
step:787/2330 train_time:31503ms step_avg:40.03ms
step:788/2330 train_time:31549ms step_avg:40.04ms
step:789/2330 train_time:31585ms step_avg:40.03ms
step:790/2330 train_time:31630ms step_avg:40.04ms
step:791/2330 train_time:31666ms step_avg:40.03ms
step:792/2330 train_time:31710ms step_avg:40.04ms
step:793/2330 train_time:31745ms step_avg:40.03ms
step:794/2330 train_time:31789ms step_avg:40.04ms
step:795/2330 train_time:31824ms step_avg:40.03ms
step:796/2330 train_time:31868ms step_avg:40.03ms
step:797/2330 train_time:31902ms step_avg:40.03ms
step:798/2330 train_time:31946ms step_avg:40.03ms
step:799/2330 train_time:31981ms step_avg:40.03ms
step:800/2330 train_time:32026ms step_avg:40.03ms
step:801/2330 train_time:32060ms step_avg:40.02ms
step:802/2330 train_time:32104ms step_avg:40.03ms
step:803/2330 train_time:32138ms step_avg:40.02ms
step:804/2330 train_time:32182ms step_avg:40.03ms
step:805/2330 train_time:32216ms step_avg:40.02ms
step:806/2330 train_time:32260ms step_avg:40.03ms
step:807/2330 train_time:32296ms step_avg:40.02ms
step:808/2330 train_time:32341ms step_avg:40.03ms
step:809/2330 train_time:32376ms step_avg:40.02ms
step:810/2330 train_time:32421ms step_avg:40.03ms
step:811/2330 train_time:32457ms step_avg:40.02ms
step:812/2330 train_time:32502ms step_avg:40.03ms
step:813/2330 train_time:32538ms step_avg:40.02ms
step:814/2330 train_time:32583ms step_avg:40.03ms
step:815/2330 train_time:32618ms step_avg:40.02ms
step:816/2330 train_time:32663ms step_avg:40.03ms
step:817/2330 train_time:32698ms step_avg:40.02ms
step:818/2330 train_time:32742ms step_avg:40.03ms
step:819/2330 train_time:32777ms step_avg:40.02ms
step:820/2330 train_time:32821ms step_avg:40.03ms
step:821/2330 train_time:32856ms step_avg:40.02ms
step:822/2330 train_time:32900ms step_avg:40.02ms
step:823/2330 train_time:32935ms step_avg:40.02ms
step:824/2330 train_time:32980ms step_avg:40.02ms
step:825/2330 train_time:33015ms step_avg:40.02ms
step:826/2330 train_time:33060ms step_avg:40.02ms
step:827/2330 train_time:33095ms step_avg:40.02ms
step:828/2330 train_time:33139ms step_avg:40.02ms
step:829/2330 train_time:33174ms step_avg:40.02ms
step:830/2330 train_time:33218ms step_avg:40.02ms
step:831/2330 train_time:33253ms step_avg:40.02ms
step:832/2330 train_time:33298ms step_avg:40.02ms
step:833/2330 train_time:33333ms step_avg:40.02ms
step:834/2330 train_time:33377ms step_avg:40.02ms
step:835/2330 train_time:33413ms step_avg:40.02ms
step:836/2330 train_time:33459ms step_avg:40.02ms
step:837/2330 train_time:33495ms step_avg:40.02ms
step:838/2330 train_time:33539ms step_avg:40.02ms
step:839/2330 train_time:33575ms step_avg:40.02ms
step:840/2330 train_time:33620ms step_avg:40.02ms
step:841/2330 train_time:33656ms step_avg:40.02ms
step:842/2330 train_time:33700ms step_avg:40.02ms
step:843/2330 train_time:33736ms step_avg:40.02ms
step:844/2330 train_time:33780ms step_avg:40.02ms
step:845/2330 train_time:33815ms step_avg:40.02ms
step:846/2330 train_time:33859ms step_avg:40.02ms
step:847/2330 train_time:33895ms step_avg:40.02ms
step:848/2330 train_time:33939ms step_avg:40.02ms
step:849/2330 train_time:33974ms step_avg:40.02ms
step:850/2330 train_time:34018ms step_avg:40.02ms
step:851/2330 train_time:34054ms step_avg:40.02ms
step:852/2330 train_time:34098ms step_avg:40.02ms
step:853/2330 train_time:34133ms step_avg:40.02ms
step:854/2330 train_time:34178ms step_avg:40.02ms
step:855/2330 train_time:34212ms step_avg:40.01ms
step:856/2330 train_time:34257ms step_avg:40.02ms
step:857/2330 train_time:34292ms step_avg:40.01ms
step:858/2330 train_time:34336ms step_avg:40.02ms
step:859/2330 train_time:34372ms step_avg:40.01ms
step:860/2330 train_time:34416ms step_avg:40.02ms
step:861/2330 train_time:34452ms step_avg:40.01ms
step:862/2330 train_time:34496ms step_avg:40.02ms
step:863/2330 train_time:34532ms step_avg:40.01ms
step:864/2330 train_time:34577ms step_avg:40.02ms
step:865/2330 train_time:34613ms step_avg:40.01ms
step:866/2330 train_time:34658ms step_avg:40.02ms
step:867/2330 train_time:34693ms step_avg:40.02ms
step:868/2330 train_time:34738ms step_avg:40.02ms
step:869/2330 train_time:34773ms step_avg:40.02ms
step:870/2330 train_time:34818ms step_avg:40.02ms
step:871/2330 train_time:34854ms step_avg:40.02ms
step:872/2330 train_time:34898ms step_avg:40.02ms
step:873/2330 train_time:34933ms step_avg:40.01ms
step:874/2330 train_time:34978ms step_avg:40.02ms
step:875/2330 train_time:35013ms step_avg:40.02ms
step:876/2330 train_time:35058ms step_avg:40.02ms
step:877/2330 train_time:35092ms step_avg:40.01ms
step:878/2330 train_time:35136ms step_avg:40.02ms
step:879/2330 train_time:35172ms step_avg:40.01ms
step:880/2330 train_time:35216ms step_avg:40.02ms
step:881/2330 train_time:35251ms step_avg:40.01ms
step:882/2330 train_time:35296ms step_avg:40.02ms
step:883/2330 train_time:35332ms step_avg:40.01ms
step:884/2330 train_time:35376ms step_avg:40.02ms
step:885/2330 train_time:35411ms step_avg:40.01ms
step:886/2330 train_time:35456ms step_avg:40.02ms
step:887/2330 train_time:35491ms step_avg:40.01ms
step:888/2330 train_time:35536ms step_avg:40.02ms
step:889/2330 train_time:35572ms step_avg:40.01ms
step:890/2330 train_time:35617ms step_avg:40.02ms
step:891/2330 train_time:35652ms step_avg:40.01ms
step:892/2330 train_time:35696ms step_avg:40.02ms
step:893/2330 train_time:35732ms step_avg:40.01ms
step:894/2330 train_time:35776ms step_avg:40.02ms
step:895/2330 train_time:35811ms step_avg:40.01ms
step:896/2330 train_time:35856ms step_avg:40.02ms
step:897/2330 train_time:35892ms step_avg:40.01ms
step:898/2330 train_time:35936ms step_avg:40.02ms
step:899/2330 train_time:35972ms step_avg:40.01ms
step:900/2330 train_time:36016ms step_avg:40.02ms
step:901/2330 train_time:36051ms step_avg:40.01ms
step:902/2330 train_time:36096ms step_avg:40.02ms
step:903/2330 train_time:36131ms step_avg:40.01ms
step:904/2330 train_time:36175ms step_avg:40.02ms
step:905/2330 train_time:36210ms step_avg:40.01ms
step:906/2330 train_time:36254ms step_avg:40.02ms
step:907/2330 train_time:36289ms step_avg:40.01ms
step:908/2330 train_time:36333ms step_avg:40.01ms
step:909/2330 train_time:36368ms step_avg:40.01ms
step:910/2330 train_time:36413ms step_avg:40.01ms
step:911/2330 train_time:36448ms step_avg:40.01ms
step:912/2330 train_time:36493ms step_avg:40.01ms
step:913/2330 train_time:36528ms step_avg:40.01ms
step:914/2330 train_time:36572ms step_avg:40.01ms
step:915/2330 train_time:36607ms step_avg:40.01ms
step:916/2330 train_time:36652ms step_avg:40.01ms
step:917/2330 train_time:36687ms step_avg:40.01ms
step:918/2330 train_time:36732ms step_avg:40.01ms
step:919/2330 train_time:36767ms step_avg:40.01ms
step:920/2330 train_time:36812ms step_avg:40.01ms
step:921/2330 train_time:36846ms step_avg:40.01ms
step:922/2330 train_time:36891ms step_avg:40.01ms
step:923/2330 train_time:36925ms step_avg:40.01ms
step:924/2330 train_time:36969ms step_avg:40.01ms
step:925/2330 train_time:37005ms step_avg:40.01ms
step:926/2330 train_time:37050ms step_avg:40.01ms
step:927/2330 train_time:37085ms step_avg:40.01ms
step:928/2330 train_time:37129ms step_avg:40.01ms
step:929/2330 train_time:37165ms step_avg:40.01ms
step:930/2330 train_time:37209ms step_avg:40.01ms
step:931/2330 train_time:37245ms step_avg:40.01ms
step:932/2330 train_time:37289ms step_avg:40.01ms
step:933/2330 train_time:37323ms step_avg:40.00ms
step:934/2330 train_time:37368ms step_avg:40.01ms
step:935/2330 train_time:37404ms step_avg:40.00ms
step:936/2330 train_time:37449ms step_avg:40.01ms
step:937/2330 train_time:37484ms step_avg:40.00ms
step:938/2330 train_time:37527ms step_avg:40.01ms
step:939/2330 train_time:37563ms step_avg:40.00ms
step:940/2330 train_time:37608ms step_avg:40.01ms
step:941/2330 train_time:37643ms step_avg:40.00ms
step:942/2330 train_time:37688ms step_avg:40.01ms
step:943/2330 train_time:37723ms step_avg:40.00ms
step:944/2330 train_time:37768ms step_avg:40.01ms
step:945/2330 train_time:37802ms step_avg:40.00ms
step:946/2330 train_time:37846ms step_avg:40.01ms
step:947/2330 train_time:37881ms step_avg:40.00ms
step:948/2330 train_time:37925ms step_avg:40.01ms
step:949/2330 train_time:37960ms step_avg:40.00ms
step:950/2330 train_time:38004ms step_avg:40.00ms
step:951/2330 train_time:38040ms step_avg:40.00ms
step:952/2330 train_time:38084ms step_avg:40.00ms
step:953/2330 train_time:38119ms step_avg:40.00ms
step:954/2330 train_time:38163ms step_avg:40.00ms
step:955/2330 train_time:38198ms step_avg:40.00ms
step:956/2330 train_time:38242ms step_avg:40.00ms
step:957/2330 train_time:38276ms step_avg:40.00ms
step:958/2330 train_time:38321ms step_avg:40.00ms
step:959/2330 train_time:38356ms step_avg:40.00ms
step:960/2330 train_time:38400ms step_avg:40.00ms
step:961/2330 train_time:38435ms step_avg:40.00ms
step:962/2330 train_time:38480ms step_avg:40.00ms
step:963/2330 train_time:38515ms step_avg:40.00ms
step:964/2330 train_time:38560ms step_avg:40.00ms
step:965/2330 train_time:38595ms step_avg:39.99ms
step:966/2330 train_time:38639ms step_avg:40.00ms
step:967/2330 train_time:38675ms step_avg:39.99ms
step:968/2330 train_time:38719ms step_avg:40.00ms
step:969/2330 train_time:38754ms step_avg:39.99ms
step:970/2330 train_time:38799ms step_avg:40.00ms
step:971/2330 train_time:38834ms step_avg:39.99ms
step:972/2330 train_time:38878ms step_avg:40.00ms
step:973/2330 train_time:38914ms step_avg:39.99ms
step:974/2330 train_time:38958ms step_avg:40.00ms
step:975/2330 train_time:38993ms step_avg:39.99ms
step:976/2330 train_time:39038ms step_avg:40.00ms
step:977/2330 train_time:39074ms step_avg:39.99ms
step:978/2330 train_time:39119ms step_avg:40.00ms
step:979/2330 train_time:39154ms step_avg:39.99ms
step:980/2330 train_time:39199ms step_avg:40.00ms
step:981/2330 train_time:39234ms step_avg:39.99ms
step:982/2330 train_time:39279ms step_avg:40.00ms
step:983/2330 train_time:39314ms step_avg:39.99ms
step:984/2330 train_time:39359ms step_avg:40.00ms
step:985/2330 train_time:39395ms step_avg:39.99ms
step:986/2330 train_time:39439ms step_avg:40.00ms
step:987/2330 train_time:39474ms step_avg:39.99ms
step:988/2330 train_time:39519ms step_avg:40.00ms
step:989/2330 train_time:39555ms step_avg:39.99ms
step:990/2330 train_time:39599ms step_avg:40.00ms
step:991/2330 train_time:39635ms step_avg:39.99ms
step:992/2330 train_time:39679ms step_avg:40.00ms
step:993/2330 train_time:39715ms step_avg:39.99ms
step:994/2330 train_time:39759ms step_avg:40.00ms
step:995/2330 train_time:39795ms step_avg:39.99ms
step:996/2330 train_time:39839ms step_avg:40.00ms
step:997/2330 train_time:39874ms step_avg:39.99ms
step:998/2330 train_time:39919ms step_avg:40.00ms
step:999/2330 train_time:39954ms step_avg:39.99ms
step:1000/2330 train_time:39999ms step_avg:40.00ms
step:1000/2330 val_loss:5.1961 train_time:40088ms step_avg:40.09ms
step:1001/2330 train_time:40101ms step_avg:40.06ms
step:1002/2330 train_time:40113ms step_avg:40.03ms
step:1003/2330 train_time:40124ms step_avg:40.00ms
step:1004/2330 train_time:40160ms step_avg:40.00ms
step:1005/2330 train_time:40194ms step_avg:39.99ms
step:1006/2330 train_time:40238ms step_avg:40.00ms
step:1007/2330 train_time:40273ms step_avg:39.99ms
step:1008/2330 train_time:40316ms step_avg:40.00ms
step:1009/2330 train_time:40350ms step_avg:39.99ms
step:1010/2330 train_time:40395ms step_avg:40.00ms
step:1011/2330 train_time:40433ms step_avg:39.99ms
step:1012/2330 train_time:40480ms step_avg:40.00ms
step:1013/2330 train_time:40517ms step_avg:40.00ms
step:1014/2330 train_time:40563ms step_avg:40.00ms
step:1015/2330 train_time:40599ms step_avg:40.00ms
step:1016/2330 train_time:40643ms step_avg:40.00ms
step:1017/2330 train_time:40678ms step_avg:40.00ms
step:1018/2330 train_time:40722ms step_avg:40.00ms
step:1019/2330 train_time:40756ms step_avg:40.00ms
step:1020/2330 train_time:40800ms step_avg:40.00ms
step:1021/2330 train_time:40835ms step_avg:39.99ms
step:1022/2330 train_time:40878ms step_avg:40.00ms
step:1023/2330 train_time:40913ms step_avg:39.99ms
step:1024/2330 train_time:40957ms step_avg:40.00ms
step:1025/2330 train_time:40993ms step_avg:39.99ms
step:1026/2330 train_time:41038ms step_avg:40.00ms
step:1027/2330 train_time:41074ms step_avg:39.99ms
step:1028/2330 train_time:41118ms step_avg:40.00ms
step:1029/2330 train_time:41153ms step_avg:39.99ms
step:1030/2330 train_time:41197ms step_avg:40.00ms
step:1031/2330 train_time:41232ms step_avg:39.99ms
step:1032/2330 train_time:41275ms step_avg:40.00ms
step:1033/2330 train_time:41310ms step_avg:39.99ms
step:1034/2330 train_time:41355ms step_avg:39.99ms
step:1035/2330 train_time:41391ms step_avg:39.99ms
step:1036/2330 train_time:41436ms step_avg:40.00ms
step:1037/2330 train_time:41473ms step_avg:39.99ms
step:1038/2330 train_time:41518ms step_avg:40.00ms
step:1039/2330 train_time:41554ms step_avg:39.99ms
step:1040/2330 train_time:41599ms step_avg:40.00ms
step:1041/2330 train_time:41634ms step_avg:39.99ms
step:1042/2330 train_time:41678ms step_avg:40.00ms
step:1043/2330 train_time:41713ms step_avg:39.99ms
step:1044/2330 train_time:41757ms step_avg:40.00ms
step:1045/2330 train_time:41792ms step_avg:39.99ms
step:1046/2330 train_time:41836ms step_avg:40.00ms
step:1047/2330 train_time:41871ms step_avg:39.99ms
step:1048/2330 train_time:41914ms step_avg:39.99ms
step:1049/2330 train_time:41949ms step_avg:39.99ms
step:1050/2330 train_time:41993ms step_avg:39.99ms
step:1051/2330 train_time:42030ms step_avg:39.99ms
step:1052/2330 train_time:42074ms step_avg:39.99ms
step:1053/2330 train_time:42110ms step_avg:39.99ms
step:1054/2330 train_time:42155ms step_avg:40.00ms
step:1055/2330 train_time:42190ms step_avg:39.99ms
step:1056/2330 train_time:42234ms step_avg:39.99ms
step:1057/2330 train_time:42270ms step_avg:39.99ms
step:1058/2330 train_time:42315ms step_avg:39.99ms
step:1059/2330 train_time:42350ms step_avg:39.99ms
step:1060/2330 train_time:42396ms step_avg:40.00ms
step:1061/2330 train_time:42431ms step_avg:39.99ms
step:1062/2330 train_time:42476ms step_avg:40.00ms
step:1063/2330 train_time:42512ms step_avg:39.99ms
step:1064/2330 train_time:42557ms step_avg:40.00ms
step:1065/2330 train_time:42592ms step_avg:39.99ms
step:1066/2330 train_time:42637ms step_avg:40.00ms
step:1067/2330 train_time:42672ms step_avg:39.99ms
step:1068/2330 train_time:42717ms step_avg:40.00ms
step:1069/2330 train_time:42752ms step_avg:39.99ms
step:1070/2330 train_time:42795ms step_avg:40.00ms
step:1071/2330 train_time:42831ms step_avg:39.99ms
step:1072/2330 train_time:42875ms step_avg:40.00ms
step:1073/2330 train_time:42910ms step_avg:39.99ms
step:1074/2330 train_time:42954ms step_avg:39.99ms
step:1075/2330 train_time:42989ms step_avg:39.99ms
step:1076/2330 train_time:43034ms step_avg:39.99ms
step:1077/2330 train_time:43070ms step_avg:39.99ms
step:1078/2330 train_time:43114ms step_avg:39.99ms
step:1079/2330 train_time:43149ms step_avg:39.99ms
step:1080/2330 train_time:43194ms step_avg:39.99ms
step:1081/2330 train_time:43228ms step_avg:39.99ms
step:1082/2330 train_time:43273ms step_avg:39.99ms
step:1083/2330 train_time:43309ms step_avg:39.99ms
step:1084/2330 train_time:43354ms step_avg:39.99ms
step:1085/2330 train_time:43390ms step_avg:39.99ms
step:1086/2330 train_time:43435ms step_avg:40.00ms
step:1087/2330 train_time:43471ms step_avg:39.99ms
step:1088/2330 train_time:43516ms step_avg:40.00ms
step:1089/2330 train_time:43552ms step_avg:39.99ms
step:1090/2330 train_time:43596ms step_avg:40.00ms
step:1091/2330 train_time:43632ms step_avg:39.99ms
step:1092/2330 train_time:43677ms step_avg:40.00ms
step:1093/2330 train_time:43712ms step_avg:39.99ms
step:1094/2330 train_time:43757ms step_avg:40.00ms
step:1095/2330 train_time:43792ms step_avg:39.99ms
step:1096/2330 train_time:43836ms step_avg:40.00ms
step:1097/2330 train_time:43871ms step_avg:39.99ms
step:1098/2330 train_time:43915ms step_avg:40.00ms
step:1099/2330 train_time:43950ms step_avg:39.99ms
step:1100/2330 train_time:43995ms step_avg:40.00ms
step:1101/2330 train_time:44030ms step_avg:39.99ms
step:1102/2330 train_time:44075ms step_avg:40.00ms
step:1103/2330 train_time:44110ms step_avg:39.99ms
step:1104/2330 train_time:44155ms step_avg:40.00ms
step:1105/2330 train_time:44190ms step_avg:39.99ms
step:1106/2330 train_time:44235ms step_avg:40.00ms
step:1107/2330 train_time:44270ms step_avg:39.99ms
step:1108/2330 train_time:44315ms step_avg:40.00ms
step:1109/2330 train_time:44350ms step_avg:39.99ms
step:1110/2330 train_time:44395ms step_avg:40.00ms
step:1111/2330 train_time:44431ms step_avg:39.99ms
step:1112/2330 train_time:44476ms step_avg:40.00ms
step:1113/2330 train_time:44511ms step_avg:39.99ms
step:1114/2330 train_time:44556ms step_avg:40.00ms
step:1115/2330 train_time:44591ms step_avg:39.99ms
step:1116/2330 train_time:44636ms step_avg:40.00ms
step:1117/2330 train_time:44671ms step_avg:39.99ms
step:1118/2330 train_time:44716ms step_avg:40.00ms
step:1119/2330 train_time:44751ms step_avg:39.99ms
step:1120/2330 train_time:44796ms step_avg:40.00ms
step:1121/2330 train_time:44831ms step_avg:39.99ms
step:1122/2330 train_time:44876ms step_avg:40.00ms
step:1123/2330 train_time:44911ms step_avg:39.99ms
step:1124/2330 train_time:44955ms step_avg:40.00ms
step:1125/2330 train_time:44989ms step_avg:39.99ms
step:1126/2330 train_time:45034ms step_avg:39.99ms
step:1127/2330 train_time:45069ms step_avg:39.99ms
step:1128/2330 train_time:45114ms step_avg:39.99ms
step:1129/2330 train_time:45149ms step_avg:39.99ms
step:1130/2330 train_time:45193ms step_avg:39.99ms
step:1131/2330 train_time:45229ms step_avg:39.99ms
step:1132/2330 train_time:45274ms step_avg:39.99ms
step:1133/2330 train_time:45310ms step_avg:39.99ms
step:1134/2330 train_time:45355ms step_avg:40.00ms
step:1135/2330 train_time:45390ms step_avg:39.99ms
step:1136/2330 train_time:45435ms step_avg:40.00ms
step:1137/2330 train_time:45471ms step_avg:39.99ms
step:1138/2330 train_time:45516ms step_avg:40.00ms
step:1139/2330 train_time:45551ms step_avg:39.99ms
step:1140/2330 train_time:45595ms step_avg:40.00ms
step:1141/2330 train_time:45631ms step_avg:39.99ms
step:1142/2330 train_time:45675ms step_avg:40.00ms
step:1143/2330 train_time:45711ms step_avg:39.99ms
step:1144/2330 train_time:45756ms step_avg:40.00ms
step:1145/2330 train_time:45791ms step_avg:39.99ms
step:1146/2330 train_time:45835ms step_avg:40.00ms
step:1147/2330 train_time:45870ms step_avg:39.99ms
step:1148/2330 train_time:45915ms step_avg:40.00ms
step:1149/2330 train_time:45949ms step_avg:39.99ms
step:1150/2330 train_time:45994ms step_avg:40.00ms
step:1151/2330 train_time:46030ms step_avg:39.99ms
step:1152/2330 train_time:46074ms step_avg:39.99ms
step:1153/2330 train_time:46109ms step_avg:39.99ms
step:1154/2330 train_time:46153ms step_avg:39.99ms
step:1155/2330 train_time:46189ms step_avg:39.99ms
step:1156/2330 train_time:46233ms step_avg:39.99ms
step:1157/2330 train_time:46268ms step_avg:39.99ms
step:1158/2330 train_time:46314ms step_avg:39.99ms
step:1159/2330 train_time:46350ms step_avg:39.99ms
step:1160/2330 train_time:46395ms step_avg:40.00ms
step:1161/2330 train_time:46430ms step_avg:39.99ms
step:1162/2330 train_time:46476ms step_avg:40.00ms
step:1163/2330 train_time:46511ms step_avg:39.99ms
step:1164/2330 train_time:46556ms step_avg:40.00ms
step:1165/2330 train_time:46591ms step_avg:39.99ms
step:1166/2330 train_time:46635ms step_avg:40.00ms
step:1167/2330 train_time:46671ms step_avg:39.99ms
step:1168/2330 train_time:46715ms step_avg:40.00ms
step:1169/2330 train_time:46751ms step_avg:39.99ms
step:1170/2330 train_time:46796ms step_avg:40.00ms
step:1171/2330 train_time:46831ms step_avg:39.99ms
step:1172/2330 train_time:46876ms step_avg:40.00ms
step:1173/2330 train_time:46910ms step_avg:39.99ms
step:1174/2330 train_time:46955ms step_avg:40.00ms
step:1175/2330 train_time:46990ms step_avg:39.99ms
step:1176/2330 train_time:47035ms step_avg:40.00ms
step:1177/2330 train_time:47070ms step_avg:39.99ms
step:1178/2330 train_time:47115ms step_avg:40.00ms
step:1179/2330 train_time:47150ms step_avg:39.99ms
step:1180/2330 train_time:47195ms step_avg:40.00ms
step:1181/2330 train_time:47230ms step_avg:39.99ms
step:1182/2330 train_time:47275ms step_avg:40.00ms
step:1183/2330 train_time:47310ms step_avg:39.99ms
step:1184/2330 train_time:47355ms step_avg:40.00ms
step:1185/2330 train_time:47391ms step_avg:39.99ms
step:1186/2330 train_time:47436ms step_avg:40.00ms
step:1187/2330 train_time:47471ms step_avg:39.99ms
step:1188/2330 train_time:47515ms step_avg:40.00ms
step:1189/2330 train_time:47551ms step_avg:39.99ms
step:1190/2330 train_time:47596ms step_avg:40.00ms
step:1191/2330 train_time:47631ms step_avg:39.99ms
step:1192/2330 train_time:47675ms step_avg:40.00ms
step:1193/2330 train_time:47710ms step_avg:39.99ms
step:1194/2330 train_time:47756ms step_avg:40.00ms
step:1195/2330 train_time:47791ms step_avg:39.99ms
step:1196/2330 train_time:47835ms step_avg:40.00ms
step:1197/2330 train_time:47871ms step_avg:39.99ms
step:1198/2330 train_time:47916ms step_avg:40.00ms
step:1199/2330 train_time:47951ms step_avg:39.99ms
step:1200/2330 train_time:47996ms step_avg:40.00ms
step:1201/2330 train_time:48031ms step_avg:39.99ms
step:1202/2330 train_time:48076ms step_avg:40.00ms
step:1203/2330 train_time:48111ms step_avg:39.99ms
step:1204/2330 train_time:48155ms step_avg:40.00ms
step:1205/2330 train_time:48190ms step_avg:39.99ms
step:1206/2330 train_time:48235ms step_avg:40.00ms
step:1207/2330 train_time:48271ms step_avg:39.99ms
step:1208/2330 train_time:48315ms step_avg:40.00ms
step:1209/2330 train_time:48351ms step_avg:39.99ms
step:1210/2330 train_time:48396ms step_avg:40.00ms
step:1211/2330 train_time:48431ms step_avg:39.99ms
step:1212/2330 train_time:48476ms step_avg:40.00ms
step:1213/2330 train_time:48511ms step_avg:39.99ms
step:1214/2330 train_time:48556ms step_avg:40.00ms
step:1215/2330 train_time:48591ms step_avg:39.99ms
step:1216/2330 train_time:48636ms step_avg:40.00ms
step:1217/2330 train_time:48671ms step_avg:39.99ms
step:1218/2330 train_time:48715ms step_avg:40.00ms
step:1219/2330 train_time:48751ms step_avg:39.99ms
step:1220/2330 train_time:48795ms step_avg:40.00ms
step:1221/2330 train_time:48831ms step_avg:39.99ms
step:1222/2330 train_time:48875ms step_avg:40.00ms
step:1223/2330 train_time:48910ms step_avg:39.99ms
step:1224/2330 train_time:48955ms step_avg:40.00ms
step:1225/2330 train_time:48990ms step_avg:39.99ms
step:1226/2330 train_time:49035ms step_avg:40.00ms
step:1227/2330 train_time:49070ms step_avg:39.99ms
step:1228/2330 train_time:49114ms step_avg:40.00ms
step:1229/2330 train_time:49150ms step_avg:39.99ms
step:1230/2330 train_time:49195ms step_avg:40.00ms
step:1231/2330 train_time:49230ms step_avg:39.99ms
step:1232/2330 train_time:49275ms step_avg:40.00ms
step:1233/2330 train_time:49310ms step_avg:39.99ms
step:1234/2330 train_time:49355ms step_avg:40.00ms
step:1235/2330 train_time:49391ms step_avg:39.99ms
step:1236/2330 train_time:49437ms step_avg:40.00ms
step:1237/2330 train_time:49472ms step_avg:39.99ms
step:1238/2330 train_time:49516ms step_avg:40.00ms
step:1239/2330 train_time:49551ms step_avg:39.99ms
step:1240/2330 train_time:49596ms step_avg:40.00ms
step:1241/2330 train_time:49631ms step_avg:39.99ms
step:1242/2330 train_time:49675ms step_avg:40.00ms
step:1243/2330 train_time:49710ms step_avg:39.99ms
step:1244/2330 train_time:49755ms step_avg:40.00ms
step:1245/2330 train_time:49789ms step_avg:39.99ms
step:1246/2330 train_time:49834ms step_avg:40.00ms
step:1247/2330 train_time:49870ms step_avg:39.99ms
step:1248/2330 train_time:49914ms step_avg:40.00ms
step:1249/2330 train_time:49950ms step_avg:39.99ms
step:1250/2330 train_time:49995ms step_avg:40.00ms
step:1250/2330 val_loss:5.1671 train_time:50082ms step_avg:40.07ms
step:1251/2330 train_time:50095ms step_avg:40.04ms
step:1252/2330 train_time:50107ms step_avg:40.02ms
step:1253/2330 train_time:50118ms step_avg:40.00ms
step:1254/2330 train_time:50155ms step_avg:40.00ms
step:1255/2330 train_time:50190ms step_avg:39.99ms
step:1256/2330 train_time:50233ms step_avg:39.99ms
step:1257/2330 train_time:50267ms step_avg:39.99ms
step:1258/2330 train_time:50310ms step_avg:39.99ms
step:1259/2330 train_time:50344ms step_avg:39.99ms
step:1260/2330 train_time:50389ms step_avg:39.99ms
step:1261/2330 train_time:50428ms step_avg:39.99ms
step:1262/2330 train_time:50474ms step_avg:40.00ms
step:1263/2330 train_time:50512ms step_avg:39.99ms
step:1264/2330 train_time:50558ms step_avg:40.00ms
step:1265/2330 train_time:50665ms step_avg:40.05ms
step:1266/2330 train_time:50707ms step_avg:40.05ms
step:1267/2330 train_time:50741ms step_avg:40.05ms
step:1268/2330 train_time:50894ms step_avg:40.14ms
step:1269/2330 train_time:50927ms step_avg:40.13ms
step:1270/2330 train_time:50970ms step_avg:40.13ms
step:1271/2330 train_time:51013ms step_avg:40.14ms
step:1272/2330 train_time:51273ms step_avg:40.31ms
step:1273/2330 train_time:51307ms step_avg:40.30ms
step:1274/2330 train_time:51350ms step_avg:40.31ms
step:1275/2330 train_time:51384ms step_avg:40.30ms
step:1276/2330 train_time:51427ms step_avg:40.30ms
step:1277/2330 train_time:51462ms step_avg:40.30ms
step:1278/2330 train_time:51505ms step_avg:40.30ms
step:1279/2330 train_time:51540ms step_avg:40.30ms
step:1280/2330 train_time:51583ms step_avg:40.30ms
step:1281/2330 train_time:51617ms step_avg:40.29ms
step:1282/2330 train_time:51661ms step_avg:40.30ms
step:1283/2330 train_time:51696ms step_avg:40.29ms
step:1284/2330 train_time:51739ms step_avg:40.30ms
step:1285/2330 train_time:51774ms step_avg:40.29ms
step:1286/2330 train_time:51817ms step_avg:40.29ms
step:1287/2330 train_time:51852ms step_avg:40.29ms
step:1288/2330 train_time:51896ms step_avg:40.29ms
step:1289/2330 train_time:51930ms step_avg:40.29ms
step:1290/2330 train_time:51974ms step_avg:40.29ms
step:1291/2330 train_time:52008ms step_avg:40.29ms
step:1292/2330 train_time:52051ms step_avg:40.29ms
step:1293/2330 train_time:52085ms step_avg:40.28ms
step:1294/2330 train_time:52131ms step_avg:40.29ms
step:1295/2330 train_time:52172ms step_avg:40.29ms
step:1296/2330 train_time:52222ms step_avg:40.29ms
step:1297/2330 train_time:52258ms step_avg:40.29ms
step:1298/2330 train_time:52304ms step_avg:40.30ms
step:1299/2330 train_time:52340ms step_avg:40.29ms
step:1300/2330 train_time:52385ms step_avg:40.30ms
step:1301/2330 train_time:52420ms step_avg:40.29ms
step:1302/2330 train_time:52463ms step_avg:40.29ms
step:1303/2330 train_time:52498ms step_avg:40.29ms
step:1304/2330 train_time:52542ms step_avg:40.29ms
step:1305/2330 train_time:52576ms step_avg:40.29ms
step:1306/2330 train_time:52620ms step_avg:40.29ms
step:1307/2330 train_time:52655ms step_avg:40.29ms
step:1308/2330 train_time:52699ms step_avg:40.29ms
step:1309/2330 train_time:52733ms step_avg:40.28ms
step:1310/2330 train_time:52776ms step_avg:40.29ms
step:1311/2330 train_time:52811ms step_avg:40.28ms
step:1312/2330 train_time:52855ms step_avg:40.29ms
step:1313/2330 train_time:52890ms step_avg:40.28ms
step:1314/2330 train_time:52935ms step_avg:40.29ms
step:1315/2330 train_time:52969ms step_avg:40.28ms
step:1316/2330 train_time:53013ms step_avg:40.28ms
step:1317/2330 train_time:53048ms step_avg:40.28ms
step:1318/2330 train_time:53092ms step_avg:40.28ms
step:1319/2330 train_time:53128ms step_avg:40.28ms
step:1320/2330 train_time:53173ms step_avg:40.28ms
step:1321/2330 train_time:53210ms step_avg:40.28ms
step:1322/2330 train_time:53257ms step_avg:40.29ms
step:1323/2330 train_time:53293ms step_avg:40.28ms
step:1324/2330 train_time:53339ms step_avg:40.29ms
step:1325/2330 train_time:53374ms step_avg:40.28ms
step:1326/2330 train_time:53419ms step_avg:40.29ms
step:1327/2330 train_time:53454ms step_avg:40.28ms
step:1328/2330 train_time:53499ms step_avg:40.29ms
step:1329/2330 train_time:53534ms step_avg:40.28ms
step:1330/2330 train_time:53579ms step_avg:40.28ms
step:1331/2330 train_time:53614ms step_avg:40.28ms
step:1332/2330 train_time:53659ms step_avg:40.28ms
step:1333/2330 train_time:53693ms step_avg:40.28ms
step:1334/2330 train_time:53737ms step_avg:40.28ms
step:1335/2330 train_time:53772ms step_avg:40.28ms
step:1336/2330 train_time:53815ms step_avg:40.28ms
step:1337/2330 train_time:53850ms step_avg:40.28ms
step:1338/2330 train_time:53893ms step_avg:40.28ms
step:1339/2330 train_time:53928ms step_avg:40.27ms
step:1340/2330 train_time:53972ms step_avg:40.28ms
step:1341/2330 train_time:54007ms step_avg:40.27ms
step:1342/2330 train_time:54051ms step_avg:40.28ms
step:1343/2330 train_time:54086ms step_avg:40.27ms
step:1344/2330 train_time:54131ms step_avg:40.28ms
step:1345/2330 train_time:54167ms step_avg:40.27ms
step:1346/2330 train_time:54212ms step_avg:40.28ms
step:1347/2330 train_time:54248ms step_avg:40.27ms
step:1348/2330 train_time:54294ms step_avg:40.28ms
step:1349/2330 train_time:54330ms step_avg:40.27ms
step:1350/2330 train_time:54375ms step_avg:40.28ms
step:1351/2330 train_time:54411ms step_avg:40.27ms
step:1352/2330 train_time:54456ms step_avg:40.28ms
step:1353/2330 train_time:54491ms step_avg:40.27ms
step:1354/2330 train_time:54536ms step_avg:40.28ms
step:1355/2330 train_time:54570ms step_avg:40.27ms
step:1356/2330 train_time:54615ms step_avg:40.28ms
step:1357/2330 train_time:54650ms step_avg:40.27ms
step:1358/2330 train_time:54694ms step_avg:40.28ms
step:1359/2330 train_time:54729ms step_avg:40.27ms
step:1360/2330 train_time:54773ms step_avg:40.27ms
step:1361/2330 train_time:54808ms step_avg:40.27ms
step:1362/2330 train_time:54852ms step_avg:40.27ms
step:1363/2330 train_time:54886ms step_avg:40.27ms
step:1364/2330 train_time:54929ms step_avg:40.27ms
step:1365/2330 train_time:54964ms step_avg:40.27ms
step:1366/2330 train_time:55009ms step_avg:40.27ms
step:1367/2330 train_time:55045ms step_avg:40.27ms
step:1368/2330 train_time:55089ms step_avg:40.27ms
step:1369/2330 train_time:55124ms step_avg:40.27ms
step:1370/2330 train_time:55168ms step_avg:40.27ms
step:1371/2330 train_time:55204ms step_avg:40.27ms
step:1372/2330 train_time:55249ms step_avg:40.27ms
step:1373/2330 train_time:55285ms step_avg:40.27ms
step:1374/2330 train_time:55330ms step_avg:40.27ms
step:1375/2330 train_time:55366ms step_avg:40.27ms
step:1376/2330 train_time:55411ms step_avg:40.27ms
step:1377/2330 train_time:55447ms step_avg:40.27ms
step:1378/2330 train_time:55492ms step_avg:40.27ms
step:1379/2330 train_time:55527ms step_avg:40.27ms
step:1380/2330 train_time:55572ms step_avg:40.27ms
step:1381/2330 train_time:55607ms step_avg:40.27ms
step:1382/2330 train_time:55652ms step_avg:40.27ms
step:1383/2330 train_time:55687ms step_avg:40.27ms
step:1384/2330 train_time:55731ms step_avg:40.27ms
step:1385/2330 train_time:55766ms step_avg:40.26ms
step:1386/2330 train_time:55810ms step_avg:40.27ms
step:1387/2330 train_time:55845ms step_avg:40.26ms
step:1388/2330 train_time:55889ms step_avg:40.27ms
step:1389/2330 train_time:55923ms step_avg:40.26ms
step:1390/2330 train_time:55967ms step_avg:40.26ms
step:1391/2330 train_time:56003ms step_avg:40.26ms
step:1392/2330 train_time:56047ms step_avg:40.26ms
step:1393/2330 train_time:56082ms step_avg:40.26ms
step:1394/2330 train_time:56127ms step_avg:40.26ms
step:1395/2330 train_time:56162ms step_avg:40.26ms
step:1396/2330 train_time:56207ms step_avg:40.26ms
step:1397/2330 train_time:56242ms step_avg:40.26ms
step:1398/2330 train_time:56287ms step_avg:40.26ms
step:1399/2330 train_time:56323ms step_avg:40.26ms
step:1400/2330 train_time:56367ms step_avg:40.26ms
step:1401/2330 train_time:56403ms step_avg:40.26ms
step:1402/2330 train_time:56448ms step_avg:40.26ms
step:1403/2330 train_time:56484ms step_avg:40.26ms
step:1404/2330 train_time:56529ms step_avg:40.26ms
step:1405/2330 train_time:56565ms step_avg:40.26ms
step:1406/2330 train_time:56610ms step_avg:40.26ms
step:1407/2330 train_time:56645ms step_avg:40.26ms
step:1408/2330 train_time:56689ms step_avg:40.26ms
step:1409/2330 train_time:56725ms step_avg:40.26ms
step:1410/2330 train_time:56769ms step_avg:40.26ms
step:1411/2330 train_time:56804ms step_avg:40.26ms
step:1412/2330 train_time:56848ms step_avg:40.26ms
step:1413/2330 train_time:56884ms step_avg:40.26ms
step:1414/2330 train_time:56928ms step_avg:40.26ms
step:1415/2330 train_time:56963ms step_avg:40.26ms
step:1416/2330 train_time:57007ms step_avg:40.26ms
step:1417/2330 train_time:57042ms step_avg:40.26ms
step:1418/2330 train_time:57086ms step_avg:40.26ms
step:1419/2330 train_time:57122ms step_avg:40.26ms
step:1420/2330 train_time:57167ms step_avg:40.26ms
step:1421/2330 train_time:57202ms step_avg:40.25ms
step:1422/2330 train_time:57247ms step_avg:40.26ms
step:1423/2330 train_time:57282ms step_avg:40.25ms
step:1424/2330 train_time:57326ms step_avg:40.26ms
step:1425/2330 train_time:57362ms step_avg:40.25ms
step:1426/2330 train_time:57407ms step_avg:40.26ms
step:1427/2330 train_time:57443ms step_avg:40.25ms
step:1428/2330 train_time:57487ms step_avg:40.26ms
step:1429/2330 train_time:57523ms step_avg:40.25ms
step:1430/2330 train_time:57568ms step_avg:40.26ms
step:1431/2330 train_time:57603ms step_avg:40.25ms
step:1432/2330 train_time:57648ms step_avg:40.26ms
step:1433/2330 train_time:57683ms step_avg:40.25ms
step:1434/2330 train_time:57728ms step_avg:40.26ms
step:1435/2330 train_time:57763ms step_avg:40.25ms
step:1436/2330 train_time:57807ms step_avg:40.26ms
step:1437/2330 train_time:57842ms step_avg:40.25ms
step:1438/2330 train_time:57886ms step_avg:40.25ms
step:1439/2330 train_time:57921ms step_avg:40.25ms
step:1440/2330 train_time:57966ms step_avg:40.25ms
step:1441/2330 train_time:58000ms step_avg:40.25ms
step:1442/2330 train_time:58045ms step_avg:40.25ms
step:1443/2330 train_time:58080ms step_avg:40.25ms
step:1444/2330 train_time:58124ms step_avg:40.25ms
step:1445/2330 train_time:58159ms step_avg:40.25ms
step:1446/2330 train_time:58204ms step_avg:40.25ms
step:1447/2330 train_time:58239ms step_avg:40.25ms
step:1448/2330 train_time:58284ms step_avg:40.25ms
step:1449/2330 train_time:58318ms step_avg:40.25ms
step:1450/2330 train_time:58364ms step_avg:40.25ms
step:1451/2330 train_time:58398ms step_avg:40.25ms
step:1452/2330 train_time:58442ms step_avg:40.25ms
step:1453/2330 train_time:58478ms step_avg:40.25ms
step:1454/2330 train_time:58523ms step_avg:40.25ms
step:1455/2330 train_time:58558ms step_avg:40.25ms
step:1456/2330 train_time:58602ms step_avg:40.25ms
step:1457/2330 train_time:58638ms step_avg:40.25ms
step:1458/2330 train_time:58682ms step_avg:40.25ms
step:1459/2330 train_time:58717ms step_avg:40.24ms
step:1460/2330 train_time:58761ms step_avg:40.25ms
step:1461/2330 train_time:58797ms step_avg:40.24ms
step:1462/2330 train_time:58841ms step_avg:40.25ms
step:1463/2330 train_time:58876ms step_avg:40.24ms
step:1464/2330 train_time:58921ms step_avg:40.25ms
step:1465/2330 train_time:58956ms step_avg:40.24ms
step:1466/2330 train_time:59000ms step_avg:40.25ms
step:1467/2330 train_time:59035ms step_avg:40.24ms
step:1468/2330 train_time:59080ms step_avg:40.25ms
step:1469/2330 train_time:59115ms step_avg:40.24ms
step:1470/2330 train_time:59160ms step_avg:40.24ms
step:1471/2330 train_time:59195ms step_avg:40.24ms
step:1472/2330 train_time:59241ms step_avg:40.25ms
step:1473/2330 train_time:59277ms step_avg:40.24ms
step:1474/2330 train_time:59321ms step_avg:40.25ms
step:1475/2330 train_time:59357ms step_avg:40.24ms
step:1476/2330 train_time:59403ms step_avg:40.25ms
step:1477/2330 train_time:59438ms step_avg:40.24ms
step:1478/2330 train_time:59483ms step_avg:40.25ms
step:1479/2330 train_time:59519ms step_avg:40.24ms
step:1480/2330 train_time:59564ms step_avg:40.25ms
step:1481/2330 train_time:59599ms step_avg:40.24ms
step:1482/2330 train_time:59643ms step_avg:40.25ms
step:1483/2330 train_time:59678ms step_avg:40.24ms
step:1484/2330 train_time:59723ms step_avg:40.24ms
step:1485/2330 train_time:59758ms step_avg:40.24ms
step:1486/2330 train_time:59803ms step_avg:40.24ms
step:1487/2330 train_time:59838ms step_avg:40.24ms
step:1488/2330 train_time:59882ms step_avg:40.24ms
step:1489/2330 train_time:59917ms step_avg:40.24ms
step:1490/2330 train_time:59961ms step_avg:40.24ms
step:1491/2330 train_time:59996ms step_avg:40.24ms
step:1492/2330 train_time:60040ms step_avg:40.24ms
step:1493/2330 train_time:60075ms step_avg:40.24ms
step:1494/2330 train_time:60121ms step_avg:40.24ms
step:1495/2330 train_time:60156ms step_avg:40.24ms
step:1496/2330 train_time:60201ms step_avg:40.24ms
step:1497/2330 train_time:60236ms step_avg:40.24ms
step:1498/2330 train_time:60281ms step_avg:40.24ms
step:1499/2330 train_time:60317ms step_avg:40.24ms
step:1500/2330 train_time:60361ms step_avg:40.24ms
step:1500/2330 val_loss:5.1490 train_time:60449ms step_avg:40.30ms
step:1501/2330 train_time:60462ms step_avg:40.28ms
step:1502/2330 train_time:60474ms step_avg:40.26ms
step:1503/2330 train_time:60485ms step_avg:40.24ms
step:1504/2330 train_time:60522ms step_avg:40.24ms
step:1505/2330 train_time:60556ms step_avg:40.24ms
step:1506/2330 train_time:60600ms step_avg:40.24ms
step:1507/2330 train_time:60634ms step_avg:40.23ms
step:1508/2330 train_time:60678ms step_avg:40.24ms
step:1509/2330 train_time:60713ms step_avg:40.23ms
step:1510/2330 train_time:60762ms step_avg:40.24ms
step:1511/2330 train_time:60801ms step_avg:40.24ms
step:1512/2330 train_time:60848ms step_avg:40.24ms
step:1513/2330 train_time:60884ms step_avg:40.24ms
step:1514/2330 train_time:60930ms step_avg:40.24ms
step:1515/2330 train_time:60964ms step_avg:40.24ms
step:1516/2330 train_time:61009ms step_avg:40.24ms
step:1517/2330 train_time:61043ms step_avg:40.24ms
step:1518/2330 train_time:61087ms step_avg:40.24ms
step:1519/2330 train_time:61292ms step_avg:40.35ms
step:1520/2330 train_time:61305ms step_avg:40.33ms
step:1521/2330 train_time:61316ms step_avg:40.31ms
step:1522/2330 train_time:61341ms step_avg:40.30ms
step:1523/2330 train_time:61375ms step_avg:40.30ms
step:1524/2330 train_time:61418ms step_avg:40.30ms
step:1525/2330 train_time:61452ms step_avg:40.30ms
step:1526/2330 train_time:61495ms step_avg:40.30ms
step:1527/2330 train_time:61530ms step_avg:40.29ms
step:1528/2330 train_time:61573ms step_avg:40.30ms
step:1529/2330 train_time:61609ms step_avg:40.29ms
step:1530/2330 train_time:61652ms step_avg:40.30ms
step:1531/2330 train_time:61685ms step_avg:40.29ms
step:1532/2330 train_time:61729ms step_avg:40.29ms
step:1533/2330 train_time:61763ms step_avg:40.29ms
step:1534/2330 train_time:61807ms step_avg:40.29ms
step:1535/2330 train_time:61841ms step_avg:40.29ms
step:1536/2330 train_time:61885ms step_avg:40.29ms
step:1537/2330 train_time:61919ms step_avg:40.29ms
step:1538/2330 train_time:61963ms step_avg:40.29ms
step:1539/2330 train_time:61997ms step_avg:40.28ms
step:1540/2330 train_time:62041ms step_avg:40.29ms
step:1541/2330 train_time:62075ms step_avg:40.28ms
step:1542/2330 train_time:62119ms step_avg:40.28ms
step:1543/2330 train_time:62153ms step_avg:40.28ms
step:1544/2330 train_time:62204ms step_avg:40.29ms
step:1545/2330 train_time:62245ms step_avg:40.29ms
step:1546/2330 train_time:62293ms step_avg:40.29ms
step:1547/2330 train_time:62329ms step_avg:40.29ms
step:1548/2330 train_time:62374ms step_avg:40.29ms
step:1549/2330 train_time:62410ms step_avg:40.29ms
step:1550/2330 train_time:62455ms step_avg:40.29ms
step:1551/2330 train_time:62490ms step_avg:40.29ms
step:1552/2330 train_time:62534ms step_avg:40.29ms
step:1553/2330 train_time:62569ms step_avg:40.29ms
step:1554/2330 train_time:62613ms step_avg:40.29ms
step:1555/2330 train_time:62648ms step_avg:40.29ms
step:1556/2330 train_time:62692ms step_avg:40.29ms
step:1557/2330 train_time:62727ms step_avg:40.29ms
step:1558/2330 train_time:62771ms step_avg:40.29ms
step:1559/2330 train_time:62805ms step_avg:40.29ms
step:1560/2330 train_time:62849ms step_avg:40.29ms
step:1561/2330 train_time:62883ms step_avg:40.28ms
step:1562/2330 train_time:62928ms step_avg:40.29ms
step:1563/2330 train_time:62962ms step_avg:40.28ms
step:1564/2330 train_time:63006ms step_avg:40.29ms
step:1565/2330 train_time:63040ms step_avg:40.28ms
step:1566/2330 train_time:63084ms step_avg:40.28ms
step:1567/2330 train_time:63119ms step_avg:40.28ms
step:1568/2330 train_time:63164ms step_avg:40.28ms
step:1569/2330 train_time:63201ms step_avg:40.28ms
step:1570/2330 train_time:63247ms step_avg:40.28ms
step:1571/2330 train_time:63284ms step_avg:40.28ms
step:1572/2330 train_time:63331ms step_avg:40.29ms
step:1573/2330 train_time:63367ms step_avg:40.28ms
step:1574/2330 train_time:63411ms step_avg:40.29ms
step:1575/2330 train_time:63447ms step_avg:40.28ms
step:1576/2330 train_time:63492ms step_avg:40.29ms
step:1577/2330 train_time:63527ms step_avg:40.28ms
step:1578/2330 train_time:63571ms step_avg:40.29ms
step:1579/2330 train_time:63606ms step_avg:40.28ms
step:1580/2330 train_time:63650ms step_avg:40.28ms
step:1581/2330 train_time:63685ms step_avg:40.28ms
step:1582/2330 train_time:63729ms step_avg:40.28ms
step:1583/2330 train_time:63764ms step_avg:40.28ms
step:1584/2330 train_time:63808ms step_avg:40.28ms
step:1585/2330 train_time:63843ms step_avg:40.28ms
step:1586/2330 train_time:63887ms step_avg:40.28ms
step:1587/2330 train_time:63921ms step_avg:40.28ms
step:1588/2330 train_time:63964ms step_avg:40.28ms
step:1589/2330 train_time:64000ms step_avg:40.28ms
step:1590/2330 train_time:64044ms step_avg:40.28ms
step:1591/2330 train_time:64078ms step_avg:40.28ms
step:1592/2330 train_time:64123ms step_avg:40.28ms
step:1593/2330 train_time:64159ms step_avg:40.28ms
step:1594/2330 train_time:64203ms step_avg:40.28ms
step:1595/2330 train_time:64240ms step_avg:40.28ms
step:1596/2330 train_time:64286ms step_avg:40.28ms
step:1597/2330 train_time:64322ms step_avg:40.28ms
step:1598/2330 train_time:64367ms step_avg:40.28ms
step:1599/2330 train_time:64403ms step_avg:40.28ms
step:1600/2330 train_time:64448ms step_avg:40.28ms
step:1601/2330 train_time:64483ms step_avg:40.28ms
step:1602/2330 train_time:64528ms step_avg:40.28ms
step:1603/2330 train_time:64563ms step_avg:40.28ms
step:1604/2330 train_time:64607ms step_avg:40.28ms
step:1605/2330 train_time:64642ms step_avg:40.28ms
step:1606/2330 train_time:64686ms step_avg:40.28ms
step:1607/2330 train_time:64721ms step_avg:40.27ms
step:1608/2330 train_time:64766ms step_avg:40.28ms
step:1609/2330 train_time:64800ms step_avg:40.27ms
step:1610/2330 train_time:64844ms step_avg:40.28ms
step:1611/2330 train_time:64879ms step_avg:40.27ms
step:1612/2330 train_time:64922ms step_avg:40.27ms
step:1613/2330 train_time:64958ms step_avg:40.27ms
step:1614/2330 train_time:65001ms step_avg:40.27ms
step:1615/2330 train_time:65036ms step_avg:40.27ms
step:1616/2330 train_time:65080ms step_avg:40.27ms
step:1617/2330 train_time:65115ms step_avg:40.27ms
step:1618/2330 train_time:65160ms step_avg:40.27ms
step:1619/2330 train_time:65195ms step_avg:40.27ms
step:1620/2330 train_time:65240ms step_avg:40.27ms
step:1621/2330 train_time:65276ms step_avg:40.27ms
step:1622/2330 train_time:65321ms step_avg:40.27ms
step:1623/2330 train_time:65357ms step_avg:40.27ms
step:1624/2330 train_time:65402ms step_avg:40.27ms
step:1625/2330 train_time:65438ms step_avg:40.27ms
step:1626/2330 train_time:65483ms step_avg:40.27ms
step:1627/2330 train_time:65519ms step_avg:40.27ms
step:1628/2330 train_time:65564ms step_avg:40.27ms
step:1629/2330 train_time:65600ms step_avg:40.27ms
step:1630/2330 train_time:65645ms step_avg:40.27ms
step:1631/2330 train_time:65680ms step_avg:40.27ms
step:1632/2330 train_time:65724ms step_avg:40.27ms
step:1633/2330 train_time:65759ms step_avg:40.27ms
step:1634/2330 train_time:65803ms step_avg:40.27ms
step:1635/2330 train_time:65838ms step_avg:40.27ms
step:1636/2330 train_time:65882ms step_avg:40.27ms
step:1637/2330 train_time:65917ms step_avg:40.27ms
step:1638/2330 train_time:65961ms step_avg:40.27ms
step:1639/2330 train_time:65996ms step_avg:40.27ms
step:1640/2330 train_time:66041ms step_avg:40.27ms
step:1641/2330 train_time:66076ms step_avg:40.27ms
step:1642/2330 train_time:66121ms step_avg:40.27ms
step:1643/2330 train_time:66156ms step_avg:40.27ms
step:1644/2330 train_time:66201ms step_avg:40.27ms
step:1645/2330 train_time:66235ms step_avg:40.26ms
step:1646/2330 train_time:66280ms step_avg:40.27ms
step:1647/2330 train_time:66316ms step_avg:40.26ms
step:1648/2330 train_time:66362ms step_avg:40.27ms
step:1649/2330 train_time:66398ms step_avg:40.27ms
step:1650/2330 train_time:66443ms step_avg:40.27ms
step:1651/2330 train_time:66479ms step_avg:40.27ms
step:1652/2330 train_time:66524ms step_avg:40.27ms
step:1653/2330 train_time:66559ms step_avg:40.27ms
step:1654/2330 train_time:66605ms step_avg:40.27ms
step:1655/2330 train_time:66640ms step_avg:40.27ms
step:1656/2330 train_time:66685ms step_avg:40.27ms
step:1657/2330 train_time:66720ms step_avg:40.27ms
step:1658/2330 train_time:66764ms step_avg:40.27ms
step:1659/2330 train_time:66799ms step_avg:40.26ms
step:1660/2330 train_time:66843ms step_avg:40.27ms
step:1661/2330 train_time:66878ms step_avg:40.26ms
step:1662/2330 train_time:66923ms step_avg:40.27ms
step:1663/2330 train_time:66959ms step_avg:40.26ms
step:1664/2330 train_time:67004ms step_avg:40.27ms
step:1665/2330 train_time:67039ms step_avg:40.26ms
step:1666/2330 train_time:67084ms step_avg:40.27ms
step:1667/2330 train_time:67118ms step_avg:40.26ms
step:1668/2330 train_time:67163ms step_avg:40.27ms
step:1669/2330 train_time:67198ms step_avg:40.26ms
step:1670/2330 train_time:67244ms step_avg:40.27ms
step:1671/2330 train_time:67279ms step_avg:40.26ms
step:1672/2330 train_time:67325ms step_avg:40.27ms
step:1673/2330 train_time:67361ms step_avg:40.26ms
step:1674/2330 train_time:67406ms step_avg:40.27ms
step:1675/2330 train_time:67441ms step_avg:40.26ms
step:1676/2330 train_time:67486ms step_avg:40.27ms
step:1677/2330 train_time:67521ms step_avg:40.26ms
step:1678/2330 train_time:67566ms step_avg:40.27ms
step:1679/2330 train_time:67600ms step_avg:40.26ms
step:1680/2330 train_time:67645ms step_avg:40.26ms
step:1681/2330 train_time:67680ms step_avg:40.26ms
step:1682/2330 train_time:67724ms step_avg:40.26ms
step:1683/2330 train_time:67759ms step_avg:40.26ms
step:1684/2330 train_time:67804ms step_avg:40.26ms
step:1685/2330 train_time:67839ms step_avg:40.26ms
step:1686/2330 train_time:67884ms step_avg:40.26ms
step:1687/2330 train_time:67919ms step_avg:40.26ms
step:1688/2330 train_time:67963ms step_avg:40.26ms
step:1689/2330 train_time:67999ms step_avg:40.26ms
step:1690/2330 train_time:68043ms step_avg:40.26ms
step:1691/2330 train_time:68079ms step_avg:40.26ms
step:1692/2330 train_time:68124ms step_avg:40.26ms
step:1693/2330 train_time:68158ms step_avg:40.26ms
step:1694/2330 train_time:68203ms step_avg:40.26ms
step:1695/2330 train_time:68238ms step_avg:40.26ms
step:1696/2330 train_time:68283ms step_avg:40.26ms
step:1697/2330 train_time:68318ms step_avg:40.26ms
step:1698/2330 train_time:68364ms step_avg:40.26ms
step:1699/2330 train_time:68400ms step_avg:40.26ms
step:1700/2330 train_time:68445ms step_avg:40.26ms
step:1701/2330 train_time:68481ms step_avg:40.26ms
step:1702/2330 train_time:68525ms step_avg:40.26ms
step:1703/2330 train_time:68561ms step_avg:40.26ms
step:1704/2330 train_time:68606ms step_avg:40.26ms
step:1705/2330 train_time:68640ms step_avg:40.26ms
step:1706/2330 train_time:68684ms step_avg:40.26ms
step:1707/2330 train_time:68720ms step_avg:40.26ms
step:1708/2330 train_time:68764ms step_avg:40.26ms
step:1709/2330 train_time:68799ms step_avg:40.26ms
step:1710/2330 train_time:68844ms step_avg:40.26ms
step:1711/2330 train_time:68880ms step_avg:40.26ms
step:1712/2330 train_time:68925ms step_avg:40.26ms
step:1713/2330 train_time:68960ms step_avg:40.26ms
step:1714/2330 train_time:69004ms step_avg:40.26ms
step:1715/2330 train_time:69039ms step_avg:40.26ms
step:1716/2330 train_time:69084ms step_avg:40.26ms
step:1717/2330 train_time:69120ms step_avg:40.26ms
step:1718/2330 train_time:69164ms step_avg:40.26ms
step:1719/2330 train_time:69199ms step_avg:40.26ms
step:1720/2330 train_time:69243ms step_avg:40.26ms
step:1721/2330 train_time:69278ms step_avg:40.25ms
step:1722/2330 train_time:69323ms step_avg:40.26ms
step:1723/2330 train_time:69359ms step_avg:40.25ms
step:1724/2330 train_time:69404ms step_avg:40.26ms
step:1725/2330 train_time:69441ms step_avg:40.26ms
step:1726/2330 train_time:69486ms step_avg:40.26ms
step:1727/2330 train_time:69521ms step_avg:40.26ms
step:1728/2330 train_time:69565ms step_avg:40.26ms
step:1729/2330 train_time:69601ms step_avg:40.26ms
step:1730/2330 train_time:69646ms step_avg:40.26ms
step:1731/2330 train_time:69680ms step_avg:40.25ms
step:1732/2330 train_time:69725ms step_avg:40.26ms
step:1733/2330 train_time:69760ms step_avg:40.25ms
step:1734/2330 train_time:69805ms step_avg:40.26ms
step:1735/2330 train_time:69840ms step_avg:40.25ms
step:1736/2330 train_time:69884ms step_avg:40.26ms
step:1737/2330 train_time:69920ms step_avg:40.25ms
step:1738/2330 train_time:69964ms step_avg:40.26ms
step:1739/2330 train_time:69999ms step_avg:40.25ms
step:1740/2330 train_time:70043ms step_avg:40.25ms
step:1741/2330 train_time:70078ms step_avg:40.25ms
step:1742/2330 train_time:70123ms step_avg:40.25ms
step:1743/2330 train_time:70158ms step_avg:40.25ms
step:1744/2330 train_time:70203ms step_avg:40.25ms
step:1745/2330 train_time:70237ms step_avg:40.25ms
step:1746/2330 train_time:70282ms step_avg:40.25ms
step:1747/2330 train_time:70318ms step_avg:40.25ms
step:1748/2330 train_time:70363ms step_avg:40.25ms
step:1749/2330 train_time:70399ms step_avg:40.25ms
step:1750/2330 train_time:70443ms step_avg:40.25ms
step:1750/2330 val_loss:5.1339 train_time:70531ms step_avg:40.30ms
step:1751/2330 train_time:70544ms step_avg:40.29ms
step:1752/2330 train_time:70555ms step_avg:40.27ms
step:1753/2330 train_time:70565ms step_avg:40.25ms
step:1754/2330 train_time:70604ms step_avg:40.25ms
step:1755/2330 train_time:70639ms step_avg:40.25ms
step:1756/2330 train_time:70682ms step_avg:40.25ms
step:1757/2330 train_time:70717ms step_avg:40.25ms
step:1758/2330 train_time:70761ms step_avg:40.25ms
step:1759/2330 train_time:70795ms step_avg:40.25ms
step:1760/2330 train_time:70840ms step_avg:40.25ms
step:1761/2330 train_time:70880ms step_avg:40.25ms
step:1762/2330 train_time:70927ms step_avg:40.25ms
step:1763/2330 train_time:70963ms step_avg:40.25ms
step:1764/2330 train_time:71009ms step_avg:40.25ms
step:1765/2330 train_time:71044ms step_avg:40.25ms
step:1766/2330 train_time:71088ms step_avg:40.25ms
step:1767/2330 train_time:71123ms step_avg:40.25ms
step:1768/2330 train_time:71167ms step_avg:40.25ms
step:1769/2330 train_time:71202ms step_avg:40.25ms
step:1770/2330 train_time:71247ms step_avg:40.25ms
step:1771/2330 train_time:71282ms step_avg:40.25ms
step:1772/2330 train_time:71325ms step_avg:40.25ms
step:1773/2330 train_time:71360ms step_avg:40.25ms
step:1774/2330 train_time:71404ms step_avg:40.25ms
step:1775/2330 train_time:71441ms step_avg:40.25ms
step:1776/2330 train_time:71487ms step_avg:40.25ms
step:1777/2330 train_time:71522ms step_avg:40.25ms
step:1778/2330 train_time:71567ms step_avg:40.25ms
step:1779/2330 train_time:71601ms step_avg:40.25ms
step:1780/2330 train_time:71645ms step_avg:40.25ms
step:1781/2330 train_time:71680ms step_avg:40.25ms
step:1782/2330 train_time:71724ms step_avg:40.25ms
step:1783/2330 train_time:71759ms step_avg:40.25ms
step:1784/2330 train_time:71805ms step_avg:40.25ms
step:1785/2330 train_time:71841ms step_avg:40.25ms
step:1786/2330 train_time:71886ms step_avg:40.25ms
step:1787/2330 train_time:71921ms step_avg:40.25ms
step:1788/2330 train_time:71966ms step_avg:40.25ms
step:1789/2330 train_time:72002ms step_avg:40.25ms
step:1790/2330 train_time:72047ms step_avg:40.25ms
step:1791/2330 train_time:72082ms step_avg:40.25ms
step:1792/2330 train_time:72127ms step_avg:40.25ms
step:1793/2330 train_time:72162ms step_avg:40.25ms
step:1794/2330 train_time:72206ms step_avg:40.25ms
step:1795/2330 train_time:72241ms step_avg:40.25ms
step:1796/2330 train_time:72284ms step_avg:40.25ms
step:1797/2330 train_time:72319ms step_avg:40.24ms
step:1798/2330 train_time:72363ms step_avg:40.25ms
step:1799/2330 train_time:72399ms step_avg:40.24ms
step:1800/2330 train_time:72444ms step_avg:40.25ms
step:1801/2330 train_time:72479ms step_avg:40.24ms
step:1802/2330 train_time:72523ms step_avg:40.25ms
step:1803/2330 train_time:72558ms step_avg:40.24ms
step:1804/2330 train_time:72602ms step_avg:40.25ms
step:1805/2330 train_time:72637ms step_avg:40.24ms
step:1806/2330 train_time:72682ms step_avg:40.24ms
step:1807/2330 train_time:72717ms step_avg:40.24ms
step:1808/2330 train_time:72762ms step_avg:40.24ms
step:1809/2330 train_time:72797ms step_avg:40.24ms
step:1810/2330 train_time:72843ms step_avg:40.24ms
step:1811/2330 train_time:72879ms step_avg:40.24ms
step:1812/2330 train_time:72924ms step_avg:40.25ms
step:1813/2330 train_time:72959ms step_avg:40.24ms
step:1814/2330 train_time:73004ms step_avg:40.25ms
step:1815/2330 train_time:73040ms step_avg:40.24ms
step:1816/2330 train_time:73084ms step_avg:40.24ms
step:1817/2330 train_time:73120ms step_avg:40.24ms
step:1818/2330 train_time:73165ms step_avg:40.24ms
step:1819/2330 train_time:73200ms step_avg:40.24ms
step:1820/2330 train_time:73244ms step_avg:40.24ms
step:1821/2330 train_time:73279ms step_avg:40.24ms
step:1822/2330 train_time:73322ms step_avg:40.24ms
step:1823/2330 train_time:73358ms step_avg:40.24ms
step:1824/2330 train_time:73403ms step_avg:40.24ms
step:1825/2330 train_time:73438ms step_avg:40.24ms
step:1826/2330 train_time:73482ms step_avg:40.24ms
step:1827/2330 train_time:73517ms step_avg:40.24ms
step:1828/2330 train_time:73561ms step_avg:40.24ms
step:1829/2330 train_time:73597ms step_avg:40.24ms
step:1830/2330 train_time:73641ms step_avg:40.24ms
step:1831/2330 train_time:73676ms step_avg:40.24ms
step:1832/2330 train_time:73720ms step_avg:40.24ms
step:1833/2330 train_time:73755ms step_avg:40.24ms
step:1834/2330 train_time:73801ms step_avg:40.24ms
step:1835/2330 train_time:73836ms step_avg:40.24ms
step:1836/2330 train_time:73882ms step_avg:40.24ms
step:1837/2330 train_time:73917ms step_avg:40.24ms
step:1838/2330 train_time:73962ms step_avg:40.24ms
step:1839/2330 train_time:73998ms step_avg:40.24ms
step:1840/2330 train_time:74044ms step_avg:40.24ms
step:1841/2330 train_time:74079ms step_avg:40.24ms
step:1842/2330 train_time:74123ms step_avg:40.24ms
step:1843/2330 train_time:74159ms step_avg:40.24ms
step:1844/2330 train_time:74203ms step_avg:40.24ms
step:1845/2330 train_time:74238ms step_avg:40.24ms
step:1846/2330 train_time:74283ms step_avg:40.24ms
step:1847/2330 train_time:74318ms step_avg:40.24ms
step:1848/2330 train_time:74362ms step_avg:40.24ms
step:1849/2330 train_time:74397ms step_avg:40.24ms
step:1850/2330 train_time:74441ms step_avg:40.24ms
step:1851/2330 train_time:74475ms step_avg:40.24ms
step:1852/2330 train_time:74521ms step_avg:40.24ms
step:1853/2330 train_time:74555ms step_avg:40.23ms
step:1854/2330 train_time:74599ms step_avg:40.24ms
step:1855/2330 train_time:74634ms step_avg:40.23ms
step:1856/2330 train_time:74678ms step_avg:40.24ms
step:1857/2330 train_time:74714ms step_avg:40.23ms
step:1858/2330 train_time:74759ms step_avg:40.24ms
step:1859/2330 train_time:74794ms step_avg:40.23ms
step:1860/2330 train_time:74839ms step_avg:40.24ms
step:1861/2330 train_time:74874ms step_avg:40.23ms
step:1862/2330 train_time:74920ms step_avg:40.24ms
step:1863/2330 train_time:74956ms step_avg:40.23ms
step:1864/2330 train_time:75001ms step_avg:40.24ms
step:1865/2330 train_time:75036ms step_avg:40.23ms
step:1866/2330 train_time:75081ms step_avg:40.24ms
step:1867/2330 train_time:75116ms step_avg:40.23ms
step:1868/2330 train_time:75161ms step_avg:40.24ms
step:1869/2330 train_time:75196ms step_avg:40.23ms
step:1870/2330 train_time:75242ms step_avg:40.24ms
step:1871/2330 train_time:75277ms step_avg:40.23ms
step:1872/2330 train_time:75322ms step_avg:40.24ms
step:1873/2330 train_time:75357ms step_avg:40.23ms
step:1874/2330 train_time:75401ms step_avg:40.24ms
step:1875/2330 train_time:75437ms step_avg:40.23ms
step:1876/2330 train_time:75482ms step_avg:40.24ms
step:1877/2330 train_time:75517ms step_avg:40.23ms
step:1878/2330 train_time:75561ms step_avg:40.23ms
step:1879/2330 train_time:75597ms step_avg:40.23ms
step:1880/2330 train_time:75642ms step_avg:40.24ms
step:1881/2330 train_time:75677ms step_avg:40.23ms
step:1882/2330 train_time:75722ms step_avg:40.23ms
step:1883/2330 train_time:75758ms step_avg:40.23ms
step:1884/2330 train_time:75803ms step_avg:40.24ms
step:1885/2330 train_time:75838ms step_avg:40.23ms
step:1886/2330 train_time:75883ms step_avg:40.23ms
step:1887/2330 train_time:75919ms step_avg:40.23ms
step:1888/2330 train_time:75964ms step_avg:40.24ms
step:1889/2330 train_time:76000ms step_avg:40.23ms
step:1890/2330 train_time:76045ms step_avg:40.24ms
step:1891/2330 train_time:76080ms step_avg:40.23ms
step:1892/2330 train_time:76124ms step_avg:40.23ms
step:1893/2330 train_time:76160ms step_avg:40.23ms
step:1894/2330 train_time:76204ms step_avg:40.23ms
step:1895/2330 train_time:76239ms step_avg:40.23ms
step:1896/2330 train_time:76283ms step_avg:40.23ms
step:1897/2330 train_time:76318ms step_avg:40.23ms
step:1898/2330 train_time:76362ms step_avg:40.23ms
step:1899/2330 train_time:76397ms step_avg:40.23ms
step:1900/2330 train_time:76441ms step_avg:40.23ms
step:1901/2330 train_time:76476ms step_avg:40.23ms
step:1902/2330 train_time:76521ms step_avg:40.23ms
step:1903/2330 train_time:76557ms step_avg:40.23ms
step:1904/2330 train_time:76602ms step_avg:40.23ms
step:1905/2330 train_time:76636ms step_avg:40.23ms
step:1906/2330 train_time:76681ms step_avg:40.23ms
step:1907/2330 train_time:76716ms step_avg:40.23ms
step:1908/2330 train_time:76761ms step_avg:40.23ms
step:1909/2330 train_time:76797ms step_avg:40.23ms
step:1910/2330 train_time:76841ms step_avg:40.23ms
step:1911/2330 train_time:76877ms step_avg:40.23ms
step:1912/2330 train_time:76922ms step_avg:40.23ms
step:1913/2330 train_time:76958ms step_avg:40.23ms
step:1914/2330 train_time:77002ms step_avg:40.23ms
step:1915/2330 train_time:77037ms step_avg:40.23ms
step:1916/2330 train_time:77083ms step_avg:40.23ms
step:1917/2330 train_time:77118ms step_avg:40.23ms
step:1918/2330 train_time:77163ms step_avg:40.23ms
step:1919/2330 train_time:77199ms step_avg:40.23ms
step:1920/2330 train_time:77243ms step_avg:40.23ms
step:1921/2330 train_time:77277ms step_avg:40.23ms
step:1922/2330 train_time:77323ms step_avg:40.23ms
step:1923/2330 train_time:77358ms step_avg:40.23ms
step:1924/2330 train_time:77403ms step_avg:40.23ms
step:1925/2330 train_time:77438ms step_avg:40.23ms
step:1926/2330 train_time:77483ms step_avg:40.23ms
step:1927/2330 train_time:77518ms step_avg:40.23ms
step:1928/2330 train_time:77562ms step_avg:40.23ms
step:1929/2330 train_time:77596ms step_avg:40.23ms
step:1930/2330 train_time:77642ms step_avg:40.23ms
step:1931/2330 train_time:77677ms step_avg:40.23ms
step:1932/2330 train_time:77721ms step_avg:40.23ms
step:1933/2330 train_time:77757ms step_avg:40.23ms
step:1934/2330 train_time:77802ms step_avg:40.23ms
step:1935/2330 train_time:77837ms step_avg:40.23ms
step:1936/2330 train_time:77882ms step_avg:40.23ms
step:1937/2330 train_time:77919ms step_avg:40.23ms
step:1938/2330 train_time:77963ms step_avg:40.23ms
step:1939/2330 train_time:77998ms step_avg:40.23ms
step:1940/2330 train_time:78042ms step_avg:40.23ms
step:1941/2330 train_time:78078ms step_avg:40.23ms
step:1942/2330 train_time:78123ms step_avg:40.23ms
step:1943/2330 train_time:78158ms step_avg:40.23ms
step:1944/2330 train_time:78203ms step_avg:40.23ms
step:1945/2330 train_time:78238ms step_avg:40.23ms
step:1946/2330 train_time:78283ms step_avg:40.23ms
step:1947/2330 train_time:78319ms step_avg:40.23ms
step:1948/2330 train_time:78363ms step_avg:40.23ms
step:1949/2330 train_time:78398ms step_avg:40.22ms
step:1950/2330 train_time:78443ms step_avg:40.23ms
step:1951/2330 train_time:78478ms step_avg:40.22ms
step:1952/2330 train_time:78522ms step_avg:40.23ms
step:1953/2330 train_time:78558ms step_avg:40.22ms
step:1954/2330 train_time:78602ms step_avg:40.23ms
step:1955/2330 train_time:78638ms step_avg:40.22ms
step:1956/2330 train_time:78683ms step_avg:40.23ms
step:1957/2330 train_time:78718ms step_avg:40.22ms
step:1958/2330 train_time:78763ms step_avg:40.23ms
step:1959/2330 train_time:78798ms step_avg:40.22ms
step:1960/2330 train_time:78843ms step_avg:40.23ms
step:1961/2330 train_time:78879ms step_avg:40.22ms
step:1962/2330 train_time:78923ms step_avg:40.23ms
step:1963/2330 train_time:78959ms step_avg:40.22ms
step:1964/2330 train_time:79004ms step_avg:40.23ms
step:1965/2330 train_time:79040ms step_avg:40.22ms
step:1966/2330 train_time:79084ms step_avg:40.23ms
step:1967/2330 train_time:79120ms step_avg:40.22ms
step:1968/2330 train_time:79164ms step_avg:40.23ms
step:1969/2330 train_time:79199ms step_avg:40.22ms
step:1970/2330 train_time:79244ms step_avg:40.23ms
step:1971/2330 train_time:79279ms step_avg:40.22ms
step:1972/2330 train_time:79324ms step_avg:40.22ms
step:1973/2330 train_time:79359ms step_avg:40.22ms
step:1974/2330 train_time:79403ms step_avg:40.22ms
step:1975/2330 train_time:79438ms step_avg:40.22ms
step:1976/2330 train_time:79483ms step_avg:40.22ms
step:1977/2330 train_time:79518ms step_avg:40.22ms
step:1978/2330 train_time:79563ms step_avg:40.22ms
step:1979/2330 train_time:79599ms step_avg:40.22ms
step:1980/2330 train_time:79644ms step_avg:40.22ms
step:1981/2330 train_time:79679ms step_avg:40.22ms
step:1982/2330 train_time:79723ms step_avg:40.22ms
step:1983/2330 train_time:79759ms step_avg:40.22ms
step:1984/2330 train_time:79804ms step_avg:40.22ms
step:1985/2330 train_time:79838ms step_avg:40.22ms
step:1986/2330 train_time:79883ms step_avg:40.22ms
step:1987/2330 train_time:79918ms step_avg:40.22ms
step:1988/2330 train_time:79963ms step_avg:40.22ms
step:1989/2330 train_time:79999ms step_avg:40.22ms
step:1990/2330 train_time:80043ms step_avg:40.22ms
step:1991/2330 train_time:80078ms step_avg:40.22ms
step:1992/2330 train_time:80123ms step_avg:40.22ms
step:1993/2330 train_time:80159ms step_avg:40.22ms
step:1994/2330 train_time:80203ms step_avg:40.22ms
step:1995/2330 train_time:80239ms step_avg:40.22ms
step:1996/2330 train_time:80283ms step_avg:40.22ms
step:1997/2330 train_time:80319ms step_avg:40.22ms
step:1998/2330 train_time:80363ms step_avg:40.22ms
step:1999/2330 train_time:80398ms step_avg:40.22ms
step:2000/2330 train_time:80443ms step_avg:40.22ms
step:2000/2330 val_loss:5.1194 train_time:80530ms step_avg:40.26ms
step:2001/2330 train_time:80543ms step_avg:40.25ms
step:2002/2330 train_time:80555ms step_avg:40.24ms
step:2003/2330 train_time:80566ms step_avg:40.22ms
step:2004/2330 train_time:80604ms step_avg:40.22ms
step:2005/2330 train_time:80638ms step_avg:40.22ms
step:2006/2330 train_time:80682ms step_avg:40.22ms
step:2007/2330 train_time:80716ms step_avg:40.22ms
step:2008/2330 train_time:80760ms step_avg:40.22ms
step:2009/2330 train_time:80795ms step_avg:40.22ms
step:2010/2330 train_time:80843ms step_avg:40.22ms
step:2011/2330 train_time:80881ms step_avg:40.22ms
step:2012/2330 train_time:80929ms step_avg:40.22ms
step:2013/2330 train_time:80965ms step_avg:40.22ms
step:2014/2330 train_time:81011ms step_avg:40.22ms
step:2015/2330 train_time:81046ms step_avg:40.22ms
step:2016/2330 train_time:81090ms step_avg:40.22ms
step:2017/2330 train_time:81125ms step_avg:40.22ms
step:2018/2330 train_time:81169ms step_avg:40.22ms
step:2019/2330 train_time:81203ms step_avg:40.22ms
step:2020/2330 train_time:81247ms step_avg:40.22ms
step:2021/2330 train_time:81282ms step_avg:40.22ms
step:2022/2330 train_time:81326ms step_avg:40.22ms
step:2023/2330 train_time:81361ms step_avg:40.22ms
step:2024/2330 train_time:81404ms step_avg:40.22ms
step:2025/2330 train_time:81439ms step_avg:40.22ms
step:2026/2330 train_time:81483ms step_avg:40.22ms
step:2027/2330 train_time:81519ms step_avg:40.22ms
step:2028/2330 train_time:81563ms step_avg:40.22ms
step:2029/2330 train_time:81597ms step_avg:40.22ms
step:2030/2330 train_time:81641ms step_avg:40.22ms
step:2031/2330 train_time:81676ms step_avg:40.21ms
step:2032/2330 train_time:81720ms step_avg:40.22ms
step:2033/2330 train_time:81755ms step_avg:40.21ms
step:2034/2330 train_time:81800ms step_avg:40.22ms
step:2035/2330 train_time:81836ms step_avg:40.21ms
step:2036/2330 train_time:81882ms step_avg:40.22ms
step:2037/2330 train_time:81918ms step_avg:40.22ms
step:2038/2330 train_time:81964ms step_avg:40.22ms
step:2039/2330 train_time:82001ms step_avg:40.22ms
step:2040/2330 train_time:82046ms step_avg:40.22ms
step:2041/2330 train_time:82082ms step_avg:40.22ms
step:2042/2330 train_time:82126ms step_avg:40.22ms
step:2043/2330 train_time:82161ms step_avg:40.22ms
step:2044/2330 train_time:82205ms step_avg:40.22ms
step:2045/2330 train_time:82240ms step_avg:40.21ms
step:2046/2330 train_time:82284ms step_avg:40.22ms
step:2047/2330 train_time:82319ms step_avg:40.21ms
step:2048/2330 train_time:82364ms step_avg:40.22ms
step:2049/2330 train_time:82398ms step_avg:40.21ms
step:2050/2330 train_time:82442ms step_avg:40.22ms
step:2051/2330 train_time:82476ms step_avg:40.21ms
step:2052/2330 train_time:82520ms step_avg:40.21ms
step:2053/2330 train_time:82554ms step_avg:40.21ms
step:2054/2330 train_time:82598ms step_avg:40.21ms
step:2055/2330 train_time:82633ms step_avg:40.21ms
step:2056/2330 train_time:82677ms step_avg:40.21ms
step:2057/2330 train_time:82711ms step_avg:40.21ms
step:2058/2330 train_time:82755ms step_avg:40.21ms
step:2059/2330 train_time:82791ms step_avg:40.21ms
step:2060/2330 train_time:82835ms step_avg:40.21ms
step:2061/2330 train_time:82871ms step_avg:40.21ms
step:2062/2330 train_time:82916ms step_avg:40.21ms
step:2063/2330 train_time:82953ms step_avg:40.21ms
step:2064/2330 train_time:82999ms step_avg:40.21ms
step:2065/2330 train_time:83035ms step_avg:40.21ms
step:2066/2330 train_time:83079ms step_avg:40.21ms
step:2067/2330 train_time:83114ms step_avg:40.21ms
step:2068/2330 train_time:83158ms step_avg:40.21ms
step:2069/2330 train_time:83193ms step_avg:40.21ms
step:2070/2330 train_time:83237ms step_avg:40.21ms
step:2071/2330 train_time:83272ms step_avg:40.21ms
step:2072/2330 train_time:83316ms step_avg:40.21ms
step:2073/2330 train_time:83351ms step_avg:40.21ms
step:2074/2330 train_time:83394ms step_avg:40.21ms
step:2075/2330 train_time:83429ms step_avg:40.21ms
step:2076/2330 train_time:83473ms step_avg:40.21ms
step:2077/2330 train_time:83508ms step_avg:40.21ms
step:2078/2330 train_time:83551ms step_avg:40.21ms
step:2079/2330 train_time:83586ms step_avg:40.20ms
step:2080/2330 train_time:83630ms step_avg:40.21ms
step:2081/2330 train_time:83665ms step_avg:40.20ms
step:2082/2330 train_time:83710ms step_avg:40.21ms
step:2083/2330 train_time:83746ms step_avg:40.20ms
step:2084/2330 train_time:83790ms step_avg:40.21ms
step:2085/2330 train_time:83825ms step_avg:40.20ms
step:2086/2330 train_time:83870ms step_avg:40.21ms
step:2087/2330 train_time:83905ms step_avg:40.20ms
step:2088/2330 train_time:83950ms step_avg:40.21ms
step:2089/2330 train_time:83985ms step_avg:40.20ms
step:2090/2330 train_time:84030ms step_avg:40.21ms
step:2091/2330 train_time:84065ms step_avg:40.20ms
step:2092/2330 train_time:84109ms step_avg:40.21ms
step:2093/2330 train_time:84144ms step_avg:40.20ms
step:2094/2330 train_time:84188ms step_avg:40.20ms
step:2095/2330 train_time:84224ms step_avg:40.20ms
step:2096/2330 train_time:84268ms step_avg:40.20ms
step:2097/2330 train_time:84303ms step_avg:40.20ms
step:2098/2330 train_time:84347ms step_avg:40.20ms
step:2099/2330 train_time:84382ms step_avg:40.20ms
step:2100/2330 train_time:84426ms step_avg:40.20ms
step:2101/2330 train_time:84461ms step_avg:40.20ms
step:2102/2330 train_time:84506ms step_avg:40.20ms
step:2103/2330 train_time:84540ms step_avg:40.20ms
step:2104/2330 train_time:84584ms step_avg:40.20ms
step:2105/2330 train_time:84620ms step_avg:40.20ms
step:2106/2330 train_time:84665ms step_avg:40.20ms
step:2107/2330 train_time:84699ms step_avg:40.20ms
step:2108/2330 train_time:84744ms step_avg:40.20ms
step:2109/2330 train_time:84780ms step_avg:40.20ms
step:2110/2330 train_time:84825ms step_avg:40.20ms
step:2111/2330 train_time:84860ms step_avg:40.20ms
step:2112/2330 train_time:84905ms step_avg:40.20ms
step:2113/2330 train_time:84941ms step_avg:40.20ms
step:2114/2330 train_time:84985ms step_avg:40.20ms
step:2115/2330 train_time:85021ms step_avg:40.20ms
step:2116/2330 train_time:85066ms step_avg:40.20ms
step:2117/2330 train_time:85102ms step_avg:40.20ms
step:2118/2330 train_time:85146ms step_avg:40.20ms
step:2119/2330 train_time:85181ms step_avg:40.20ms
step:2120/2330 train_time:85225ms step_avg:40.20ms
step:2121/2330 train_time:85261ms step_avg:40.20ms
step:2122/2330 train_time:85306ms step_avg:40.20ms
step:2123/2330 train_time:85341ms step_avg:40.20ms
step:2124/2330 train_time:85385ms step_avg:40.20ms
step:2125/2330 train_time:85420ms step_avg:40.20ms
step:2126/2330 train_time:85464ms step_avg:40.20ms
step:2127/2330 train_time:85498ms step_avg:40.20ms
step:2128/2330 train_time:85543ms step_avg:40.20ms
step:2129/2330 train_time:85578ms step_avg:40.20ms
step:2130/2330 train_time:85623ms step_avg:40.20ms
step:2131/2330 train_time:85658ms step_avg:40.20ms
step:2132/2330 train_time:85702ms step_avg:40.20ms
step:2133/2330 train_time:85737ms step_avg:40.20ms
step:2134/2330 train_time:85783ms step_avg:40.20ms
step:2135/2330 train_time:85817ms step_avg:40.20ms
step:2136/2330 train_time:85863ms step_avg:40.20ms
step:2137/2330 train_time:85898ms step_avg:40.20ms
step:2138/2330 train_time:85943ms step_avg:40.20ms
step:2139/2330 train_time:85979ms step_avg:40.20ms
step:2140/2330 train_time:86024ms step_avg:40.20ms
step:2141/2330 train_time:86060ms step_avg:40.20ms
step:2142/2330 train_time:86105ms step_avg:40.20ms
step:2143/2330 train_time:86140ms step_avg:40.20ms
step:2144/2330 train_time:86185ms step_avg:40.20ms
step:2145/2330 train_time:86220ms step_avg:40.20ms
step:2146/2330 train_time:86265ms step_avg:40.20ms
step:2147/2330 train_time:86300ms step_avg:40.20ms
step:2148/2330 train_time:86345ms step_avg:40.20ms
step:2149/2330 train_time:86379ms step_avg:40.20ms
step:2150/2330 train_time:86424ms step_avg:40.20ms
step:2151/2330 train_time:86458ms step_avg:40.19ms
step:2152/2330 train_time:86502ms step_avg:40.20ms
step:2153/2330 train_time:86537ms step_avg:40.19ms
step:2154/2330 train_time:86581ms step_avg:40.20ms
step:2155/2330 train_time:86616ms step_avg:40.19ms
step:2156/2330 train_time:86660ms step_avg:40.19ms
step:2157/2330 train_time:86696ms step_avg:40.19ms
step:2158/2330 train_time:86740ms step_avg:40.19ms
step:2159/2330 train_time:86775ms step_avg:40.19ms
step:2160/2330 train_time:86819ms step_avg:40.19ms
step:2161/2330 train_time:86854ms step_avg:40.19ms
step:2162/2330 train_time:86900ms step_avg:40.19ms
step:2163/2330 train_time:86935ms step_avg:40.19ms
step:2164/2330 train_time:86979ms step_avg:40.19ms
step:2165/2330 train_time:87015ms step_avg:40.19ms
step:2166/2330 train_time:87061ms step_avg:40.19ms
step:2167/2330 train_time:87096ms step_avg:40.19ms
step:2168/2330 train_time:87141ms step_avg:40.19ms
step:2169/2330 train_time:87177ms step_avg:40.19ms
step:2170/2330 train_time:87221ms step_avg:40.19ms
step:2171/2330 train_time:87257ms step_avg:40.19ms
step:2172/2330 train_time:87301ms step_avg:40.19ms
step:2173/2330 train_time:87336ms step_avg:40.19ms
step:2174/2330 train_time:87380ms step_avg:40.19ms
step:2175/2330 train_time:87415ms step_avg:40.19ms
step:2176/2330 train_time:87459ms step_avg:40.19ms
step:2177/2330 train_time:87494ms step_avg:40.19ms
step:2178/2330 train_time:87539ms step_avg:40.19ms
step:2179/2330 train_time:87575ms step_avg:40.19ms
step:2180/2330 train_time:87619ms step_avg:40.19ms
step:2181/2330 train_time:87653ms step_avg:40.19ms
step:2182/2330 train_time:87698ms step_avg:40.19ms
step:2183/2330 train_time:87733ms step_avg:40.19ms
step:2184/2330 train_time:87777ms step_avg:40.19ms
step:2185/2330 train_time:87812ms step_avg:40.19ms
step:2186/2330 train_time:87856ms step_avg:40.19ms
step:2187/2330 train_time:87891ms step_avg:40.19ms
step:2188/2330 train_time:87935ms step_avg:40.19ms
step:2189/2330 train_time:87970ms step_avg:40.19ms
step:2190/2330 train_time:88014ms step_avg:40.19ms
step:2191/2330 train_time:88050ms step_avg:40.19ms
step:2192/2330 train_time:88094ms step_avg:40.19ms
step:2193/2330 train_time:88130ms step_avg:40.19ms
step:2194/2330 train_time:88174ms step_avg:40.19ms
step:2195/2330 train_time:88209ms step_avg:40.19ms
step:2196/2330 train_time:88254ms step_avg:40.19ms
step:2197/2330 train_time:88289ms step_avg:40.19ms
step:2198/2330 train_time:88334ms step_avg:40.19ms
step:2199/2330 train_time:88370ms step_avg:40.19ms
step:2200/2330 train_time:88414ms step_avg:40.19ms
step:2201/2330 train_time:88449ms step_avg:40.19ms
step:2202/2330 train_time:88494ms step_avg:40.19ms
step:2203/2330 train_time:88528ms step_avg:40.19ms
step:2204/2330 train_time:88572ms step_avg:40.19ms
step:2205/2330 train_time:88607ms step_avg:40.18ms
step:2206/2330 train_time:88651ms step_avg:40.19ms
step:2207/2330 train_time:88687ms step_avg:40.18ms
step:2208/2330 train_time:88731ms step_avg:40.19ms
step:2209/2330 train_time:88766ms step_avg:40.18ms
step:2210/2330 train_time:88810ms step_avg:40.19ms
step:2211/2330 train_time:88845ms step_avg:40.18ms
step:2212/2330 train_time:88889ms step_avg:40.18ms
step:2213/2330 train_time:88924ms step_avg:40.18ms
step:2214/2330 train_time:88969ms step_avg:40.18ms
step:2215/2330 train_time:89004ms step_avg:40.18ms
step:2216/2330 train_time:89048ms step_avg:40.18ms
step:2217/2330 train_time:89083ms step_avg:40.18ms
step:2218/2330 train_time:89129ms step_avg:40.18ms
step:2219/2330 train_time:89164ms step_avg:40.18ms
step:2220/2330 train_time:89208ms step_avg:40.18ms
step:2221/2330 train_time:89243ms step_avg:40.18ms
step:2222/2330 train_time:89287ms step_avg:40.18ms
step:2223/2330 train_time:89323ms step_avg:40.18ms
step:2224/2330 train_time:89367ms step_avg:40.18ms
step:2225/2330 train_time:89402ms step_avg:40.18ms
step:2226/2330 train_time:89446ms step_avg:40.18ms
step:2227/2330 train_time:89482ms step_avg:40.18ms
step:2228/2330 train_time:89526ms step_avg:40.18ms
step:2229/2330 train_time:89562ms step_avg:40.18ms
step:2230/2330 train_time:89606ms step_avg:40.18ms
step:2231/2330 train_time:89641ms step_avg:40.18ms
step:2232/2330 train_time:89686ms step_avg:40.18ms
step:2233/2330 train_time:89721ms step_avg:40.18ms
step:2234/2330 train_time:89765ms step_avg:40.18ms
step:2235/2330 train_time:89801ms step_avg:40.18ms
step:2236/2330 train_time:89845ms step_avg:40.18ms
step:2237/2330 train_time:89880ms step_avg:40.18ms
step:2238/2330 train_time:89924ms step_avg:40.18ms
step:2239/2330 train_time:89960ms step_avg:40.18ms
step:2240/2330 train_time:90004ms step_avg:40.18ms
step:2241/2330 train_time:90039ms step_avg:40.18ms
step:2242/2330 train_time:90085ms step_avg:40.18ms
step:2243/2330 train_time:90120ms step_avg:40.18ms
step:2244/2330 train_time:90166ms step_avg:40.18ms
step:2245/2330 train_time:90201ms step_avg:40.18ms
step:2246/2330 train_time:90245ms step_avg:40.18ms
step:2247/2330 train_time:90281ms step_avg:40.18ms
step:2248/2330 train_time:90326ms step_avg:40.18ms
step:2249/2330 train_time:90361ms step_avg:40.18ms
step:2250/2330 train_time:90405ms step_avg:40.18ms
step:2250/2330 val_loss:5.1110 train_time:90492ms step_avg:40.22ms
step:2251/2330 train_time:90505ms step_avg:40.21ms
step:2252/2330 train_time:90517ms step_avg:40.19ms
step:2253/2330 train_time:90528ms step_avg:40.18ms
step:2254/2330 train_time:90564ms step_avg:40.18ms
step:2255/2330 train_time:90598ms step_avg:40.18ms
step:2256/2330 train_time:90642ms step_avg:40.18ms
step:2257/2330 train_time:90677ms step_avg:40.18ms
step:2258/2330 train_time:90720ms step_avg:40.18ms
step:2259/2330 train_time:90755ms step_avg:40.17ms
step:2260/2330 train_time:90798ms step_avg:40.18ms
step:2261/2330 train_time:90837ms step_avg:40.18ms
step:2262/2330 train_time:90885ms step_avg:40.18ms
step:2263/2330 train_time:90920ms step_avg:40.18ms
step:2264/2330 train_time:90966ms step_avg:40.18ms
step:2265/2330 train_time:91001ms step_avg:40.18ms
step:2266/2330 train_time:91045ms step_avg:40.18ms
step:2267/2330 train_time:91081ms step_avg:40.18ms
step:2268/2330 train_time:91125ms step_avg:40.18ms
step:2269/2330 train_time:91160ms step_avg:40.18ms
step:2270/2330 train_time:91204ms step_avg:40.18ms
step:2271/2330 train_time:91239ms step_avg:40.18ms
step:2272/2330 train_time:91283ms step_avg:40.18ms
step:2273/2330 train_time:91318ms step_avg:40.18ms
step:2274/2330 train_time:91362ms step_avg:40.18ms
step:2275/2330 train_time:91397ms step_avg:40.17ms
step:2276/2330 train_time:91443ms step_avg:40.18ms
step:2277/2330 train_time:91479ms step_avg:40.18ms
step:2278/2330 train_time:91523ms step_avg:40.18ms
step:2279/2330 train_time:91558ms step_avg:40.17ms
step:2280/2330 train_time:91602ms step_avg:40.18ms
step:2281/2330 train_time:91637ms step_avg:40.17ms
step:2282/2330 train_time:91681ms step_avg:40.18ms
step:2283/2330 train_time:91716ms step_avg:40.17ms
step:2284/2330 train_time:91761ms step_avg:40.18ms
step:2285/2330 train_time:91797ms step_avg:40.17ms
step:2286/2330 train_time:91842ms step_avg:40.18ms
step:2287/2330 train_time:91878ms step_avg:40.17ms
step:2288/2330 train_time:91924ms step_avg:40.18ms
step:2289/2330 train_time:91959ms step_avg:40.17ms
step:2290/2330 train_time:92004ms step_avg:40.18ms
step:2291/2330 train_time:92040ms step_avg:40.17ms
step:2292/2330 train_time:92085ms step_avg:40.18ms
step:2293/2330 train_time:92120ms step_avg:40.17ms
step:2294/2330 train_time:92164ms step_avg:40.18ms
step:2295/2330 train_time:92199ms step_avg:40.17ms
step:2296/2330 train_time:92243ms step_avg:40.18ms
step:2297/2330 train_time:92277ms step_avg:40.17ms
step:2298/2330 train_time:92321ms step_avg:40.17ms
step:2299/2330 train_time:92356ms step_avg:40.17ms
step:2300/2330 train_time:92401ms step_avg:40.17ms
step:2301/2330 train_time:92436ms step_avg:40.17ms
step:2302/2330 train_time:92480ms step_avg:40.17ms
step:2303/2330 train_time:92515ms step_avg:40.17ms
step:2304/2330 train_time:92559ms step_avg:40.17ms
step:2305/2330 train_time:92593ms step_avg:40.17ms
step:2306/2330 train_time:92637ms step_avg:40.17ms
step:2307/2330 train_time:92672ms step_avg:40.17ms
step:2308/2330 train_time:92716ms step_avg:40.17ms
step:2309/2330 train_time:92751ms step_avg:40.17ms
step:2310/2330 train_time:92797ms step_avg:40.17ms
step:2311/2330 train_time:92832ms step_avg:40.17ms
step:2312/2330 train_time:92877ms step_avg:40.17ms
step:2313/2330 train_time:92913ms step_avg:40.17ms
step:2314/2330 train_time:92958ms step_avg:40.17ms
step:2315/2330 train_time:92993ms step_avg:40.17ms
step:2316/2330 train_time:93038ms step_avg:40.17ms
step:2317/2330 train_time:93073ms step_avg:40.17ms
step:2318/2330 train_time:93117ms step_avg:40.17ms
step:2319/2330 train_time:93153ms step_avg:40.17ms
step:2320/2330 train_time:93197ms step_avg:40.17ms
step:2321/2330 train_time:93232ms step_avg:40.17ms
step:2322/2330 train_time:93276ms step_avg:40.17ms
step:2323/2330 train_time:93311ms step_avg:40.17ms
step:2324/2330 train_time:93356ms step_avg:40.17ms
step:2325/2330 train_time:93391ms step_avg:40.17ms
step:2326/2330 train_time:93435ms step_avg:40.17ms
step:2327/2330 train_time:93470ms step_avg:40.17ms
step:2328/2330 train_time:93514ms step_avg:40.17ms
step:2329/2330 train_time:93550ms step_avg:40.17ms
step:2330/2330 train_time:93593ms step_avg:40.17ms
step:2330/2330 val_loss:5.1063 train_time:93680ms step_avg:40.21ms
peak memory allocated: 29494 MiB reserved: 38888 MiB
