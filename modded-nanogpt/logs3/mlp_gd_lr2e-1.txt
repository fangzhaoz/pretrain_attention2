import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = divide_by_norm(batched)
                # v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = divide_by_norm(batched_update_grads)
                # v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        # self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "mlp_gd_lr2e-1"
    os.makedirs("logs3", exist_ok=True)
    logfile = f"logs3/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
# embed_params = [p for n, p in model.named_parameters() if "embed" in n]
embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=2e-1, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    return 1.0

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum_min + 0.5 * (momentum_max - momentum_min)

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs3/{run_id}", exist_ok=True)
            torch.save(log, f"logs3/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 08:19:47 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   33C    P0             112W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   31C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   28C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   31C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             116W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.05ms
step:1/2330 train_time:85ms step_avg:85.25ms
step:2/2330 train_time:146ms step_avg:73.16ms
step:3/2330 train_time:159ms step_avg:52.98ms
step:4/2330 train_time:171ms step_avg:42.85ms
step:5/2330 train_time:182ms step_avg:36.42ms
step:6/2330 train_time:207ms step_avg:34.43ms
step:7/2330 train_time:228ms step_avg:32.53ms
step:8/2330 train_time:282ms step_avg:35.26ms
step:9/2330 train_time:304ms step_avg:33.83ms
step:10/2330 train_time:360ms step_avg:35.95ms
step:11/2330 train_time:382ms step_avg:34.71ms
step:12/2330 train_time:437ms step_avg:36.43ms
step:13/2330 train_time:459ms step_avg:35.31ms
step:14/2330 train_time:515ms step_avg:36.76ms
step:15/2330 train_time:536ms step_avg:35.75ms
step:16/2330 train_time:592ms step_avg:36.97ms
step:17/2330 train_time:613ms step_avg:36.08ms
step:18/2330 train_time:669ms step_avg:37.16ms
step:19/2330 train_time:690ms step_avg:36.33ms
step:20/2330 train_time:745ms step_avg:37.26ms
step:21/2330 train_time:767ms step_avg:36.52ms
step:22/2330 train_time:822ms step_avg:37.36ms
step:23/2330 train_time:844ms step_avg:36.68ms
step:24/2330 train_time:898ms step_avg:37.44ms
step:25/2330 train_time:920ms step_avg:36.82ms
step:26/2330 train_time:977ms step_avg:37.57ms
step:27/2330 train_time:1001ms step_avg:37.08ms
step:28/2330 train_time:1062ms step_avg:37.91ms
step:29/2330 train_time:1086ms step_avg:37.45ms
step:30/2330 train_time:1143ms step_avg:38.11ms
step:31/2330 train_time:1166ms step_avg:37.63ms
step:32/2330 train_time:1222ms step_avg:38.19ms
step:33/2330 train_time:1245ms step_avg:37.71ms
step:34/2330 train_time:1300ms step_avg:38.22ms
step:35/2330 train_time:1321ms step_avg:37.75ms
step:36/2330 train_time:1376ms step_avg:38.22ms
step:37/2330 train_time:1397ms step_avg:37.76ms
step:38/2330 train_time:1452ms step_avg:38.21ms
step:39/2330 train_time:1473ms step_avg:37.77ms
step:40/2330 train_time:1528ms step_avg:38.20ms
step:41/2330 train_time:1550ms step_avg:37.79ms
step:42/2330 train_time:1604ms step_avg:38.19ms
step:43/2330 train_time:1626ms step_avg:37.80ms
step:44/2330 train_time:1680ms step_avg:38.18ms
step:45/2330 train_time:1701ms step_avg:37.81ms
step:46/2330 train_time:1756ms step_avg:38.16ms
step:47/2330 train_time:1776ms step_avg:37.80ms
step:48/2330 train_time:1831ms step_avg:38.15ms
step:49/2330 train_time:1852ms step_avg:37.80ms
step:50/2330 train_time:1907ms step_avg:38.15ms
step:51/2330 train_time:1929ms step_avg:37.83ms
step:52/2330 train_time:1986ms step_avg:38.20ms
step:53/2330 train_time:2010ms step_avg:37.92ms
step:54/2330 train_time:2067ms step_avg:38.27ms
step:55/2330 train_time:2090ms step_avg:37.99ms
step:56/2330 train_time:2146ms step_avg:38.32ms
step:57/2330 train_time:2169ms step_avg:38.05ms
step:58/2330 train_time:2225ms step_avg:38.37ms
step:59/2330 train_time:2248ms step_avg:38.11ms
step:60/2330 train_time:2303ms step_avg:38.39ms
step:61/2330 train_time:2326ms step_avg:38.13ms
step:62/2330 train_time:2381ms step_avg:38.40ms
step:63/2330 train_time:2402ms step_avg:38.13ms
step:64/2330 train_time:2458ms step_avg:38.40ms
step:65/2330 train_time:2479ms step_avg:38.15ms
step:66/2330 train_time:2534ms step_avg:38.40ms
step:67/2330 train_time:2556ms step_avg:38.15ms
step:68/2330 train_time:2611ms step_avg:38.39ms
step:69/2330 train_time:2632ms step_avg:38.14ms
step:70/2330 train_time:2686ms step_avg:38.38ms
step:71/2330 train_time:2708ms step_avg:38.13ms
step:72/2330 train_time:2762ms step_avg:38.36ms
step:73/2330 train_time:2784ms step_avg:38.14ms
step:74/2330 train_time:2839ms step_avg:38.36ms
step:75/2330 train_time:2861ms step_avg:38.14ms
step:76/2330 train_time:2916ms step_avg:38.37ms
step:77/2330 train_time:2939ms step_avg:38.17ms
step:78/2330 train_time:2995ms step_avg:38.40ms
step:79/2330 train_time:3018ms step_avg:38.20ms
step:80/2330 train_time:3075ms step_avg:38.43ms
step:81/2330 train_time:3097ms step_avg:38.24ms
step:82/2330 train_time:3153ms step_avg:38.45ms
step:83/2330 train_time:3176ms step_avg:38.26ms
step:84/2330 train_time:3232ms step_avg:38.48ms
step:85/2330 train_time:3254ms step_avg:38.28ms
step:86/2330 train_time:3309ms step_avg:38.48ms
step:87/2330 train_time:3331ms step_avg:38.29ms
step:88/2330 train_time:3386ms step_avg:38.48ms
step:89/2330 train_time:3408ms step_avg:38.30ms
step:90/2330 train_time:3463ms step_avg:38.48ms
step:91/2330 train_time:3486ms step_avg:38.30ms
step:92/2330 train_time:3540ms step_avg:38.48ms
step:93/2330 train_time:3562ms step_avg:38.30ms
step:94/2330 train_time:3617ms step_avg:38.48ms
step:95/2330 train_time:3638ms step_avg:38.30ms
step:96/2330 train_time:3693ms step_avg:38.47ms
step:97/2330 train_time:3714ms step_avg:38.29ms
step:98/2330 train_time:3768ms step_avg:38.45ms
step:99/2330 train_time:3790ms step_avg:38.28ms
step:100/2330 train_time:3845ms step_avg:38.45ms
step:101/2330 train_time:3867ms step_avg:38.29ms
step:102/2330 train_time:3923ms step_avg:38.46ms
step:103/2330 train_time:3945ms step_avg:38.30ms
step:104/2330 train_time:4001ms step_avg:38.47ms
step:105/2330 train_time:4024ms step_avg:38.33ms
step:106/2330 train_time:4080ms step_avg:38.49ms
step:107/2330 train_time:4102ms step_avg:38.34ms
step:108/2330 train_time:4158ms step_avg:38.50ms
step:109/2330 train_time:4181ms step_avg:38.35ms
step:110/2330 train_time:4236ms step_avg:38.51ms
step:111/2330 train_time:4258ms step_avg:38.36ms
step:112/2330 train_time:4314ms step_avg:38.52ms
step:113/2330 train_time:4335ms step_avg:38.37ms
step:114/2330 train_time:4391ms step_avg:38.52ms
step:115/2330 train_time:4412ms step_avg:38.37ms
step:116/2330 train_time:4467ms step_avg:38.51ms
step:117/2330 train_time:4489ms step_avg:38.36ms
step:118/2330 train_time:4544ms step_avg:38.51ms
step:119/2330 train_time:4567ms step_avg:38.38ms
step:120/2330 train_time:4622ms step_avg:38.51ms
step:121/2330 train_time:4644ms step_avg:38.38ms
step:122/2330 train_time:4699ms step_avg:38.51ms
step:123/2330 train_time:4720ms step_avg:38.38ms
step:124/2330 train_time:4775ms step_avg:38.51ms
step:125/2330 train_time:4797ms step_avg:38.37ms
step:126/2330 train_time:4852ms step_avg:38.51ms
step:127/2330 train_time:4874ms step_avg:38.38ms
step:128/2330 train_time:4929ms step_avg:38.51ms
step:129/2330 train_time:4951ms step_avg:38.38ms
step:130/2330 train_time:5006ms step_avg:38.51ms
step:131/2330 train_time:5029ms step_avg:38.39ms
step:132/2330 train_time:5084ms step_avg:38.52ms
step:133/2330 train_time:5107ms step_avg:38.40ms
step:134/2330 train_time:5163ms step_avg:38.53ms
step:135/2330 train_time:5185ms step_avg:38.41ms
step:136/2330 train_time:5240ms step_avg:38.53ms
step:137/2330 train_time:5262ms step_avg:38.41ms
step:138/2330 train_time:5317ms step_avg:38.53ms
step:139/2330 train_time:5340ms step_avg:38.42ms
step:140/2330 train_time:5396ms step_avg:38.54ms
step:141/2330 train_time:5418ms step_avg:38.42ms
step:142/2330 train_time:5473ms step_avg:38.54ms
step:143/2330 train_time:5495ms step_avg:38.43ms
step:144/2330 train_time:5551ms step_avg:38.55ms
step:145/2330 train_time:5572ms step_avg:38.43ms
step:146/2330 train_time:5627ms step_avg:38.54ms
step:147/2330 train_time:5648ms step_avg:38.42ms
step:148/2330 train_time:5703ms step_avg:38.53ms
step:149/2330 train_time:5725ms step_avg:38.42ms
step:150/2330 train_time:5780ms step_avg:38.53ms
step:151/2330 train_time:5802ms step_avg:38.42ms
step:152/2330 train_time:5857ms step_avg:38.53ms
step:153/2330 train_time:5879ms step_avg:38.43ms
step:154/2330 train_time:5934ms step_avg:38.53ms
step:155/2330 train_time:5956ms step_avg:38.43ms
step:156/2330 train_time:6011ms step_avg:38.53ms
step:157/2330 train_time:6034ms step_avg:38.43ms
step:158/2330 train_time:6090ms step_avg:38.54ms
step:159/2330 train_time:6112ms step_avg:38.44ms
step:160/2330 train_time:6167ms step_avg:38.55ms
step:161/2330 train_time:6190ms step_avg:38.44ms
step:162/2330 train_time:6245ms step_avg:38.55ms
step:163/2330 train_time:6267ms step_avg:38.45ms
step:164/2330 train_time:6323ms step_avg:38.55ms
step:165/2330 train_time:6345ms step_avg:38.45ms
step:166/2330 train_time:6400ms step_avg:38.55ms
step:167/2330 train_time:6422ms step_avg:38.45ms
step:168/2330 train_time:6478ms step_avg:38.56ms
step:169/2330 train_time:6500ms step_avg:38.46ms
step:170/2330 train_time:6556ms step_avg:38.56ms
step:171/2330 train_time:6578ms step_avg:38.47ms
step:172/2330 train_time:6634ms step_avg:38.57ms
step:173/2330 train_time:6655ms step_avg:38.47ms
step:174/2330 train_time:6711ms step_avg:38.57ms
step:175/2330 train_time:6732ms step_avg:38.47ms
step:176/2330 train_time:6788ms step_avg:38.57ms
step:177/2330 train_time:6809ms step_avg:38.47ms
step:178/2330 train_time:6865ms step_avg:38.57ms
step:179/2330 train_time:6887ms step_avg:38.47ms
step:180/2330 train_time:6942ms step_avg:38.56ms
step:181/2330 train_time:6964ms step_avg:38.47ms
step:182/2330 train_time:7019ms step_avg:38.57ms
step:183/2330 train_time:7042ms step_avg:38.48ms
step:184/2330 train_time:7098ms step_avg:38.57ms
step:185/2330 train_time:7120ms step_avg:38.49ms
step:186/2330 train_time:7176ms step_avg:38.58ms
step:187/2330 train_time:7198ms step_avg:38.49ms
step:188/2330 train_time:7254ms step_avg:38.59ms
step:189/2330 train_time:7276ms step_avg:38.50ms
step:190/2330 train_time:7331ms step_avg:38.58ms
step:191/2330 train_time:7353ms step_avg:38.50ms
step:192/2330 train_time:7408ms step_avg:38.59ms
step:193/2330 train_time:7431ms step_avg:38.50ms
step:194/2330 train_time:7486ms step_avg:38.59ms
step:195/2330 train_time:7508ms step_avg:38.50ms
step:196/2330 train_time:7564ms step_avg:38.59ms
step:197/2330 train_time:7586ms step_avg:38.51ms
step:198/2330 train_time:7641ms step_avg:38.59ms
step:199/2330 train_time:7663ms step_avg:38.51ms
step:200/2330 train_time:7719ms step_avg:38.59ms
step:201/2330 train_time:7740ms step_avg:38.51ms
step:202/2330 train_time:7796ms step_avg:38.59ms
step:203/2330 train_time:7818ms step_avg:38.51ms
step:204/2330 train_time:7873ms step_avg:38.59ms
step:205/2330 train_time:7895ms step_avg:38.51ms
step:206/2330 train_time:7951ms step_avg:38.60ms
step:207/2330 train_time:7972ms step_avg:38.51ms
step:208/2330 train_time:8028ms step_avg:38.59ms
step:209/2330 train_time:8050ms step_avg:38.51ms
step:210/2330 train_time:8105ms step_avg:38.60ms
step:211/2330 train_time:8129ms step_avg:38.52ms
step:212/2330 train_time:8184ms step_avg:38.60ms
step:213/2330 train_time:8207ms step_avg:38.53ms
step:214/2330 train_time:8261ms step_avg:38.61ms
step:215/2330 train_time:8284ms step_avg:38.53ms
step:216/2330 train_time:8339ms step_avg:38.60ms
step:217/2330 train_time:8361ms step_avg:38.53ms
step:218/2330 train_time:8416ms step_avg:38.61ms
step:219/2330 train_time:8438ms step_avg:38.53ms
step:220/2330 train_time:8494ms step_avg:38.61ms
step:221/2330 train_time:8516ms step_avg:38.54ms
step:222/2330 train_time:8572ms step_avg:38.61ms
step:223/2330 train_time:8594ms step_avg:38.54ms
step:224/2330 train_time:8649ms step_avg:38.61ms
step:225/2330 train_time:8671ms step_avg:38.54ms
step:226/2330 train_time:8727ms step_avg:38.61ms
step:227/2330 train_time:8749ms step_avg:38.54ms
step:228/2330 train_time:8804ms step_avg:38.61ms
step:229/2330 train_time:8826ms step_avg:38.54ms
step:230/2330 train_time:8881ms step_avg:38.61ms
step:231/2330 train_time:8903ms step_avg:38.54ms
step:232/2330 train_time:8959ms step_avg:38.62ms
step:233/2330 train_time:8980ms step_avg:38.54ms
step:234/2330 train_time:9037ms step_avg:38.62ms
step:235/2330 train_time:9059ms step_avg:38.55ms
step:236/2330 train_time:9116ms step_avg:38.63ms
step:237/2330 train_time:9137ms step_avg:38.55ms
step:238/2330 train_time:9193ms step_avg:38.63ms
step:239/2330 train_time:9215ms step_avg:38.56ms
step:240/2330 train_time:9271ms step_avg:38.63ms
step:241/2330 train_time:9292ms step_avg:38.56ms
step:242/2330 train_time:9347ms step_avg:38.63ms
step:243/2330 train_time:9369ms step_avg:38.56ms
step:244/2330 train_time:9425ms step_avg:38.63ms
step:245/2330 train_time:9447ms step_avg:38.56ms
step:246/2330 train_time:9502ms step_avg:38.63ms
step:247/2330 train_time:9525ms step_avg:38.56ms
step:248/2330 train_time:9580ms step_avg:38.63ms
step:249/2330 train_time:9603ms step_avg:38.56ms
step:250/2330 train_time:9658ms step_avg:38.63ms
step:250/2330 val_loss:5.5955 train_time:9753ms step_avg:39.01ms
step:251/2330 train_time:9766ms step_avg:38.91ms
step:252/2330 train_time:9777ms step_avg:38.80ms
step:253/2330 train_time:9788ms step_avg:38.69ms
step:254/2330 train_time:9815ms step_avg:38.64ms
step:255/2330 train_time:9836ms step_avg:38.57ms
step:256/2330 train_time:9891ms step_avg:38.63ms
step:257/2330 train_time:9912ms step_avg:38.57ms
step:258/2330 train_time:9967ms step_avg:38.63ms
step:259/2330 train_time:9989ms step_avg:38.57ms
step:260/2330 train_time:10045ms step_avg:38.63ms
step:261/2330 train_time:10070ms step_avg:38.58ms
step:262/2330 train_time:10130ms step_avg:38.66ms
step:263/2330 train_time:10155ms step_avg:38.61ms
step:264/2330 train_time:10211ms step_avg:38.68ms
step:265/2330 train_time:10234ms step_avg:38.62ms
step:266/2330 train_time:10291ms step_avg:38.69ms
step:267/2330 train_time:10313ms step_avg:38.62ms
step:268/2330 train_time:10368ms step_avg:38.69ms
step:269/2330 train_time:10390ms step_avg:38.62ms
step:270/2330 train_time:10445ms step_avg:38.69ms
step:271/2330 train_time:10467ms step_avg:38.62ms
step:272/2330 train_time:10522ms step_avg:38.68ms
step:273/2330 train_time:10543ms step_avg:38.62ms
step:274/2330 train_time:10598ms step_avg:38.68ms
step:275/2330 train_time:10619ms step_avg:38.62ms
step:276/2330 train_time:10675ms step_avg:38.68ms
step:277/2330 train_time:10698ms step_avg:38.62ms
step:278/2330 train_time:10753ms step_avg:38.68ms
step:279/2330 train_time:10775ms step_avg:38.62ms
step:280/2330 train_time:10830ms step_avg:38.68ms
step:281/2330 train_time:10852ms step_avg:38.62ms
step:282/2330 train_time:10907ms step_avg:38.68ms
step:283/2330 train_time:10929ms step_avg:38.62ms
step:284/2330 train_time:10984ms step_avg:38.68ms
step:285/2330 train_time:11007ms step_avg:38.62ms
step:286/2330 train_time:11064ms step_avg:38.68ms
step:287/2330 train_time:11087ms step_avg:38.63ms
step:288/2330 train_time:11144ms step_avg:38.69ms
step:289/2330 train_time:11166ms step_avg:38.64ms
step:290/2330 train_time:11223ms step_avg:38.70ms
step:291/2330 train_time:11245ms step_avg:38.64ms
step:292/2330 train_time:11301ms step_avg:38.70ms
step:293/2330 train_time:11323ms step_avg:38.65ms
step:294/2330 train_time:11379ms step_avg:38.70ms
step:295/2330 train_time:11401ms step_avg:38.65ms
step:296/2330 train_time:11457ms step_avg:38.71ms
step:297/2330 train_time:11479ms step_avg:38.65ms
step:298/2330 train_time:11534ms step_avg:38.71ms
step:299/2330 train_time:11557ms step_avg:38.65ms
step:300/2330 train_time:11612ms step_avg:38.71ms
step:301/2330 train_time:11634ms step_avg:38.65ms
step:302/2330 train_time:11690ms step_avg:38.71ms
step:303/2330 train_time:11712ms step_avg:38.65ms
step:304/2330 train_time:11767ms step_avg:38.71ms
step:305/2330 train_time:11789ms step_avg:38.65ms
step:306/2330 train_time:11844ms step_avg:38.71ms
step:307/2330 train_time:11866ms step_avg:38.65ms
step:308/2330 train_time:11922ms step_avg:38.71ms
step:309/2330 train_time:11944ms step_avg:38.65ms
step:310/2330 train_time:11999ms step_avg:38.71ms
step:311/2330 train_time:12021ms step_avg:38.65ms
step:312/2330 train_time:12077ms step_avg:38.71ms
step:313/2330 train_time:12099ms step_avg:38.66ms
step:314/2330 train_time:12156ms step_avg:38.71ms
step:315/2330 train_time:12179ms step_avg:38.66ms
step:316/2330 train_time:12235ms step_avg:38.72ms
step:317/2330 train_time:12259ms step_avg:38.67ms
step:318/2330 train_time:12315ms step_avg:38.73ms
step:319/2330 train_time:12338ms step_avg:38.68ms
step:320/2330 train_time:12395ms step_avg:38.73ms
step:321/2330 train_time:12417ms step_avg:38.68ms
step:322/2330 train_time:12473ms step_avg:38.74ms
step:323/2330 train_time:12495ms step_avg:38.68ms
step:324/2330 train_time:12550ms step_avg:38.74ms
step:325/2330 train_time:12572ms step_avg:38.68ms
step:326/2330 train_time:12627ms step_avg:38.73ms
step:327/2330 train_time:12649ms step_avg:38.68ms
step:328/2330 train_time:12705ms step_avg:38.74ms
step:329/2330 train_time:12727ms step_avg:38.68ms
step:330/2330 train_time:12783ms step_avg:38.74ms
step:331/2330 train_time:12805ms step_avg:38.68ms
step:332/2330 train_time:12861ms step_avg:38.74ms
step:333/2330 train_time:12882ms step_avg:38.69ms
step:334/2330 train_time:12938ms step_avg:38.74ms
step:335/2330 train_time:12960ms step_avg:38.69ms
step:336/2330 train_time:13015ms step_avg:38.73ms
step:337/2330 train_time:13038ms step_avg:38.69ms
step:338/2330 train_time:13094ms step_avg:38.74ms
step:339/2330 train_time:13117ms step_avg:38.69ms
step:340/2330 train_time:13173ms step_avg:38.74ms
step:341/2330 train_time:13195ms step_avg:38.70ms
step:342/2330 train_time:13251ms step_avg:38.75ms
step:343/2330 train_time:13274ms step_avg:38.70ms
step:344/2330 train_time:13329ms step_avg:38.75ms
step:345/2330 train_time:13351ms step_avg:38.70ms
step:346/2330 train_time:13407ms step_avg:38.75ms
step:347/2330 train_time:13429ms step_avg:38.70ms
step:348/2330 train_time:13485ms step_avg:38.75ms
step:349/2330 train_time:13507ms step_avg:38.70ms
step:350/2330 train_time:13563ms step_avg:38.75ms
step:351/2330 train_time:13584ms step_avg:38.70ms
step:352/2330 train_time:13640ms step_avg:38.75ms
step:353/2330 train_time:13661ms step_avg:38.70ms
step:354/2330 train_time:13717ms step_avg:38.75ms
step:355/2330 train_time:13740ms step_avg:38.70ms
step:356/2330 train_time:13795ms step_avg:38.75ms
step:357/2330 train_time:13817ms step_avg:38.70ms
step:358/2330 train_time:13873ms step_avg:38.75ms
step:359/2330 train_time:13895ms step_avg:38.70ms
step:360/2330 train_time:13950ms step_avg:38.75ms
step:361/2330 train_time:13973ms step_avg:38.71ms
step:362/2330 train_time:14029ms step_avg:38.75ms
step:363/2330 train_time:14051ms step_avg:38.71ms
step:364/2330 train_time:14108ms step_avg:38.76ms
step:365/2330 train_time:14130ms step_avg:38.71ms
step:366/2330 train_time:14187ms step_avg:38.76ms
step:367/2330 train_time:14210ms step_avg:38.72ms
step:368/2330 train_time:14266ms step_avg:38.77ms
step:369/2330 train_time:14288ms step_avg:38.72ms
step:370/2330 train_time:14344ms step_avg:38.77ms
step:371/2330 train_time:14366ms step_avg:38.72ms
step:372/2330 train_time:14422ms step_avg:38.77ms
step:373/2330 train_time:14444ms step_avg:38.72ms
step:374/2330 train_time:14500ms step_avg:38.77ms
step:375/2330 train_time:14521ms step_avg:38.72ms
step:376/2330 train_time:14576ms step_avg:38.77ms
step:377/2330 train_time:14598ms step_avg:38.72ms
step:378/2330 train_time:14653ms step_avg:38.76ms
step:379/2330 train_time:14675ms step_avg:38.72ms
step:380/2330 train_time:14731ms step_avg:38.76ms
step:381/2330 train_time:14753ms step_avg:38.72ms
step:382/2330 train_time:14809ms step_avg:38.77ms
step:383/2330 train_time:14831ms step_avg:38.72ms
step:384/2330 train_time:14887ms step_avg:38.77ms
step:385/2330 train_time:14909ms step_avg:38.72ms
step:386/2330 train_time:14965ms step_avg:38.77ms
step:387/2330 train_time:14987ms step_avg:38.73ms
step:388/2330 train_time:15043ms step_avg:38.77ms
step:389/2330 train_time:15065ms step_avg:38.73ms
step:390/2330 train_time:15121ms step_avg:38.77ms
step:391/2330 train_time:15143ms step_avg:38.73ms
step:392/2330 train_time:15199ms step_avg:38.77ms
step:393/2330 train_time:15221ms step_avg:38.73ms
step:394/2330 train_time:15278ms step_avg:38.78ms
step:395/2330 train_time:15300ms step_avg:38.73ms
step:396/2330 train_time:15356ms step_avg:38.78ms
step:397/2330 train_time:15378ms step_avg:38.74ms
step:398/2330 train_time:15434ms step_avg:38.78ms
step:399/2330 train_time:15456ms step_avg:38.74ms
step:400/2330 train_time:15512ms step_avg:38.78ms
step:401/2330 train_time:15535ms step_avg:38.74ms
step:402/2330 train_time:15590ms step_avg:38.78ms
step:403/2330 train_time:15613ms step_avg:38.74ms
step:404/2330 train_time:15668ms step_avg:38.78ms
step:405/2330 train_time:15690ms step_avg:38.74ms
step:406/2330 train_time:15746ms step_avg:38.78ms
step:407/2330 train_time:15768ms step_avg:38.74ms
step:408/2330 train_time:15823ms step_avg:38.78ms
step:409/2330 train_time:15846ms step_avg:38.74ms
step:410/2330 train_time:15901ms step_avg:38.78ms
step:411/2330 train_time:15923ms step_avg:38.74ms
step:412/2330 train_time:15978ms step_avg:38.78ms
step:413/2330 train_time:16001ms step_avg:38.74ms
step:414/2330 train_time:16056ms step_avg:38.78ms
step:415/2330 train_time:16079ms step_avg:38.74ms
step:416/2330 train_time:16134ms step_avg:38.78ms
step:417/2330 train_time:16157ms step_avg:38.75ms
step:418/2330 train_time:16213ms step_avg:38.79ms
step:419/2330 train_time:16235ms step_avg:38.75ms
step:420/2330 train_time:16291ms step_avg:38.79ms
step:421/2330 train_time:16313ms step_avg:38.75ms
step:422/2330 train_time:16369ms step_avg:38.79ms
step:423/2330 train_time:16391ms step_avg:38.75ms
step:424/2330 train_time:16447ms step_avg:38.79ms
step:425/2330 train_time:16469ms step_avg:38.75ms
step:426/2330 train_time:16525ms step_avg:38.79ms
step:427/2330 train_time:16548ms step_avg:38.75ms
step:428/2330 train_time:16603ms step_avg:38.79ms
step:429/2330 train_time:16625ms step_avg:38.75ms
step:430/2330 train_time:16681ms step_avg:38.79ms
step:431/2330 train_time:16702ms step_avg:38.75ms
step:432/2330 train_time:16758ms step_avg:38.79ms
step:433/2330 train_time:16780ms step_avg:38.75ms
step:434/2330 train_time:16836ms step_avg:38.79ms
step:435/2330 train_time:16858ms step_avg:38.75ms
step:436/2330 train_time:16914ms step_avg:38.79ms
step:437/2330 train_time:16937ms step_avg:38.76ms
step:438/2330 train_time:16992ms step_avg:38.79ms
step:439/2330 train_time:17014ms step_avg:38.76ms
step:440/2330 train_time:17070ms step_avg:38.80ms
step:441/2330 train_time:17093ms step_avg:38.76ms
step:442/2330 train_time:17149ms step_avg:38.80ms
step:443/2330 train_time:17171ms step_avg:38.76ms
step:444/2330 train_time:17227ms step_avg:38.80ms
step:445/2330 train_time:17249ms step_avg:38.76ms
step:446/2330 train_time:17306ms step_avg:38.80ms
step:447/2330 train_time:17328ms step_avg:38.76ms
step:448/2330 train_time:17384ms step_avg:38.80ms
step:449/2330 train_time:17406ms step_avg:38.77ms
step:450/2330 train_time:17462ms step_avg:38.81ms
step:451/2330 train_time:17485ms step_avg:38.77ms
step:452/2330 train_time:17541ms step_avg:38.81ms
step:453/2330 train_time:17562ms step_avg:38.77ms
step:454/2330 train_time:17618ms step_avg:38.81ms
step:455/2330 train_time:17641ms step_avg:38.77ms
step:456/2330 train_time:17696ms step_avg:38.81ms
step:457/2330 train_time:17718ms step_avg:38.77ms
step:458/2330 train_time:17774ms step_avg:38.81ms
step:459/2330 train_time:17796ms step_avg:38.77ms
step:460/2330 train_time:17852ms step_avg:38.81ms
step:461/2330 train_time:17874ms step_avg:38.77ms
step:462/2330 train_time:17930ms step_avg:38.81ms
step:463/2330 train_time:17952ms step_avg:38.77ms
step:464/2330 train_time:18008ms step_avg:38.81ms
step:465/2330 train_time:18030ms step_avg:38.78ms
step:466/2330 train_time:18087ms step_avg:38.81ms
step:467/2330 train_time:18109ms step_avg:38.78ms
step:468/2330 train_time:18165ms step_avg:38.81ms
step:469/2330 train_time:18187ms step_avg:38.78ms
step:470/2330 train_time:18243ms step_avg:38.82ms
step:471/2330 train_time:18265ms step_avg:38.78ms
step:472/2330 train_time:18321ms step_avg:38.82ms
step:473/2330 train_time:18343ms step_avg:38.78ms
step:474/2330 train_time:18399ms step_avg:38.82ms
step:475/2330 train_time:18421ms step_avg:38.78ms
step:476/2330 train_time:18476ms step_avg:38.82ms
step:477/2330 train_time:18499ms step_avg:38.78ms
step:478/2330 train_time:18554ms step_avg:38.82ms
step:479/2330 train_time:18576ms step_avg:38.78ms
step:480/2330 train_time:18632ms step_avg:38.82ms
step:481/2330 train_time:18654ms step_avg:38.78ms
step:482/2330 train_time:18710ms step_avg:38.82ms
step:483/2330 train_time:18732ms step_avg:38.78ms
step:484/2330 train_time:18788ms step_avg:38.82ms
step:485/2330 train_time:18810ms step_avg:38.78ms
step:486/2330 train_time:18865ms step_avg:38.82ms
step:487/2330 train_time:18887ms step_avg:38.78ms
step:488/2330 train_time:18943ms step_avg:38.82ms
step:489/2330 train_time:18965ms step_avg:38.78ms
step:490/2330 train_time:19021ms step_avg:38.82ms
step:491/2330 train_time:19043ms step_avg:38.78ms
step:492/2330 train_time:19099ms step_avg:38.82ms
step:493/2330 train_time:19121ms step_avg:38.79ms
step:494/2330 train_time:19177ms step_avg:38.82ms
step:495/2330 train_time:19199ms step_avg:38.79ms
step:496/2330 train_time:19255ms step_avg:38.82ms
step:497/2330 train_time:19278ms step_avg:38.79ms
step:498/2330 train_time:19333ms step_avg:38.82ms
step:499/2330 train_time:19356ms step_avg:38.79ms
step:500/2330 train_time:19412ms step_avg:38.82ms
step:500/2330 val_loss:5.4837 train_time:19507ms step_avg:39.01ms
step:501/2330 train_time:19520ms step_avg:38.96ms
step:502/2330 train_time:19532ms step_avg:38.91ms
step:503/2330 train_time:19542ms step_avg:38.85ms
step:504/2330 train_time:19569ms step_avg:38.83ms
step:505/2330 train_time:19590ms step_avg:38.79ms
step:506/2330 train_time:19644ms step_avg:38.82ms
step:507/2330 train_time:19666ms step_avg:38.79ms
step:508/2330 train_time:19721ms step_avg:38.82ms
step:509/2330 train_time:19742ms step_avg:38.79ms
step:510/2330 train_time:19797ms step_avg:38.82ms
step:511/2330 train_time:19821ms step_avg:38.79ms
step:512/2330 train_time:19880ms step_avg:38.83ms
step:513/2330 train_time:19903ms step_avg:38.80ms
step:514/2330 train_time:19960ms step_avg:38.83ms
step:515/2330 train_time:19983ms step_avg:38.80ms
step:516/2330 train_time:20039ms step_avg:38.83ms
step:517/2330 train_time:20061ms step_avg:38.80ms
step:518/2330 train_time:20116ms step_avg:38.83ms
step:519/2330 train_time:20138ms step_avg:38.80ms
step:520/2330 train_time:20194ms step_avg:38.83ms
step:521/2330 train_time:20216ms step_avg:38.80ms
step:522/2330 train_time:20271ms step_avg:38.83ms
step:523/2330 train_time:20293ms step_avg:38.80ms
step:524/2330 train_time:20348ms step_avg:38.83ms
step:525/2330 train_time:20370ms step_avg:38.80ms
step:526/2330 train_time:20426ms step_avg:38.83ms
step:527/2330 train_time:20449ms step_avg:38.80ms
step:528/2330 train_time:20504ms step_avg:38.83ms
step:529/2330 train_time:20527ms step_avg:38.80ms
step:530/2330 train_time:20583ms step_avg:38.84ms
step:531/2330 train_time:20605ms step_avg:38.80ms
step:532/2330 train_time:20660ms step_avg:38.83ms
step:533/2330 train_time:20682ms step_avg:38.80ms
step:534/2330 train_time:20738ms step_avg:38.84ms
step:535/2330 train_time:20761ms step_avg:38.80ms
step:536/2330 train_time:20817ms step_avg:38.84ms
step:537/2330 train_time:20839ms step_avg:38.81ms
step:538/2330 train_time:20896ms step_avg:38.84ms
step:539/2330 train_time:20919ms step_avg:38.81ms
step:540/2330 train_time:20976ms step_avg:38.84ms
step:541/2330 train_time:20998ms step_avg:38.81ms
step:542/2330 train_time:21054ms step_avg:38.85ms
step:543/2330 train_time:21077ms step_avg:38.82ms
step:544/2330 train_time:21132ms step_avg:38.85ms
step:545/2330 train_time:21154ms step_avg:38.82ms
step:546/2330 train_time:21210ms step_avg:38.85ms
step:547/2330 train_time:21231ms step_avg:38.81ms
step:548/2330 train_time:21287ms step_avg:38.84ms
step:549/2330 train_time:21309ms step_avg:38.81ms
step:550/2330 train_time:21364ms step_avg:38.84ms
step:551/2330 train_time:21387ms step_avg:38.81ms
step:552/2330 train_time:21443ms step_avg:38.85ms
step:553/2330 train_time:21465ms step_avg:38.82ms
step:554/2330 train_time:21521ms step_avg:38.85ms
step:555/2330 train_time:21543ms step_avg:38.82ms
step:556/2330 train_time:21599ms step_avg:38.85ms
step:557/2330 train_time:21621ms step_avg:38.82ms
step:558/2330 train_time:21677ms step_avg:38.85ms
step:559/2330 train_time:21698ms step_avg:38.82ms
step:560/2330 train_time:21755ms step_avg:38.85ms
step:561/2330 train_time:21777ms step_avg:38.82ms
step:562/2330 train_time:21834ms step_avg:38.85ms
step:563/2330 train_time:21856ms step_avg:38.82ms
step:564/2330 train_time:21912ms step_avg:38.85ms
step:565/2330 train_time:21934ms step_avg:38.82ms
step:566/2330 train_time:21990ms step_avg:38.85ms
step:567/2330 train_time:22012ms step_avg:38.82ms
step:568/2330 train_time:22068ms step_avg:38.85ms
step:569/2330 train_time:22090ms step_avg:38.82ms
step:570/2330 train_time:22145ms step_avg:38.85ms
step:571/2330 train_time:22168ms step_avg:38.82ms
step:572/2330 train_time:22223ms step_avg:38.85ms
step:573/2330 train_time:22246ms step_avg:38.82ms
step:574/2330 train_time:22301ms step_avg:38.85ms
step:575/2330 train_time:22324ms step_avg:38.82ms
step:576/2330 train_time:22379ms step_avg:38.85ms
step:577/2330 train_time:22402ms step_avg:38.82ms
step:578/2330 train_time:22458ms step_avg:38.85ms
step:579/2330 train_time:22480ms step_avg:38.83ms
step:580/2330 train_time:22536ms step_avg:38.86ms
step:581/2330 train_time:22558ms step_avg:38.83ms
step:582/2330 train_time:22614ms step_avg:38.85ms
step:583/2330 train_time:22635ms step_avg:38.83ms
step:584/2330 train_time:22691ms step_avg:38.85ms
step:585/2330 train_time:22713ms step_avg:38.82ms
step:586/2330 train_time:22769ms step_avg:38.85ms
step:587/2330 train_time:22791ms step_avg:38.83ms
step:588/2330 train_time:22847ms step_avg:38.86ms
step:589/2330 train_time:22869ms step_avg:38.83ms
step:590/2330 train_time:22925ms step_avg:38.86ms
step:591/2330 train_time:22948ms step_avg:38.83ms
step:592/2330 train_time:23003ms step_avg:38.86ms
step:593/2330 train_time:23026ms step_avg:38.83ms
step:594/2330 train_time:23081ms step_avg:38.86ms
step:595/2330 train_time:23104ms step_avg:38.83ms
step:596/2330 train_time:23159ms step_avg:38.86ms
step:597/2330 train_time:23181ms step_avg:38.83ms
step:598/2330 train_time:23237ms step_avg:38.86ms
step:599/2330 train_time:23259ms step_avg:38.83ms
step:600/2330 train_time:23315ms step_avg:38.86ms
step:601/2330 train_time:23337ms step_avg:38.83ms
step:602/2330 train_time:23393ms step_avg:38.86ms
step:603/2330 train_time:23414ms step_avg:38.83ms
step:604/2330 train_time:23471ms step_avg:38.86ms
step:605/2330 train_time:23493ms step_avg:38.83ms
step:606/2330 train_time:23548ms step_avg:38.86ms
step:607/2330 train_time:23570ms step_avg:38.83ms
step:608/2330 train_time:23626ms step_avg:38.86ms
step:609/2330 train_time:23648ms step_avg:38.83ms
step:610/2330 train_time:23704ms step_avg:38.86ms
step:611/2330 train_time:23726ms step_avg:38.83ms
step:612/2330 train_time:23782ms step_avg:38.86ms
step:613/2330 train_time:23804ms step_avg:38.83ms
step:614/2330 train_time:23860ms step_avg:38.86ms
step:615/2330 train_time:23882ms step_avg:38.83ms
step:616/2330 train_time:23938ms step_avg:38.86ms
step:617/2330 train_time:23961ms step_avg:38.83ms
step:618/2330 train_time:24017ms step_avg:38.86ms
step:619/2330 train_time:24040ms step_avg:38.84ms
step:620/2330 train_time:24096ms step_avg:38.86ms
step:621/2330 train_time:24118ms step_avg:38.84ms
step:622/2330 train_time:24174ms step_avg:38.86ms
step:623/2330 train_time:24196ms step_avg:38.84ms
step:624/2330 train_time:24252ms step_avg:38.87ms
step:625/2330 train_time:24274ms step_avg:38.84ms
step:626/2330 train_time:24329ms step_avg:38.86ms
step:627/2330 train_time:24351ms step_avg:38.84ms
step:628/2330 train_time:24407ms step_avg:38.86ms
step:629/2330 train_time:24429ms step_avg:38.84ms
step:630/2330 train_time:24484ms step_avg:38.86ms
step:631/2330 train_time:24507ms step_avg:38.84ms
step:632/2330 train_time:24562ms step_avg:38.86ms
step:633/2330 train_time:24584ms step_avg:38.84ms
step:634/2330 train_time:24640ms step_avg:38.86ms
step:635/2330 train_time:24662ms step_avg:38.84ms
step:636/2330 train_time:24718ms step_avg:38.86ms
step:637/2330 train_time:24740ms step_avg:38.84ms
step:638/2330 train_time:24797ms step_avg:38.87ms
step:639/2330 train_time:24819ms step_avg:38.84ms
step:640/2330 train_time:24875ms step_avg:38.87ms
step:641/2330 train_time:24897ms step_avg:38.84ms
step:642/2330 train_time:24953ms step_avg:38.87ms
step:643/2330 train_time:24975ms step_avg:38.84ms
step:644/2330 train_time:25031ms step_avg:38.87ms
step:645/2330 train_time:25052ms step_avg:38.84ms
step:646/2330 train_time:25108ms step_avg:38.87ms
step:647/2330 train_time:25131ms step_avg:38.84ms
step:648/2330 train_time:25186ms step_avg:38.87ms
step:649/2330 train_time:25208ms step_avg:38.84ms
step:650/2330 train_time:25264ms step_avg:38.87ms
step:651/2330 train_time:25288ms step_avg:38.84ms
step:652/2330 train_time:25344ms step_avg:38.87ms
step:653/2330 train_time:25366ms step_avg:38.85ms
step:654/2330 train_time:25421ms step_avg:38.87ms
step:655/2330 train_time:25443ms step_avg:38.84ms
step:656/2330 train_time:25499ms step_avg:38.87ms
step:657/2330 train_time:25521ms step_avg:38.84ms
step:658/2330 train_time:25577ms step_avg:38.87ms
step:659/2330 train_time:25599ms step_avg:38.85ms
step:660/2330 train_time:25654ms step_avg:38.87ms
step:661/2330 train_time:25676ms step_avg:38.84ms
step:662/2330 train_time:25733ms step_avg:38.87ms
step:663/2330 train_time:25755ms step_avg:38.85ms
step:664/2330 train_time:25811ms step_avg:38.87ms
step:665/2330 train_time:25833ms step_avg:38.85ms
step:666/2330 train_time:25889ms step_avg:38.87ms
step:667/2330 train_time:25911ms step_avg:38.85ms
step:668/2330 train_time:25967ms step_avg:38.87ms
step:669/2330 train_time:25989ms step_avg:38.85ms
step:670/2330 train_time:26045ms step_avg:38.87ms
step:671/2330 train_time:26067ms step_avg:38.85ms
step:672/2330 train_time:26123ms step_avg:38.87ms
step:673/2330 train_time:26145ms step_avg:38.85ms
step:674/2330 train_time:26201ms step_avg:38.87ms
step:675/2330 train_time:26223ms step_avg:38.85ms
step:676/2330 train_time:26279ms step_avg:38.87ms
step:677/2330 train_time:26302ms step_avg:38.85ms
step:678/2330 train_time:26358ms step_avg:38.88ms
step:679/2330 train_time:26381ms step_avg:38.85ms
step:680/2330 train_time:26436ms step_avg:38.88ms
step:681/2330 train_time:26458ms step_avg:38.85ms
step:682/2330 train_time:26514ms step_avg:38.88ms
step:683/2330 train_time:26535ms step_avg:38.85ms
step:684/2330 train_time:26591ms step_avg:38.88ms
step:685/2330 train_time:26613ms step_avg:38.85ms
step:686/2330 train_time:26669ms step_avg:38.88ms
step:687/2330 train_time:26691ms step_avg:38.85ms
step:688/2330 train_time:26746ms step_avg:38.87ms
step:689/2330 train_time:26768ms step_avg:38.85ms
step:690/2330 train_time:26824ms step_avg:38.88ms
step:691/2330 train_time:26847ms step_avg:38.85ms
step:692/2330 train_time:26903ms step_avg:38.88ms
step:693/2330 train_time:26925ms step_avg:38.85ms
step:694/2330 train_time:26981ms step_avg:38.88ms
step:695/2330 train_time:27004ms step_avg:38.85ms
step:696/2330 train_time:27060ms step_avg:38.88ms
step:697/2330 train_time:27082ms step_avg:38.85ms
step:698/2330 train_time:27138ms step_avg:38.88ms
step:699/2330 train_time:27161ms step_avg:38.86ms
step:700/2330 train_time:27217ms step_avg:38.88ms
step:701/2330 train_time:27239ms step_avg:38.86ms
step:702/2330 train_time:27295ms step_avg:38.88ms
step:703/2330 train_time:27318ms step_avg:38.86ms
step:704/2330 train_time:27374ms step_avg:38.88ms
step:705/2330 train_time:27396ms step_avg:38.86ms
step:706/2330 train_time:27451ms step_avg:38.88ms
step:707/2330 train_time:27473ms step_avg:38.86ms
step:708/2330 train_time:27530ms step_avg:38.88ms
step:709/2330 train_time:27552ms step_avg:38.86ms
step:710/2330 train_time:27608ms step_avg:38.88ms
step:711/2330 train_time:27630ms step_avg:38.86ms
step:712/2330 train_time:27686ms step_avg:38.88ms
step:713/2330 train_time:27709ms step_avg:38.86ms
step:714/2330 train_time:27765ms step_avg:38.89ms
step:715/2330 train_time:27788ms step_avg:38.86ms
step:716/2330 train_time:27843ms step_avg:38.89ms
step:717/2330 train_time:27866ms step_avg:38.86ms
step:718/2330 train_time:27921ms step_avg:38.89ms
step:719/2330 train_time:27944ms step_avg:38.86ms
step:720/2330 train_time:27999ms step_avg:38.89ms
step:721/2330 train_time:28022ms step_avg:38.86ms
step:722/2330 train_time:28078ms step_avg:38.89ms
step:723/2330 train_time:28100ms step_avg:38.87ms
step:724/2330 train_time:28156ms step_avg:38.89ms
step:725/2330 train_time:28178ms step_avg:38.87ms
step:726/2330 train_time:28234ms step_avg:38.89ms
step:727/2330 train_time:28256ms step_avg:38.87ms
step:728/2330 train_time:28313ms step_avg:38.89ms
step:729/2330 train_time:28334ms step_avg:38.87ms
step:730/2330 train_time:28390ms step_avg:38.89ms
step:731/2330 train_time:28412ms step_avg:38.87ms
step:732/2330 train_time:28468ms step_avg:38.89ms
step:733/2330 train_time:28491ms step_avg:38.87ms
step:734/2330 train_time:28546ms step_avg:38.89ms
step:735/2330 train_time:28568ms step_avg:38.87ms
step:736/2330 train_time:28624ms step_avg:38.89ms
step:737/2330 train_time:28646ms step_avg:38.87ms
step:738/2330 train_time:28701ms step_avg:38.89ms
step:739/2330 train_time:28723ms step_avg:38.87ms
step:740/2330 train_time:28779ms step_avg:38.89ms
step:741/2330 train_time:28802ms step_avg:38.87ms
step:742/2330 train_time:28857ms step_avg:38.89ms
step:743/2330 train_time:28879ms step_avg:38.87ms
step:744/2330 train_time:28935ms step_avg:38.89ms
step:745/2330 train_time:28958ms step_avg:38.87ms
step:746/2330 train_time:29013ms step_avg:38.89ms
step:747/2330 train_time:29035ms step_avg:38.87ms
step:748/2330 train_time:29091ms step_avg:38.89ms
step:749/2330 train_time:29113ms step_avg:38.87ms
step:750/2330 train_time:29169ms step_avg:38.89ms
step:750/2330 val_loss:5.4286 train_time:29263ms step_avg:39.02ms
step:751/2330 train_time:29276ms step_avg:38.98ms
step:752/2330 train_time:29288ms step_avg:38.95ms
step:753/2330 train_time:29297ms step_avg:38.91ms
step:754/2330 train_time:29326ms step_avg:38.89ms
step:755/2330 train_time:29347ms step_avg:38.87ms
step:756/2330 train_time:29402ms step_avg:38.89ms
step:757/2330 train_time:29424ms step_avg:38.87ms
step:758/2330 train_time:29479ms step_avg:38.89ms
step:759/2330 train_time:29501ms step_avg:38.87ms
step:760/2330 train_time:29557ms step_avg:38.89ms
step:761/2330 train_time:29581ms step_avg:38.87ms
step:762/2330 train_time:29640ms step_avg:38.90ms
step:763/2330 train_time:29663ms step_avg:38.88ms
step:764/2330 train_time:29720ms step_avg:38.90ms
step:765/2330 train_time:29745ms step_avg:38.88ms
step:766/2330 train_time:29800ms step_avg:38.90ms
step:767/2330 train_time:29822ms step_avg:38.88ms
step:768/2330 train_time:29878ms step_avg:38.90ms
step:769/2330 train_time:29900ms step_avg:38.88ms
step:770/2330 train_time:29955ms step_avg:38.90ms
step:771/2330 train_time:29977ms step_avg:38.88ms
step:772/2330 train_time:30032ms step_avg:38.90ms
step:773/2330 train_time:30055ms step_avg:38.88ms
step:774/2330 train_time:30110ms step_avg:38.90ms
step:775/2330 train_time:30132ms step_avg:38.88ms
step:776/2330 train_time:30188ms step_avg:38.90ms
step:777/2330 train_time:30211ms step_avg:38.88ms
step:778/2330 train_time:30266ms step_avg:38.90ms
step:779/2330 train_time:30288ms step_avg:38.88ms
step:780/2330 train_time:30344ms step_avg:38.90ms
step:781/2330 train_time:30366ms step_avg:38.88ms
step:782/2330 train_time:30421ms step_avg:38.90ms
step:783/2330 train_time:30444ms step_avg:38.88ms
step:784/2330 train_time:30500ms step_avg:38.90ms
step:785/2330 train_time:30523ms step_avg:38.88ms
step:786/2330 train_time:30579ms step_avg:38.90ms
step:787/2330 train_time:30602ms step_avg:38.88ms
step:788/2330 train_time:30659ms step_avg:38.91ms
step:789/2330 train_time:30683ms step_avg:38.89ms
step:790/2330 train_time:30740ms step_avg:38.91ms
step:791/2330 train_time:30763ms step_avg:38.89ms
step:792/2330 train_time:30819ms step_avg:38.91ms
step:793/2330 train_time:30841ms step_avg:38.89ms
step:794/2330 train_time:30897ms step_avg:38.91ms
step:795/2330 train_time:30920ms step_avg:38.89ms
step:796/2330 train_time:30976ms step_avg:38.91ms
step:797/2330 train_time:30998ms step_avg:38.89ms
step:798/2330 train_time:31053ms step_avg:38.91ms
step:799/2330 train_time:31076ms step_avg:38.89ms
step:800/2330 train_time:31132ms step_avg:38.92ms
step:801/2330 train_time:31154ms step_avg:38.89ms
step:802/2330 train_time:31209ms step_avg:38.91ms
step:803/2330 train_time:31231ms step_avg:38.89ms
step:804/2330 train_time:31286ms step_avg:38.91ms
step:805/2330 train_time:31308ms step_avg:38.89ms
step:806/2330 train_time:31363ms step_avg:38.91ms
step:807/2330 train_time:31385ms step_avg:38.89ms
step:808/2330 train_time:31441ms step_avg:38.91ms
step:809/2330 train_time:31463ms step_avg:38.89ms
step:810/2330 train_time:31519ms step_avg:38.91ms
step:811/2330 train_time:31541ms step_avg:38.89ms
step:812/2330 train_time:31597ms step_avg:38.91ms
step:813/2330 train_time:31619ms step_avg:38.89ms
step:814/2330 train_time:31676ms step_avg:38.91ms
step:815/2330 train_time:31698ms step_avg:38.89ms
step:816/2330 train_time:31755ms step_avg:38.92ms
step:817/2330 train_time:31777ms step_avg:38.89ms
step:818/2330 train_time:31833ms step_avg:38.92ms
step:819/2330 train_time:31856ms step_avg:38.90ms
step:820/2330 train_time:31911ms step_avg:38.92ms
step:821/2330 train_time:31934ms step_avg:38.90ms
step:822/2330 train_time:31990ms step_avg:38.92ms
step:823/2330 train_time:32011ms step_avg:38.90ms
step:824/2330 train_time:32066ms step_avg:38.92ms
step:825/2330 train_time:32089ms step_avg:38.90ms
step:826/2330 train_time:32144ms step_avg:38.92ms
step:827/2330 train_time:32166ms step_avg:38.90ms
step:828/2330 train_time:32222ms step_avg:38.91ms
step:829/2330 train_time:32244ms step_avg:38.89ms
step:830/2330 train_time:32300ms step_avg:38.92ms
step:831/2330 train_time:32321ms step_avg:38.89ms
step:832/2330 train_time:32377ms step_avg:38.91ms
step:833/2330 train_time:32399ms step_avg:38.89ms
step:834/2330 train_time:32455ms step_avg:38.91ms
step:835/2330 train_time:32477ms step_avg:38.89ms
step:836/2330 train_time:32533ms step_avg:38.91ms
step:837/2330 train_time:32555ms step_avg:38.90ms
step:838/2330 train_time:32611ms step_avg:38.92ms
step:839/2330 train_time:32634ms step_avg:38.90ms
step:840/2330 train_time:32690ms step_avg:38.92ms
step:841/2330 train_time:32713ms step_avg:38.90ms
step:842/2330 train_time:32770ms step_avg:38.92ms
step:843/2330 train_time:32792ms step_avg:38.90ms
step:844/2330 train_time:32848ms step_avg:38.92ms
step:845/2330 train_time:32871ms step_avg:38.90ms
step:846/2330 train_time:32927ms step_avg:38.92ms
step:847/2330 train_time:32949ms step_avg:38.90ms
step:848/2330 train_time:33005ms step_avg:38.92ms
step:849/2330 train_time:33027ms step_avg:38.90ms
step:850/2330 train_time:33083ms step_avg:38.92ms
step:851/2330 train_time:33105ms step_avg:38.90ms
step:852/2330 train_time:33161ms step_avg:38.92ms
step:853/2330 train_time:33183ms step_avg:38.90ms
step:854/2330 train_time:33238ms step_avg:38.92ms
step:855/2330 train_time:33260ms step_avg:38.90ms
step:856/2330 train_time:33315ms step_avg:38.92ms
step:857/2330 train_time:33337ms step_avg:38.90ms
step:858/2330 train_time:33392ms step_avg:38.92ms
step:859/2330 train_time:33415ms step_avg:38.90ms
step:860/2330 train_time:33471ms step_avg:38.92ms
step:861/2330 train_time:33493ms step_avg:38.90ms
step:862/2330 train_time:33549ms step_avg:38.92ms
step:863/2330 train_time:33572ms step_avg:38.90ms
step:864/2330 train_time:33629ms step_avg:38.92ms
step:865/2330 train_time:33651ms step_avg:38.90ms
step:866/2330 train_time:33707ms step_avg:38.92ms
step:867/2330 train_time:33730ms step_avg:38.90ms
step:868/2330 train_time:33786ms step_avg:38.92ms
step:869/2330 train_time:33808ms step_avg:38.91ms
step:870/2330 train_time:33865ms step_avg:38.93ms
step:871/2330 train_time:33888ms step_avg:38.91ms
step:872/2330 train_time:33943ms step_avg:38.93ms
step:873/2330 train_time:33965ms step_avg:38.91ms
step:874/2330 train_time:34021ms step_avg:38.93ms
step:875/2330 train_time:34043ms step_avg:38.91ms
step:876/2330 train_time:34099ms step_avg:38.93ms
step:877/2330 train_time:34121ms step_avg:38.91ms
step:878/2330 train_time:34176ms step_avg:38.93ms
step:879/2330 train_time:34198ms step_avg:38.91ms
step:880/2330 train_time:34254ms step_avg:38.92ms
step:881/2330 train_time:34276ms step_avg:38.91ms
step:882/2330 train_time:34331ms step_avg:38.92ms
step:883/2330 train_time:34353ms step_avg:38.91ms
step:884/2330 train_time:34409ms step_avg:38.92ms
step:885/2330 train_time:34431ms step_avg:38.91ms
step:886/2330 train_time:34486ms step_avg:38.92ms
step:887/2330 train_time:34509ms step_avg:38.91ms
step:888/2330 train_time:34564ms step_avg:38.92ms
step:889/2330 train_time:34587ms step_avg:38.91ms
step:890/2330 train_time:34643ms step_avg:38.92ms
step:891/2330 train_time:34666ms step_avg:38.91ms
step:892/2330 train_time:34722ms step_avg:38.93ms
step:893/2330 train_time:34745ms step_avg:38.91ms
step:894/2330 train_time:34801ms step_avg:38.93ms
step:895/2330 train_time:34824ms step_avg:38.91ms
step:896/2330 train_time:34880ms step_avg:38.93ms
step:897/2330 train_time:34902ms step_avg:38.91ms
step:898/2330 train_time:34958ms step_avg:38.93ms
step:899/2330 train_time:34980ms step_avg:38.91ms
step:900/2330 train_time:35037ms step_avg:38.93ms
step:901/2330 train_time:35059ms step_avg:38.91ms
step:902/2330 train_time:35114ms step_avg:38.93ms
step:903/2330 train_time:35136ms step_avg:38.91ms
step:904/2330 train_time:35192ms step_avg:38.93ms
step:905/2330 train_time:35214ms step_avg:38.91ms
step:906/2330 train_time:35269ms step_avg:38.93ms
step:907/2330 train_time:35291ms step_avg:38.91ms
step:908/2330 train_time:35346ms step_avg:38.93ms
step:909/2330 train_time:35369ms step_avg:38.91ms
step:910/2330 train_time:35424ms step_avg:38.93ms
step:911/2330 train_time:35447ms step_avg:38.91ms
step:912/2330 train_time:35503ms step_avg:38.93ms
step:913/2330 train_time:35525ms step_avg:38.91ms
step:914/2330 train_time:35580ms step_avg:38.93ms
step:915/2330 train_time:35603ms step_avg:38.91ms
step:916/2330 train_time:35659ms step_avg:38.93ms
step:917/2330 train_time:35682ms step_avg:38.91ms
step:918/2330 train_time:35738ms step_avg:38.93ms
step:919/2330 train_time:35760ms step_avg:38.91ms
step:920/2330 train_time:35817ms step_avg:38.93ms
step:921/2330 train_time:35840ms step_avg:38.91ms
step:922/2330 train_time:35896ms step_avg:38.93ms
step:923/2330 train_time:35919ms step_avg:38.92ms
step:924/2330 train_time:35974ms step_avg:38.93ms
step:925/2330 train_time:35997ms step_avg:38.92ms
step:926/2330 train_time:36053ms step_avg:38.93ms
step:927/2330 train_time:36075ms step_avg:38.92ms
step:928/2330 train_time:36130ms step_avg:38.93ms
step:929/2330 train_time:36152ms step_avg:38.92ms
step:930/2330 train_time:36207ms step_avg:38.93ms
step:931/2330 train_time:36229ms step_avg:38.91ms
step:932/2330 train_time:36285ms step_avg:38.93ms
step:933/2330 train_time:36307ms step_avg:38.91ms
step:934/2330 train_time:36362ms step_avg:38.93ms
step:935/2330 train_time:36384ms step_avg:38.91ms
step:936/2330 train_time:36440ms step_avg:38.93ms
step:937/2330 train_time:36463ms step_avg:38.91ms
step:938/2330 train_time:36519ms step_avg:38.93ms
step:939/2330 train_time:36541ms step_avg:38.91ms
step:940/2330 train_time:36598ms step_avg:38.93ms
step:941/2330 train_time:36620ms step_avg:38.92ms
step:942/2330 train_time:36675ms step_avg:38.93ms
step:943/2330 train_time:36698ms step_avg:38.92ms
step:944/2330 train_time:36754ms step_avg:38.93ms
step:945/2330 train_time:36776ms step_avg:38.92ms
step:946/2330 train_time:36833ms step_avg:38.94ms
step:947/2330 train_time:36854ms step_avg:38.92ms
step:948/2330 train_time:36910ms step_avg:38.93ms
step:949/2330 train_time:36932ms step_avg:38.92ms
step:950/2330 train_time:36988ms step_avg:38.93ms
step:951/2330 train_time:37010ms step_avg:38.92ms
step:952/2330 train_time:37065ms step_avg:38.93ms
step:953/2330 train_time:37088ms step_avg:38.92ms
step:954/2330 train_time:37144ms step_avg:38.94ms
step:955/2330 train_time:37167ms step_avg:38.92ms
step:956/2330 train_time:37222ms step_avg:38.94ms
step:957/2330 train_time:37244ms step_avg:38.92ms
step:958/2330 train_time:37299ms step_avg:38.93ms
step:959/2330 train_time:37322ms step_avg:38.92ms
step:960/2330 train_time:37378ms step_avg:38.93ms
step:961/2330 train_time:37400ms step_avg:38.92ms
step:962/2330 train_time:37456ms step_avg:38.94ms
step:963/2330 train_time:37479ms step_avg:38.92ms
step:964/2330 train_time:37535ms step_avg:38.94ms
step:965/2330 train_time:37557ms step_avg:38.92ms
step:966/2330 train_time:37613ms step_avg:38.94ms
step:967/2330 train_time:37635ms step_avg:38.92ms
step:968/2330 train_time:37691ms step_avg:38.94ms
step:969/2330 train_time:37713ms step_avg:38.92ms
step:970/2330 train_time:37770ms step_avg:38.94ms
step:971/2330 train_time:37792ms step_avg:38.92ms
step:972/2330 train_time:37848ms step_avg:38.94ms
step:973/2330 train_time:37870ms step_avg:38.92ms
step:974/2330 train_time:37925ms step_avg:38.94ms
step:975/2330 train_time:37948ms step_avg:38.92ms
step:976/2330 train_time:38004ms step_avg:38.94ms
step:977/2330 train_time:38027ms step_avg:38.92ms
step:978/2330 train_time:38083ms step_avg:38.94ms
step:979/2330 train_time:38105ms step_avg:38.92ms
step:980/2330 train_time:38161ms step_avg:38.94ms
step:981/2330 train_time:38183ms step_avg:38.92ms
step:982/2330 train_time:38238ms step_avg:38.94ms
step:983/2330 train_time:38261ms step_avg:38.92ms
step:984/2330 train_time:38316ms step_avg:38.94ms
step:985/2330 train_time:38339ms step_avg:38.92ms
step:986/2330 train_time:38395ms step_avg:38.94ms
step:987/2330 train_time:38417ms step_avg:38.92ms
step:988/2330 train_time:38472ms step_avg:38.94ms
step:989/2330 train_time:38495ms step_avg:38.92ms
step:990/2330 train_time:38551ms step_avg:38.94ms
step:991/2330 train_time:38573ms step_avg:38.92ms
step:992/2330 train_time:38629ms step_avg:38.94ms
step:993/2330 train_time:38651ms step_avg:38.92ms
step:994/2330 train_time:38707ms step_avg:38.94ms
step:995/2330 train_time:38729ms step_avg:38.92ms
step:996/2330 train_time:38785ms step_avg:38.94ms
step:997/2330 train_time:38808ms step_avg:38.92ms
step:998/2330 train_time:38863ms step_avg:38.94ms
step:999/2330 train_time:38886ms step_avg:38.92ms
step:1000/2330 train_time:38942ms step_avg:38.94ms
step:1000/2330 val_loss:5.3942 train_time:39038ms step_avg:39.04ms
step:1001/2330 train_time:39051ms step_avg:39.01ms
step:1002/2330 train_time:39062ms step_avg:38.98ms
step:1003/2330 train_time:39073ms step_avg:38.96ms
step:1004/2330 train_time:39101ms step_avg:38.94ms
step:1005/2330 train_time:39122ms step_avg:38.93ms
step:1006/2330 train_time:39177ms step_avg:38.94ms
step:1007/2330 train_time:39198ms step_avg:38.93ms
step:1008/2330 train_time:39253ms step_avg:38.94ms
step:1009/2330 train_time:39275ms step_avg:38.92ms
step:1010/2330 train_time:39330ms step_avg:38.94ms
step:1011/2330 train_time:39353ms step_avg:38.93ms
step:1012/2330 train_time:39413ms step_avg:38.95ms
step:1013/2330 train_time:39439ms step_avg:38.93ms
step:1014/2330 train_time:39496ms step_avg:38.95ms
step:1015/2330 train_time:39520ms step_avg:38.94ms
step:1016/2330 train_time:39576ms step_avg:38.95ms
step:1017/2330 train_time:39598ms step_avg:38.94ms
step:1018/2330 train_time:39653ms step_avg:38.95ms
step:1019/2330 train_time:39676ms step_avg:38.94ms
step:1020/2330 train_time:39731ms step_avg:38.95ms
step:1021/2330 train_time:39754ms step_avg:38.94ms
step:1022/2330 train_time:39809ms step_avg:38.95ms
step:1023/2330 train_time:39831ms step_avg:38.94ms
step:1024/2330 train_time:39886ms step_avg:38.95ms
step:1025/2330 train_time:39908ms step_avg:38.93ms
step:1026/2330 train_time:39964ms step_avg:38.95ms
step:1027/2330 train_time:39986ms step_avg:38.93ms
step:1028/2330 train_time:40042ms step_avg:38.95ms
step:1029/2330 train_time:40064ms step_avg:38.93ms
step:1030/2330 train_time:40121ms step_avg:38.95ms
step:1031/2330 train_time:40142ms step_avg:38.94ms
step:1032/2330 train_time:40198ms step_avg:38.95ms
step:1033/2330 train_time:40220ms step_avg:38.93ms
step:1034/2330 train_time:40275ms step_avg:38.95ms
step:1035/2330 train_time:40297ms step_avg:38.93ms
step:1036/2330 train_time:40354ms step_avg:38.95ms
step:1037/2330 train_time:40378ms step_avg:38.94ms
step:1038/2330 train_time:40435ms step_avg:38.95ms
step:1039/2330 train_time:40458ms step_avg:38.94ms
step:1040/2330 train_time:40514ms step_avg:38.96ms
step:1041/2330 train_time:40538ms step_avg:38.94ms
step:1042/2330 train_time:40594ms step_avg:38.96ms
step:1043/2330 train_time:40618ms step_avg:38.94ms
step:1044/2330 train_time:40674ms step_avg:38.96ms
step:1045/2330 train_time:40697ms step_avg:38.94ms
step:1046/2330 train_time:40752ms step_avg:38.96ms
step:1047/2330 train_time:40775ms step_avg:38.94ms
step:1048/2330 train_time:40831ms step_avg:38.96ms
step:1049/2330 train_time:40853ms step_avg:38.95ms
step:1050/2330 train_time:40909ms step_avg:38.96ms
step:1051/2330 train_time:40931ms step_avg:38.95ms
step:1052/2330 train_time:40987ms step_avg:38.96ms
step:1053/2330 train_time:41009ms step_avg:38.94ms
step:1054/2330 train_time:41064ms step_avg:38.96ms
step:1055/2330 train_time:41087ms step_avg:38.94ms
step:1056/2330 train_time:41142ms step_avg:38.96ms
step:1057/2330 train_time:41165ms step_avg:38.95ms
step:1058/2330 train_time:41221ms step_avg:38.96ms
step:1059/2330 train_time:41244ms step_avg:38.95ms
step:1060/2330 train_time:41300ms step_avg:38.96ms
step:1061/2330 train_time:41323ms step_avg:38.95ms
step:1062/2330 train_time:41380ms step_avg:38.96ms
step:1063/2330 train_time:41402ms step_avg:38.95ms
step:1064/2330 train_time:41459ms step_avg:38.96ms
step:1065/2330 train_time:41481ms step_avg:38.95ms
step:1066/2330 train_time:41538ms step_avg:38.97ms
step:1067/2330 train_time:41560ms step_avg:38.95ms
step:1068/2330 train_time:41616ms step_avg:38.97ms
step:1069/2330 train_time:41637ms step_avg:38.95ms
step:1070/2330 train_time:41693ms step_avg:38.97ms
step:1071/2330 train_time:41715ms step_avg:38.95ms
step:1072/2330 train_time:41772ms step_avg:38.97ms
step:1073/2330 train_time:41794ms step_avg:38.95ms
step:1074/2330 train_time:41850ms step_avg:38.97ms
step:1075/2330 train_time:41872ms step_avg:38.95ms
step:1076/2330 train_time:41928ms step_avg:38.97ms
step:1077/2330 train_time:41951ms step_avg:38.95ms
step:1078/2330 train_time:42007ms step_avg:38.97ms
step:1079/2330 train_time:42029ms step_avg:38.95ms
step:1080/2330 train_time:42085ms step_avg:38.97ms
step:1081/2330 train_time:42108ms step_avg:38.95ms
step:1082/2330 train_time:42163ms step_avg:38.97ms
step:1083/2330 train_time:42185ms step_avg:38.95ms
step:1084/2330 train_time:42241ms step_avg:38.97ms
step:1085/2330 train_time:42264ms step_avg:38.95ms
step:1086/2330 train_time:42320ms step_avg:38.97ms
step:1087/2330 train_time:42343ms step_avg:38.95ms
step:1088/2330 train_time:42400ms step_avg:38.97ms
step:1089/2330 train_time:42423ms step_avg:38.96ms
step:1090/2330 train_time:42481ms step_avg:38.97ms
step:1091/2330 train_time:42503ms step_avg:38.96ms
step:1092/2330 train_time:42560ms step_avg:38.97ms
step:1093/2330 train_time:42582ms step_avg:38.96ms
step:1094/2330 train_time:42638ms step_avg:38.97ms
step:1095/2330 train_time:42660ms step_avg:38.96ms
step:1096/2330 train_time:42716ms step_avg:38.97ms
step:1097/2330 train_time:42738ms step_avg:38.96ms
step:1098/2330 train_time:42794ms step_avg:38.97ms
step:1099/2330 train_time:42816ms step_avg:38.96ms
step:1100/2330 train_time:42872ms step_avg:38.97ms
step:1101/2330 train_time:42894ms step_avg:38.96ms
step:1102/2330 train_time:42950ms step_avg:38.97ms
step:1103/2330 train_time:42973ms step_avg:38.96ms
step:1104/2330 train_time:43029ms step_avg:38.98ms
step:1105/2330 train_time:43052ms step_avg:38.96ms
step:1106/2330 train_time:43107ms step_avg:38.98ms
step:1107/2330 train_time:43129ms step_avg:38.96ms
step:1108/2330 train_time:43186ms step_avg:38.98ms
step:1109/2330 train_time:43208ms step_avg:38.96ms
step:1110/2330 train_time:43263ms step_avg:38.98ms
step:1111/2330 train_time:43286ms step_avg:38.96ms
step:1112/2330 train_time:43343ms step_avg:38.98ms
step:1113/2330 train_time:43367ms step_avg:38.96ms
step:1114/2330 train_time:43423ms step_avg:38.98ms
step:1115/2330 train_time:43446ms step_avg:38.97ms
step:1116/2330 train_time:43503ms step_avg:38.98ms
step:1117/2330 train_time:43526ms step_avg:38.97ms
step:1118/2330 train_time:43582ms step_avg:38.98ms
step:1119/2330 train_time:43605ms step_avg:38.97ms
step:1120/2330 train_time:43661ms step_avg:38.98ms
step:1121/2330 train_time:43684ms step_avg:38.97ms
step:1122/2330 train_time:43740ms step_avg:38.98ms
step:1123/2330 train_time:43762ms step_avg:38.97ms
step:1124/2330 train_time:43818ms step_avg:38.98ms
step:1125/2330 train_time:43840ms step_avg:38.97ms
step:1126/2330 train_time:43896ms step_avg:38.98ms
step:1127/2330 train_time:43918ms step_avg:38.97ms
step:1128/2330 train_time:43974ms step_avg:38.98ms
step:1129/2330 train_time:43997ms step_avg:38.97ms
step:1130/2330 train_time:44053ms step_avg:38.98ms
step:1131/2330 train_time:44075ms step_avg:38.97ms
step:1132/2330 train_time:44131ms step_avg:38.98ms
step:1133/2330 train_time:44154ms step_avg:38.97ms
step:1134/2330 train_time:44209ms step_avg:38.99ms
step:1135/2330 train_time:44232ms step_avg:38.97ms
step:1136/2330 train_time:44288ms step_avg:38.99ms
step:1137/2330 train_time:44312ms step_avg:38.97ms
step:1138/2330 train_time:44368ms step_avg:38.99ms
step:1139/2330 train_time:44391ms step_avg:38.97ms
step:1140/2330 train_time:44447ms step_avg:38.99ms
step:1141/2330 train_time:44470ms step_avg:38.97ms
step:1142/2330 train_time:44526ms step_avg:38.99ms
step:1143/2330 train_time:44548ms step_avg:38.97ms
step:1144/2330 train_time:44604ms step_avg:38.99ms
step:1145/2330 train_time:44627ms step_avg:38.98ms
step:1146/2330 train_time:44684ms step_avg:38.99ms
step:1147/2330 train_time:44706ms step_avg:38.98ms
step:1148/2330 train_time:44763ms step_avg:38.99ms
step:1149/2330 train_time:44786ms step_avg:38.98ms
step:1150/2330 train_time:44842ms step_avg:38.99ms
step:1151/2330 train_time:44864ms step_avg:38.98ms
step:1152/2330 train_time:44921ms step_avg:38.99ms
step:1153/2330 train_time:44943ms step_avg:38.98ms
step:1154/2330 train_time:45000ms step_avg:39.00ms
step:1155/2330 train_time:45022ms step_avg:38.98ms
step:1156/2330 train_time:45078ms step_avg:39.00ms
step:1157/2330 train_time:45100ms step_avg:38.98ms
step:1158/2330 train_time:45155ms step_avg:38.99ms
step:1159/2330 train_time:45177ms step_avg:38.98ms
step:1160/2330 train_time:45234ms step_avg:38.99ms
step:1161/2330 train_time:45257ms step_avg:38.98ms
step:1162/2330 train_time:45312ms step_avg:39.00ms
step:1163/2330 train_time:45335ms step_avg:38.98ms
step:1164/2330 train_time:45392ms step_avg:39.00ms
step:1165/2330 train_time:45415ms step_avg:38.98ms
step:1166/2330 train_time:45470ms step_avg:39.00ms
step:1167/2330 train_time:45493ms step_avg:38.98ms
step:1168/2330 train_time:45549ms step_avg:39.00ms
step:1169/2330 train_time:45571ms step_avg:38.98ms
step:1170/2330 train_time:45627ms step_avg:39.00ms
step:1171/2330 train_time:45650ms step_avg:38.98ms
step:1172/2330 train_time:45707ms step_avg:39.00ms
step:1173/2330 train_time:45729ms step_avg:38.99ms
step:1174/2330 train_time:45785ms step_avg:39.00ms
step:1175/2330 train_time:45808ms step_avg:38.99ms
step:1176/2330 train_time:45863ms step_avg:39.00ms
step:1177/2330 train_time:45886ms step_avg:38.99ms
step:1178/2330 train_time:45942ms step_avg:39.00ms
step:1179/2330 train_time:45964ms step_avg:38.99ms
step:1180/2330 train_time:46020ms step_avg:39.00ms
step:1181/2330 train_time:46043ms step_avg:38.99ms
step:1182/2330 train_time:46099ms step_avg:39.00ms
step:1183/2330 train_time:46121ms step_avg:38.99ms
step:1184/2330 train_time:46176ms step_avg:39.00ms
step:1185/2330 train_time:46198ms step_avg:38.99ms
step:1186/2330 train_time:46254ms step_avg:39.00ms
step:1187/2330 train_time:46276ms step_avg:38.99ms
step:1188/2330 train_time:46331ms step_avg:39.00ms
step:1189/2330 train_time:46354ms step_avg:38.99ms
step:1190/2330 train_time:46410ms step_avg:39.00ms
step:1191/2330 train_time:46432ms step_avg:38.99ms
step:1192/2330 train_time:46489ms step_avg:39.00ms
step:1193/2330 train_time:46511ms step_avg:38.99ms
step:1194/2330 train_time:46567ms step_avg:39.00ms
step:1195/2330 train_time:46589ms step_avg:38.99ms
step:1196/2330 train_time:46645ms step_avg:39.00ms
step:1197/2330 train_time:46668ms step_avg:38.99ms
step:1198/2330 train_time:46725ms step_avg:39.00ms
step:1199/2330 train_time:46747ms step_avg:38.99ms
step:1200/2330 train_time:46803ms step_avg:39.00ms
step:1201/2330 train_time:46826ms step_avg:38.99ms
step:1202/2330 train_time:46882ms step_avg:39.00ms
step:1203/2330 train_time:46904ms step_avg:38.99ms
step:1204/2330 train_time:46959ms step_avg:39.00ms
step:1205/2330 train_time:46981ms step_avg:38.99ms
step:1206/2330 train_time:47037ms step_avg:39.00ms
step:1207/2330 train_time:47059ms step_avg:38.99ms
step:1208/2330 train_time:47114ms step_avg:39.00ms
step:1209/2330 train_time:47136ms step_avg:38.99ms
step:1210/2330 train_time:47192ms step_avg:39.00ms
step:1211/2330 train_time:47214ms step_avg:38.99ms
step:1212/2330 train_time:47270ms step_avg:39.00ms
step:1213/2330 train_time:47292ms step_avg:38.99ms
step:1214/2330 train_time:47348ms step_avg:39.00ms
step:1215/2330 train_time:47369ms step_avg:38.99ms
step:1216/2330 train_time:47426ms step_avg:39.00ms
step:1217/2330 train_time:47448ms step_avg:38.99ms
step:1218/2330 train_time:47504ms step_avg:39.00ms
step:1219/2330 train_time:47526ms step_avg:38.99ms
step:1220/2330 train_time:47582ms step_avg:39.00ms
step:1221/2330 train_time:47604ms step_avg:38.99ms
step:1222/2330 train_time:47660ms step_avg:39.00ms
step:1223/2330 train_time:47682ms step_avg:38.99ms
step:1224/2330 train_time:47738ms step_avg:39.00ms
step:1225/2330 train_time:47761ms step_avg:38.99ms
step:1226/2330 train_time:47817ms step_avg:39.00ms
step:1227/2330 train_time:47839ms step_avg:38.99ms
step:1228/2330 train_time:47895ms step_avg:39.00ms
step:1229/2330 train_time:47917ms step_avg:38.99ms
step:1230/2330 train_time:47972ms step_avg:39.00ms
step:1231/2330 train_time:47995ms step_avg:38.99ms
step:1232/2330 train_time:48050ms step_avg:39.00ms
step:1233/2330 train_time:48072ms step_avg:38.99ms
step:1234/2330 train_time:48128ms step_avg:39.00ms
step:1235/2330 train_time:48151ms step_avg:38.99ms
step:1236/2330 train_time:48206ms step_avg:39.00ms
step:1237/2330 train_time:48229ms step_avg:38.99ms
step:1238/2330 train_time:48284ms step_avg:39.00ms
step:1239/2330 train_time:48306ms step_avg:38.99ms
step:1240/2330 train_time:48363ms step_avg:39.00ms
step:1241/2330 train_time:48385ms step_avg:38.99ms
step:1242/2330 train_time:48441ms step_avg:39.00ms
step:1243/2330 train_time:48463ms step_avg:38.99ms
step:1244/2330 train_time:48518ms step_avg:39.00ms
step:1245/2330 train_time:48541ms step_avg:38.99ms
step:1246/2330 train_time:48597ms step_avg:39.00ms
step:1247/2330 train_time:48620ms step_avg:38.99ms
step:1248/2330 train_time:48676ms step_avg:39.00ms
step:1249/2330 train_time:48699ms step_avg:38.99ms
step:1250/2330 train_time:48755ms step_avg:39.00ms
step:1250/2330 val_loss:5.3688 train_time:48852ms step_avg:39.08ms
step:1251/2330 train_time:48864ms step_avg:39.06ms
step:1252/2330 train_time:48876ms step_avg:39.04ms
step:1253/2330 train_time:48886ms step_avg:39.02ms
step:1254/2330 train_time:48914ms step_avg:39.01ms
step:1255/2330 train_time:48935ms step_avg:38.99ms
step:1256/2330 train_time:48990ms step_avg:39.00ms
step:1257/2330 train_time:49011ms step_avg:38.99ms
step:1258/2330 train_time:49066ms step_avg:39.00ms
step:1259/2330 train_time:49088ms step_avg:38.99ms
step:1260/2330 train_time:49143ms step_avg:39.00ms
step:1261/2330 train_time:49169ms step_avg:38.99ms
step:1262/2330 train_time:49228ms step_avg:39.01ms
step:1263/2330 train_time:49252ms step_avg:39.00ms
step:1264/2330 train_time:49309ms step_avg:39.01ms
step:1265/2330 train_time:49331ms step_avg:39.00ms
step:1266/2330 train_time:49387ms step_avg:39.01ms
step:1267/2330 train_time:49409ms step_avg:39.00ms
step:1268/2330 train_time:49464ms step_avg:39.01ms
step:1269/2330 train_time:49486ms step_avg:39.00ms
step:1270/2330 train_time:49542ms step_avg:39.01ms
step:1271/2330 train_time:49564ms step_avg:39.00ms
step:1272/2330 train_time:49618ms step_avg:39.01ms
step:1273/2330 train_time:49641ms step_avg:38.99ms
step:1274/2330 train_time:49696ms step_avg:39.01ms
step:1275/2330 train_time:49718ms step_avg:38.99ms
step:1276/2330 train_time:49773ms step_avg:39.01ms
step:1277/2330 train_time:49795ms step_avg:38.99ms
step:1278/2330 train_time:49851ms step_avg:39.01ms
step:1279/2330 train_time:49873ms step_avg:38.99ms
step:1280/2330 train_time:49928ms step_avg:39.01ms
step:1281/2330 train_time:49949ms step_avg:38.99ms
step:1282/2330 train_time:50004ms step_avg:39.00ms
step:1283/2330 train_time:50027ms step_avg:38.99ms
step:1284/2330 train_time:50082ms step_avg:39.00ms
step:1285/2330 train_time:50105ms step_avg:38.99ms
step:1286/2330 train_time:50161ms step_avg:39.01ms
step:1287/2330 train_time:50184ms step_avg:38.99ms
step:1288/2330 train_time:50241ms step_avg:39.01ms
step:1289/2330 train_time:50264ms step_avg:38.99ms
step:1290/2330 train_time:50320ms step_avg:39.01ms
step:1291/2330 train_time:50343ms step_avg:39.00ms
step:1292/2330 train_time:50399ms step_avg:39.01ms
step:1293/2330 train_time:50421ms step_avg:39.00ms
step:1294/2330 train_time:50476ms step_avg:39.01ms
step:1295/2330 train_time:50499ms step_avg:39.00ms
step:1296/2330 train_time:50555ms step_avg:39.01ms
step:1297/2330 train_time:50577ms step_avg:39.00ms
step:1298/2330 train_time:50633ms step_avg:39.01ms
step:1299/2330 train_time:50654ms step_avg:38.99ms
step:1300/2330 train_time:50710ms step_avg:39.01ms
step:1301/2330 train_time:50732ms step_avg:38.99ms
step:1302/2330 train_time:50787ms step_avg:39.01ms
step:1303/2330 train_time:50808ms step_avg:38.99ms
step:1304/2330 train_time:50863ms step_avg:39.01ms
step:1305/2330 train_time:50886ms step_avg:38.99ms
step:1306/2330 train_time:50941ms step_avg:39.01ms
step:1307/2330 train_time:50963ms step_avg:38.99ms
step:1308/2330 train_time:51019ms step_avg:39.01ms
step:1309/2330 train_time:51041ms step_avg:38.99ms
step:1310/2330 train_time:51097ms step_avg:39.01ms
step:1311/2330 train_time:51119ms step_avg:38.99ms
step:1312/2330 train_time:51176ms step_avg:39.01ms
step:1313/2330 train_time:51198ms step_avg:38.99ms
step:1314/2330 train_time:51255ms step_avg:39.01ms
step:1315/2330 train_time:51278ms step_avg:38.99ms
step:1316/2330 train_time:51334ms step_avg:39.01ms
step:1317/2330 train_time:51356ms step_avg:38.99ms
step:1318/2330 train_time:51412ms step_avg:39.01ms
step:1319/2330 train_time:51435ms step_avg:39.00ms
step:1320/2330 train_time:51491ms step_avg:39.01ms
step:1321/2330 train_time:51512ms step_avg:39.00ms
step:1322/2330 train_time:51568ms step_avg:39.01ms
step:1323/2330 train_time:51590ms step_avg:38.99ms
step:1324/2330 train_time:51645ms step_avg:39.01ms
step:1325/2330 train_time:51667ms step_avg:38.99ms
step:1326/2330 train_time:51723ms step_avg:39.01ms
step:1327/2330 train_time:51745ms step_avg:38.99ms
step:1328/2330 train_time:51800ms step_avg:39.01ms
step:1329/2330 train_time:51822ms step_avg:38.99ms
step:1330/2330 train_time:51878ms step_avg:39.01ms
step:1331/2330 train_time:51900ms step_avg:38.99ms
step:1332/2330 train_time:51956ms step_avg:39.01ms
step:1333/2330 train_time:51977ms step_avg:38.99ms
step:1334/2330 train_time:52033ms step_avg:39.01ms
step:1335/2330 train_time:52055ms step_avg:38.99ms
step:1336/2330 train_time:52112ms step_avg:39.01ms
step:1337/2330 train_time:52134ms step_avg:38.99ms
step:1338/2330 train_time:52191ms step_avg:39.01ms
step:1339/2330 train_time:52213ms step_avg:38.99ms
step:1340/2330 train_time:52269ms step_avg:39.01ms
step:1341/2330 train_time:52291ms step_avg:38.99ms
step:1342/2330 train_time:52346ms step_avg:39.01ms
step:1343/2330 train_time:52368ms step_avg:38.99ms
step:1344/2330 train_time:52424ms step_avg:39.01ms
step:1345/2330 train_time:52447ms step_avg:38.99ms
step:1346/2330 train_time:52503ms step_avg:39.01ms
step:1347/2330 train_time:52526ms step_avg:38.99ms
step:1348/2330 train_time:52581ms step_avg:39.01ms
step:1349/2330 train_time:52603ms step_avg:38.99ms
step:1350/2330 train_time:52659ms step_avg:39.01ms
step:1351/2330 train_time:52681ms step_avg:38.99ms
step:1352/2330 train_time:52736ms step_avg:39.01ms
step:1353/2330 train_time:52759ms step_avg:38.99ms
step:1354/2330 train_time:52814ms step_avg:39.01ms
step:1355/2330 train_time:52837ms step_avg:38.99ms
step:1356/2330 train_time:52892ms step_avg:39.01ms
step:1357/2330 train_time:52914ms step_avg:38.99ms
step:1358/2330 train_time:52970ms step_avg:39.01ms
step:1359/2330 train_time:52991ms step_avg:38.99ms
step:1360/2330 train_time:53047ms step_avg:39.00ms
step:1361/2330 train_time:53069ms step_avg:38.99ms
step:1362/2330 train_time:53124ms step_avg:39.00ms
step:1363/2330 train_time:53147ms step_avg:38.99ms
step:1364/2330 train_time:53202ms step_avg:39.00ms
step:1365/2330 train_time:53225ms step_avg:38.99ms
step:1366/2330 train_time:53281ms step_avg:39.00ms
step:1367/2330 train_time:53304ms step_avg:38.99ms
step:1368/2330 train_time:53359ms step_avg:39.01ms
step:1369/2330 train_time:53382ms step_avg:38.99ms
step:1370/2330 train_time:53437ms step_avg:39.01ms
step:1371/2330 train_time:53460ms step_avg:38.99ms
step:1372/2330 train_time:53515ms step_avg:39.01ms
step:1373/2330 train_time:53539ms step_avg:38.99ms
step:1374/2330 train_time:53595ms step_avg:39.01ms
step:1375/2330 train_time:53617ms step_avg:38.99ms
step:1376/2330 train_time:53672ms step_avg:39.01ms
step:1377/2330 train_time:53694ms step_avg:38.99ms
step:1378/2330 train_time:53749ms step_avg:39.01ms
step:1379/2330 train_time:53772ms step_avg:38.99ms
step:1380/2330 train_time:53827ms step_avg:39.01ms
step:1381/2330 train_time:53849ms step_avg:38.99ms
step:1382/2330 train_time:53904ms step_avg:39.00ms
step:1383/2330 train_time:53926ms step_avg:38.99ms
step:1384/2330 train_time:53982ms step_avg:39.00ms
step:1385/2330 train_time:54005ms step_avg:38.99ms
step:1386/2330 train_time:54061ms step_avg:39.00ms
step:1387/2330 train_time:54083ms step_avg:38.99ms
step:1388/2330 train_time:54139ms step_avg:39.00ms
step:1389/2330 train_time:54161ms step_avg:38.99ms
step:1390/2330 train_time:54217ms step_avg:39.00ms
step:1391/2330 train_time:54240ms step_avg:38.99ms
step:1392/2330 train_time:54295ms step_avg:39.01ms
step:1393/2330 train_time:54317ms step_avg:38.99ms
step:1394/2330 train_time:54374ms step_avg:39.01ms
step:1395/2330 train_time:54395ms step_avg:38.99ms
step:1396/2330 train_time:54452ms step_avg:39.01ms
step:1397/2330 train_time:54474ms step_avg:38.99ms
step:1398/2330 train_time:54529ms step_avg:39.01ms
step:1399/2330 train_time:54551ms step_avg:38.99ms
step:1400/2330 train_time:54607ms step_avg:39.01ms
step:1401/2330 train_time:54629ms step_avg:38.99ms
step:1402/2330 train_time:54684ms step_avg:39.00ms
step:1403/2330 train_time:54707ms step_avg:38.99ms
step:1404/2330 train_time:54762ms step_avg:39.00ms
step:1405/2330 train_time:54784ms step_avg:38.99ms
step:1406/2330 train_time:54840ms step_avg:39.00ms
step:1407/2330 train_time:54862ms step_avg:38.99ms
step:1408/2330 train_time:54918ms step_avg:39.00ms
step:1409/2330 train_time:54940ms step_avg:38.99ms
step:1410/2330 train_time:54995ms step_avg:39.00ms
step:1411/2330 train_time:55017ms step_avg:38.99ms
step:1412/2330 train_time:55073ms step_avg:39.00ms
step:1413/2330 train_time:55095ms step_avg:38.99ms
step:1414/2330 train_time:55151ms step_avg:39.00ms
step:1415/2330 train_time:55173ms step_avg:38.99ms
step:1416/2330 train_time:55229ms step_avg:39.00ms
step:1417/2330 train_time:55251ms step_avg:38.99ms
step:1418/2330 train_time:55306ms step_avg:39.00ms
step:1419/2330 train_time:55328ms step_avg:38.99ms
step:1420/2330 train_time:55384ms step_avg:39.00ms
step:1421/2330 train_time:55407ms step_avg:38.99ms
step:1422/2330 train_time:55463ms step_avg:39.00ms
step:1423/2330 train_time:55486ms step_avg:38.99ms
step:1424/2330 train_time:55542ms step_avg:39.00ms
step:1425/2330 train_time:55564ms step_avg:38.99ms
step:1426/2330 train_time:55619ms step_avg:39.00ms
step:1427/2330 train_time:55642ms step_avg:38.99ms
step:1428/2330 train_time:55697ms step_avg:39.00ms
step:1429/2330 train_time:55719ms step_avg:38.99ms
step:1430/2330 train_time:55775ms step_avg:39.00ms
step:1431/2330 train_time:55797ms step_avg:38.99ms
step:1432/2330 train_time:55853ms step_avg:39.00ms
step:1433/2330 train_time:55874ms step_avg:38.99ms
step:1434/2330 train_time:55930ms step_avg:39.00ms
step:1435/2330 train_time:55952ms step_avg:38.99ms
step:1436/2330 train_time:56008ms step_avg:39.00ms
step:1437/2330 train_time:56030ms step_avg:38.99ms
step:1438/2330 train_time:56086ms step_avg:39.00ms
step:1439/2330 train_time:56108ms step_avg:38.99ms
step:1440/2330 train_time:56163ms step_avg:39.00ms
step:1441/2330 train_time:56186ms step_avg:38.99ms
step:1442/2330 train_time:56241ms step_avg:39.00ms
step:1443/2330 train_time:56264ms step_avg:38.99ms
step:1444/2330 train_time:56320ms step_avg:39.00ms
step:1445/2330 train_time:56342ms step_avg:38.99ms
step:1446/2330 train_time:56398ms step_avg:39.00ms
step:1447/2330 train_time:56421ms step_avg:38.99ms
step:1448/2330 train_time:56476ms step_avg:39.00ms
step:1449/2330 train_time:56498ms step_avg:38.99ms
step:1450/2330 train_time:56555ms step_avg:39.00ms
step:1451/2330 train_time:56577ms step_avg:38.99ms
step:1452/2330 train_time:56633ms step_avg:39.00ms
step:1453/2330 train_time:56655ms step_avg:38.99ms
step:1454/2330 train_time:56711ms step_avg:39.00ms
step:1455/2330 train_time:56733ms step_avg:38.99ms
step:1456/2330 train_time:56790ms step_avg:39.00ms
step:1457/2330 train_time:56812ms step_avg:38.99ms
step:1458/2330 train_time:56867ms step_avg:39.00ms
step:1459/2330 train_time:56889ms step_avg:38.99ms
step:1460/2330 train_time:56944ms step_avg:39.00ms
step:1461/2330 train_time:56967ms step_avg:38.99ms
step:1462/2330 train_time:57022ms step_avg:39.00ms
step:1463/2330 train_time:57045ms step_avg:38.99ms
step:1464/2330 train_time:57100ms step_avg:39.00ms
step:1465/2330 train_time:57122ms step_avg:38.99ms
step:1466/2330 train_time:57178ms step_avg:39.00ms
step:1467/2330 train_time:57200ms step_avg:38.99ms
step:1468/2330 train_time:57255ms step_avg:39.00ms
step:1469/2330 train_time:57277ms step_avg:38.99ms
step:1470/2330 train_time:57333ms step_avg:39.00ms
step:1471/2330 train_time:57355ms step_avg:38.99ms
step:1472/2330 train_time:57411ms step_avg:39.00ms
step:1473/2330 train_time:57433ms step_avg:38.99ms
step:1474/2330 train_time:57489ms step_avg:39.00ms
step:1475/2330 train_time:57511ms step_avg:38.99ms
step:1476/2330 train_time:57567ms step_avg:39.00ms
step:1477/2330 train_time:57589ms step_avg:38.99ms
step:1478/2330 train_time:57644ms step_avg:39.00ms
step:1479/2330 train_time:57666ms step_avg:38.99ms
step:1480/2330 train_time:57722ms step_avg:39.00ms
step:1481/2330 train_time:57744ms step_avg:38.99ms
step:1482/2330 train_time:57800ms step_avg:39.00ms
step:1483/2330 train_time:57823ms step_avg:38.99ms
step:1484/2330 train_time:57878ms step_avg:39.00ms
step:1485/2330 train_time:57900ms step_avg:38.99ms
step:1486/2330 train_time:57956ms step_avg:39.00ms
step:1487/2330 train_time:57979ms step_avg:38.99ms
step:1488/2330 train_time:58034ms step_avg:39.00ms
step:1489/2330 train_time:58056ms step_avg:38.99ms
step:1490/2330 train_time:58112ms step_avg:39.00ms
step:1491/2330 train_time:58134ms step_avg:38.99ms
step:1492/2330 train_time:58190ms step_avg:39.00ms
step:1493/2330 train_time:58211ms step_avg:38.99ms
step:1494/2330 train_time:58268ms step_avg:39.00ms
step:1495/2330 train_time:58290ms step_avg:38.99ms
step:1496/2330 train_time:58346ms step_avg:39.00ms
step:1497/2330 train_time:58368ms step_avg:38.99ms
step:1498/2330 train_time:58424ms step_avg:39.00ms
step:1499/2330 train_time:58447ms step_avg:38.99ms
step:1500/2330 train_time:58502ms step_avg:39.00ms
step:1500/2330 val_loss:5.3468 train_time:58598ms step_avg:39.07ms
step:1501/2330 train_time:58611ms step_avg:39.05ms
step:1502/2330 train_time:58623ms step_avg:39.03ms
step:1503/2330 train_time:58634ms step_avg:39.01ms
step:1504/2330 train_time:58662ms step_avg:39.00ms
step:1505/2330 train_time:58683ms step_avg:38.99ms
step:1506/2330 train_time:58737ms step_avg:39.00ms
step:1507/2330 train_time:58759ms step_avg:38.99ms
step:1508/2330 train_time:58814ms step_avg:39.00ms
step:1509/2330 train_time:58836ms step_avg:38.99ms
step:1510/2330 train_time:58891ms step_avg:39.00ms
step:1511/2330 train_time:58916ms step_avg:38.99ms
step:1512/2330 train_time:58975ms step_avg:39.00ms
step:1513/2330 train_time:58998ms step_avg:38.99ms
step:1514/2330 train_time:59054ms step_avg:39.01ms
step:1515/2330 train_time:59076ms step_avg:38.99ms
step:1516/2330 train_time:59131ms step_avg:39.00ms
step:1517/2330 train_time:59153ms step_avg:38.99ms
step:1518/2330 train_time:59208ms step_avg:39.00ms
step:1519/2330 train_time:59230ms step_avg:38.99ms
step:1520/2330 train_time:59285ms step_avg:39.00ms
step:1521/2330 train_time:59307ms step_avg:38.99ms
step:1522/2330 train_time:59363ms step_avg:39.00ms
step:1523/2330 train_time:59385ms step_avg:38.99ms
step:1524/2330 train_time:59440ms step_avg:39.00ms
step:1525/2330 train_time:59462ms step_avg:38.99ms
step:1526/2330 train_time:59520ms step_avg:39.00ms
step:1527/2330 train_time:59545ms step_avg:38.99ms
step:1528/2330 train_time:59603ms step_avg:39.01ms
step:1529/2330 train_time:59627ms step_avg:39.00ms
step:1530/2330 train_time:59683ms step_avg:39.01ms
step:1531/2330 train_time:59704ms step_avg:39.00ms
step:1532/2330 train_time:59759ms step_avg:39.01ms
step:1533/2330 train_time:59782ms step_avg:39.00ms
step:1534/2330 train_time:59838ms step_avg:39.01ms
step:1535/2330 train_time:59862ms step_avg:39.00ms
step:1536/2330 train_time:59918ms step_avg:39.01ms
step:1537/2330 train_time:59941ms step_avg:39.00ms
step:1538/2330 train_time:59997ms step_avg:39.01ms
step:1539/2330 train_time:60019ms step_avg:39.00ms
step:1540/2330 train_time:60075ms step_avg:39.01ms
step:1541/2330 train_time:60096ms step_avg:39.00ms
step:1542/2330 train_time:60152ms step_avg:39.01ms
step:1543/2330 train_time:60173ms step_avg:39.00ms
step:1544/2330 train_time:60228ms step_avg:39.01ms
step:1545/2330 train_time:60250ms step_avg:39.00ms
step:1546/2330 train_time:60306ms step_avg:39.01ms
step:1547/2330 train_time:60329ms step_avg:39.00ms
step:1548/2330 train_time:60384ms step_avg:39.01ms
step:1549/2330 train_time:60406ms step_avg:39.00ms
step:1550/2330 train_time:60462ms step_avg:39.01ms
step:1551/2330 train_time:60485ms step_avg:39.00ms
step:1552/2330 train_time:60541ms step_avg:39.01ms
step:1553/2330 train_time:60563ms step_avg:39.00ms
step:1554/2330 train_time:60620ms step_avg:39.01ms
step:1555/2330 train_time:60642ms step_avg:39.00ms
step:1556/2330 train_time:60697ms step_avg:39.01ms
step:1557/2330 train_time:60719ms step_avg:39.00ms
step:1558/2330 train_time:60775ms step_avg:39.01ms
step:1559/2330 train_time:60798ms step_avg:39.00ms
step:1560/2330 train_time:60854ms step_avg:39.01ms
step:1561/2330 train_time:60876ms step_avg:39.00ms
step:1562/2330 train_time:60932ms step_avg:39.01ms
step:1563/2330 train_time:60955ms step_avg:39.00ms
step:1564/2330 train_time:61010ms step_avg:39.01ms
step:1565/2330 train_time:61032ms step_avg:39.00ms
step:1566/2330 train_time:61088ms step_avg:39.01ms
step:1567/2330 train_time:61110ms step_avg:39.00ms
step:1568/2330 train_time:61165ms step_avg:39.01ms
step:1569/2330 train_time:61187ms step_avg:39.00ms
step:1570/2330 train_time:61243ms step_avg:39.01ms
step:1571/2330 train_time:61265ms step_avg:39.00ms
step:1572/2330 train_time:61320ms step_avg:39.01ms
step:1573/2330 train_time:61342ms step_avg:39.00ms
step:1574/2330 train_time:61397ms step_avg:39.01ms
step:1575/2330 train_time:61419ms step_avg:39.00ms
step:1576/2330 train_time:61475ms step_avg:39.01ms
step:1577/2330 train_time:61498ms step_avg:39.00ms
step:1578/2330 train_time:61554ms step_avg:39.01ms
step:1579/2330 train_time:61576ms step_avg:39.00ms
step:1580/2330 train_time:61632ms step_avg:39.01ms
step:1581/2330 train_time:61653ms step_avg:39.00ms
step:1582/2330 train_time:61709ms step_avg:39.01ms
step:1583/2330 train_time:61732ms step_avg:39.00ms
step:1584/2330 train_time:61788ms step_avg:39.01ms
step:1585/2330 train_time:61810ms step_avg:39.00ms
step:1586/2330 train_time:61866ms step_avg:39.01ms
step:1587/2330 train_time:61889ms step_avg:39.00ms
step:1588/2330 train_time:61944ms step_avg:39.01ms
step:1589/2330 train_time:61967ms step_avg:39.00ms
step:1590/2330 train_time:62023ms step_avg:39.01ms
step:1591/2330 train_time:62045ms step_avg:39.00ms
step:1592/2330 train_time:62101ms step_avg:39.01ms
step:1593/2330 train_time:62123ms step_avg:39.00ms
step:1594/2330 train_time:62178ms step_avg:39.01ms
step:1595/2330 train_time:62200ms step_avg:39.00ms
step:1596/2330 train_time:62256ms step_avg:39.01ms
step:1597/2330 train_time:62278ms step_avg:39.00ms
step:1598/2330 train_time:62333ms step_avg:39.01ms
step:1599/2330 train_time:62355ms step_avg:39.00ms
step:1600/2330 train_time:62411ms step_avg:39.01ms
step:1601/2330 train_time:62434ms step_avg:39.00ms
step:1602/2330 train_time:62490ms step_avg:39.01ms
step:1603/2330 train_time:62512ms step_avg:39.00ms
step:1604/2330 train_time:62567ms step_avg:39.01ms
step:1605/2330 train_time:62590ms step_avg:39.00ms
step:1606/2330 train_time:62645ms step_avg:39.01ms
step:1607/2330 train_time:62668ms step_avg:39.00ms
step:1608/2330 train_time:62724ms step_avg:39.01ms
step:1609/2330 train_time:62746ms step_avg:39.00ms
step:1610/2330 train_time:62802ms step_avg:39.01ms
step:1611/2330 train_time:62824ms step_avg:39.00ms
step:1612/2330 train_time:62881ms step_avg:39.01ms
step:1613/2330 train_time:62903ms step_avg:39.00ms
step:1614/2330 train_time:62959ms step_avg:39.01ms
step:1615/2330 train_time:62981ms step_avg:39.00ms
step:1616/2330 train_time:63037ms step_avg:39.01ms
step:1617/2330 train_time:63059ms step_avg:39.00ms
step:1618/2330 train_time:63115ms step_avg:39.01ms
step:1619/2330 train_time:63137ms step_avg:39.00ms
step:1620/2330 train_time:63192ms step_avg:39.01ms
step:1621/2330 train_time:63214ms step_avg:39.00ms
step:1622/2330 train_time:63269ms step_avg:39.01ms
step:1623/2330 train_time:63291ms step_avg:39.00ms
step:1624/2330 train_time:63346ms step_avg:39.01ms
step:1625/2330 train_time:63369ms step_avg:39.00ms
step:1626/2330 train_time:63425ms step_avg:39.01ms
step:1627/2330 train_time:63447ms step_avg:39.00ms
step:1628/2330 train_time:63504ms step_avg:39.01ms
step:1629/2330 train_time:63526ms step_avg:39.00ms
step:1630/2330 train_time:63582ms step_avg:39.01ms
step:1631/2330 train_time:63604ms step_avg:39.00ms
step:1632/2330 train_time:63661ms step_avg:39.01ms
step:1633/2330 train_time:63683ms step_avg:39.00ms
step:1634/2330 train_time:63739ms step_avg:39.01ms
step:1635/2330 train_time:63761ms step_avg:39.00ms
step:1636/2330 train_time:63818ms step_avg:39.01ms
step:1637/2330 train_time:63839ms step_avg:39.00ms
step:1638/2330 train_time:63895ms step_avg:39.01ms
step:1639/2330 train_time:63917ms step_avg:39.00ms
step:1640/2330 train_time:63973ms step_avg:39.01ms
step:1641/2330 train_time:63995ms step_avg:39.00ms
step:1642/2330 train_time:64051ms step_avg:39.01ms
step:1643/2330 train_time:64073ms step_avg:39.00ms
step:1644/2330 train_time:64128ms step_avg:39.01ms
step:1645/2330 train_time:64150ms step_avg:39.00ms
step:1646/2330 train_time:64206ms step_avg:39.01ms
step:1647/2330 train_time:64228ms step_avg:39.00ms
step:1648/2330 train_time:64283ms step_avg:39.01ms
step:1649/2330 train_time:64305ms step_avg:39.00ms
step:1650/2330 train_time:64362ms step_avg:39.01ms
step:1651/2330 train_time:64384ms step_avg:39.00ms
step:1652/2330 train_time:64440ms step_avg:39.01ms
step:1653/2330 train_time:64462ms step_avg:39.00ms
step:1654/2330 train_time:64518ms step_avg:39.01ms
step:1655/2330 train_time:64541ms step_avg:39.00ms
step:1656/2330 train_time:64597ms step_avg:39.01ms
step:1657/2330 train_time:64618ms step_avg:39.00ms
step:1658/2330 train_time:64674ms step_avg:39.01ms
step:1659/2330 train_time:64696ms step_avg:39.00ms
step:1660/2330 train_time:64752ms step_avg:39.01ms
step:1661/2330 train_time:64773ms step_avg:39.00ms
step:1662/2330 train_time:64829ms step_avg:39.01ms
step:1663/2330 train_time:64851ms step_avg:39.00ms
step:1664/2330 train_time:64907ms step_avg:39.01ms
step:1665/2330 train_time:64930ms step_avg:39.00ms
step:1666/2330 train_time:64986ms step_avg:39.01ms
step:1667/2330 train_time:65008ms step_avg:39.00ms
step:1668/2330 train_time:65063ms step_avg:39.01ms
step:1669/2330 train_time:65086ms step_avg:39.00ms
step:1670/2330 train_time:65141ms step_avg:39.01ms
step:1671/2330 train_time:65163ms step_avg:39.00ms
step:1672/2330 train_time:65219ms step_avg:39.01ms
step:1673/2330 train_time:65241ms step_avg:39.00ms
step:1674/2330 train_time:65297ms step_avg:39.01ms
step:1675/2330 train_time:65319ms step_avg:39.00ms
step:1676/2330 train_time:65375ms step_avg:39.01ms
step:1677/2330 train_time:65397ms step_avg:39.00ms
step:1678/2330 train_time:65453ms step_avg:39.01ms
step:1679/2330 train_time:65475ms step_avg:39.00ms
step:1680/2330 train_time:65531ms step_avg:39.01ms
step:1681/2330 train_time:65553ms step_avg:39.00ms
step:1682/2330 train_time:65609ms step_avg:39.01ms
step:1683/2330 train_time:65633ms step_avg:39.00ms
step:1684/2330 train_time:65688ms step_avg:39.01ms
step:1685/2330 train_time:65711ms step_avg:39.00ms
step:1686/2330 train_time:65766ms step_avg:39.01ms
step:1687/2330 train_time:65789ms step_avg:39.00ms
step:1688/2330 train_time:65844ms step_avg:39.01ms
step:1689/2330 train_time:65866ms step_avg:39.00ms
step:1690/2330 train_time:65922ms step_avg:39.01ms
step:1691/2330 train_time:65945ms step_avg:39.00ms
step:1692/2330 train_time:66000ms step_avg:39.01ms
step:1693/2330 train_time:66023ms step_avg:39.00ms
step:1694/2330 train_time:66079ms step_avg:39.01ms
step:1695/2330 train_time:66102ms step_avg:39.00ms
step:1696/2330 train_time:66158ms step_avg:39.01ms
step:1697/2330 train_time:66180ms step_avg:39.00ms
step:1698/2330 train_time:66235ms step_avg:39.01ms
step:1699/2330 train_time:66257ms step_avg:39.00ms
step:1700/2330 train_time:66312ms step_avg:39.01ms
step:1701/2330 train_time:66334ms step_avg:39.00ms
step:1702/2330 train_time:66390ms step_avg:39.01ms
step:1703/2330 train_time:66412ms step_avg:39.00ms
step:1704/2330 train_time:66468ms step_avg:39.01ms
step:1705/2330 train_time:66490ms step_avg:39.00ms
step:1706/2330 train_time:66546ms step_avg:39.01ms
step:1707/2330 train_time:66568ms step_avg:39.00ms
step:1708/2330 train_time:66624ms step_avg:39.01ms
step:1709/2330 train_time:66646ms step_avg:39.00ms
step:1710/2330 train_time:66703ms step_avg:39.01ms
step:1711/2330 train_time:66725ms step_avg:39.00ms
step:1712/2330 train_time:66781ms step_avg:39.01ms
step:1713/2330 train_time:66803ms step_avg:39.00ms
step:1714/2330 train_time:66859ms step_avg:39.01ms
step:1715/2330 train_time:66881ms step_avg:39.00ms
step:1716/2330 train_time:66937ms step_avg:39.01ms
step:1717/2330 train_time:66958ms step_avg:39.00ms
step:1718/2330 train_time:67015ms step_avg:39.01ms
step:1719/2330 train_time:67037ms step_avg:39.00ms
step:1720/2330 train_time:67092ms step_avg:39.01ms
step:1721/2330 train_time:67114ms step_avg:39.00ms
step:1722/2330 train_time:67170ms step_avg:39.01ms
step:1723/2330 train_time:67192ms step_avg:39.00ms
step:1724/2330 train_time:67248ms step_avg:39.01ms
step:1725/2330 train_time:67270ms step_avg:39.00ms
step:1726/2330 train_time:67325ms step_avg:39.01ms
step:1727/2330 train_time:67348ms step_avg:39.00ms
step:1728/2330 train_time:67403ms step_avg:39.01ms
step:1729/2330 train_time:67426ms step_avg:39.00ms
step:1730/2330 train_time:67482ms step_avg:39.01ms
step:1731/2330 train_time:67505ms step_avg:39.00ms
step:1732/2330 train_time:67561ms step_avg:39.01ms
step:1733/2330 train_time:67582ms step_avg:39.00ms
step:1734/2330 train_time:67638ms step_avg:39.01ms
step:1735/2330 train_time:67660ms step_avg:39.00ms
step:1736/2330 train_time:67716ms step_avg:39.01ms
step:1737/2330 train_time:67738ms step_avg:39.00ms
step:1738/2330 train_time:67794ms step_avg:39.01ms
step:1739/2330 train_time:67816ms step_avg:39.00ms
step:1740/2330 train_time:67872ms step_avg:39.01ms
step:1741/2330 train_time:67894ms step_avg:39.00ms
step:1742/2330 train_time:67950ms step_avg:39.01ms
step:1743/2330 train_time:67972ms step_avg:39.00ms
step:1744/2330 train_time:68027ms step_avg:39.01ms
step:1745/2330 train_time:68050ms step_avg:39.00ms
step:1746/2330 train_time:68106ms step_avg:39.01ms
step:1747/2330 train_time:68128ms step_avg:39.00ms
step:1748/2330 train_time:68184ms step_avg:39.01ms
step:1749/2330 train_time:68206ms step_avg:39.00ms
step:1750/2330 train_time:68261ms step_avg:39.01ms
step:1750/2330 val_loss:5.3335 train_time:68357ms step_avg:39.06ms
step:1751/2330 train_time:68370ms step_avg:39.05ms
step:1752/2330 train_time:68382ms step_avg:39.03ms
step:1753/2330 train_time:68393ms step_avg:39.01ms
step:1754/2330 train_time:68419ms step_avg:39.01ms
step:1755/2330 train_time:68440ms step_avg:39.00ms
step:1756/2330 train_time:68495ms step_avg:39.01ms
step:1757/2330 train_time:68517ms step_avg:39.00ms
step:1758/2330 train_time:68572ms step_avg:39.01ms
step:1759/2330 train_time:68593ms step_avg:39.00ms
step:1760/2330 train_time:68648ms step_avg:39.00ms
step:1761/2330 train_time:68672ms step_avg:39.00ms
step:1762/2330 train_time:68733ms step_avg:39.01ms
step:1763/2330 train_time:68755ms step_avg:39.00ms
step:1764/2330 train_time:68812ms step_avg:39.01ms
step:1765/2330 train_time:68836ms step_avg:39.00ms
step:1766/2330 train_time:68892ms step_avg:39.01ms
step:1767/2330 train_time:68915ms step_avg:39.00ms
step:1768/2330 train_time:68970ms step_avg:39.01ms
step:1769/2330 train_time:68992ms step_avg:39.00ms
step:1770/2330 train_time:69048ms step_avg:39.01ms
step:1771/2330 train_time:69069ms step_avg:39.00ms
step:1772/2330 train_time:69124ms step_avg:39.01ms
step:1773/2330 train_time:69146ms step_avg:39.00ms
step:1774/2330 train_time:69200ms step_avg:39.01ms
step:1775/2330 train_time:69222ms step_avg:39.00ms
step:1776/2330 train_time:69279ms step_avg:39.01ms
step:1777/2330 train_time:69301ms step_avg:39.00ms
step:1778/2330 train_time:69358ms step_avg:39.01ms
step:1779/2330 train_time:69380ms step_avg:39.00ms
step:1780/2330 train_time:69435ms step_avg:39.01ms
step:1781/2330 train_time:69457ms step_avg:39.00ms
step:1782/2330 train_time:69513ms step_avg:39.01ms
step:1783/2330 train_time:69535ms step_avg:39.00ms
step:1784/2330 train_time:69591ms step_avg:39.01ms
step:1785/2330 train_time:69613ms step_avg:39.00ms
step:1786/2330 train_time:69670ms step_avg:39.01ms
step:1787/2330 train_time:69692ms step_avg:39.00ms
step:1788/2330 train_time:69751ms step_avg:39.01ms
step:1789/2330 train_time:69773ms step_avg:39.00ms
step:1790/2330 train_time:69829ms step_avg:39.01ms
step:1791/2330 train_time:69851ms step_avg:39.00ms
step:1792/2330 train_time:69908ms step_avg:39.01ms
step:1793/2330 train_time:69930ms step_avg:39.00ms
step:1794/2330 train_time:69986ms step_avg:39.01ms
step:1795/2330 train_time:70008ms step_avg:39.00ms
step:1796/2330 train_time:70063ms step_avg:39.01ms
step:1797/2330 train_time:70084ms step_avg:39.00ms
step:1798/2330 train_time:70139ms step_avg:39.01ms
step:1799/2330 train_time:70161ms step_avg:39.00ms
step:1800/2330 train_time:70218ms step_avg:39.01ms
step:1801/2330 train_time:70239ms step_avg:39.00ms
step:1802/2330 train_time:70295ms step_avg:39.01ms
step:1803/2330 train_time:70317ms step_avg:39.00ms
step:1804/2330 train_time:70373ms step_avg:39.01ms
step:1805/2330 train_time:70395ms step_avg:39.00ms
step:1806/2330 train_time:70450ms step_avg:39.01ms
step:1807/2330 train_time:70472ms step_avg:39.00ms
step:1808/2330 train_time:70528ms step_avg:39.01ms
step:1809/2330 train_time:70550ms step_avg:39.00ms
step:1810/2330 train_time:70606ms step_avg:39.01ms
step:1811/2330 train_time:70628ms step_avg:39.00ms
step:1812/2330 train_time:70685ms step_avg:39.01ms
step:1813/2330 train_time:70708ms step_avg:39.00ms
step:1814/2330 train_time:70764ms step_avg:39.01ms
step:1815/2330 train_time:70787ms step_avg:39.00ms
step:1816/2330 train_time:70844ms step_avg:39.01ms
step:1817/2330 train_time:70865ms step_avg:39.00ms
step:1818/2330 train_time:70921ms step_avg:39.01ms
step:1819/2330 train_time:70944ms step_avg:39.00ms
step:1820/2330 train_time:71000ms step_avg:39.01ms
step:1821/2330 train_time:71022ms step_avg:39.00ms
step:1822/2330 train_time:71077ms step_avg:39.01ms
step:1823/2330 train_time:71099ms step_avg:39.00ms
step:1824/2330 train_time:71154ms step_avg:39.01ms
step:1825/2330 train_time:71177ms step_avg:39.00ms
step:1826/2330 train_time:71233ms step_avg:39.01ms
step:1827/2330 train_time:71255ms step_avg:39.00ms
step:1828/2330 train_time:71311ms step_avg:39.01ms
step:1829/2330 train_time:71332ms step_avg:39.00ms
step:1830/2330 train_time:71388ms step_avg:39.01ms
step:1831/2330 train_time:71409ms step_avg:39.00ms
step:1832/2330 train_time:71465ms step_avg:39.01ms
step:1833/2330 train_time:71488ms step_avg:39.00ms
step:1834/2330 train_time:71544ms step_avg:39.01ms
step:1835/2330 train_time:71565ms step_avg:39.00ms
step:1836/2330 train_time:71621ms step_avg:39.01ms
step:1837/2330 train_time:71645ms step_avg:39.00ms
step:1838/2330 train_time:71701ms step_avg:39.01ms
step:1839/2330 train_time:71724ms step_avg:39.00ms
step:1840/2330 train_time:71780ms step_avg:39.01ms
step:1841/2330 train_time:71803ms step_avg:39.00ms
step:1842/2330 train_time:71859ms step_avg:39.01ms
step:1843/2330 train_time:71881ms step_avg:39.00ms
step:1844/2330 train_time:71937ms step_avg:39.01ms
step:1845/2330 train_time:71959ms step_avg:39.00ms
step:1846/2330 train_time:72015ms step_avg:39.01ms
step:1847/2330 train_time:72037ms step_avg:39.00ms
step:1848/2330 train_time:72093ms step_avg:39.01ms
step:1849/2330 train_time:72115ms step_avg:39.00ms
step:1850/2330 train_time:72171ms step_avg:39.01ms
step:1851/2330 train_time:72192ms step_avg:39.00ms
step:1852/2330 train_time:72249ms step_avg:39.01ms
step:1853/2330 train_time:72270ms step_avg:39.00ms
step:1854/2330 train_time:72326ms step_avg:39.01ms
step:1855/2330 train_time:72348ms step_avg:39.00ms
step:1856/2330 train_time:72404ms step_avg:39.01ms
step:1857/2330 train_time:72425ms step_avg:39.00ms
step:1858/2330 train_time:72481ms step_avg:39.01ms
step:1859/2330 train_time:72503ms step_avg:39.00ms
step:1860/2330 train_time:72559ms step_avg:39.01ms
step:1861/2330 train_time:72581ms step_avg:39.00ms
step:1862/2330 train_time:72637ms step_avg:39.01ms
step:1863/2330 train_time:72660ms step_avg:39.00ms
step:1864/2330 train_time:72717ms step_avg:39.01ms
step:1865/2330 train_time:72739ms step_avg:39.00ms
step:1866/2330 train_time:72795ms step_avg:39.01ms
step:1867/2330 train_time:72817ms step_avg:39.00ms
step:1868/2330 train_time:72873ms step_avg:39.01ms
step:1869/2330 train_time:72896ms step_avg:39.00ms
step:1870/2330 train_time:72953ms step_avg:39.01ms
step:1871/2330 train_time:72975ms step_avg:39.00ms
step:1872/2330 train_time:73030ms step_avg:39.01ms
step:1873/2330 train_time:73052ms step_avg:39.00ms
step:1874/2330 train_time:73107ms step_avg:39.01ms
step:1875/2330 train_time:73129ms step_avg:39.00ms
step:1876/2330 train_time:73185ms step_avg:39.01ms
step:1877/2330 train_time:73207ms step_avg:39.00ms
step:1878/2330 train_time:73262ms step_avg:39.01ms
step:1879/2330 train_time:73284ms step_avg:39.00ms
step:1880/2330 train_time:73340ms step_avg:39.01ms
step:1881/2330 train_time:73362ms step_avg:39.00ms
step:1882/2330 train_time:73418ms step_avg:39.01ms
step:1883/2330 train_time:73440ms step_avg:39.00ms
step:1884/2330 train_time:73496ms step_avg:39.01ms
step:1885/2330 train_time:73519ms step_avg:39.00ms
step:1886/2330 train_time:73574ms step_avg:39.01ms
step:1887/2330 train_time:73597ms step_avg:39.00ms
step:1888/2330 train_time:73653ms step_avg:39.01ms
step:1889/2330 train_time:73675ms step_avg:39.00ms
step:1890/2330 train_time:73731ms step_avg:39.01ms
step:1891/2330 train_time:73753ms step_avg:39.00ms
step:1892/2330 train_time:73810ms step_avg:39.01ms
step:1893/2330 train_time:73832ms step_avg:39.00ms
step:1894/2330 train_time:73887ms step_avg:39.01ms
step:1895/2330 train_time:73910ms step_avg:39.00ms
step:1896/2330 train_time:73966ms step_avg:39.01ms
step:1897/2330 train_time:73988ms step_avg:39.00ms
step:1898/2330 train_time:74043ms step_avg:39.01ms
step:1899/2330 train_time:74066ms step_avg:39.00ms
step:1900/2330 train_time:74121ms step_avg:39.01ms
step:1901/2330 train_time:74144ms step_avg:39.00ms
step:1902/2330 train_time:74200ms step_avg:39.01ms
step:1903/2330 train_time:74223ms step_avg:39.00ms
step:1904/2330 train_time:74278ms step_avg:39.01ms
step:1905/2330 train_time:74300ms step_avg:39.00ms
step:1906/2330 train_time:74356ms step_avg:39.01ms
step:1907/2330 train_time:74378ms step_avg:39.00ms
step:1908/2330 train_time:74434ms step_avg:39.01ms
step:1909/2330 train_time:74455ms step_avg:39.00ms
step:1910/2330 train_time:74511ms step_avg:39.01ms
step:1911/2330 train_time:74533ms step_avg:39.00ms
step:1912/2330 train_time:74589ms step_avg:39.01ms
step:1913/2330 train_time:74611ms step_avg:39.00ms
step:1914/2330 train_time:74667ms step_avg:39.01ms
step:1915/2330 train_time:74689ms step_avg:39.00ms
step:1916/2330 train_time:74745ms step_avg:39.01ms
step:1917/2330 train_time:74767ms step_avg:39.00ms
step:1918/2330 train_time:74823ms step_avg:39.01ms
step:1919/2330 train_time:74845ms step_avg:39.00ms
step:1920/2330 train_time:74901ms step_avg:39.01ms
step:1921/2330 train_time:74924ms step_avg:39.00ms
step:1922/2330 train_time:74980ms step_avg:39.01ms
step:1923/2330 train_time:75003ms step_avg:39.00ms
step:1924/2330 train_time:75058ms step_avg:39.01ms
step:1925/2330 train_time:75081ms step_avg:39.00ms
step:1926/2330 train_time:75136ms step_avg:39.01ms
step:1927/2330 train_time:75158ms step_avg:39.00ms
step:1928/2330 train_time:75214ms step_avg:39.01ms
step:1929/2330 train_time:75236ms step_avg:39.00ms
step:1930/2330 train_time:75291ms step_avg:39.01ms
step:1931/2330 train_time:75314ms step_avg:39.00ms
step:1932/2330 train_time:75369ms step_avg:39.01ms
step:1933/2330 train_time:75391ms step_avg:39.00ms
step:1934/2330 train_time:75447ms step_avg:39.01ms
step:1935/2330 train_time:75469ms step_avg:39.00ms
step:1936/2330 train_time:75524ms step_avg:39.01ms
step:1937/2330 train_time:75546ms step_avg:39.00ms
step:1938/2330 train_time:75603ms step_avg:39.01ms
step:1939/2330 train_time:75625ms step_avg:39.00ms
step:1940/2330 train_time:75681ms step_avg:39.01ms
step:1941/2330 train_time:75704ms step_avg:39.00ms
step:1942/2330 train_time:75759ms step_avg:39.01ms
step:1943/2330 train_time:75782ms step_avg:39.00ms
step:1944/2330 train_time:75838ms step_avg:39.01ms
step:1945/2330 train_time:75860ms step_avg:39.00ms
step:1946/2330 train_time:75916ms step_avg:39.01ms
step:1947/2330 train_time:75938ms step_avg:39.00ms
step:1948/2330 train_time:75994ms step_avg:39.01ms
step:1949/2330 train_time:76016ms step_avg:39.00ms
step:1950/2330 train_time:76071ms step_avg:39.01ms
step:1951/2330 train_time:76093ms step_avg:39.00ms
step:1952/2330 train_time:76149ms step_avg:39.01ms
step:1953/2330 train_time:76171ms step_avg:39.00ms
step:1954/2330 train_time:76227ms step_avg:39.01ms
step:1955/2330 train_time:76249ms step_avg:39.00ms
step:1956/2330 train_time:76304ms step_avg:39.01ms
step:1957/2330 train_time:76327ms step_avg:39.00ms
step:1958/2330 train_time:76382ms step_avg:39.01ms
step:1959/2330 train_time:76404ms step_avg:39.00ms
step:1960/2330 train_time:76460ms step_avg:39.01ms
step:1961/2330 train_time:76483ms step_avg:39.00ms
step:1962/2330 train_time:76539ms step_avg:39.01ms
step:1963/2330 train_time:76562ms step_avg:39.00ms
step:1964/2330 train_time:76618ms step_avg:39.01ms
step:1965/2330 train_time:76640ms step_avg:39.00ms
step:1966/2330 train_time:76696ms step_avg:39.01ms
step:1967/2330 train_time:76719ms step_avg:39.00ms
step:1968/2330 train_time:76775ms step_avg:39.01ms
step:1969/2330 train_time:76797ms step_avg:39.00ms
step:1970/2330 train_time:76853ms step_avg:39.01ms
step:1971/2330 train_time:76875ms step_avg:39.00ms
step:1972/2330 train_time:76931ms step_avg:39.01ms
step:1973/2330 train_time:76953ms step_avg:39.00ms
step:1974/2330 train_time:77009ms step_avg:39.01ms
step:1975/2330 train_time:77031ms step_avg:39.00ms
step:1976/2330 train_time:77088ms step_avg:39.01ms
step:1977/2330 train_time:77109ms step_avg:39.00ms
step:1978/2330 train_time:77165ms step_avg:39.01ms
step:1979/2330 train_time:77187ms step_avg:39.00ms
step:1980/2330 train_time:77243ms step_avg:39.01ms
step:1981/2330 train_time:77265ms step_avg:39.00ms
step:1982/2330 train_time:77321ms step_avg:39.01ms
step:1983/2330 train_time:77343ms step_avg:39.00ms
step:1984/2330 train_time:77399ms step_avg:39.01ms
step:1985/2330 train_time:77422ms step_avg:39.00ms
step:1986/2330 train_time:77477ms step_avg:39.01ms
step:1987/2330 train_time:77500ms step_avg:39.00ms
step:1988/2330 train_time:77556ms step_avg:39.01ms
step:1989/2330 train_time:77578ms step_avg:39.00ms
step:1990/2330 train_time:77635ms step_avg:39.01ms
step:1991/2330 train_time:77656ms step_avg:39.00ms
step:1992/2330 train_time:77712ms step_avg:39.01ms
step:1993/2330 train_time:77734ms step_avg:39.00ms
step:1994/2330 train_time:77790ms step_avg:39.01ms
step:1995/2330 train_time:77812ms step_avg:39.00ms
step:1996/2330 train_time:77868ms step_avg:39.01ms
step:1997/2330 train_time:77890ms step_avg:39.00ms
step:1998/2330 train_time:77946ms step_avg:39.01ms
step:1999/2330 train_time:77968ms step_avg:39.00ms
step:2000/2330 train_time:78024ms step_avg:39.01ms
step:2000/2330 val_loss:5.3218 train_time:78119ms step_avg:39.06ms
step:2001/2330 train_time:78132ms step_avg:39.05ms
step:2002/2330 train_time:78143ms step_avg:39.03ms
step:2003/2330 train_time:78153ms step_avg:39.02ms
step:2004/2330 train_time:78181ms step_avg:39.01ms
step:2005/2330 train_time:78203ms step_avg:39.00ms
step:2006/2330 train_time:78258ms step_avg:39.01ms
step:2007/2330 train_time:78279ms step_avg:39.00ms
step:2008/2330 train_time:78334ms step_avg:39.01ms
step:2009/2330 train_time:78355ms step_avg:39.00ms
step:2010/2330 train_time:78410ms step_avg:39.01ms
step:2011/2330 train_time:78435ms step_avg:39.00ms
step:2012/2330 train_time:78495ms step_avg:39.01ms
step:2013/2330 train_time:78518ms step_avg:39.01ms
step:2014/2330 train_time:78575ms step_avg:39.01ms
step:2015/2330 train_time:78598ms step_avg:39.01ms
step:2016/2330 train_time:78654ms step_avg:39.01ms
step:2017/2330 train_time:78676ms step_avg:39.01ms
step:2018/2330 train_time:78731ms step_avg:39.01ms
step:2019/2330 train_time:78753ms step_avg:39.01ms
step:2020/2330 train_time:78808ms step_avg:39.01ms
step:2021/2330 train_time:78830ms step_avg:39.01ms
step:2022/2330 train_time:78886ms step_avg:39.01ms
step:2023/2330 train_time:78908ms step_avg:39.01ms
step:2024/2330 train_time:78964ms step_avg:39.01ms
step:2025/2330 train_time:78986ms step_avg:39.01ms
step:2026/2330 train_time:79041ms step_avg:39.01ms
step:2027/2330 train_time:79064ms step_avg:39.01ms
step:2028/2330 train_time:79119ms step_avg:39.01ms
step:2029/2330 train_time:79142ms step_avg:39.01ms
step:2030/2330 train_time:79197ms step_avg:39.01ms
step:2031/2330 train_time:79219ms step_avg:39.00ms
step:2032/2330 train_time:79274ms step_avg:39.01ms
step:2033/2330 train_time:79296ms step_avg:39.00ms
step:2034/2330 train_time:79351ms step_avg:39.01ms
step:2035/2330 train_time:79373ms step_avg:39.00ms
step:2036/2330 train_time:79430ms step_avg:39.01ms
step:2037/2330 train_time:79455ms step_avg:39.01ms
step:2038/2330 train_time:79512ms step_avg:39.01ms
step:2039/2330 train_time:79535ms step_avg:39.01ms
step:2040/2330 train_time:79591ms step_avg:39.02ms
step:2041/2330 train_time:79614ms step_avg:39.01ms
step:2042/2330 train_time:79669ms step_avg:39.02ms
step:2043/2330 train_time:79693ms step_avg:39.01ms
step:2044/2330 train_time:79748ms step_avg:39.02ms
step:2045/2330 train_time:79770ms step_avg:39.01ms
step:2046/2330 train_time:79826ms step_avg:39.02ms
step:2047/2330 train_time:79848ms step_avg:39.01ms
step:2048/2330 train_time:79903ms step_avg:39.02ms
step:2049/2330 train_time:79925ms step_avg:39.01ms
step:2050/2330 train_time:79981ms step_avg:39.01ms
step:2051/2330 train_time:80003ms step_avg:39.01ms
step:2052/2330 train_time:80059ms step_avg:39.01ms
step:2053/2330 train_time:80080ms step_avg:39.01ms
step:2054/2330 train_time:80136ms step_avg:39.01ms
step:2055/2330 train_time:80158ms step_avg:39.01ms
step:2056/2330 train_time:80214ms step_avg:39.01ms
step:2057/2330 train_time:80236ms step_avg:39.01ms
step:2058/2330 train_time:80291ms step_avg:39.01ms
step:2059/2330 train_time:80314ms step_avg:39.01ms
step:2060/2330 train_time:80369ms step_avg:39.01ms
step:2061/2330 train_time:80392ms step_avg:39.01ms
step:2062/2330 train_time:80448ms step_avg:39.01ms
step:2063/2330 train_time:80470ms step_avg:39.01ms
step:2064/2330 train_time:80526ms step_avg:39.01ms
step:2065/2330 train_time:80548ms step_avg:39.01ms
step:2066/2330 train_time:80605ms step_avg:39.01ms
step:2067/2330 train_time:80627ms step_avg:39.01ms
step:2068/2330 train_time:80684ms step_avg:39.02ms
step:2069/2330 train_time:80706ms step_avg:39.01ms
step:2070/2330 train_time:80762ms step_avg:39.02ms
step:2071/2330 train_time:80784ms step_avg:39.01ms
step:2072/2330 train_time:80839ms step_avg:39.01ms
step:2073/2330 train_time:80861ms step_avg:39.01ms
step:2074/2330 train_time:80916ms step_avg:39.01ms
step:2075/2330 train_time:80938ms step_avg:39.01ms
step:2076/2330 train_time:80993ms step_avg:39.01ms
step:2077/2330 train_time:81016ms step_avg:39.01ms
step:2078/2330 train_time:81071ms step_avg:39.01ms
step:2079/2330 train_time:81094ms step_avg:39.01ms
step:2080/2330 train_time:81149ms step_avg:39.01ms
step:2081/2330 train_time:81172ms step_avg:39.01ms
step:2082/2330 train_time:81227ms step_avg:39.01ms
step:2083/2330 train_time:81250ms step_avg:39.01ms
step:2084/2330 train_time:81305ms step_avg:39.01ms
step:2085/2330 train_time:81327ms step_avg:39.01ms
step:2086/2330 train_time:81383ms step_avg:39.01ms
step:2087/2330 train_time:81406ms step_avg:39.01ms
step:2088/2330 train_time:81462ms step_avg:39.01ms
step:2089/2330 train_time:81484ms step_avg:39.01ms
step:2090/2330 train_time:81541ms step_avg:39.01ms
step:2091/2330 train_time:81563ms step_avg:39.01ms
step:2092/2330 train_time:81619ms step_avg:39.01ms
step:2093/2330 train_time:81641ms step_avg:39.01ms
step:2094/2330 train_time:81697ms step_avg:39.01ms
step:2095/2330 train_time:81719ms step_avg:39.01ms
step:2096/2330 train_time:81775ms step_avg:39.01ms
step:2097/2330 train_time:81797ms step_avg:39.01ms
step:2098/2330 train_time:81853ms step_avg:39.01ms
step:2099/2330 train_time:81875ms step_avg:39.01ms
step:2100/2330 train_time:81930ms step_avg:39.01ms
step:2101/2330 train_time:81953ms step_avg:39.01ms
step:2102/2330 train_time:82008ms step_avg:39.01ms
step:2103/2330 train_time:82030ms step_avg:39.01ms
step:2104/2330 train_time:82086ms step_avg:39.01ms
step:2105/2330 train_time:82108ms step_avg:39.01ms
step:2106/2330 train_time:82164ms step_avg:39.01ms
step:2107/2330 train_time:82186ms step_avg:39.01ms
step:2108/2330 train_time:82242ms step_avg:39.01ms
step:2109/2330 train_time:82264ms step_avg:39.01ms
step:2110/2330 train_time:82320ms step_avg:39.01ms
step:2111/2330 train_time:82342ms step_avg:39.01ms
step:2112/2330 train_time:82398ms step_avg:39.01ms
step:2113/2330 train_time:82420ms step_avg:39.01ms
step:2114/2330 train_time:82476ms step_avg:39.01ms
step:2115/2330 train_time:82498ms step_avg:39.01ms
step:2116/2330 train_time:82554ms step_avg:39.01ms
step:2117/2330 train_time:82576ms step_avg:39.01ms
step:2118/2330 train_time:82632ms step_avg:39.01ms
step:2119/2330 train_time:82655ms step_avg:39.01ms
step:2120/2330 train_time:82711ms step_avg:39.01ms
step:2121/2330 train_time:82735ms step_avg:39.01ms
step:2122/2330 train_time:82791ms step_avg:39.02ms
step:2123/2330 train_time:82813ms step_avg:39.01ms
step:2124/2330 train_time:82869ms step_avg:39.02ms
step:2125/2330 train_time:82891ms step_avg:39.01ms
step:2126/2330 train_time:82947ms step_avg:39.02ms
step:2127/2330 train_time:82969ms step_avg:39.01ms
step:2128/2330 train_time:83025ms step_avg:39.02ms
step:2129/2330 train_time:83047ms step_avg:39.01ms
step:2130/2330 train_time:83102ms step_avg:39.02ms
step:2131/2330 train_time:83124ms step_avg:39.01ms
step:2132/2330 train_time:83180ms step_avg:39.02ms
step:2133/2330 train_time:83202ms step_avg:39.01ms
step:2134/2330 train_time:83258ms step_avg:39.02ms
step:2135/2330 train_time:83280ms step_avg:39.01ms
step:2136/2330 train_time:83335ms step_avg:39.01ms
step:2137/2330 train_time:83357ms step_avg:39.01ms
step:2138/2330 train_time:83413ms step_avg:39.01ms
step:2139/2330 train_time:83435ms step_avg:39.01ms
step:2140/2330 train_time:83490ms step_avg:39.01ms
step:2141/2330 train_time:83513ms step_avg:39.01ms
step:2142/2330 train_time:83569ms step_avg:39.01ms
step:2143/2330 train_time:83591ms step_avg:39.01ms
step:2144/2330 train_time:83647ms step_avg:39.01ms
step:2145/2330 train_time:83669ms step_avg:39.01ms
step:2146/2330 train_time:83725ms step_avg:39.01ms
step:2147/2330 train_time:83748ms step_avg:39.01ms
step:2148/2330 train_time:83804ms step_avg:39.01ms
step:2149/2330 train_time:83825ms step_avg:39.01ms
step:2150/2330 train_time:83881ms step_avg:39.01ms
step:2151/2330 train_time:83903ms step_avg:39.01ms
step:2152/2330 train_time:83959ms step_avg:39.01ms
step:2153/2330 train_time:83981ms step_avg:39.01ms
step:2154/2330 train_time:84037ms step_avg:39.01ms
step:2155/2330 train_time:84058ms step_avg:39.01ms
step:2156/2330 train_time:84114ms step_avg:39.01ms
step:2157/2330 train_time:84136ms step_avg:39.01ms
step:2158/2330 train_time:84191ms step_avg:39.01ms
step:2159/2330 train_time:84214ms step_avg:39.01ms
step:2160/2330 train_time:84269ms step_avg:39.01ms
step:2161/2330 train_time:84292ms step_avg:39.01ms
step:2162/2330 train_time:84348ms step_avg:39.01ms
step:2163/2330 train_time:84370ms step_avg:39.01ms
step:2164/2330 train_time:84426ms step_avg:39.01ms
step:2165/2330 train_time:84448ms step_avg:39.01ms
step:2166/2330 train_time:84503ms step_avg:39.01ms
step:2167/2330 train_time:84525ms step_avg:39.01ms
step:2168/2330 train_time:84582ms step_avg:39.01ms
step:2169/2330 train_time:84604ms step_avg:39.01ms
step:2170/2330 train_time:84660ms step_avg:39.01ms
step:2171/2330 train_time:84682ms step_avg:39.01ms
step:2172/2330 train_time:84738ms step_avg:39.01ms
step:2173/2330 train_time:84760ms step_avg:39.01ms
step:2174/2330 train_time:84816ms step_avg:39.01ms
step:2175/2330 train_time:84837ms step_avg:39.01ms
step:2176/2330 train_time:84892ms step_avg:39.01ms
step:2177/2330 train_time:84915ms step_avg:39.01ms
step:2178/2330 train_time:84970ms step_avg:39.01ms
step:2179/2330 train_time:84994ms step_avg:39.01ms
step:2180/2330 train_time:85049ms step_avg:39.01ms
step:2181/2330 train_time:85072ms step_avg:39.01ms
step:2182/2330 train_time:85128ms step_avg:39.01ms
step:2183/2330 train_time:85150ms step_avg:39.01ms
step:2184/2330 train_time:85206ms step_avg:39.01ms
step:2185/2330 train_time:85228ms step_avg:39.01ms
step:2186/2330 train_time:85284ms step_avg:39.01ms
step:2187/2330 train_time:85306ms step_avg:39.01ms
step:2188/2330 train_time:85362ms step_avg:39.01ms
step:2189/2330 train_time:85384ms step_avg:39.01ms
step:2190/2330 train_time:85440ms step_avg:39.01ms
step:2191/2330 train_time:85462ms step_avg:39.01ms
step:2192/2330 train_time:85519ms step_avg:39.01ms
step:2193/2330 train_time:85540ms step_avg:39.01ms
step:2194/2330 train_time:85596ms step_avg:39.01ms
step:2195/2330 train_time:85618ms step_avg:39.01ms
step:2196/2330 train_time:85674ms step_avg:39.01ms
step:2197/2330 train_time:85696ms step_avg:39.01ms
step:2198/2330 train_time:85752ms step_avg:39.01ms
step:2199/2330 train_time:85774ms step_avg:39.01ms
step:2200/2330 train_time:85830ms step_avg:39.01ms
step:2201/2330 train_time:85853ms step_avg:39.01ms
step:2202/2330 train_time:85909ms step_avg:39.01ms
step:2203/2330 train_time:85931ms step_avg:39.01ms
step:2204/2330 train_time:85987ms step_avg:39.01ms
step:2205/2330 train_time:86009ms step_avg:39.01ms
step:2206/2330 train_time:86065ms step_avg:39.01ms
step:2207/2330 train_time:86087ms step_avg:39.01ms
step:2208/2330 train_time:86142ms step_avg:39.01ms
step:2209/2330 train_time:86164ms step_avg:39.01ms
step:2210/2330 train_time:86220ms step_avg:39.01ms
step:2211/2330 train_time:86242ms step_avg:39.01ms
step:2212/2330 train_time:86297ms step_avg:39.01ms
step:2213/2330 train_time:86319ms step_avg:39.01ms
step:2214/2330 train_time:86375ms step_avg:39.01ms
step:2215/2330 train_time:86397ms step_avg:39.01ms
step:2216/2330 train_time:86453ms step_avg:39.01ms
step:2217/2330 train_time:86475ms step_avg:39.01ms
step:2218/2330 train_time:86531ms step_avg:39.01ms
step:2219/2330 train_time:86553ms step_avg:39.01ms
step:2220/2330 train_time:86610ms step_avg:39.01ms
step:2221/2330 train_time:86632ms step_avg:39.01ms
step:2222/2330 train_time:86688ms step_avg:39.01ms
step:2223/2330 train_time:86710ms step_avg:39.01ms
step:2224/2330 train_time:86766ms step_avg:39.01ms
step:2225/2330 train_time:86789ms step_avg:39.01ms
step:2226/2330 train_time:86844ms step_avg:39.01ms
step:2227/2330 train_time:86867ms step_avg:39.01ms
step:2228/2330 train_time:86923ms step_avg:39.01ms
step:2229/2330 train_time:86945ms step_avg:39.01ms
step:2230/2330 train_time:87001ms step_avg:39.01ms
step:2231/2330 train_time:87023ms step_avg:39.01ms
step:2232/2330 train_time:87079ms step_avg:39.01ms
step:2233/2330 train_time:87101ms step_avg:39.01ms
step:2234/2330 train_time:87157ms step_avg:39.01ms
step:2235/2330 train_time:87178ms step_avg:39.01ms
step:2236/2330 train_time:87234ms step_avg:39.01ms
step:2237/2330 train_time:87256ms step_avg:39.01ms
step:2238/2330 train_time:87312ms step_avg:39.01ms
step:2239/2330 train_time:87334ms step_avg:39.01ms
step:2240/2330 train_time:87390ms step_avg:39.01ms
step:2241/2330 train_time:87412ms step_avg:39.01ms
step:2242/2330 train_time:87468ms step_avg:39.01ms
step:2243/2330 train_time:87491ms step_avg:39.01ms
step:2244/2330 train_time:87547ms step_avg:39.01ms
step:2245/2330 train_time:87568ms step_avg:39.01ms
step:2246/2330 train_time:87625ms step_avg:39.01ms
step:2247/2330 train_time:87647ms step_avg:39.01ms
step:2248/2330 train_time:87703ms step_avg:39.01ms
step:2249/2330 train_time:87724ms step_avg:39.01ms
step:2250/2330 train_time:87780ms step_avg:39.01ms
step:2250/2330 val_loss:5.3142 train_time:87875ms step_avg:39.06ms
step:2251/2330 train_time:87888ms step_avg:39.04ms
step:2252/2330 train_time:87899ms step_avg:39.03ms
step:2253/2330 train_time:87908ms step_avg:39.02ms
step:2254/2330 train_time:87937ms step_avg:39.01ms
step:2255/2330 train_time:87959ms step_avg:39.01ms
step:2256/2330 train_time:88013ms step_avg:39.01ms
step:2257/2330 train_time:88035ms step_avg:39.01ms
step:2258/2330 train_time:88091ms step_avg:39.01ms
step:2259/2330 train_time:88112ms step_avg:39.00ms
step:2260/2330 train_time:88167ms step_avg:39.01ms
step:2261/2330 train_time:88193ms step_avg:39.01ms
step:2262/2330 train_time:88251ms step_avg:39.01ms
step:2263/2330 train_time:88274ms step_avg:39.01ms
step:2264/2330 train_time:88332ms step_avg:39.02ms
step:2265/2330 train_time:88355ms step_avg:39.01ms
step:2266/2330 train_time:88411ms step_avg:39.02ms
step:2267/2330 train_time:88433ms step_avg:39.01ms
step:2268/2330 train_time:88488ms step_avg:39.02ms
step:2269/2330 train_time:88510ms step_avg:39.01ms
step:2270/2330 train_time:88565ms step_avg:39.02ms
step:2271/2330 train_time:88587ms step_avg:39.01ms
step:2272/2330 train_time:88642ms step_avg:39.01ms
step:2273/2330 train_time:88664ms step_avg:39.01ms
step:2274/2330 train_time:88719ms step_avg:39.01ms
step:2275/2330 train_time:88740ms step_avg:39.01ms
step:2276/2330 train_time:88797ms step_avg:39.01ms
step:2277/2330 train_time:88819ms step_avg:39.01ms
step:2278/2330 train_time:88876ms step_avg:39.01ms
step:2279/2330 train_time:88898ms step_avg:39.01ms
step:2280/2330 train_time:88953ms step_avg:39.01ms
step:2281/2330 train_time:88975ms step_avg:39.01ms
step:2282/2330 train_time:89031ms step_avg:39.01ms
step:2283/2330 train_time:89053ms step_avg:39.01ms
step:2284/2330 train_time:89109ms step_avg:39.01ms
step:2285/2330 train_time:89131ms step_avg:39.01ms
step:2286/2330 train_time:89187ms step_avg:39.01ms
step:2287/2330 train_time:89210ms step_avg:39.01ms
step:2288/2330 train_time:89267ms step_avg:39.02ms
step:2289/2330 train_time:89289ms step_avg:39.01ms
step:2290/2330 train_time:89346ms step_avg:39.02ms
step:2291/2330 train_time:89368ms step_avg:39.01ms
step:2292/2330 train_time:89424ms step_avg:39.02ms
step:2293/2330 train_time:89445ms step_avg:39.01ms
step:2294/2330 train_time:89501ms step_avg:39.02ms
step:2295/2330 train_time:89523ms step_avg:39.01ms
step:2296/2330 train_time:89578ms step_avg:39.01ms
step:2297/2330 train_time:89600ms step_avg:39.01ms
step:2298/2330 train_time:89656ms step_avg:39.01ms
step:2299/2330 train_time:89677ms step_avg:39.01ms
step:2300/2330 train_time:89733ms step_avg:39.01ms
step:2301/2330 train_time:89755ms step_avg:39.01ms
step:2302/2330 train_time:89811ms step_avg:39.01ms
step:2303/2330 train_time:89833ms step_avg:39.01ms
step:2304/2330 train_time:89890ms step_avg:39.01ms
step:2305/2330 train_time:89911ms step_avg:39.01ms
step:2306/2330 train_time:89967ms step_avg:39.01ms
step:2307/2330 train_time:89989ms step_avg:39.01ms
step:2308/2330 train_time:90045ms step_avg:39.01ms
step:2309/2330 train_time:90066ms step_avg:39.01ms
step:2310/2330 train_time:90122ms step_avg:39.01ms
step:2311/2330 train_time:90144ms step_avg:39.01ms
step:2312/2330 train_time:90201ms step_avg:39.01ms
step:2313/2330 train_time:90224ms step_avg:39.01ms
step:2314/2330 train_time:90279ms step_avg:39.01ms
step:2315/2330 train_time:90304ms step_avg:39.01ms
step:2316/2330 train_time:90360ms step_avg:39.02ms
step:2317/2330 train_time:90383ms step_avg:39.01ms
step:2318/2330 train_time:90439ms step_avg:39.02ms
step:2319/2330 train_time:90461ms step_avg:39.01ms
step:2320/2330 train_time:90517ms step_avg:39.02ms
step:2321/2330 train_time:90539ms step_avg:39.01ms
step:2322/2330 train_time:90594ms step_avg:39.02ms
step:2323/2330 train_time:90616ms step_avg:39.01ms
step:2324/2330 train_time:90672ms step_avg:39.02ms
step:2325/2330 train_time:90694ms step_avg:39.01ms
step:2326/2330 train_time:90750ms step_avg:39.02ms
step:2327/2330 train_time:90771ms step_avg:39.01ms
step:2328/2330 train_time:90827ms step_avg:39.02ms
step:2329/2330 train_time:90849ms step_avg:39.01ms
step:2330/2330 train_time:90905ms step_avg:39.01ms
step:2330/2330 val_loss:5.3093 train_time:90999ms step_avg:39.06ms
peak memory allocated: 29226 MiB reserved: 38888 MiB
