import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                # v_chunk = divide_by_norm(batched)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                # v_chunk = divide_by_norm(batched_update_grads)
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "muon_lr1e-1"
    os.makedirs("logs3", exist_ok=True)
    logfile = f"logs3/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
# embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=1e-1, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    return 1.0

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum_min + 0.5 * (momentum_max - momentum_min)

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs3/{run_id}", exist_ok=True)
            torch.save(log, f"logs3/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 07:28:34 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   29C    P0             110W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   27C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   27C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   28C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   27C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   27C    P0             108W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:74ms step_avg:74.07ms
step:2/2330 train_time:195ms step_avg:97.31ms
step:3/2330 train_time:216ms step_avg:72.16ms
step:4/2330 train_time:252ms step_avg:62.92ms
step:5/2330 train_time:309ms step_avg:61.81ms
step:6/2330 train_time:371ms step_avg:61.77ms
step:7/2330 train_time:429ms step_avg:61.29ms
step:8/2330 train_time:491ms step_avg:61.35ms
step:9/2330 train_time:549ms step_avg:61.05ms
step:10/2330 train_time:611ms step_avg:61.13ms
step:11/2330 train_time:670ms step_avg:60.89ms
step:12/2330 train_time:731ms step_avg:60.95ms
step:13/2330 train_time:790ms step_avg:60.74ms
step:14/2330 train_time:851ms step_avg:60.79ms
step:15/2330 train_time:910ms step_avg:60.64ms
step:16/2330 train_time:971ms step_avg:60.69ms
step:17/2330 train_time:1031ms step_avg:60.63ms
step:18/2330 train_time:1096ms step_avg:60.90ms
step:19/2330 train_time:1158ms step_avg:60.95ms
step:20/2330 train_time:1221ms step_avg:61.06ms
step:21/2330 train_time:1281ms step_avg:60.99ms
step:22/2330 train_time:1343ms step_avg:61.06ms
step:23/2330 train_time:1402ms step_avg:60.98ms
step:24/2330 train_time:1465ms step_avg:61.04ms
step:25/2330 train_time:1524ms step_avg:60.97ms
step:26/2330 train_time:1587ms step_avg:61.02ms
step:27/2330 train_time:1646ms step_avg:60.95ms
step:28/2330 train_time:1707ms step_avg:60.97ms
step:29/2330 train_time:1767ms step_avg:60.91ms
step:30/2330 train_time:1829ms step_avg:60.96ms
step:31/2330 train_time:1888ms step_avg:60.90ms
step:32/2330 train_time:1950ms step_avg:60.93ms
step:33/2330 train_time:2010ms step_avg:60.90ms
step:34/2330 train_time:2074ms step_avg:60.99ms
step:35/2330 train_time:2134ms step_avg:60.97ms
step:36/2330 train_time:2197ms step_avg:61.04ms
step:37/2330 train_time:2257ms step_avg:61.00ms
step:38/2330 train_time:2319ms step_avg:61.04ms
step:39/2330 train_time:2379ms step_avg:61.00ms
step:40/2330 train_time:2442ms step_avg:61.04ms
step:41/2330 train_time:2501ms step_avg:61.00ms
step:42/2330 train_time:2563ms step_avg:61.03ms
step:43/2330 train_time:2622ms step_avg:60.98ms
step:44/2330 train_time:2684ms step_avg:61.00ms
step:45/2330 train_time:2743ms step_avg:60.96ms
step:46/2330 train_time:2805ms step_avg:60.98ms
step:47/2330 train_time:2865ms step_avg:60.95ms
step:48/2330 train_time:2928ms step_avg:60.99ms
step:49/2330 train_time:2987ms step_avg:60.96ms
step:50/2330 train_time:3049ms step_avg:60.99ms
step:51/2330 train_time:3109ms step_avg:60.96ms
step:52/2330 train_time:3172ms step_avg:61.01ms
step:53/2330 train_time:3232ms step_avg:60.99ms
step:54/2330 train_time:3295ms step_avg:61.03ms
step:55/2330 train_time:3355ms step_avg:61.00ms
step:56/2330 train_time:3417ms step_avg:61.01ms
step:57/2330 train_time:3476ms step_avg:60.98ms
step:58/2330 train_time:3538ms step_avg:60.99ms
step:59/2330 train_time:3597ms step_avg:60.97ms
step:60/2330 train_time:3660ms step_avg:60.99ms
step:61/2330 train_time:3719ms step_avg:60.96ms
step:62/2330 train_time:3782ms step_avg:61.00ms
step:63/2330 train_time:3842ms step_avg:60.98ms
step:64/2330 train_time:3904ms step_avg:61.00ms
step:65/2330 train_time:3963ms step_avg:60.97ms
step:66/2330 train_time:4026ms step_avg:61.00ms
step:67/2330 train_time:4086ms step_avg:60.99ms
step:68/2330 train_time:4149ms step_avg:61.01ms
step:69/2330 train_time:4208ms step_avg:60.99ms
step:70/2330 train_time:4270ms step_avg:61.00ms
step:71/2330 train_time:4329ms step_avg:60.97ms
step:72/2330 train_time:4391ms step_avg:60.99ms
step:73/2330 train_time:4451ms step_avg:60.97ms
step:74/2330 train_time:4513ms step_avg:60.99ms
step:75/2330 train_time:4573ms step_avg:60.97ms
step:76/2330 train_time:4635ms step_avg:60.99ms
step:77/2330 train_time:4694ms step_avg:60.97ms
step:78/2330 train_time:4756ms step_avg:60.98ms
step:79/2330 train_time:4815ms step_avg:60.96ms
step:80/2330 train_time:4878ms step_avg:60.97ms
step:81/2330 train_time:4937ms step_avg:60.95ms
step:82/2330 train_time:4999ms step_avg:60.97ms
step:83/2330 train_time:5059ms step_avg:60.95ms
step:84/2330 train_time:5121ms step_avg:60.97ms
step:85/2330 train_time:5181ms step_avg:60.96ms
step:86/2330 train_time:5243ms step_avg:60.97ms
step:87/2330 train_time:5302ms step_avg:60.94ms
step:88/2330 train_time:5365ms step_avg:60.96ms
step:89/2330 train_time:5425ms step_avg:60.95ms
step:90/2330 train_time:5487ms step_avg:60.97ms
step:91/2330 train_time:5547ms step_avg:60.95ms
step:92/2330 train_time:5609ms step_avg:60.97ms
step:93/2330 train_time:5669ms step_avg:60.95ms
step:94/2330 train_time:5731ms step_avg:60.97ms
step:95/2330 train_time:5791ms step_avg:60.96ms
step:96/2330 train_time:5853ms step_avg:60.97ms
step:97/2330 train_time:5912ms step_avg:60.95ms
step:98/2330 train_time:5975ms step_avg:60.97ms
step:99/2330 train_time:6035ms step_avg:60.96ms
step:100/2330 train_time:6097ms step_avg:60.97ms
step:101/2330 train_time:6155ms step_avg:60.94ms
step:102/2330 train_time:6218ms step_avg:60.96ms
step:103/2330 train_time:6277ms step_avg:60.94ms
step:104/2330 train_time:6340ms step_avg:60.96ms
step:105/2330 train_time:6399ms step_avg:60.94ms
step:106/2330 train_time:6461ms step_avg:60.95ms
step:107/2330 train_time:6520ms step_avg:60.93ms
step:108/2330 train_time:6582ms step_avg:60.95ms
step:109/2330 train_time:6642ms step_avg:60.94ms
step:110/2330 train_time:6704ms step_avg:60.95ms
step:111/2330 train_time:6764ms step_avg:60.93ms
step:112/2330 train_time:6827ms step_avg:60.95ms
step:113/2330 train_time:6886ms step_avg:60.94ms
step:114/2330 train_time:6949ms step_avg:60.95ms
step:115/2330 train_time:7008ms step_avg:60.94ms
step:116/2330 train_time:7070ms step_avg:60.94ms
step:117/2330 train_time:7129ms step_avg:60.93ms
step:118/2330 train_time:7191ms step_avg:60.94ms
step:119/2330 train_time:7251ms step_avg:60.93ms
step:120/2330 train_time:7313ms step_avg:60.94ms
step:121/2330 train_time:7373ms step_avg:60.93ms
step:122/2330 train_time:7436ms step_avg:60.95ms
step:123/2330 train_time:7494ms step_avg:60.93ms
step:124/2330 train_time:7557ms step_avg:60.94ms
step:125/2330 train_time:7616ms step_avg:60.93ms
step:126/2330 train_time:7679ms step_avg:60.94ms
step:127/2330 train_time:7738ms step_avg:60.93ms
step:128/2330 train_time:7800ms step_avg:60.94ms
step:129/2330 train_time:7860ms step_avg:60.93ms
step:130/2330 train_time:7922ms step_avg:60.94ms
step:131/2330 train_time:7981ms step_avg:60.92ms
step:132/2330 train_time:8043ms step_avg:60.93ms
step:133/2330 train_time:8102ms step_avg:60.92ms
step:134/2330 train_time:8164ms step_avg:60.93ms
step:135/2330 train_time:8224ms step_avg:60.92ms
step:136/2330 train_time:8287ms step_avg:60.93ms
step:137/2330 train_time:8347ms step_avg:60.92ms
step:138/2330 train_time:8409ms step_avg:60.93ms
step:139/2330 train_time:8468ms step_avg:60.92ms
step:140/2330 train_time:8530ms step_avg:60.93ms
step:141/2330 train_time:8589ms step_avg:60.92ms
step:142/2330 train_time:8652ms step_avg:60.93ms
step:143/2330 train_time:8711ms step_avg:60.92ms
step:144/2330 train_time:8774ms step_avg:60.93ms
step:145/2330 train_time:8834ms step_avg:60.92ms
step:146/2330 train_time:8896ms step_avg:60.93ms
step:147/2330 train_time:8955ms step_avg:60.92ms
step:148/2330 train_time:9017ms step_avg:60.93ms
step:149/2330 train_time:9076ms step_avg:60.91ms
step:150/2330 train_time:9138ms step_avg:60.92ms
step:151/2330 train_time:9197ms step_avg:60.91ms
step:152/2330 train_time:9260ms step_avg:60.92ms
step:153/2330 train_time:9319ms step_avg:60.91ms
step:154/2330 train_time:9382ms step_avg:60.92ms
step:155/2330 train_time:9441ms step_avg:60.91ms
step:156/2330 train_time:9503ms step_avg:60.92ms
step:157/2330 train_time:9564ms step_avg:60.91ms
step:158/2330 train_time:9627ms step_avg:60.93ms
step:159/2330 train_time:9686ms step_avg:60.92ms
step:160/2330 train_time:9748ms step_avg:60.93ms
step:161/2330 train_time:9807ms step_avg:60.92ms
step:162/2330 train_time:9869ms step_avg:60.92ms
step:163/2330 train_time:9929ms step_avg:60.91ms
step:164/2330 train_time:9991ms step_avg:60.92ms
step:165/2330 train_time:10051ms step_avg:60.91ms
step:166/2330 train_time:10114ms step_avg:60.93ms
step:167/2330 train_time:10174ms step_avg:60.92ms
step:168/2330 train_time:10236ms step_avg:60.93ms
step:169/2330 train_time:10295ms step_avg:60.92ms
step:170/2330 train_time:10357ms step_avg:60.92ms
step:171/2330 train_time:10416ms step_avg:60.91ms
step:172/2330 train_time:10478ms step_avg:60.92ms
step:173/2330 train_time:10538ms step_avg:60.91ms
step:174/2330 train_time:10600ms step_avg:60.92ms
step:175/2330 train_time:10660ms step_avg:60.92ms
step:176/2330 train_time:10723ms step_avg:60.92ms
step:177/2330 train_time:10782ms step_avg:60.91ms
step:178/2330 train_time:10844ms step_avg:60.92ms
step:179/2330 train_time:10904ms step_avg:60.92ms
step:180/2330 train_time:10967ms step_avg:60.93ms
step:181/2330 train_time:11026ms step_avg:60.92ms
step:182/2330 train_time:11089ms step_avg:60.93ms
step:183/2330 train_time:11148ms step_avg:60.92ms
step:184/2330 train_time:11210ms step_avg:60.92ms
step:185/2330 train_time:11269ms step_avg:60.91ms
step:186/2330 train_time:11332ms step_avg:60.93ms
step:187/2330 train_time:11392ms step_avg:60.92ms
step:188/2330 train_time:11454ms step_avg:60.92ms
step:189/2330 train_time:11513ms step_avg:60.92ms
step:190/2330 train_time:11576ms step_avg:60.93ms
step:191/2330 train_time:11635ms step_avg:60.92ms
step:192/2330 train_time:11697ms step_avg:60.92ms
step:193/2330 train_time:11756ms step_avg:60.91ms
step:194/2330 train_time:11818ms step_avg:60.92ms
step:195/2330 train_time:11878ms step_avg:60.91ms
step:196/2330 train_time:11940ms step_avg:60.92ms
step:197/2330 train_time:11999ms step_avg:60.91ms
step:198/2330 train_time:12061ms step_avg:60.91ms
step:199/2330 train_time:12120ms step_avg:60.91ms
step:200/2330 train_time:12183ms step_avg:60.91ms
step:201/2330 train_time:12242ms step_avg:60.91ms
step:202/2330 train_time:12305ms step_avg:60.92ms
step:203/2330 train_time:12365ms step_avg:60.91ms
step:204/2330 train_time:12427ms step_avg:60.92ms
step:205/2330 train_time:12486ms step_avg:60.91ms
step:206/2330 train_time:12548ms step_avg:60.91ms
step:207/2330 train_time:12607ms step_avg:60.90ms
step:208/2330 train_time:12670ms step_avg:60.91ms
step:209/2330 train_time:12729ms step_avg:60.90ms
step:210/2330 train_time:12792ms step_avg:60.91ms
step:211/2330 train_time:12851ms step_avg:60.90ms
step:212/2330 train_time:12913ms step_avg:60.91ms
step:213/2330 train_time:12973ms step_avg:60.91ms
step:214/2330 train_time:13035ms step_avg:60.91ms
step:215/2330 train_time:13094ms step_avg:60.90ms
step:216/2330 train_time:13156ms step_avg:60.91ms
step:217/2330 train_time:13216ms step_avg:60.90ms
step:218/2330 train_time:13278ms step_avg:60.91ms
step:219/2330 train_time:13337ms step_avg:60.90ms
step:220/2330 train_time:13400ms step_avg:60.91ms
step:221/2330 train_time:13459ms step_avg:60.90ms
step:222/2330 train_time:13521ms step_avg:60.91ms
step:223/2330 train_time:13581ms step_avg:60.90ms
step:224/2330 train_time:13643ms step_avg:60.91ms
step:225/2330 train_time:13702ms step_avg:60.90ms
step:226/2330 train_time:13765ms step_avg:60.91ms
step:227/2330 train_time:13824ms step_avg:60.90ms
step:228/2330 train_time:13886ms step_avg:60.90ms
step:229/2330 train_time:13946ms step_avg:60.90ms
step:230/2330 train_time:14007ms step_avg:60.90ms
step:231/2330 train_time:14067ms step_avg:60.90ms
step:232/2330 train_time:14129ms step_avg:60.90ms
step:233/2330 train_time:14189ms step_avg:60.90ms
step:234/2330 train_time:14251ms step_avg:60.90ms
step:235/2330 train_time:14311ms step_avg:60.90ms
step:236/2330 train_time:14374ms step_avg:60.91ms
step:237/2330 train_time:14433ms step_avg:60.90ms
step:238/2330 train_time:14495ms step_avg:60.91ms
step:239/2330 train_time:14554ms step_avg:60.90ms
step:240/2330 train_time:14616ms step_avg:60.90ms
step:241/2330 train_time:14675ms step_avg:60.89ms
step:242/2330 train_time:14737ms step_avg:60.90ms
step:243/2330 train_time:14796ms step_avg:60.89ms
step:244/2330 train_time:14858ms step_avg:60.89ms
step:245/2330 train_time:14918ms step_avg:60.89ms
step:246/2330 train_time:14980ms step_avg:60.89ms
step:247/2330 train_time:15039ms step_avg:60.89ms
step:248/2330 train_time:15102ms step_avg:60.89ms
step:249/2330 train_time:15161ms step_avg:60.89ms
step:250/2330 train_time:15224ms step_avg:60.90ms
step:250/2330 val_loss:4.1406 train_time:15288ms step_avg:61.15ms
step:251/2330 train_time:15311ms step_avg:61.00ms
step:252/2330 train_time:15347ms step_avg:60.90ms
step:253/2330 train_time:15410ms step_avg:60.91ms
step:254/2330 train_time:15475ms step_avg:60.93ms
step:255/2330 train_time:15534ms step_avg:60.92ms
step:256/2330 train_time:15596ms step_avg:60.92ms
step:257/2330 train_time:15656ms step_avg:60.92ms
step:258/2330 train_time:15717ms step_avg:60.92ms
step:259/2330 train_time:15776ms step_avg:60.91ms
step:260/2330 train_time:15837ms step_avg:60.91ms
step:261/2330 train_time:15895ms step_avg:60.90ms
step:262/2330 train_time:15957ms step_avg:60.90ms
step:263/2330 train_time:16015ms step_avg:60.89ms
step:264/2330 train_time:16076ms step_avg:60.90ms
step:265/2330 train_time:16135ms step_avg:60.89ms
step:266/2330 train_time:16197ms step_avg:60.89ms
step:267/2330 train_time:16258ms step_avg:60.89ms
step:268/2330 train_time:16321ms step_avg:60.90ms
step:269/2330 train_time:16382ms step_avg:60.90ms
step:270/2330 train_time:16445ms step_avg:60.91ms
step:271/2330 train_time:16506ms step_avg:60.91ms
step:272/2330 train_time:16569ms step_avg:60.91ms
step:273/2330 train_time:16628ms step_avg:60.91ms
step:274/2330 train_time:16690ms step_avg:60.91ms
step:275/2330 train_time:16749ms step_avg:60.90ms
step:276/2330 train_time:16811ms step_avg:60.91ms
step:277/2330 train_time:16869ms step_avg:60.90ms
step:278/2330 train_time:16931ms step_avg:60.90ms
step:279/2330 train_time:16990ms step_avg:60.90ms
step:280/2330 train_time:17053ms step_avg:60.90ms
step:281/2330 train_time:17111ms step_avg:60.89ms
step:282/2330 train_time:17172ms step_avg:60.90ms
step:283/2330 train_time:17232ms step_avg:60.89ms
step:284/2330 train_time:17295ms step_avg:60.90ms
step:285/2330 train_time:17355ms step_avg:60.90ms
step:286/2330 train_time:17418ms step_avg:60.90ms
step:287/2330 train_time:17478ms step_avg:60.90ms
step:288/2330 train_time:17541ms step_avg:60.91ms
step:289/2330 train_time:17600ms step_avg:60.90ms
step:290/2330 train_time:17663ms step_avg:60.91ms
step:291/2330 train_time:17723ms step_avg:60.90ms
step:292/2330 train_time:17784ms step_avg:60.91ms
step:293/2330 train_time:17844ms step_avg:60.90ms
step:294/2330 train_time:17906ms step_avg:60.90ms
step:295/2330 train_time:17965ms step_avg:60.90ms
step:296/2330 train_time:18027ms step_avg:60.90ms
step:297/2330 train_time:18085ms step_avg:60.89ms
step:298/2330 train_time:18148ms step_avg:60.90ms
step:299/2330 train_time:18208ms step_avg:60.90ms
step:300/2330 train_time:18271ms step_avg:60.90ms
step:301/2330 train_time:18330ms step_avg:60.90ms
step:302/2330 train_time:18393ms step_avg:60.90ms
step:303/2330 train_time:18452ms step_avg:60.90ms
step:304/2330 train_time:18514ms step_avg:60.90ms
step:305/2330 train_time:18573ms step_avg:60.89ms
step:306/2330 train_time:18635ms step_avg:60.90ms
step:307/2330 train_time:18693ms step_avg:60.89ms
step:308/2330 train_time:18756ms step_avg:60.90ms
step:309/2330 train_time:18815ms step_avg:60.89ms
step:310/2330 train_time:18878ms step_avg:60.90ms
step:311/2330 train_time:18937ms step_avg:60.89ms
step:312/2330 train_time:18999ms step_avg:60.89ms
step:313/2330 train_time:19058ms step_avg:60.89ms
step:314/2330 train_time:19121ms step_avg:60.89ms
step:315/2330 train_time:19180ms step_avg:60.89ms
step:316/2330 train_time:19243ms step_avg:60.90ms
step:317/2330 train_time:19303ms step_avg:60.89ms
step:318/2330 train_time:19364ms step_avg:60.89ms
step:319/2330 train_time:19423ms step_avg:60.89ms
step:320/2330 train_time:19485ms step_avg:60.89ms
step:321/2330 train_time:19545ms step_avg:60.89ms
step:322/2330 train_time:19608ms step_avg:60.89ms
step:323/2330 train_time:19667ms step_avg:60.89ms
step:324/2330 train_time:19729ms step_avg:60.89ms
step:325/2330 train_time:19788ms step_avg:60.89ms
step:326/2330 train_time:19851ms step_avg:60.89ms
step:327/2330 train_time:19910ms step_avg:60.89ms
step:328/2330 train_time:19972ms step_avg:60.89ms
step:329/2330 train_time:20030ms step_avg:60.88ms
step:330/2330 train_time:20092ms step_avg:60.89ms
step:331/2330 train_time:20152ms step_avg:60.88ms
step:332/2330 train_time:20214ms step_avg:60.89ms
step:333/2330 train_time:20273ms step_avg:60.88ms
step:334/2330 train_time:20335ms step_avg:60.88ms
step:335/2330 train_time:20394ms step_avg:60.88ms
step:336/2330 train_time:20457ms step_avg:60.88ms
step:337/2330 train_time:20516ms step_avg:60.88ms
step:338/2330 train_time:20579ms step_avg:60.88ms
step:339/2330 train_time:20638ms step_avg:60.88ms
step:340/2330 train_time:20701ms step_avg:60.89ms
step:341/2330 train_time:20761ms step_avg:60.88ms
step:342/2330 train_time:20823ms step_avg:60.89ms
step:343/2330 train_time:20882ms step_avg:60.88ms
step:344/2330 train_time:20945ms step_avg:60.89ms
step:345/2330 train_time:21005ms step_avg:60.88ms
step:346/2330 train_time:21067ms step_avg:60.89ms
step:347/2330 train_time:21126ms step_avg:60.88ms
step:348/2330 train_time:21188ms step_avg:60.89ms
step:349/2330 train_time:21248ms step_avg:60.88ms
step:350/2330 train_time:21311ms step_avg:60.89ms
step:351/2330 train_time:21371ms step_avg:60.88ms
step:352/2330 train_time:21433ms step_avg:60.89ms
step:353/2330 train_time:21491ms step_avg:60.88ms
step:354/2330 train_time:21554ms step_avg:60.89ms
step:355/2330 train_time:21613ms step_avg:60.88ms
step:356/2330 train_time:21675ms step_avg:60.89ms
step:357/2330 train_time:21736ms step_avg:60.88ms
step:358/2330 train_time:21798ms step_avg:60.89ms
step:359/2330 train_time:21857ms step_avg:60.88ms
step:360/2330 train_time:21919ms step_avg:60.89ms
step:361/2330 train_time:21979ms step_avg:60.88ms
step:362/2330 train_time:22041ms step_avg:60.89ms
step:363/2330 train_time:22100ms step_avg:60.88ms
step:364/2330 train_time:22164ms step_avg:60.89ms
step:365/2330 train_time:22224ms step_avg:60.89ms
step:366/2330 train_time:22286ms step_avg:60.89ms
step:367/2330 train_time:22345ms step_avg:60.89ms
step:368/2330 train_time:22407ms step_avg:60.89ms
step:369/2330 train_time:22468ms step_avg:60.89ms
step:370/2330 train_time:22530ms step_avg:60.89ms
step:371/2330 train_time:22590ms step_avg:60.89ms
step:372/2330 train_time:22652ms step_avg:60.89ms
step:373/2330 train_time:22712ms step_avg:60.89ms
step:374/2330 train_time:22774ms step_avg:60.89ms
step:375/2330 train_time:22833ms step_avg:60.89ms
step:376/2330 train_time:22896ms step_avg:60.89ms
step:377/2330 train_time:22957ms step_avg:60.89ms
step:378/2330 train_time:23018ms step_avg:60.90ms
step:379/2330 train_time:23078ms step_avg:60.89ms
step:380/2330 train_time:23140ms step_avg:60.90ms
step:381/2330 train_time:23199ms step_avg:60.89ms
step:382/2330 train_time:23262ms step_avg:60.89ms
step:383/2330 train_time:23322ms step_avg:60.89ms
step:384/2330 train_time:23384ms step_avg:60.90ms
step:385/2330 train_time:23443ms step_avg:60.89ms
step:386/2330 train_time:23506ms step_avg:60.90ms
step:387/2330 train_time:23565ms step_avg:60.89ms
step:388/2330 train_time:23628ms step_avg:60.90ms
step:389/2330 train_time:23688ms step_avg:60.90ms
step:390/2330 train_time:23752ms step_avg:60.90ms
step:391/2330 train_time:23811ms step_avg:60.90ms
step:392/2330 train_time:23873ms step_avg:60.90ms
step:393/2330 train_time:23932ms step_avg:60.90ms
step:394/2330 train_time:23994ms step_avg:60.90ms
step:395/2330 train_time:24053ms step_avg:60.89ms
step:396/2330 train_time:24115ms step_avg:60.90ms
step:397/2330 train_time:24174ms step_avg:60.89ms
step:398/2330 train_time:24236ms step_avg:60.89ms
step:399/2330 train_time:24297ms step_avg:60.89ms
step:400/2330 train_time:24359ms step_avg:60.90ms
step:401/2330 train_time:24419ms step_avg:60.90ms
step:402/2330 train_time:24481ms step_avg:60.90ms
step:403/2330 train_time:24541ms step_avg:60.90ms
step:404/2330 train_time:24604ms step_avg:60.90ms
step:405/2330 train_time:24664ms step_avg:60.90ms
step:406/2330 train_time:24726ms step_avg:60.90ms
step:407/2330 train_time:24785ms step_avg:60.90ms
step:408/2330 train_time:24847ms step_avg:60.90ms
step:409/2330 train_time:24907ms step_avg:60.90ms
step:410/2330 train_time:24969ms step_avg:60.90ms
step:411/2330 train_time:25028ms step_avg:60.89ms
step:412/2330 train_time:25091ms step_avg:60.90ms
step:413/2330 train_time:25150ms step_avg:60.90ms
step:414/2330 train_time:25212ms step_avg:60.90ms
step:415/2330 train_time:25272ms step_avg:60.90ms
step:416/2330 train_time:25334ms step_avg:60.90ms
step:417/2330 train_time:25393ms step_avg:60.90ms
step:418/2330 train_time:25456ms step_avg:60.90ms
step:419/2330 train_time:25516ms step_avg:60.90ms
step:420/2330 train_time:25578ms step_avg:60.90ms
step:421/2330 train_time:25638ms step_avg:60.90ms
step:422/2330 train_time:25700ms step_avg:60.90ms
step:423/2330 train_time:25760ms step_avg:60.90ms
step:424/2330 train_time:25822ms step_avg:60.90ms
step:425/2330 train_time:25881ms step_avg:60.90ms
step:426/2330 train_time:25943ms step_avg:60.90ms
step:427/2330 train_time:26003ms step_avg:60.90ms
step:428/2330 train_time:26066ms step_avg:60.90ms
step:429/2330 train_time:26125ms step_avg:60.90ms
step:430/2330 train_time:26188ms step_avg:60.90ms
step:431/2330 train_time:26248ms step_avg:60.90ms
step:432/2330 train_time:26311ms step_avg:60.91ms
step:433/2330 train_time:26371ms step_avg:60.90ms
step:434/2330 train_time:26433ms step_avg:60.91ms
step:435/2330 train_time:26492ms step_avg:60.90ms
step:436/2330 train_time:26555ms step_avg:60.91ms
step:437/2330 train_time:26614ms step_avg:60.90ms
step:438/2330 train_time:26676ms step_avg:60.90ms
step:439/2330 train_time:26735ms step_avg:60.90ms
step:440/2330 train_time:26797ms step_avg:60.90ms
step:441/2330 train_time:26857ms step_avg:60.90ms
step:442/2330 train_time:26919ms step_avg:60.90ms
step:443/2330 train_time:26979ms step_avg:60.90ms
step:444/2330 train_time:27041ms step_avg:60.90ms
step:445/2330 train_time:27101ms step_avg:60.90ms
step:446/2330 train_time:27164ms step_avg:60.90ms
step:447/2330 train_time:27223ms step_avg:60.90ms
step:448/2330 train_time:27285ms step_avg:60.90ms
step:449/2330 train_time:27345ms step_avg:60.90ms
step:450/2330 train_time:27407ms step_avg:60.91ms
step:451/2330 train_time:27467ms step_avg:60.90ms
step:452/2330 train_time:27529ms step_avg:60.91ms
step:453/2330 train_time:27589ms step_avg:60.90ms
step:454/2330 train_time:27652ms step_avg:60.91ms
step:455/2330 train_time:27712ms step_avg:60.90ms
step:456/2330 train_time:27774ms step_avg:60.91ms
step:457/2330 train_time:27833ms step_avg:60.90ms
step:458/2330 train_time:27896ms step_avg:60.91ms
step:459/2330 train_time:27956ms step_avg:60.91ms
step:460/2330 train_time:28018ms step_avg:60.91ms
step:461/2330 train_time:28077ms step_avg:60.91ms
step:462/2330 train_time:28139ms step_avg:60.91ms
step:463/2330 train_time:28199ms step_avg:60.90ms
step:464/2330 train_time:28261ms step_avg:60.91ms
step:465/2330 train_time:28320ms step_avg:60.90ms
step:466/2330 train_time:28383ms step_avg:60.91ms
step:467/2330 train_time:28443ms step_avg:60.91ms
step:468/2330 train_time:28505ms step_avg:60.91ms
step:469/2330 train_time:28565ms step_avg:60.91ms
step:470/2330 train_time:28627ms step_avg:60.91ms
step:471/2330 train_time:28687ms step_avg:60.91ms
step:472/2330 train_time:28750ms step_avg:60.91ms
step:473/2330 train_time:28809ms step_avg:60.91ms
step:474/2330 train_time:28872ms step_avg:60.91ms
step:475/2330 train_time:28931ms step_avg:60.91ms
step:476/2330 train_time:28994ms step_avg:60.91ms
step:477/2330 train_time:29053ms step_avg:60.91ms
step:478/2330 train_time:29115ms step_avg:60.91ms
step:479/2330 train_time:29174ms step_avg:60.91ms
step:480/2330 train_time:29236ms step_avg:60.91ms
step:481/2330 train_time:29296ms step_avg:60.91ms
step:482/2330 train_time:29359ms step_avg:60.91ms
step:483/2330 train_time:29419ms step_avg:60.91ms
step:484/2330 train_time:29481ms step_avg:60.91ms
step:485/2330 train_time:29540ms step_avg:60.91ms
step:486/2330 train_time:29603ms step_avg:60.91ms
step:487/2330 train_time:29663ms step_avg:60.91ms
step:488/2330 train_time:29725ms step_avg:60.91ms
step:489/2330 train_time:29785ms step_avg:60.91ms
step:490/2330 train_time:29847ms step_avg:60.91ms
step:491/2330 train_time:29907ms step_avg:60.91ms
step:492/2330 train_time:29970ms step_avg:60.91ms
step:493/2330 train_time:30030ms step_avg:60.91ms
step:494/2330 train_time:30092ms step_avg:60.92ms
step:495/2330 train_time:30152ms step_avg:60.91ms
step:496/2330 train_time:30214ms step_avg:60.91ms
step:497/2330 train_time:30273ms step_avg:60.91ms
step:498/2330 train_time:30335ms step_avg:60.91ms
step:499/2330 train_time:30394ms step_avg:60.91ms
step:500/2330 train_time:30456ms step_avg:60.91ms
step:500/2330 val_loss:3.8516 train_time:30521ms step_avg:61.04ms
step:501/2330 train_time:30544ms step_avg:60.97ms
step:502/2330 train_time:30584ms step_avg:60.92ms
step:503/2330 train_time:30648ms step_avg:60.93ms
step:504/2330 train_time:30714ms step_avg:60.94ms
step:505/2330 train_time:30772ms step_avg:60.94ms
step:506/2330 train_time:30834ms step_avg:60.94ms
step:507/2330 train_time:30893ms step_avg:60.93ms
step:508/2330 train_time:30955ms step_avg:60.94ms
step:509/2330 train_time:31014ms step_avg:60.93ms
step:510/2330 train_time:31076ms step_avg:60.93ms
step:511/2330 train_time:31135ms step_avg:60.93ms
step:512/2330 train_time:31197ms step_avg:60.93ms
step:513/2330 train_time:31255ms step_avg:60.93ms
step:514/2330 train_time:31318ms step_avg:60.93ms
step:515/2330 train_time:31376ms step_avg:60.92ms
step:516/2330 train_time:31439ms step_avg:60.93ms
step:517/2330 train_time:31499ms step_avg:60.93ms
step:518/2330 train_time:31564ms step_avg:60.93ms
step:519/2330 train_time:31624ms step_avg:60.93ms
step:520/2330 train_time:31687ms step_avg:60.94ms
step:521/2330 train_time:31747ms step_avg:60.94ms
step:522/2330 train_time:31810ms step_avg:60.94ms
step:523/2330 train_time:31869ms step_avg:60.94ms
step:524/2330 train_time:31931ms step_avg:60.94ms
step:525/2330 train_time:31990ms step_avg:60.93ms
step:526/2330 train_time:32052ms step_avg:60.93ms
step:527/2330 train_time:32111ms step_avg:60.93ms
step:528/2330 train_time:32173ms step_avg:60.93ms
step:529/2330 train_time:32231ms step_avg:60.93ms
step:530/2330 train_time:32293ms step_avg:60.93ms
step:531/2330 train_time:32353ms step_avg:60.93ms
step:532/2330 train_time:32415ms step_avg:60.93ms
step:533/2330 train_time:32474ms step_avg:60.93ms
step:534/2330 train_time:32538ms step_avg:60.93ms
step:535/2330 train_time:32598ms step_avg:60.93ms
step:536/2330 train_time:32661ms step_avg:60.94ms
step:537/2330 train_time:32721ms step_avg:60.93ms
step:538/2330 train_time:32784ms step_avg:60.94ms
step:539/2330 train_time:32843ms step_avg:60.93ms
step:540/2330 train_time:32906ms step_avg:60.94ms
step:541/2330 train_time:32966ms step_avg:60.93ms
step:542/2330 train_time:33029ms step_avg:60.94ms
step:543/2330 train_time:33089ms step_avg:60.94ms
step:544/2330 train_time:33151ms step_avg:60.94ms
step:545/2330 train_time:33210ms step_avg:60.94ms
step:546/2330 train_time:33272ms step_avg:60.94ms
step:547/2330 train_time:33331ms step_avg:60.94ms
step:548/2330 train_time:33394ms step_avg:60.94ms
step:549/2330 train_time:33453ms step_avg:60.93ms
step:550/2330 train_time:33515ms step_avg:60.94ms
step:551/2330 train_time:33575ms step_avg:60.93ms
step:552/2330 train_time:33638ms step_avg:60.94ms
step:553/2330 train_time:33697ms step_avg:60.94ms
step:554/2330 train_time:33760ms step_avg:60.94ms
step:555/2330 train_time:33820ms step_avg:60.94ms
step:556/2330 train_time:33883ms step_avg:60.94ms
step:557/2330 train_time:33943ms step_avg:60.94ms
step:558/2330 train_time:34006ms step_avg:60.94ms
step:559/2330 train_time:34066ms step_avg:60.94ms
step:560/2330 train_time:34129ms step_avg:60.94ms
step:561/2330 train_time:34188ms step_avg:60.94ms
step:562/2330 train_time:34251ms step_avg:60.94ms
step:563/2330 train_time:34310ms step_avg:60.94ms
step:564/2330 train_time:34372ms step_avg:60.94ms
step:565/2330 train_time:34430ms step_avg:60.94ms
step:566/2330 train_time:34493ms step_avg:60.94ms
step:567/2330 train_time:34552ms step_avg:60.94ms
step:568/2330 train_time:34614ms step_avg:60.94ms
step:569/2330 train_time:34674ms step_avg:60.94ms
step:570/2330 train_time:34737ms step_avg:60.94ms
step:571/2330 train_time:34797ms step_avg:60.94ms
step:572/2330 train_time:34860ms step_avg:60.94ms
step:573/2330 train_time:34919ms step_avg:60.94ms
step:574/2330 train_time:34982ms step_avg:60.94ms
step:575/2330 train_time:35042ms step_avg:60.94ms
step:576/2330 train_time:35104ms step_avg:60.95ms
step:577/2330 train_time:35164ms step_avg:60.94ms
step:578/2330 train_time:35227ms step_avg:60.95ms
step:579/2330 train_time:35287ms step_avg:60.94ms
step:580/2330 train_time:35350ms step_avg:60.95ms
step:581/2330 train_time:35409ms step_avg:60.95ms
step:582/2330 train_time:35472ms step_avg:60.95ms
step:583/2330 train_time:35531ms step_avg:60.95ms
step:584/2330 train_time:35594ms step_avg:60.95ms
step:585/2330 train_time:35653ms step_avg:60.95ms
step:586/2330 train_time:35715ms step_avg:60.95ms
step:587/2330 train_time:35775ms step_avg:60.95ms
step:588/2330 train_time:35837ms step_avg:60.95ms
step:589/2330 train_time:35897ms step_avg:60.95ms
step:590/2330 train_time:35960ms step_avg:60.95ms
step:591/2330 train_time:36020ms step_avg:60.95ms
step:592/2330 train_time:36083ms step_avg:60.95ms
step:593/2330 train_time:36143ms step_avg:60.95ms
step:594/2330 train_time:36205ms step_avg:60.95ms
step:595/2330 train_time:36264ms step_avg:60.95ms
step:596/2330 train_time:36326ms step_avg:60.95ms
step:597/2330 train_time:36386ms step_avg:60.95ms
step:598/2330 train_time:36450ms step_avg:60.95ms
step:599/2330 train_time:36509ms step_avg:60.95ms
step:600/2330 train_time:36571ms step_avg:60.95ms
step:601/2330 train_time:36630ms step_avg:60.95ms
step:602/2330 train_time:36694ms step_avg:60.95ms
step:603/2330 train_time:36752ms step_avg:60.95ms
step:604/2330 train_time:36814ms step_avg:60.95ms
step:605/2330 train_time:36874ms step_avg:60.95ms
step:606/2330 train_time:36936ms step_avg:60.95ms
step:607/2330 train_time:36996ms step_avg:60.95ms
step:608/2330 train_time:37059ms step_avg:60.95ms
step:609/2330 train_time:37118ms step_avg:60.95ms
step:610/2330 train_time:37181ms step_avg:60.95ms
step:611/2330 train_time:37241ms step_avg:60.95ms
step:612/2330 train_time:37303ms step_avg:60.95ms
step:613/2330 train_time:37364ms step_avg:60.95ms
step:614/2330 train_time:37426ms step_avg:60.95ms
step:615/2330 train_time:37486ms step_avg:60.95ms
step:616/2330 train_time:37550ms step_avg:60.96ms
step:617/2330 train_time:37609ms step_avg:60.96ms
step:618/2330 train_time:37672ms step_avg:60.96ms
step:619/2330 train_time:37731ms step_avg:60.95ms
step:620/2330 train_time:37793ms step_avg:60.96ms
step:621/2330 train_time:37852ms step_avg:60.95ms
step:622/2330 train_time:37914ms step_avg:60.95ms
step:623/2330 train_time:37974ms step_avg:60.95ms
step:624/2330 train_time:38036ms step_avg:60.96ms
step:625/2330 train_time:38095ms step_avg:60.95ms
step:626/2330 train_time:38158ms step_avg:60.96ms
step:627/2330 train_time:38218ms step_avg:60.95ms
step:628/2330 train_time:38281ms step_avg:60.96ms
step:629/2330 train_time:38340ms step_avg:60.95ms
step:630/2330 train_time:38403ms step_avg:60.96ms
step:631/2330 train_time:38463ms step_avg:60.96ms
step:632/2330 train_time:38525ms step_avg:60.96ms
step:633/2330 train_time:38585ms step_avg:60.96ms
step:634/2330 train_time:38648ms step_avg:60.96ms
step:635/2330 train_time:38708ms step_avg:60.96ms
step:636/2330 train_time:38771ms step_avg:60.96ms
step:637/2330 train_time:38830ms step_avg:60.96ms
step:638/2330 train_time:38894ms step_avg:60.96ms
step:639/2330 train_time:38952ms step_avg:60.96ms
step:640/2330 train_time:39014ms step_avg:60.96ms
step:641/2330 train_time:39074ms step_avg:60.96ms
step:642/2330 train_time:39137ms step_avg:60.96ms
step:643/2330 train_time:39197ms step_avg:60.96ms
step:644/2330 train_time:39259ms step_avg:60.96ms
step:645/2330 train_time:39319ms step_avg:60.96ms
step:646/2330 train_time:39381ms step_avg:60.96ms
step:647/2330 train_time:39442ms step_avg:60.96ms
step:648/2330 train_time:39504ms step_avg:60.96ms
step:649/2330 train_time:39563ms step_avg:60.96ms
step:650/2330 train_time:39625ms step_avg:60.96ms
step:651/2330 train_time:39685ms step_avg:60.96ms
step:652/2330 train_time:39748ms step_avg:60.96ms
step:653/2330 train_time:39808ms step_avg:60.96ms
step:654/2330 train_time:39870ms step_avg:60.96ms
step:655/2330 train_time:39930ms step_avg:60.96ms
step:656/2330 train_time:39993ms step_avg:60.97ms
step:657/2330 train_time:40053ms step_avg:60.96ms
step:658/2330 train_time:40115ms step_avg:60.97ms
step:659/2330 train_time:40175ms step_avg:60.96ms
step:660/2330 train_time:40237ms step_avg:60.97ms
step:661/2330 train_time:40297ms step_avg:60.96ms
step:662/2330 train_time:40359ms step_avg:60.97ms
step:663/2330 train_time:40419ms step_avg:60.96ms
step:664/2330 train_time:40481ms step_avg:60.97ms
step:665/2330 train_time:40541ms step_avg:60.96ms
step:666/2330 train_time:40604ms step_avg:60.97ms
step:667/2330 train_time:40663ms step_avg:60.96ms
step:668/2330 train_time:40726ms step_avg:60.97ms
step:669/2330 train_time:40785ms step_avg:60.96ms
step:670/2330 train_time:40848ms step_avg:60.97ms
step:671/2330 train_time:40908ms step_avg:60.97ms
step:672/2330 train_time:40971ms step_avg:60.97ms
step:673/2330 train_time:41030ms step_avg:60.97ms
step:674/2330 train_time:41093ms step_avg:60.97ms
step:675/2330 train_time:41152ms step_avg:60.97ms
step:676/2330 train_time:41214ms step_avg:60.97ms
step:677/2330 train_time:41274ms step_avg:60.97ms
step:678/2330 train_time:41336ms step_avg:60.97ms
step:679/2330 train_time:41396ms step_avg:60.97ms
step:680/2330 train_time:41458ms step_avg:60.97ms
step:681/2330 train_time:41518ms step_avg:60.97ms
step:682/2330 train_time:41580ms step_avg:60.97ms
step:683/2330 train_time:41640ms step_avg:60.97ms
step:684/2330 train_time:41703ms step_avg:60.97ms
step:685/2330 train_time:41762ms step_avg:60.97ms
step:686/2330 train_time:41824ms step_avg:60.97ms
step:687/2330 train_time:41884ms step_avg:60.97ms
step:688/2330 train_time:41947ms step_avg:60.97ms
step:689/2330 train_time:42007ms step_avg:60.97ms
step:690/2330 train_time:42069ms step_avg:60.97ms
step:691/2330 train_time:42129ms step_avg:60.97ms
step:692/2330 train_time:42192ms step_avg:60.97ms
step:693/2330 train_time:42251ms step_avg:60.97ms
step:694/2330 train_time:42312ms step_avg:60.97ms
step:695/2330 train_time:42372ms step_avg:60.97ms
step:696/2330 train_time:42435ms step_avg:60.97ms
step:697/2330 train_time:42495ms step_avg:60.97ms
step:698/2330 train_time:42557ms step_avg:60.97ms
step:699/2330 train_time:42617ms step_avg:60.97ms
step:700/2330 train_time:42679ms step_avg:60.97ms
step:701/2330 train_time:42740ms step_avg:60.97ms
step:702/2330 train_time:42803ms step_avg:60.97ms
step:703/2330 train_time:42863ms step_avg:60.97ms
step:704/2330 train_time:42925ms step_avg:60.97ms
step:705/2330 train_time:42984ms step_avg:60.97ms
step:706/2330 train_time:43047ms step_avg:60.97ms
step:707/2330 train_time:43106ms step_avg:60.97ms
step:708/2330 train_time:43169ms step_avg:60.97ms
step:709/2330 train_time:43229ms step_avg:60.97ms
step:710/2330 train_time:43292ms step_avg:60.97ms
step:711/2330 train_time:43351ms step_avg:60.97ms
step:712/2330 train_time:43413ms step_avg:60.97ms
step:713/2330 train_time:43472ms step_avg:60.97ms
step:714/2330 train_time:43534ms step_avg:60.97ms
step:715/2330 train_time:43594ms step_avg:60.97ms
step:716/2330 train_time:43657ms step_avg:60.97ms
step:717/2330 train_time:43717ms step_avg:60.97ms
step:718/2330 train_time:43780ms step_avg:60.97ms
step:719/2330 train_time:43839ms step_avg:60.97ms
step:720/2330 train_time:43902ms step_avg:60.98ms
step:721/2330 train_time:43962ms step_avg:60.97ms
step:722/2330 train_time:44024ms step_avg:60.98ms
step:723/2330 train_time:44084ms step_avg:60.97ms
step:724/2330 train_time:44147ms step_avg:60.98ms
step:725/2330 train_time:44207ms step_avg:60.97ms
step:726/2330 train_time:44270ms step_avg:60.98ms
step:727/2330 train_time:44330ms step_avg:60.98ms
step:728/2330 train_time:44393ms step_avg:60.98ms
step:729/2330 train_time:44452ms step_avg:60.98ms
step:730/2330 train_time:44514ms step_avg:60.98ms
step:731/2330 train_time:44573ms step_avg:60.98ms
step:732/2330 train_time:44636ms step_avg:60.98ms
step:733/2330 train_time:44696ms step_avg:60.98ms
step:734/2330 train_time:44758ms step_avg:60.98ms
step:735/2330 train_time:44817ms step_avg:60.98ms
step:736/2330 train_time:44880ms step_avg:60.98ms
step:737/2330 train_time:44940ms step_avg:60.98ms
step:738/2330 train_time:45003ms step_avg:60.98ms
step:739/2330 train_time:45063ms step_avg:60.98ms
step:740/2330 train_time:45125ms step_avg:60.98ms
step:741/2330 train_time:45186ms step_avg:60.98ms
step:742/2330 train_time:45249ms step_avg:60.98ms
step:743/2330 train_time:45309ms step_avg:60.98ms
step:744/2330 train_time:45372ms step_avg:60.98ms
step:745/2330 train_time:45431ms step_avg:60.98ms
step:746/2330 train_time:45494ms step_avg:60.98ms
step:747/2330 train_time:45553ms step_avg:60.98ms
step:748/2330 train_time:45615ms step_avg:60.98ms
step:749/2330 train_time:45674ms step_avg:60.98ms
step:750/2330 train_time:45736ms step_avg:60.98ms
step:750/2330 val_loss:3.7144 train_time:45801ms step_avg:61.07ms
step:751/2330 train_time:45823ms step_avg:61.02ms
step:752/2330 train_time:45860ms step_avg:60.98ms
step:753/2330 train_time:45923ms step_avg:60.99ms
step:754/2330 train_time:45988ms step_avg:60.99ms
step:755/2330 train_time:46047ms step_avg:60.99ms
step:756/2330 train_time:46109ms step_avg:60.99ms
step:757/2330 train_time:46168ms step_avg:60.99ms
step:758/2330 train_time:46230ms step_avg:60.99ms
step:759/2330 train_time:46289ms step_avg:60.99ms
step:760/2330 train_time:46350ms step_avg:60.99ms
step:761/2330 train_time:46409ms step_avg:60.98ms
step:762/2330 train_time:46470ms step_avg:60.98ms
step:763/2330 train_time:46529ms step_avg:60.98ms
step:764/2330 train_time:46591ms step_avg:60.98ms
step:765/2330 train_time:46649ms step_avg:60.98ms
step:766/2330 train_time:46712ms step_avg:60.98ms
step:767/2330 train_time:46774ms step_avg:60.98ms
step:768/2330 train_time:46839ms step_avg:60.99ms
step:769/2330 train_time:46901ms step_avg:60.99ms
step:770/2330 train_time:46965ms step_avg:60.99ms
step:771/2330 train_time:47026ms step_avg:60.99ms
step:772/2330 train_time:47089ms step_avg:61.00ms
step:773/2330 train_time:47148ms step_avg:60.99ms
step:774/2330 train_time:47211ms step_avg:61.00ms
step:775/2330 train_time:47271ms step_avg:60.99ms
step:776/2330 train_time:47333ms step_avg:61.00ms
step:777/2330 train_time:47393ms step_avg:60.99ms
step:778/2330 train_time:47455ms step_avg:61.00ms
step:779/2330 train_time:47515ms step_avg:60.99ms
step:780/2330 train_time:47577ms step_avg:61.00ms
step:781/2330 train_time:47637ms step_avg:60.99ms
step:782/2330 train_time:47700ms step_avg:61.00ms
step:783/2330 train_time:47759ms step_avg:61.00ms
step:784/2330 train_time:47823ms step_avg:61.00ms
step:785/2330 train_time:47883ms step_avg:61.00ms
step:786/2330 train_time:47947ms step_avg:61.00ms
step:787/2330 train_time:48008ms step_avg:61.00ms
step:788/2330 train_time:48070ms step_avg:61.00ms
step:789/2330 train_time:48130ms step_avg:61.00ms
step:790/2330 train_time:48194ms step_avg:61.00ms
step:791/2330 train_time:48253ms step_avg:61.00ms
step:792/2330 train_time:48316ms step_avg:61.01ms
step:793/2330 train_time:48376ms step_avg:61.00ms
step:794/2330 train_time:48438ms step_avg:61.01ms
step:795/2330 train_time:48497ms step_avg:61.00ms
step:796/2330 train_time:48560ms step_avg:61.00ms
step:797/2330 train_time:48620ms step_avg:61.00ms
step:798/2330 train_time:48683ms step_avg:61.01ms
step:799/2330 train_time:48743ms step_avg:61.00ms
step:800/2330 train_time:48806ms step_avg:61.01ms
step:801/2330 train_time:48866ms step_avg:61.01ms
step:802/2330 train_time:48930ms step_avg:61.01ms
step:803/2330 train_time:48990ms step_avg:61.01ms
step:804/2330 train_time:49053ms step_avg:61.01ms
step:805/2330 train_time:49113ms step_avg:61.01ms
step:806/2330 train_time:49176ms step_avg:61.01ms
step:807/2330 train_time:49236ms step_avg:61.01ms
step:808/2330 train_time:49299ms step_avg:61.01ms
step:809/2330 train_time:49358ms step_avg:61.01ms
step:810/2330 train_time:49422ms step_avg:61.01ms
step:811/2330 train_time:49481ms step_avg:61.01ms
step:812/2330 train_time:49544ms step_avg:61.01ms
step:813/2330 train_time:49604ms step_avg:61.01ms
step:814/2330 train_time:49666ms step_avg:61.02ms
step:815/2330 train_time:49726ms step_avg:61.01ms
step:816/2330 train_time:49790ms step_avg:61.02ms
step:817/2330 train_time:49850ms step_avg:61.02ms
step:818/2330 train_time:49913ms step_avg:61.02ms
step:819/2330 train_time:49973ms step_avg:61.02ms
step:820/2330 train_time:50036ms step_avg:61.02ms
step:821/2330 train_time:50096ms step_avg:61.02ms
step:822/2330 train_time:50159ms step_avg:61.02ms
step:823/2330 train_time:50220ms step_avg:61.02ms
step:824/2330 train_time:50282ms step_avg:61.02ms
step:825/2330 train_time:50342ms step_avg:61.02ms
step:826/2330 train_time:50406ms step_avg:61.02ms
step:827/2330 train_time:50465ms step_avg:61.02ms
step:828/2330 train_time:50529ms step_avg:61.02ms
step:829/2330 train_time:50588ms step_avg:61.02ms
step:830/2330 train_time:50651ms step_avg:61.02ms
step:831/2330 train_time:50710ms step_avg:61.02ms
step:832/2330 train_time:50773ms step_avg:61.03ms
step:833/2330 train_time:50833ms step_avg:61.02ms
step:834/2330 train_time:50896ms step_avg:61.03ms
step:835/2330 train_time:50956ms step_avg:61.03ms
step:836/2330 train_time:51019ms step_avg:61.03ms
step:837/2330 train_time:51079ms step_avg:61.03ms
step:838/2330 train_time:51142ms step_avg:61.03ms
step:839/2330 train_time:51202ms step_avg:61.03ms
step:840/2330 train_time:51265ms step_avg:61.03ms
step:841/2330 train_time:51324ms step_avg:61.03ms
step:842/2330 train_time:51388ms step_avg:61.03ms
step:843/2330 train_time:51448ms step_avg:61.03ms
step:844/2330 train_time:51511ms step_avg:61.03ms
step:845/2330 train_time:51571ms step_avg:61.03ms
step:846/2330 train_time:51634ms step_avg:61.03ms
step:847/2330 train_time:51694ms step_avg:61.03ms
step:848/2330 train_time:51756ms step_avg:61.03ms
step:849/2330 train_time:51817ms step_avg:61.03ms
step:850/2330 train_time:51880ms step_avg:61.03ms
step:851/2330 train_time:51939ms step_avg:61.03ms
step:852/2330 train_time:52003ms step_avg:61.04ms
step:853/2330 train_time:52063ms step_avg:61.03ms
step:854/2330 train_time:52125ms step_avg:61.04ms
step:855/2330 train_time:52185ms step_avg:61.04ms
step:856/2330 train_time:52248ms step_avg:61.04ms
step:857/2330 train_time:52308ms step_avg:61.04ms
step:858/2330 train_time:52371ms step_avg:61.04ms
step:859/2330 train_time:52431ms step_avg:61.04ms
step:860/2330 train_time:52494ms step_avg:61.04ms
step:861/2330 train_time:52554ms step_avg:61.04ms
step:862/2330 train_time:52616ms step_avg:61.04ms
step:863/2330 train_time:52677ms step_avg:61.04ms
step:864/2330 train_time:52740ms step_avg:61.04ms
step:865/2330 train_time:52799ms step_avg:61.04ms
step:866/2330 train_time:52862ms step_avg:61.04ms
step:867/2330 train_time:52923ms step_avg:61.04ms
step:868/2330 train_time:52986ms step_avg:61.04ms
step:869/2330 train_time:53046ms step_avg:61.04ms
step:870/2330 train_time:53109ms step_avg:61.04ms
step:871/2330 train_time:53169ms step_avg:61.04ms
step:872/2330 train_time:53232ms step_avg:61.05ms
step:873/2330 train_time:53292ms step_avg:61.04ms
step:874/2330 train_time:53355ms step_avg:61.05ms
step:875/2330 train_time:53415ms step_avg:61.05ms
step:876/2330 train_time:53478ms step_avg:61.05ms
step:877/2330 train_time:53538ms step_avg:61.05ms
step:878/2330 train_time:53601ms step_avg:61.05ms
step:879/2330 train_time:53660ms step_avg:61.05ms
step:880/2330 train_time:53723ms step_avg:61.05ms
step:881/2330 train_time:53783ms step_avg:61.05ms
step:882/2330 train_time:53846ms step_avg:61.05ms
step:883/2330 train_time:53906ms step_avg:61.05ms
step:884/2330 train_time:53969ms step_avg:61.05ms
step:885/2330 train_time:54030ms step_avg:61.05ms
step:886/2330 train_time:54092ms step_avg:61.05ms
step:887/2330 train_time:54152ms step_avg:61.05ms
step:888/2330 train_time:54215ms step_avg:61.05ms
step:889/2330 train_time:54275ms step_avg:61.05ms
step:890/2330 train_time:54338ms step_avg:61.05ms
step:891/2330 train_time:54398ms step_avg:61.05ms
step:892/2330 train_time:54461ms step_avg:61.05ms
step:893/2330 train_time:54520ms step_avg:61.05ms
step:894/2330 train_time:54583ms step_avg:61.05ms
step:895/2330 train_time:54643ms step_avg:61.05ms
step:896/2330 train_time:54706ms step_avg:61.06ms
step:897/2330 train_time:54766ms step_avg:61.05ms
step:898/2330 train_time:54830ms step_avg:61.06ms
step:899/2330 train_time:54890ms step_avg:61.06ms
step:900/2330 train_time:54953ms step_avg:61.06ms
step:901/2330 train_time:55013ms step_avg:61.06ms
step:902/2330 train_time:55076ms step_avg:61.06ms
step:903/2330 train_time:55136ms step_avg:61.06ms
step:904/2330 train_time:55199ms step_avg:61.06ms
step:905/2330 train_time:55260ms step_avg:61.06ms
step:906/2330 train_time:55323ms step_avg:61.06ms
step:907/2330 train_time:55383ms step_avg:61.06ms
step:908/2330 train_time:55447ms step_avg:61.06ms
step:909/2330 train_time:55507ms step_avg:61.06ms
step:910/2330 train_time:55570ms step_avg:61.07ms
step:911/2330 train_time:55629ms step_avg:61.06ms
step:912/2330 train_time:55692ms step_avg:61.07ms
step:913/2330 train_time:55752ms step_avg:61.06ms
step:914/2330 train_time:55815ms step_avg:61.07ms
step:915/2330 train_time:55875ms step_avg:61.07ms
step:916/2330 train_time:55938ms step_avg:61.07ms
step:917/2330 train_time:55998ms step_avg:61.07ms
step:918/2330 train_time:56061ms step_avg:61.07ms
step:919/2330 train_time:56121ms step_avg:61.07ms
step:920/2330 train_time:56184ms step_avg:61.07ms
step:921/2330 train_time:56244ms step_avg:61.07ms
step:922/2330 train_time:56307ms step_avg:61.07ms
step:923/2330 train_time:56367ms step_avg:61.07ms
step:924/2330 train_time:56431ms step_avg:61.07ms
step:925/2330 train_time:56490ms step_avg:61.07ms
step:926/2330 train_time:56553ms step_avg:61.07ms
step:927/2330 train_time:56613ms step_avg:61.07ms
step:928/2330 train_time:56676ms step_avg:61.07ms
step:929/2330 train_time:56736ms step_avg:61.07ms
step:930/2330 train_time:56799ms step_avg:61.07ms
step:931/2330 train_time:56859ms step_avg:61.07ms
step:932/2330 train_time:56922ms step_avg:61.07ms
step:933/2330 train_time:56982ms step_avg:61.07ms
step:934/2330 train_time:57045ms step_avg:61.08ms
step:935/2330 train_time:57104ms step_avg:61.07ms
step:936/2330 train_time:57167ms step_avg:61.08ms
step:937/2330 train_time:57227ms step_avg:61.08ms
step:938/2330 train_time:57290ms step_avg:61.08ms
step:939/2330 train_time:57350ms step_avg:61.08ms
step:940/2330 train_time:57413ms step_avg:61.08ms
step:941/2330 train_time:57473ms step_avg:61.08ms
step:942/2330 train_time:57535ms step_avg:61.08ms
step:943/2330 train_time:57595ms step_avg:61.08ms
step:944/2330 train_time:57658ms step_avg:61.08ms
step:945/2330 train_time:57718ms step_avg:61.08ms
step:946/2330 train_time:57781ms step_avg:61.08ms
step:947/2330 train_time:57841ms step_avg:61.08ms
step:948/2330 train_time:57903ms step_avg:61.08ms
step:949/2330 train_time:57963ms step_avg:61.08ms
step:950/2330 train_time:58027ms step_avg:61.08ms
step:951/2330 train_time:58087ms step_avg:61.08ms
step:952/2330 train_time:58150ms step_avg:61.08ms
step:953/2330 train_time:58210ms step_avg:61.08ms
step:954/2330 train_time:58273ms step_avg:61.08ms
step:955/2330 train_time:58333ms step_avg:61.08ms
step:956/2330 train_time:58396ms step_avg:61.08ms
step:957/2330 train_time:58456ms step_avg:61.08ms
step:958/2330 train_time:58519ms step_avg:61.08ms
step:959/2330 train_time:58579ms step_avg:61.08ms
step:960/2330 train_time:58642ms step_avg:61.09ms
step:961/2330 train_time:58702ms step_avg:61.08ms
step:962/2330 train_time:58765ms step_avg:61.09ms
step:963/2330 train_time:58825ms step_avg:61.09ms
step:964/2330 train_time:58888ms step_avg:61.09ms
step:965/2330 train_time:58947ms step_avg:61.09ms
step:966/2330 train_time:59010ms step_avg:61.09ms
step:967/2330 train_time:59070ms step_avg:61.09ms
step:968/2330 train_time:59133ms step_avg:61.09ms
step:969/2330 train_time:59193ms step_avg:61.09ms
step:970/2330 train_time:59256ms step_avg:61.09ms
step:971/2330 train_time:59316ms step_avg:61.09ms
step:972/2330 train_time:59379ms step_avg:61.09ms
step:973/2330 train_time:59439ms step_avg:61.09ms
step:974/2330 train_time:59501ms step_avg:61.09ms
step:975/2330 train_time:59561ms step_avg:61.09ms
step:976/2330 train_time:59624ms step_avg:61.09ms
step:977/2330 train_time:59685ms step_avg:61.09ms
step:978/2330 train_time:59748ms step_avg:61.09ms
step:979/2330 train_time:59808ms step_avg:61.09ms
step:980/2330 train_time:59871ms step_avg:61.09ms
step:981/2330 train_time:59930ms step_avg:61.09ms
step:982/2330 train_time:59993ms step_avg:61.09ms
step:983/2330 train_time:60053ms step_avg:61.09ms
step:984/2330 train_time:60116ms step_avg:61.09ms
step:985/2330 train_time:60176ms step_avg:61.09ms
step:986/2330 train_time:60239ms step_avg:61.09ms
step:987/2330 train_time:60298ms step_avg:61.09ms
step:988/2330 train_time:60362ms step_avg:61.09ms
step:989/2330 train_time:60421ms step_avg:61.09ms
step:990/2330 train_time:60484ms step_avg:61.10ms
step:991/2330 train_time:60544ms step_avg:61.09ms
step:992/2330 train_time:60607ms step_avg:61.10ms
step:993/2330 train_time:60667ms step_avg:61.10ms
step:994/2330 train_time:60731ms step_avg:61.10ms
step:995/2330 train_time:60791ms step_avg:61.10ms
step:996/2330 train_time:60853ms step_avg:61.10ms
step:997/2330 train_time:60913ms step_avg:61.10ms
step:998/2330 train_time:60976ms step_avg:61.10ms
step:999/2330 train_time:61035ms step_avg:61.10ms
step:1000/2330 train_time:61098ms step_avg:61.10ms
step:1000/2330 val_loss:3.5981 train_time:61163ms step_avg:61.16ms
step:1001/2330 train_time:61186ms step_avg:61.12ms
step:1002/2330 train_time:61224ms step_avg:61.10ms
step:1003/2330 train_time:61289ms step_avg:61.11ms
step:1004/2330 train_time:61357ms step_avg:61.11ms
step:1005/2330 train_time:61419ms step_avg:61.11ms
step:1006/2330 train_time:61483ms step_avg:61.12ms
step:1007/2330 train_time:61543ms step_avg:61.11ms
step:1008/2330 train_time:61605ms step_avg:61.12ms
step:1009/2330 train_time:61664ms step_avg:61.11ms
step:1010/2330 train_time:61727ms step_avg:61.12ms
step:1011/2330 train_time:61786ms step_avg:61.11ms
step:1012/2330 train_time:61848ms step_avg:61.12ms
step:1013/2330 train_time:61907ms step_avg:61.11ms
step:1014/2330 train_time:61970ms step_avg:61.11ms
step:1015/2330 train_time:62028ms step_avg:61.11ms
step:1016/2330 train_time:62093ms step_avg:61.11ms
step:1017/2330 train_time:62154ms step_avg:61.11ms
step:1018/2330 train_time:62218ms step_avg:61.12ms
step:1019/2330 train_time:62281ms step_avg:61.12ms
step:1020/2330 train_time:62347ms step_avg:61.12ms
step:1021/2330 train_time:62408ms step_avg:61.12ms
step:1022/2330 train_time:62472ms step_avg:61.13ms
step:1023/2330 train_time:62533ms step_avg:61.13ms
step:1024/2330 train_time:62596ms step_avg:61.13ms
step:1025/2330 train_time:62655ms step_avg:61.13ms
step:1026/2330 train_time:62718ms step_avg:61.13ms
step:1027/2330 train_time:62778ms step_avg:61.13ms
step:1028/2330 train_time:62841ms step_avg:61.13ms
step:1029/2330 train_time:62900ms step_avg:61.13ms
step:1030/2330 train_time:62963ms step_avg:61.13ms
step:1031/2330 train_time:63022ms step_avg:61.13ms
step:1032/2330 train_time:63086ms step_avg:61.13ms
step:1033/2330 train_time:63146ms step_avg:61.13ms
step:1034/2330 train_time:63209ms step_avg:61.13ms
step:1035/2330 train_time:63270ms step_avg:61.13ms
step:1036/2330 train_time:63334ms step_avg:61.13ms
step:1037/2330 train_time:63394ms step_avg:61.13ms
step:1038/2330 train_time:63457ms step_avg:61.13ms
step:1039/2330 train_time:63518ms step_avg:61.13ms
step:1040/2330 train_time:63581ms step_avg:61.14ms
step:1041/2330 train_time:63641ms step_avg:61.13ms
step:1042/2330 train_time:63705ms step_avg:61.14ms
step:1043/2330 train_time:63764ms step_avg:61.14ms
step:1044/2330 train_time:63827ms step_avg:61.14ms
step:1045/2330 train_time:63886ms step_avg:61.14ms
step:1046/2330 train_time:63949ms step_avg:61.14ms
step:1047/2330 train_time:64008ms step_avg:61.14ms
step:1048/2330 train_time:64071ms step_avg:61.14ms
step:1049/2330 train_time:64131ms step_avg:61.14ms
step:1050/2330 train_time:64194ms step_avg:61.14ms
step:1051/2330 train_time:64254ms step_avg:61.14ms
step:1052/2330 train_time:64318ms step_avg:61.14ms
step:1053/2330 train_time:64378ms step_avg:61.14ms
step:1054/2330 train_time:64442ms step_avg:61.14ms
step:1055/2330 train_time:64502ms step_avg:61.14ms
step:1056/2330 train_time:64567ms step_avg:61.14ms
step:1057/2330 train_time:64627ms step_avg:61.14ms
step:1058/2330 train_time:64690ms step_avg:61.14ms
step:1059/2330 train_time:64749ms step_avg:61.14ms
step:1060/2330 train_time:64812ms step_avg:61.14ms
step:1061/2330 train_time:64872ms step_avg:61.14ms
step:1062/2330 train_time:64935ms step_avg:61.14ms
step:1063/2330 train_time:64995ms step_avg:61.14ms
step:1064/2330 train_time:65058ms step_avg:61.15ms
step:1065/2330 train_time:65118ms step_avg:61.14ms
step:1066/2330 train_time:65181ms step_avg:61.15ms
step:1067/2330 train_time:65241ms step_avg:61.14ms
step:1068/2330 train_time:65304ms step_avg:61.15ms
step:1069/2330 train_time:65364ms step_avg:61.15ms
step:1070/2330 train_time:65428ms step_avg:61.15ms
step:1071/2330 train_time:65488ms step_avg:61.15ms
step:1072/2330 train_time:65552ms step_avg:61.15ms
step:1073/2330 train_time:65612ms step_avg:61.15ms
step:1074/2330 train_time:65675ms step_avg:61.15ms
step:1075/2330 train_time:65735ms step_avg:61.15ms
step:1076/2330 train_time:65798ms step_avg:61.15ms
step:1077/2330 train_time:65857ms step_avg:61.15ms
step:1078/2330 train_time:65920ms step_avg:61.15ms
step:1079/2330 train_time:65980ms step_avg:61.15ms
step:1080/2330 train_time:66042ms step_avg:61.15ms
step:1081/2330 train_time:66102ms step_avg:61.15ms
step:1082/2330 train_time:66165ms step_avg:61.15ms
step:1083/2330 train_time:66225ms step_avg:61.15ms
step:1084/2330 train_time:66288ms step_avg:61.15ms
step:1085/2330 train_time:66349ms step_avg:61.15ms
step:1086/2330 train_time:66411ms step_avg:61.15ms
step:1087/2330 train_time:66472ms step_avg:61.15ms
step:1088/2330 train_time:66535ms step_avg:61.15ms
step:1089/2330 train_time:66595ms step_avg:61.15ms
step:1090/2330 train_time:66657ms step_avg:61.15ms
step:1091/2330 train_time:66717ms step_avg:61.15ms
step:1092/2330 train_time:66781ms step_avg:61.15ms
step:1093/2330 train_time:66841ms step_avg:61.15ms
step:1094/2330 train_time:66904ms step_avg:61.16ms
step:1095/2330 train_time:66964ms step_avg:61.15ms
step:1096/2330 train_time:67027ms step_avg:61.16ms
step:1097/2330 train_time:67087ms step_avg:61.16ms
step:1098/2330 train_time:67150ms step_avg:61.16ms
step:1099/2330 train_time:67210ms step_avg:61.16ms
step:1100/2330 train_time:67273ms step_avg:61.16ms
step:1101/2330 train_time:67334ms step_avg:61.16ms
step:1102/2330 train_time:67396ms step_avg:61.16ms
step:1103/2330 train_time:67457ms step_avg:61.16ms
step:1104/2330 train_time:67520ms step_avg:61.16ms
step:1105/2330 train_time:67580ms step_avg:61.16ms
step:1106/2330 train_time:67644ms step_avg:61.16ms
step:1107/2330 train_time:67704ms step_avg:61.16ms
step:1108/2330 train_time:67768ms step_avg:61.16ms
step:1109/2330 train_time:67827ms step_avg:61.16ms
step:1110/2330 train_time:67890ms step_avg:61.16ms
step:1111/2330 train_time:67950ms step_avg:61.16ms
step:1112/2330 train_time:68012ms step_avg:61.16ms
step:1113/2330 train_time:68072ms step_avg:61.16ms
step:1114/2330 train_time:68135ms step_avg:61.16ms
step:1115/2330 train_time:68195ms step_avg:61.16ms
step:1116/2330 train_time:68258ms step_avg:61.16ms
step:1117/2330 train_time:68318ms step_avg:61.16ms
step:1118/2330 train_time:68382ms step_avg:61.16ms
step:1119/2330 train_time:68442ms step_avg:61.16ms
step:1120/2330 train_time:68505ms step_avg:61.17ms
step:1121/2330 train_time:68566ms step_avg:61.17ms
step:1122/2330 train_time:68629ms step_avg:61.17ms
step:1123/2330 train_time:68689ms step_avg:61.17ms
step:1124/2330 train_time:68752ms step_avg:61.17ms
step:1125/2330 train_time:68812ms step_avg:61.17ms
step:1126/2330 train_time:68875ms step_avg:61.17ms
step:1127/2330 train_time:68935ms step_avg:61.17ms
step:1128/2330 train_time:68998ms step_avg:61.17ms
step:1129/2330 train_time:69058ms step_avg:61.17ms
step:1130/2330 train_time:69121ms step_avg:61.17ms
step:1131/2330 train_time:69181ms step_avg:61.17ms
step:1132/2330 train_time:69245ms step_avg:61.17ms
step:1133/2330 train_time:69305ms step_avg:61.17ms
step:1134/2330 train_time:69368ms step_avg:61.17ms
step:1135/2330 train_time:69428ms step_avg:61.17ms
step:1136/2330 train_time:69491ms step_avg:61.17ms
step:1137/2330 train_time:69551ms step_avg:61.17ms
step:1138/2330 train_time:69614ms step_avg:61.17ms
step:1139/2330 train_time:69674ms step_avg:61.17ms
step:1140/2330 train_time:69737ms step_avg:61.17ms
step:1141/2330 train_time:69797ms step_avg:61.17ms
step:1142/2330 train_time:69859ms step_avg:61.17ms
step:1143/2330 train_time:69919ms step_avg:61.17ms
step:1144/2330 train_time:69983ms step_avg:61.17ms
step:1145/2330 train_time:70042ms step_avg:61.17ms
step:1146/2330 train_time:70106ms step_avg:61.17ms
step:1147/2330 train_time:70165ms step_avg:61.17ms
step:1148/2330 train_time:70228ms step_avg:61.17ms
step:1149/2330 train_time:70288ms step_avg:61.17ms
step:1150/2330 train_time:70351ms step_avg:61.17ms
step:1151/2330 train_time:70411ms step_avg:61.17ms
step:1152/2330 train_time:70474ms step_avg:61.18ms
step:1153/2330 train_time:70534ms step_avg:61.17ms
step:1154/2330 train_time:70598ms step_avg:61.18ms
step:1155/2330 train_time:70658ms step_avg:61.18ms
step:1156/2330 train_time:70721ms step_avg:61.18ms
step:1157/2330 train_time:70781ms step_avg:61.18ms
step:1158/2330 train_time:70844ms step_avg:61.18ms
step:1159/2330 train_time:70904ms step_avg:61.18ms
step:1160/2330 train_time:70967ms step_avg:61.18ms
step:1161/2330 train_time:71028ms step_avg:61.18ms
step:1162/2330 train_time:71090ms step_avg:61.18ms
step:1163/2330 train_time:71150ms step_avg:61.18ms
step:1164/2330 train_time:71213ms step_avg:61.18ms
step:1165/2330 train_time:71273ms step_avg:61.18ms
step:1166/2330 train_time:71336ms step_avg:61.18ms
step:1167/2330 train_time:71396ms step_avg:61.18ms
step:1168/2330 train_time:71459ms step_avg:61.18ms
step:1169/2330 train_time:71520ms step_avg:61.18ms
step:1170/2330 train_time:71583ms step_avg:61.18ms
step:1171/2330 train_time:71643ms step_avg:61.18ms
step:1172/2330 train_time:71705ms step_avg:61.18ms
step:1173/2330 train_time:71765ms step_avg:61.18ms
step:1174/2330 train_time:71828ms step_avg:61.18ms
step:1175/2330 train_time:71887ms step_avg:61.18ms
step:1176/2330 train_time:71950ms step_avg:61.18ms
step:1177/2330 train_time:72009ms step_avg:61.18ms
step:1178/2330 train_time:72073ms step_avg:61.18ms
step:1179/2330 train_time:72133ms step_avg:61.18ms
step:1180/2330 train_time:72195ms step_avg:61.18ms
step:1181/2330 train_time:72255ms step_avg:61.18ms
step:1182/2330 train_time:72318ms step_avg:61.18ms
step:1183/2330 train_time:72378ms step_avg:61.18ms
step:1184/2330 train_time:72442ms step_avg:61.18ms
step:1185/2330 train_time:72502ms step_avg:61.18ms
step:1186/2330 train_time:72565ms step_avg:61.18ms
step:1187/2330 train_time:72625ms step_avg:61.18ms
step:1188/2330 train_time:72688ms step_avg:61.19ms
step:1189/2330 train_time:72748ms step_avg:61.18ms
step:1190/2330 train_time:72811ms step_avg:61.19ms
step:1191/2330 train_time:72871ms step_avg:61.18ms
step:1192/2330 train_time:72934ms step_avg:61.19ms
step:1193/2330 train_time:72994ms step_avg:61.19ms
step:1194/2330 train_time:73058ms step_avg:61.19ms
step:1195/2330 train_time:73117ms step_avg:61.19ms
step:1196/2330 train_time:73181ms step_avg:61.19ms
step:1197/2330 train_time:73240ms step_avg:61.19ms
step:1198/2330 train_time:73304ms step_avg:61.19ms
step:1199/2330 train_time:73364ms step_avg:61.19ms
step:1200/2330 train_time:73427ms step_avg:61.19ms
step:1201/2330 train_time:73486ms step_avg:61.19ms
step:1202/2330 train_time:73549ms step_avg:61.19ms
step:1203/2330 train_time:73609ms step_avg:61.19ms
step:1204/2330 train_time:73671ms step_avg:61.19ms
step:1205/2330 train_time:73731ms step_avg:61.19ms
step:1206/2330 train_time:73794ms step_avg:61.19ms
step:1207/2330 train_time:73854ms step_avg:61.19ms
step:1208/2330 train_time:73918ms step_avg:61.19ms
step:1209/2330 train_time:73978ms step_avg:61.19ms
step:1210/2330 train_time:74041ms step_avg:61.19ms
step:1211/2330 train_time:74100ms step_avg:61.19ms
step:1212/2330 train_time:74163ms step_avg:61.19ms
step:1213/2330 train_time:74223ms step_avg:61.19ms
step:1214/2330 train_time:74285ms step_avg:61.19ms
step:1215/2330 train_time:74345ms step_avg:61.19ms
step:1216/2330 train_time:74408ms step_avg:61.19ms
step:1217/2330 train_time:74467ms step_avg:61.19ms
step:1218/2330 train_time:74530ms step_avg:61.19ms
step:1219/2330 train_time:74590ms step_avg:61.19ms
step:1220/2330 train_time:74652ms step_avg:61.19ms
step:1221/2330 train_time:74713ms step_avg:61.19ms
step:1222/2330 train_time:74775ms step_avg:61.19ms
step:1223/2330 train_time:74836ms step_avg:61.19ms
step:1224/2330 train_time:74899ms step_avg:61.19ms
step:1225/2330 train_time:74959ms step_avg:61.19ms
step:1226/2330 train_time:75022ms step_avg:61.19ms
step:1227/2330 train_time:75082ms step_avg:61.19ms
step:1228/2330 train_time:75146ms step_avg:61.19ms
step:1229/2330 train_time:75205ms step_avg:61.19ms
step:1230/2330 train_time:75268ms step_avg:61.19ms
step:1231/2330 train_time:75328ms step_avg:61.19ms
step:1232/2330 train_time:75391ms step_avg:61.19ms
step:1233/2330 train_time:75451ms step_avg:61.19ms
step:1234/2330 train_time:75514ms step_avg:61.19ms
step:1235/2330 train_time:75574ms step_avg:61.19ms
step:1236/2330 train_time:75637ms step_avg:61.19ms
step:1237/2330 train_time:75697ms step_avg:61.19ms
step:1238/2330 train_time:75760ms step_avg:61.20ms
step:1239/2330 train_time:75821ms step_avg:61.20ms
step:1240/2330 train_time:75884ms step_avg:61.20ms
step:1241/2330 train_time:75945ms step_avg:61.20ms
step:1242/2330 train_time:76008ms step_avg:61.20ms
step:1243/2330 train_time:76068ms step_avg:61.20ms
step:1244/2330 train_time:76131ms step_avg:61.20ms
step:1245/2330 train_time:76191ms step_avg:61.20ms
step:1246/2330 train_time:76254ms step_avg:61.20ms
step:1247/2330 train_time:76314ms step_avg:61.20ms
step:1248/2330 train_time:76378ms step_avg:61.20ms
step:1249/2330 train_time:76438ms step_avg:61.20ms
step:1250/2330 train_time:76501ms step_avg:61.20ms
step:1250/2330 val_loss:3.5377 train_time:76566ms step_avg:61.25ms
step:1251/2330 train_time:76589ms step_avg:61.22ms
step:1252/2330 train_time:76628ms step_avg:61.20ms
step:1253/2330 train_time:76692ms step_avg:61.21ms
step:1254/2330 train_time:76756ms step_avg:61.21ms
step:1255/2330 train_time:76816ms step_avg:61.21ms
step:1256/2330 train_time:76878ms step_avg:61.21ms
step:1257/2330 train_time:76938ms step_avg:61.21ms
step:1258/2330 train_time:77001ms step_avg:61.21ms
step:1259/2330 train_time:77060ms step_avg:61.21ms
step:1260/2330 train_time:77123ms step_avg:61.21ms
step:1261/2330 train_time:77182ms step_avg:61.21ms
step:1262/2330 train_time:77244ms step_avg:61.21ms
step:1263/2330 train_time:77304ms step_avg:61.21ms
step:1264/2330 train_time:77366ms step_avg:61.21ms
step:1265/2330 train_time:77426ms step_avg:61.21ms
step:1266/2330 train_time:77489ms step_avg:61.21ms
step:1267/2330 train_time:77550ms step_avg:61.21ms
step:1268/2330 train_time:77614ms step_avg:61.21ms
step:1269/2330 train_time:77676ms step_avg:61.21ms
step:1270/2330 train_time:77739ms step_avg:61.21ms
step:1271/2330 train_time:77799ms step_avg:61.21ms
step:1272/2330 train_time:77862ms step_avg:61.21ms
step:1273/2330 train_time:77923ms step_avg:61.21ms
step:1274/2330 train_time:77986ms step_avg:61.21ms
step:1275/2330 train_time:78046ms step_avg:61.21ms
step:1276/2330 train_time:78109ms step_avg:61.21ms
step:1277/2330 train_time:78168ms step_avg:61.21ms
step:1278/2330 train_time:78231ms step_avg:61.21ms
step:1279/2330 train_time:78291ms step_avg:61.21ms
step:1280/2330 train_time:78353ms step_avg:61.21ms
step:1281/2330 train_time:78413ms step_avg:61.21ms
step:1282/2330 train_time:78476ms step_avg:61.21ms
step:1283/2330 train_time:78537ms step_avg:61.21ms
step:1284/2330 train_time:78601ms step_avg:61.22ms
step:1285/2330 train_time:78661ms step_avg:61.22ms
step:1286/2330 train_time:78725ms step_avg:61.22ms
step:1287/2330 train_time:78785ms step_avg:61.22ms
step:1288/2330 train_time:78848ms step_avg:61.22ms
step:1289/2330 train_time:78908ms step_avg:61.22ms
step:1290/2330 train_time:78971ms step_avg:61.22ms
step:1291/2330 train_time:79031ms step_avg:61.22ms
step:1292/2330 train_time:79094ms step_avg:61.22ms
step:1293/2330 train_time:79154ms step_avg:61.22ms
step:1294/2330 train_time:79216ms step_avg:61.22ms
step:1295/2330 train_time:79276ms step_avg:61.22ms
step:1296/2330 train_time:79338ms step_avg:61.22ms
step:1297/2330 train_time:79398ms step_avg:61.22ms
step:1298/2330 train_time:79461ms step_avg:61.22ms
step:1299/2330 train_time:79521ms step_avg:61.22ms
step:1300/2330 train_time:79584ms step_avg:61.22ms
step:1301/2330 train_time:79645ms step_avg:61.22ms
step:1302/2330 train_time:79708ms step_avg:61.22ms
step:1303/2330 train_time:79768ms step_avg:61.22ms
step:1304/2330 train_time:79831ms step_avg:61.22ms
step:1305/2330 train_time:79892ms step_avg:61.22ms
step:1306/2330 train_time:79955ms step_avg:61.22ms
step:1307/2330 train_time:80015ms step_avg:61.22ms
step:1308/2330 train_time:80077ms step_avg:61.22ms
step:1309/2330 train_time:80137ms step_avg:61.22ms
step:1310/2330 train_time:80200ms step_avg:61.22ms
step:1311/2330 train_time:80260ms step_avg:61.22ms
step:1312/2330 train_time:80322ms step_avg:61.22ms
step:1313/2330 train_time:80382ms step_avg:61.22ms
step:1314/2330 train_time:80446ms step_avg:61.22ms
step:1315/2330 train_time:80505ms step_avg:61.22ms
step:1316/2330 train_time:80568ms step_avg:61.22ms
step:1317/2330 train_time:80628ms step_avg:61.22ms
step:1318/2330 train_time:80691ms step_avg:61.22ms
step:1319/2330 train_time:80751ms step_avg:61.22ms
step:1320/2330 train_time:80815ms step_avg:61.22ms
step:1321/2330 train_time:80875ms step_avg:61.22ms
step:1322/2330 train_time:80938ms step_avg:61.22ms
step:1323/2330 train_time:80998ms step_avg:61.22ms
step:1324/2330 train_time:81062ms step_avg:61.22ms
step:1325/2330 train_time:81121ms step_avg:61.22ms
step:1326/2330 train_time:81184ms step_avg:61.22ms
step:1327/2330 train_time:81244ms step_avg:61.22ms
step:1328/2330 train_time:81307ms step_avg:61.23ms
step:1329/2330 train_time:81366ms step_avg:61.22ms
step:1330/2330 train_time:81429ms step_avg:61.23ms
step:1331/2330 train_time:81489ms step_avg:61.22ms
step:1332/2330 train_time:81552ms step_avg:61.23ms
step:1333/2330 train_time:81613ms step_avg:61.22ms
step:1334/2330 train_time:81675ms step_avg:61.23ms
step:1335/2330 train_time:81735ms step_avg:61.22ms
step:1336/2330 train_time:81798ms step_avg:61.23ms
step:1337/2330 train_time:81858ms step_avg:61.23ms
step:1338/2330 train_time:81921ms step_avg:61.23ms
step:1339/2330 train_time:81982ms step_avg:61.23ms
step:1340/2330 train_time:82046ms step_avg:61.23ms
step:1341/2330 train_time:82105ms step_avg:61.23ms
step:1342/2330 train_time:82168ms step_avg:61.23ms
step:1343/2330 train_time:82228ms step_avg:61.23ms
step:1344/2330 train_time:82290ms step_avg:61.23ms
step:1345/2330 train_time:82350ms step_avg:61.23ms
step:1346/2330 train_time:82413ms step_avg:61.23ms
step:1347/2330 train_time:82472ms step_avg:61.23ms
step:1348/2330 train_time:82535ms step_avg:61.23ms
step:1349/2330 train_time:82595ms step_avg:61.23ms
step:1350/2330 train_time:82658ms step_avg:61.23ms
step:1351/2330 train_time:82718ms step_avg:61.23ms
step:1352/2330 train_time:82781ms step_avg:61.23ms
step:1353/2330 train_time:82841ms step_avg:61.23ms
step:1354/2330 train_time:82905ms step_avg:61.23ms
step:1355/2330 train_time:82964ms step_avg:61.23ms
step:1356/2330 train_time:83028ms step_avg:61.23ms
step:1357/2330 train_time:83088ms step_avg:61.23ms
step:1358/2330 train_time:83151ms step_avg:61.23ms
step:1359/2330 train_time:83210ms step_avg:61.23ms
step:1360/2330 train_time:83273ms step_avg:61.23ms
step:1361/2330 train_time:83333ms step_avg:61.23ms
step:1362/2330 train_time:83396ms step_avg:61.23ms
step:1363/2330 train_time:83455ms step_avg:61.23ms
step:1364/2330 train_time:83518ms step_avg:61.23ms
step:1365/2330 train_time:83577ms step_avg:61.23ms
step:1366/2330 train_time:83640ms step_avg:61.23ms
step:1367/2330 train_time:83700ms step_avg:61.23ms
step:1368/2330 train_time:83764ms step_avg:61.23ms
step:1369/2330 train_time:83824ms step_avg:61.23ms
step:1370/2330 train_time:83887ms step_avg:61.23ms
step:1371/2330 train_time:83947ms step_avg:61.23ms
step:1372/2330 train_time:84011ms step_avg:61.23ms
step:1373/2330 train_time:84071ms step_avg:61.23ms
step:1374/2330 train_time:84134ms step_avg:61.23ms
step:1375/2330 train_time:84194ms step_avg:61.23ms
step:1376/2330 train_time:84258ms step_avg:61.23ms
step:1377/2330 train_time:84318ms step_avg:61.23ms
step:1378/2330 train_time:84381ms step_avg:61.23ms
step:1379/2330 train_time:84442ms step_avg:61.23ms
step:1380/2330 train_time:84505ms step_avg:61.24ms
step:1381/2330 train_time:84564ms step_avg:61.23ms
step:1382/2330 train_time:84627ms step_avg:61.24ms
step:1383/2330 train_time:84688ms step_avg:61.23ms
step:1384/2330 train_time:84751ms step_avg:61.24ms
step:1385/2330 train_time:84812ms step_avg:61.24ms
step:1386/2330 train_time:84875ms step_avg:61.24ms
step:1387/2330 train_time:84935ms step_avg:61.24ms
step:1388/2330 train_time:84998ms step_avg:61.24ms
step:1389/2330 train_time:85057ms step_avg:61.24ms
step:1390/2330 train_time:85120ms step_avg:61.24ms
step:1391/2330 train_time:85180ms step_avg:61.24ms
step:1392/2330 train_time:85244ms step_avg:61.24ms
step:1393/2330 train_time:85303ms step_avg:61.24ms
step:1394/2330 train_time:85366ms step_avg:61.24ms
step:1395/2330 train_time:85426ms step_avg:61.24ms
step:1396/2330 train_time:85489ms step_avg:61.24ms
step:1397/2330 train_time:85549ms step_avg:61.24ms
step:1398/2330 train_time:85612ms step_avg:61.24ms
step:1399/2330 train_time:85672ms step_avg:61.24ms
step:1400/2330 train_time:85735ms step_avg:61.24ms
step:1401/2330 train_time:85795ms step_avg:61.24ms
step:1402/2330 train_time:85858ms step_avg:61.24ms
step:1403/2330 train_time:85918ms step_avg:61.24ms
step:1404/2330 train_time:85981ms step_avg:61.24ms
step:1405/2330 train_time:86041ms step_avg:61.24ms
step:1406/2330 train_time:86104ms step_avg:61.24ms
step:1407/2330 train_time:86164ms step_avg:61.24ms
step:1408/2330 train_time:86227ms step_avg:61.24ms
step:1409/2330 train_time:86287ms step_avg:61.24ms
step:1410/2330 train_time:86350ms step_avg:61.24ms
step:1411/2330 train_time:86411ms step_avg:61.24ms
step:1412/2330 train_time:86473ms step_avg:61.24ms
step:1413/2330 train_time:86533ms step_avg:61.24ms
step:1414/2330 train_time:86596ms step_avg:61.24ms
step:1415/2330 train_time:86656ms step_avg:61.24ms
step:1416/2330 train_time:86719ms step_avg:61.24ms
step:1417/2330 train_time:86779ms step_avg:61.24ms
step:1418/2330 train_time:86842ms step_avg:61.24ms
step:1419/2330 train_time:86902ms step_avg:61.24ms
step:1420/2330 train_time:86965ms step_avg:61.24ms
step:1421/2330 train_time:87025ms step_avg:61.24ms
step:1422/2330 train_time:87087ms step_avg:61.24ms
step:1423/2330 train_time:87147ms step_avg:61.24ms
step:1424/2330 train_time:87210ms step_avg:61.24ms
step:1425/2330 train_time:87269ms step_avg:61.24ms
step:1426/2330 train_time:87332ms step_avg:61.24ms
step:1427/2330 train_time:87393ms step_avg:61.24ms
step:1428/2330 train_time:87456ms step_avg:61.24ms
step:1429/2330 train_time:87515ms step_avg:61.24ms
step:1430/2330 train_time:87578ms step_avg:61.24ms
step:1431/2330 train_time:87638ms step_avg:61.24ms
step:1432/2330 train_time:87701ms step_avg:61.24ms
step:1433/2330 train_time:87760ms step_avg:61.24ms
step:1434/2330 train_time:87823ms step_avg:61.24ms
step:1435/2330 train_time:87883ms step_avg:61.24ms
step:1436/2330 train_time:87947ms step_avg:61.24ms
step:1437/2330 train_time:88007ms step_avg:61.24ms
step:1438/2330 train_time:88070ms step_avg:61.24ms
step:1439/2330 train_time:88130ms step_avg:61.24ms
step:1440/2330 train_time:88193ms step_avg:61.25ms
step:1441/2330 train_time:88253ms step_avg:61.24ms
step:1442/2330 train_time:88316ms step_avg:61.25ms
step:1443/2330 train_time:88375ms step_avg:61.24ms
step:1444/2330 train_time:88438ms step_avg:61.25ms
step:1445/2330 train_time:88498ms step_avg:61.24ms
step:1446/2330 train_time:88560ms step_avg:61.25ms
step:1447/2330 train_time:88620ms step_avg:61.24ms
step:1448/2330 train_time:88683ms step_avg:61.25ms
step:1449/2330 train_time:88743ms step_avg:61.24ms
step:1450/2330 train_time:88806ms step_avg:61.25ms
step:1451/2330 train_time:88866ms step_avg:61.24ms
step:1452/2330 train_time:88929ms step_avg:61.25ms
step:1453/2330 train_time:88989ms step_avg:61.25ms
step:1454/2330 train_time:89052ms step_avg:61.25ms
step:1455/2330 train_time:89112ms step_avg:61.25ms
step:1456/2330 train_time:89175ms step_avg:61.25ms
step:1457/2330 train_time:89235ms step_avg:61.25ms
step:1458/2330 train_time:89298ms step_avg:61.25ms
step:1459/2330 train_time:89358ms step_avg:61.25ms
step:1460/2330 train_time:89421ms step_avg:61.25ms
step:1461/2330 train_time:89481ms step_avg:61.25ms
step:1462/2330 train_time:89544ms step_avg:61.25ms
step:1463/2330 train_time:89604ms step_avg:61.25ms
step:1464/2330 train_time:89667ms step_avg:61.25ms
step:1465/2330 train_time:89727ms step_avg:61.25ms
step:1466/2330 train_time:89790ms step_avg:61.25ms
step:1467/2330 train_time:89850ms step_avg:61.25ms
step:1468/2330 train_time:89913ms step_avg:61.25ms
step:1469/2330 train_time:89973ms step_avg:61.25ms
step:1470/2330 train_time:90036ms step_avg:61.25ms
step:1471/2330 train_time:90096ms step_avg:61.25ms
step:1472/2330 train_time:90159ms step_avg:61.25ms
step:1473/2330 train_time:90219ms step_avg:61.25ms
step:1474/2330 train_time:90283ms step_avg:61.25ms
step:1475/2330 train_time:90343ms step_avg:61.25ms
step:1476/2330 train_time:90406ms step_avg:61.25ms
step:1477/2330 train_time:90466ms step_avg:61.25ms
step:1478/2330 train_time:90529ms step_avg:61.25ms
step:1479/2330 train_time:90590ms step_avg:61.25ms
step:1480/2330 train_time:90653ms step_avg:61.25ms
step:1481/2330 train_time:90713ms step_avg:61.25ms
step:1482/2330 train_time:90776ms step_avg:61.25ms
step:1483/2330 train_time:90836ms step_avg:61.25ms
step:1484/2330 train_time:90900ms step_avg:61.25ms
step:1485/2330 train_time:90959ms step_avg:61.25ms
step:1486/2330 train_time:91022ms step_avg:61.25ms
step:1487/2330 train_time:91082ms step_avg:61.25ms
step:1488/2330 train_time:91146ms step_avg:61.25ms
step:1489/2330 train_time:91206ms step_avg:61.25ms
step:1490/2330 train_time:91268ms step_avg:61.25ms
step:1491/2330 train_time:91329ms step_avg:61.25ms
step:1492/2330 train_time:91393ms step_avg:61.26ms
step:1493/2330 train_time:91453ms step_avg:61.25ms
step:1494/2330 train_time:91516ms step_avg:61.26ms
step:1495/2330 train_time:91575ms step_avg:61.25ms
step:1496/2330 train_time:91638ms step_avg:61.26ms
step:1497/2330 train_time:91698ms step_avg:61.25ms
step:1498/2330 train_time:91761ms step_avg:61.26ms
step:1499/2330 train_time:91821ms step_avg:61.25ms
step:1500/2330 train_time:91884ms step_avg:61.26ms
step:1500/2330 val_loss:3.4953 train_time:91949ms step_avg:61.30ms
step:1501/2330 train_time:91972ms step_avg:61.27ms
step:1502/2330 train_time:92010ms step_avg:61.26ms
step:1503/2330 train_time:92074ms step_avg:61.26ms
step:1504/2330 train_time:92138ms step_avg:61.26ms
step:1505/2330 train_time:92198ms step_avg:61.26ms
step:1506/2330 train_time:92261ms step_avg:61.26ms
step:1507/2330 train_time:92321ms step_avg:61.26ms
step:1508/2330 train_time:92384ms step_avg:61.26ms
step:1509/2330 train_time:92443ms step_avg:61.26ms
step:1510/2330 train_time:92505ms step_avg:61.26ms
step:1511/2330 train_time:92564ms step_avg:61.26ms
step:1512/2330 train_time:92627ms step_avg:61.26ms
step:1513/2330 train_time:92685ms step_avg:61.26ms
step:1514/2330 train_time:92748ms step_avg:61.26ms
step:1515/2330 train_time:92807ms step_avg:61.26ms
step:1516/2330 train_time:92871ms step_avg:61.26ms
step:1517/2330 train_time:92933ms step_avg:61.26ms
step:1518/2330 train_time:92998ms step_avg:61.26ms
step:1519/2330 train_time:93058ms step_avg:61.26ms
step:1520/2330 train_time:93123ms step_avg:61.27ms
step:1521/2330 train_time:93184ms step_avg:61.26ms
step:1522/2330 train_time:93247ms step_avg:61.27ms
step:1523/2330 train_time:93306ms step_avg:61.26ms
step:1524/2330 train_time:93369ms step_avg:61.27ms
step:1525/2330 train_time:93429ms step_avg:61.26ms
step:1526/2330 train_time:93492ms step_avg:61.27ms
step:1527/2330 train_time:93551ms step_avg:61.26ms
step:1528/2330 train_time:93614ms step_avg:61.27ms
step:1529/2330 train_time:93673ms step_avg:61.26ms
step:1530/2330 train_time:93736ms step_avg:61.27ms
step:1531/2330 train_time:93796ms step_avg:61.26ms
step:1532/2330 train_time:93859ms step_avg:61.27ms
step:1533/2330 train_time:93920ms step_avg:61.27ms
step:1534/2330 train_time:93983ms step_avg:61.27ms
step:1535/2330 train_time:94045ms step_avg:61.27ms
step:1536/2330 train_time:94109ms step_avg:61.27ms
step:1537/2330 train_time:94170ms step_avg:61.27ms
step:1538/2330 train_time:94234ms step_avg:61.27ms
step:1539/2330 train_time:94294ms step_avg:61.27ms
step:1540/2330 train_time:94358ms step_avg:61.27ms
step:1541/2330 train_time:94419ms step_avg:61.27ms
step:1542/2330 train_time:94483ms step_avg:61.27ms
step:1543/2330 train_time:94542ms step_avg:61.27ms
step:1544/2330 train_time:94605ms step_avg:61.27ms
step:1545/2330 train_time:94665ms step_avg:61.27ms
step:1546/2330 train_time:94728ms step_avg:61.27ms
step:1547/2330 train_time:94789ms step_avg:61.27ms
step:1548/2330 train_time:94852ms step_avg:61.27ms
step:1549/2330 train_time:94913ms step_avg:61.27ms
step:1550/2330 train_time:94977ms step_avg:61.28ms
step:1551/2330 train_time:95037ms step_avg:61.27ms
step:1552/2330 train_time:95101ms step_avg:61.28ms
step:1553/2330 train_time:95162ms step_avg:61.28ms
step:1554/2330 train_time:95226ms step_avg:61.28ms
step:1555/2330 train_time:95287ms step_avg:61.28ms
step:1556/2330 train_time:95351ms step_avg:61.28ms
step:1557/2330 train_time:95411ms step_avg:61.28ms
step:1558/2330 train_time:95475ms step_avg:61.28ms
step:1559/2330 train_time:95535ms step_avg:61.28ms
step:1560/2330 train_time:95598ms step_avg:61.28ms
step:1561/2330 train_time:95658ms step_avg:61.28ms
step:1562/2330 train_time:95722ms step_avg:61.28ms
step:1563/2330 train_time:95782ms step_avg:61.28ms
step:1564/2330 train_time:95846ms step_avg:61.28ms
step:1565/2330 train_time:95906ms step_avg:61.28ms
step:1566/2330 train_time:95970ms step_avg:61.28ms
step:1567/2330 train_time:96031ms step_avg:61.28ms
step:1568/2330 train_time:96095ms step_avg:61.29ms
step:1569/2330 train_time:96156ms step_avg:61.28ms
step:1570/2330 train_time:96219ms step_avg:61.29ms
step:1571/2330 train_time:96280ms step_avg:61.29ms
step:1572/2330 train_time:96343ms step_avg:61.29ms
step:1573/2330 train_time:96403ms step_avg:61.29ms
step:1574/2330 train_time:96467ms step_avg:61.29ms
step:1575/2330 train_time:96528ms step_avg:61.29ms
step:1576/2330 train_time:96591ms step_avg:61.29ms
step:1577/2330 train_time:96651ms step_avg:61.29ms
step:1578/2330 train_time:96715ms step_avg:61.29ms
step:1579/2330 train_time:96775ms step_avg:61.29ms
step:1580/2330 train_time:96839ms step_avg:61.29ms
step:1581/2330 train_time:96899ms step_avg:61.29ms
step:1582/2330 train_time:96963ms step_avg:61.29ms
step:1583/2330 train_time:97025ms step_avg:61.29ms
step:1584/2330 train_time:97088ms step_avg:61.29ms
step:1585/2330 train_time:97149ms step_avg:61.29ms
step:1586/2330 train_time:97212ms step_avg:61.29ms
step:1587/2330 train_time:97272ms step_avg:61.29ms
step:1588/2330 train_time:97336ms step_avg:61.29ms
step:1589/2330 train_time:97397ms step_avg:61.29ms
step:1590/2330 train_time:97461ms step_avg:61.30ms
step:1591/2330 train_time:97522ms step_avg:61.30ms
step:1592/2330 train_time:97585ms step_avg:61.30ms
step:1593/2330 train_time:97645ms step_avg:61.30ms
step:1594/2330 train_time:97709ms step_avg:61.30ms
step:1595/2330 train_time:97770ms step_avg:61.30ms
step:1596/2330 train_time:97833ms step_avg:61.30ms
step:1597/2330 train_time:97894ms step_avg:61.30ms
step:1598/2330 train_time:97957ms step_avg:61.30ms
step:1599/2330 train_time:98019ms step_avg:61.30ms
step:1600/2330 train_time:98082ms step_avg:61.30ms
step:1601/2330 train_time:98143ms step_avg:61.30ms
step:1602/2330 train_time:98206ms step_avg:61.30ms
step:1603/2330 train_time:98266ms step_avg:61.30ms
step:1604/2330 train_time:98330ms step_avg:61.30ms
step:1605/2330 train_time:98391ms step_avg:61.30ms
step:1606/2330 train_time:98454ms step_avg:61.30ms
step:1607/2330 train_time:98514ms step_avg:61.30ms
step:1608/2330 train_time:98578ms step_avg:61.30ms
step:1609/2330 train_time:98638ms step_avg:61.30ms
step:1610/2330 train_time:98702ms step_avg:61.31ms
step:1611/2330 train_time:98763ms step_avg:61.31ms
step:1612/2330 train_time:98826ms step_avg:61.31ms
step:1613/2330 train_time:98887ms step_avg:61.31ms
step:1614/2330 train_time:98951ms step_avg:61.31ms
step:1615/2330 train_time:99012ms step_avg:61.31ms
step:1616/2330 train_time:99076ms step_avg:61.31ms
step:1617/2330 train_time:99136ms step_avg:61.31ms
step:1618/2330 train_time:99199ms step_avg:61.31ms
step:1619/2330 train_time:99259ms step_avg:61.31ms
step:1620/2330 train_time:99323ms step_avg:61.31ms
step:1621/2330 train_time:99383ms step_avg:61.31ms
step:1622/2330 train_time:99447ms step_avg:61.31ms
step:1623/2330 train_time:99507ms step_avg:61.31ms
step:1624/2330 train_time:99571ms step_avg:61.31ms
step:1625/2330 train_time:99631ms step_avg:61.31ms
step:1626/2330 train_time:99694ms step_avg:61.31ms
step:1627/2330 train_time:99755ms step_avg:61.31ms
step:1628/2330 train_time:99818ms step_avg:61.31ms
step:1629/2330 train_time:99879ms step_avg:61.31ms
step:1630/2330 train_time:99942ms step_avg:61.31ms
step:1631/2330 train_time:100002ms step_avg:61.31ms
step:1632/2330 train_time:100066ms step_avg:61.31ms
step:1633/2330 train_time:100126ms step_avg:61.31ms
step:1634/2330 train_time:100190ms step_avg:61.32ms
step:1635/2330 train_time:100250ms step_avg:61.32ms
step:1636/2330 train_time:100314ms step_avg:61.32ms
step:1637/2330 train_time:100375ms step_avg:61.32ms
step:1638/2330 train_time:100439ms step_avg:61.32ms
step:1639/2330 train_time:100498ms step_avg:61.32ms
step:1640/2330 train_time:100562ms step_avg:61.32ms
step:1641/2330 train_time:100623ms step_avg:61.32ms
step:1642/2330 train_time:100686ms step_avg:61.32ms
step:1643/2330 train_time:100747ms step_avg:61.32ms
step:1644/2330 train_time:100810ms step_avg:61.32ms
step:1645/2330 train_time:100871ms step_avg:61.32ms
step:1646/2330 train_time:100935ms step_avg:61.32ms
step:1647/2330 train_time:100995ms step_avg:61.32ms
step:1648/2330 train_time:101058ms step_avg:61.32ms
step:1649/2330 train_time:101119ms step_avg:61.32ms
step:1650/2330 train_time:101183ms step_avg:61.32ms
step:1651/2330 train_time:101244ms step_avg:61.32ms
step:1652/2330 train_time:101308ms step_avg:61.32ms
step:1653/2330 train_time:101368ms step_avg:61.32ms
step:1654/2330 train_time:101432ms step_avg:61.33ms
step:1655/2330 train_time:101492ms step_avg:61.32ms
step:1656/2330 train_time:101556ms step_avg:61.33ms
step:1657/2330 train_time:101616ms step_avg:61.33ms
step:1658/2330 train_time:101679ms step_avg:61.33ms
step:1659/2330 train_time:101740ms step_avg:61.33ms
step:1660/2330 train_time:101804ms step_avg:61.33ms
step:1661/2330 train_time:101865ms step_avg:61.33ms
step:1662/2330 train_time:101928ms step_avg:61.33ms
step:1663/2330 train_time:101989ms step_avg:61.33ms
step:1664/2330 train_time:102051ms step_avg:61.33ms
step:1665/2330 train_time:102112ms step_avg:61.33ms
step:1666/2330 train_time:102176ms step_avg:61.33ms
step:1667/2330 train_time:102236ms step_avg:61.33ms
step:1668/2330 train_time:102300ms step_avg:61.33ms
step:1669/2330 train_time:102361ms step_avg:61.33ms
step:1670/2330 train_time:102424ms step_avg:61.33ms
step:1671/2330 train_time:102484ms step_avg:61.33ms
step:1672/2330 train_time:102548ms step_avg:61.33ms
step:1673/2330 train_time:102609ms step_avg:61.33ms
step:1674/2330 train_time:102673ms step_avg:61.33ms
step:1675/2330 train_time:102733ms step_avg:61.33ms
step:1676/2330 train_time:102797ms step_avg:61.33ms
step:1677/2330 train_time:102857ms step_avg:61.33ms
step:1678/2330 train_time:102921ms step_avg:61.34ms
step:1679/2330 train_time:102981ms step_avg:61.33ms
step:1680/2330 train_time:103045ms step_avg:61.34ms
step:1681/2330 train_time:103105ms step_avg:61.34ms
step:1682/2330 train_time:103169ms step_avg:61.34ms
step:1683/2330 train_time:103229ms step_avg:61.34ms
step:1684/2330 train_time:103293ms step_avg:61.34ms
step:1685/2330 train_time:103353ms step_avg:61.34ms
step:1686/2330 train_time:103417ms step_avg:61.34ms
step:1687/2330 train_time:103477ms step_avg:61.34ms
step:1688/2330 train_time:103542ms step_avg:61.34ms
step:1689/2330 train_time:103603ms step_avg:61.34ms
step:1690/2330 train_time:103666ms step_avg:61.34ms
step:1691/2330 train_time:103726ms step_avg:61.34ms
step:1692/2330 train_time:103790ms step_avg:61.34ms
step:1693/2330 train_time:103850ms step_avg:61.34ms
step:1694/2330 train_time:103913ms step_avg:61.34ms
step:1695/2330 train_time:103974ms step_avg:61.34ms
step:1696/2330 train_time:104037ms step_avg:61.34ms
step:1697/2330 train_time:104097ms step_avg:61.34ms
step:1698/2330 train_time:104162ms step_avg:61.34ms
step:1699/2330 train_time:104222ms step_avg:61.34ms
step:1700/2330 train_time:104285ms step_avg:61.34ms
step:1701/2330 train_time:104346ms step_avg:61.34ms
step:1702/2330 train_time:104409ms step_avg:61.35ms
step:1703/2330 train_time:104470ms step_avg:61.34ms
step:1704/2330 train_time:104533ms step_avg:61.35ms
step:1705/2330 train_time:104594ms step_avg:61.35ms
step:1706/2330 train_time:104657ms step_avg:61.35ms
step:1707/2330 train_time:104718ms step_avg:61.35ms
step:1708/2330 train_time:104781ms step_avg:61.35ms
step:1709/2330 train_time:104842ms step_avg:61.35ms
step:1710/2330 train_time:104905ms step_avg:61.35ms
step:1711/2330 train_time:104966ms step_avg:61.35ms
step:1712/2330 train_time:105029ms step_avg:61.35ms
step:1713/2330 train_time:105090ms step_avg:61.35ms
step:1714/2330 train_time:105154ms step_avg:61.35ms
step:1715/2330 train_time:105215ms step_avg:61.35ms
step:1716/2330 train_time:105279ms step_avg:61.35ms
step:1717/2330 train_time:105339ms step_avg:61.35ms
step:1718/2330 train_time:105403ms step_avg:61.35ms
step:1719/2330 train_time:105463ms step_avg:61.35ms
step:1720/2330 train_time:105526ms step_avg:61.35ms
step:1721/2330 train_time:105586ms step_avg:61.35ms
step:1722/2330 train_time:105650ms step_avg:61.35ms
step:1723/2330 train_time:105711ms step_avg:61.35ms
step:1724/2330 train_time:105774ms step_avg:61.35ms
step:1725/2330 train_time:105835ms step_avg:61.35ms
step:1726/2330 train_time:105898ms step_avg:61.35ms
step:1727/2330 train_time:105959ms step_avg:61.35ms
step:1728/2330 train_time:106023ms step_avg:61.36ms
step:1729/2330 train_time:106083ms step_avg:61.36ms
step:1730/2330 train_time:106147ms step_avg:61.36ms
step:1731/2330 train_time:106207ms step_avg:61.36ms
step:1732/2330 train_time:106271ms step_avg:61.36ms
step:1733/2330 train_time:106331ms step_avg:61.36ms
step:1734/2330 train_time:106395ms step_avg:61.36ms
step:1735/2330 train_time:106455ms step_avg:61.36ms
step:1736/2330 train_time:106518ms step_avg:61.36ms
step:1737/2330 train_time:106579ms step_avg:61.36ms
step:1738/2330 train_time:106642ms step_avg:61.36ms
step:1739/2330 train_time:106703ms step_avg:61.36ms
step:1740/2330 train_time:106766ms step_avg:61.36ms
step:1741/2330 train_time:106827ms step_avg:61.36ms
step:1742/2330 train_time:106890ms step_avg:61.36ms
step:1743/2330 train_time:106951ms step_avg:61.36ms
step:1744/2330 train_time:107015ms step_avg:61.36ms
step:1745/2330 train_time:107075ms step_avg:61.36ms
step:1746/2330 train_time:107139ms step_avg:61.36ms
step:1747/2330 train_time:107200ms step_avg:61.36ms
step:1748/2330 train_time:107263ms step_avg:61.36ms
step:1749/2330 train_time:107324ms step_avg:61.36ms
step:1750/2330 train_time:107388ms step_avg:61.36ms
step:1750/2330 val_loss:3.4548 train_time:107453ms step_avg:61.40ms
step:1751/2330 train_time:107475ms step_avg:61.38ms
step:1752/2330 train_time:107514ms step_avg:61.37ms
step:1753/2330 train_time:107580ms step_avg:61.37ms
step:1754/2330 train_time:107647ms step_avg:61.37ms
step:1755/2330 train_time:107707ms step_avg:61.37ms
step:1756/2330 train_time:107770ms step_avg:61.37ms
step:1757/2330 train_time:107829ms step_avg:61.37ms
step:1758/2330 train_time:107892ms step_avg:61.37ms
step:1759/2330 train_time:107952ms step_avg:61.37ms
step:1760/2330 train_time:108015ms step_avg:61.37ms
step:1761/2330 train_time:108074ms step_avg:61.37ms
step:1762/2330 train_time:108137ms step_avg:61.37ms
step:1763/2330 train_time:108196ms step_avg:61.37ms
step:1764/2330 train_time:108258ms step_avg:61.37ms
step:1765/2330 train_time:108319ms step_avg:61.37ms
step:1766/2330 train_time:108387ms step_avg:61.37ms
step:1767/2330 train_time:108449ms step_avg:61.37ms
step:1768/2330 train_time:108513ms step_avg:61.38ms
step:1769/2330 train_time:108575ms step_avg:61.38ms
step:1770/2330 train_time:108639ms step_avg:61.38ms
step:1771/2330 train_time:108699ms step_avg:61.38ms
step:1772/2330 train_time:108763ms step_avg:61.38ms
step:1773/2330 train_time:108822ms step_avg:61.38ms
step:1774/2330 train_time:108886ms step_avg:61.38ms
step:1775/2330 train_time:108946ms step_avg:61.38ms
step:1776/2330 train_time:109009ms step_avg:61.38ms
step:1777/2330 train_time:109069ms step_avg:61.38ms
step:1778/2330 train_time:109133ms step_avg:61.38ms
step:1779/2330 train_time:109192ms step_avg:61.38ms
step:1780/2330 train_time:109255ms step_avg:61.38ms
step:1781/2330 train_time:109315ms step_avg:61.38ms
step:1782/2330 train_time:109380ms step_avg:61.38ms
step:1783/2330 train_time:109441ms step_avg:61.38ms
step:1784/2330 train_time:109506ms step_avg:61.38ms
step:1785/2330 train_time:109567ms step_avg:61.38ms
step:1786/2330 train_time:109631ms step_avg:61.38ms
step:1787/2330 train_time:109692ms step_avg:61.38ms
step:1788/2330 train_time:109756ms step_avg:61.38ms
step:1789/2330 train_time:109817ms step_avg:61.38ms
step:1790/2330 train_time:109881ms step_avg:61.39ms
step:1791/2330 train_time:109941ms step_avg:61.39ms
step:1792/2330 train_time:110004ms step_avg:61.39ms
step:1793/2330 train_time:110064ms step_avg:61.39ms
step:1794/2330 train_time:110129ms step_avg:61.39ms
step:1795/2330 train_time:110188ms step_avg:61.39ms
step:1796/2330 train_time:110251ms step_avg:61.39ms
step:1797/2330 train_time:110312ms step_avg:61.39ms
step:1798/2330 train_time:110375ms step_avg:61.39ms
step:1799/2330 train_time:110436ms step_avg:61.39ms
step:1800/2330 train_time:110501ms step_avg:61.39ms
step:1801/2330 train_time:110562ms step_avg:61.39ms
step:1802/2330 train_time:110626ms step_avg:61.39ms
step:1803/2330 train_time:110687ms step_avg:61.39ms
step:1804/2330 train_time:110750ms step_avg:61.39ms
step:1805/2330 train_time:110811ms step_avg:61.39ms
step:1806/2330 train_time:110874ms step_avg:61.39ms
step:1807/2330 train_time:110935ms step_avg:61.39ms
step:1808/2330 train_time:110999ms step_avg:61.39ms
step:1809/2330 train_time:111060ms step_avg:61.39ms
step:1810/2330 train_time:111122ms step_avg:61.39ms
step:1811/2330 train_time:111183ms step_avg:61.39ms
step:1812/2330 train_time:111247ms step_avg:61.39ms
step:1813/2330 train_time:111308ms step_avg:61.39ms
step:1814/2330 train_time:111371ms step_avg:61.40ms
step:1815/2330 train_time:111432ms step_avg:61.39ms
step:1816/2330 train_time:111495ms step_avg:61.40ms
step:1817/2330 train_time:111556ms step_avg:61.40ms
step:1818/2330 train_time:111620ms step_avg:61.40ms
step:1819/2330 train_time:111681ms step_avg:61.40ms
step:1820/2330 train_time:111744ms step_avg:61.40ms
step:1821/2330 train_time:111804ms step_avg:61.40ms
step:1822/2330 train_time:111868ms step_avg:61.40ms
step:1823/2330 train_time:111928ms step_avg:61.40ms
step:1824/2330 train_time:111991ms step_avg:61.40ms
step:1825/2330 train_time:112051ms step_avg:61.40ms
step:1826/2330 train_time:112115ms step_avg:61.40ms
step:1827/2330 train_time:112176ms step_avg:61.40ms
step:1828/2330 train_time:112240ms step_avg:61.40ms
step:1829/2330 train_time:112301ms step_avg:61.40ms
step:1830/2330 train_time:112364ms step_avg:61.40ms
step:1831/2330 train_time:112425ms step_avg:61.40ms
step:1832/2330 train_time:112488ms step_avg:61.40ms
step:1833/2330 train_time:112549ms step_avg:61.40ms
step:1834/2330 train_time:112612ms step_avg:61.40ms
step:1835/2330 train_time:112673ms step_avg:61.40ms
step:1836/2330 train_time:112737ms step_avg:61.40ms
step:1837/2330 train_time:112797ms step_avg:61.40ms
step:1838/2330 train_time:112862ms step_avg:61.40ms
step:1839/2330 train_time:112922ms step_avg:61.40ms
step:1840/2330 train_time:112985ms step_avg:61.40ms
step:1841/2330 train_time:113044ms step_avg:61.40ms
step:1842/2330 train_time:113107ms step_avg:61.40ms
step:1843/2330 train_time:113167ms step_avg:61.40ms
step:1844/2330 train_time:113231ms step_avg:61.41ms
step:1845/2330 train_time:113292ms step_avg:61.40ms
step:1846/2330 train_time:113355ms step_avg:61.41ms
step:1847/2330 train_time:113415ms step_avg:61.41ms
step:1848/2330 train_time:113479ms step_avg:61.41ms
step:1849/2330 train_time:113540ms step_avg:61.41ms
step:1850/2330 train_time:113603ms step_avg:61.41ms
step:1851/2330 train_time:113663ms step_avg:61.41ms
step:1852/2330 train_time:113726ms step_avg:61.41ms
step:1853/2330 train_time:113788ms step_avg:61.41ms
step:1854/2330 train_time:113851ms step_avg:61.41ms
step:1855/2330 train_time:113911ms step_avg:61.41ms
step:1856/2330 train_time:113975ms step_avg:61.41ms
step:1857/2330 train_time:114036ms step_avg:61.41ms
step:1858/2330 train_time:114099ms step_avg:61.41ms
step:1859/2330 train_time:114161ms step_avg:61.41ms
step:1860/2330 train_time:114224ms step_avg:61.41ms
step:1861/2330 train_time:114284ms step_avg:61.41ms
step:1862/2330 train_time:114347ms step_avg:61.41ms
step:1863/2330 train_time:114408ms step_avg:61.41ms
step:1864/2330 train_time:114472ms step_avg:61.41ms
step:1865/2330 train_time:114532ms step_avg:61.41ms
step:1866/2330 train_time:114595ms step_avg:61.41ms
step:1867/2330 train_time:114656ms step_avg:61.41ms
step:1868/2330 train_time:114720ms step_avg:61.41ms
step:1869/2330 train_time:114782ms step_avg:61.41ms
step:1870/2330 train_time:114845ms step_avg:61.41ms
step:1871/2330 train_time:114906ms step_avg:61.41ms
step:1872/2330 train_time:114970ms step_avg:61.42ms
step:1873/2330 train_time:115030ms step_avg:61.42ms
step:1874/2330 train_time:115094ms step_avg:61.42ms
step:1875/2330 train_time:115154ms step_avg:61.42ms
step:1876/2330 train_time:115218ms step_avg:61.42ms
step:1877/2330 train_time:115278ms step_avg:61.42ms
step:1878/2330 train_time:115342ms step_avg:61.42ms
step:1879/2330 train_time:115402ms step_avg:61.42ms
step:1880/2330 train_time:115466ms step_avg:61.42ms
step:1881/2330 train_time:115527ms step_avg:61.42ms
step:1882/2330 train_time:115590ms step_avg:61.42ms
step:1883/2330 train_time:115651ms step_avg:61.42ms
step:1884/2330 train_time:115715ms step_avg:61.42ms
step:1885/2330 train_time:115775ms step_avg:61.42ms
step:1886/2330 train_time:115839ms step_avg:61.42ms
step:1887/2330 train_time:115900ms step_avg:61.42ms
step:1888/2330 train_time:115964ms step_avg:61.42ms
step:1889/2330 train_time:116025ms step_avg:61.42ms
step:1890/2330 train_time:116089ms step_avg:61.42ms
step:1891/2330 train_time:116150ms step_avg:61.42ms
step:1892/2330 train_time:116213ms step_avg:61.42ms
step:1893/2330 train_time:116273ms step_avg:61.42ms
step:1894/2330 train_time:116337ms step_avg:61.42ms
step:1895/2330 train_time:116397ms step_avg:61.42ms
step:1896/2330 train_time:116461ms step_avg:61.42ms
step:1897/2330 train_time:116522ms step_avg:61.42ms
step:1898/2330 train_time:116585ms step_avg:61.43ms
step:1899/2330 train_time:116645ms step_avg:61.42ms
step:1900/2330 train_time:116709ms step_avg:61.43ms
step:1901/2330 train_time:116770ms step_avg:61.43ms
step:1902/2330 train_time:116834ms step_avg:61.43ms
step:1903/2330 train_time:116894ms step_avg:61.43ms
step:1904/2330 train_time:116958ms step_avg:61.43ms
step:1905/2330 train_time:117018ms step_avg:61.43ms
step:1906/2330 train_time:117082ms step_avg:61.43ms
step:1907/2330 train_time:117142ms step_avg:61.43ms
step:1908/2330 train_time:117205ms step_avg:61.43ms
step:1909/2330 train_time:117267ms step_avg:61.43ms
step:1910/2330 train_time:117330ms step_avg:61.43ms
step:1911/2330 train_time:117391ms step_avg:61.43ms
step:1912/2330 train_time:117454ms step_avg:61.43ms
step:1913/2330 train_time:117515ms step_avg:61.43ms
step:1914/2330 train_time:117579ms step_avg:61.43ms
step:1915/2330 train_time:117640ms step_avg:61.43ms
step:1916/2330 train_time:117703ms step_avg:61.43ms
step:1917/2330 train_time:117763ms step_avg:61.43ms
step:1918/2330 train_time:117826ms step_avg:61.43ms
step:1919/2330 train_time:117887ms step_avg:61.43ms
step:1920/2330 train_time:117951ms step_avg:61.43ms
step:1921/2330 train_time:118011ms step_avg:61.43ms
step:1922/2330 train_time:118075ms step_avg:61.43ms
step:1923/2330 train_time:118135ms step_avg:61.43ms
step:1924/2330 train_time:118199ms step_avg:61.43ms
step:1925/2330 train_time:118260ms step_avg:61.43ms
step:1926/2330 train_time:118324ms step_avg:61.43ms
step:1927/2330 train_time:118384ms step_avg:61.43ms
step:1928/2330 train_time:118447ms step_avg:61.44ms
step:1929/2330 train_time:118508ms step_avg:61.43ms
step:1930/2330 train_time:118572ms step_avg:61.44ms
step:1931/2330 train_time:118632ms step_avg:61.44ms
step:1932/2330 train_time:118695ms step_avg:61.44ms
step:1933/2330 train_time:118756ms step_avg:61.44ms
step:1934/2330 train_time:118819ms step_avg:61.44ms
step:1935/2330 train_time:118881ms step_avg:61.44ms
step:1936/2330 train_time:118944ms step_avg:61.44ms
step:1937/2330 train_time:119004ms step_avg:61.44ms
step:1938/2330 train_time:119068ms step_avg:61.44ms
step:1939/2330 train_time:119128ms step_avg:61.44ms
step:1940/2330 train_time:119192ms step_avg:61.44ms
step:1941/2330 train_time:119252ms step_avg:61.44ms
step:1942/2330 train_time:119315ms step_avg:61.44ms
step:1943/2330 train_time:119376ms step_avg:61.44ms
step:1944/2330 train_time:119440ms step_avg:61.44ms
step:1945/2330 train_time:119500ms step_avg:61.44ms
step:1946/2330 train_time:119563ms step_avg:61.44ms
step:1947/2330 train_time:119624ms step_avg:61.44ms
step:1948/2330 train_time:119688ms step_avg:61.44ms
step:1949/2330 train_time:119748ms step_avg:61.44ms
step:1950/2330 train_time:119811ms step_avg:61.44ms
step:1951/2330 train_time:119872ms step_avg:61.44ms
step:1952/2330 train_time:119935ms step_avg:61.44ms
step:1953/2330 train_time:119995ms step_avg:61.44ms
step:1954/2330 train_time:120059ms step_avg:61.44ms
step:1955/2330 train_time:120119ms step_avg:61.44ms
step:1956/2330 train_time:120183ms step_avg:61.44ms
step:1957/2330 train_time:120243ms step_avg:61.44ms
step:1958/2330 train_time:120307ms step_avg:61.44ms
step:1959/2330 train_time:120367ms step_avg:61.44ms
step:1960/2330 train_time:120431ms step_avg:61.44ms
step:1961/2330 train_time:120491ms step_avg:61.44ms
step:1962/2330 train_time:120555ms step_avg:61.44ms
step:1963/2330 train_time:120615ms step_avg:61.44ms
step:1964/2330 train_time:120680ms step_avg:61.45ms
step:1965/2330 train_time:120740ms step_avg:61.45ms
step:1966/2330 train_time:120803ms step_avg:61.45ms
step:1967/2330 train_time:120864ms step_avg:61.45ms
step:1968/2330 train_time:120927ms step_avg:61.45ms
step:1969/2330 train_time:120989ms step_avg:61.45ms
step:1970/2330 train_time:121052ms step_avg:61.45ms
step:1971/2330 train_time:121113ms step_avg:61.45ms
step:1972/2330 train_time:121176ms step_avg:61.45ms
step:1973/2330 train_time:121237ms step_avg:61.45ms
step:1974/2330 train_time:121301ms step_avg:61.45ms
step:1975/2330 train_time:121361ms step_avg:61.45ms
step:1976/2330 train_time:121425ms step_avg:61.45ms
step:1977/2330 train_time:121486ms step_avg:61.45ms
step:1978/2330 train_time:121550ms step_avg:61.45ms
step:1979/2330 train_time:121610ms step_avg:61.45ms
step:1980/2330 train_time:121673ms step_avg:61.45ms
step:1981/2330 train_time:121735ms step_avg:61.45ms
step:1982/2330 train_time:121797ms step_avg:61.45ms
step:1983/2330 train_time:121858ms step_avg:61.45ms
step:1984/2330 train_time:121921ms step_avg:61.45ms
step:1985/2330 train_time:121982ms step_avg:61.45ms
step:1986/2330 train_time:122047ms step_avg:61.45ms
step:1987/2330 train_time:122108ms step_avg:61.45ms
step:1988/2330 train_time:122172ms step_avg:61.45ms
step:1989/2330 train_time:122233ms step_avg:61.45ms
step:1990/2330 train_time:122296ms step_avg:61.46ms
step:1991/2330 train_time:122356ms step_avg:61.45ms
step:1992/2330 train_time:122420ms step_avg:61.46ms
step:1993/2330 train_time:122481ms step_avg:61.46ms
step:1994/2330 train_time:122545ms step_avg:61.46ms
step:1995/2330 train_time:122605ms step_avg:61.46ms
step:1996/2330 train_time:122668ms step_avg:61.46ms
step:1997/2330 train_time:122729ms step_avg:61.46ms
step:1998/2330 train_time:122792ms step_avg:61.46ms
step:1999/2330 train_time:122852ms step_avg:61.46ms
step:2000/2330 train_time:122916ms step_avg:61.46ms
step:2000/2330 val_loss:3.4269 train_time:122982ms step_avg:61.49ms
step:2001/2330 train_time:123005ms step_avg:61.47ms
step:2002/2330 train_time:123044ms step_avg:61.46ms
step:2003/2330 train_time:123109ms step_avg:61.46ms
step:2004/2330 train_time:123173ms step_avg:61.46ms
step:2005/2330 train_time:123234ms step_avg:61.46ms
step:2006/2330 train_time:123297ms step_avg:61.46ms
step:2007/2330 train_time:123356ms step_avg:61.46ms
step:2008/2330 train_time:123419ms step_avg:61.46ms
step:2009/2330 train_time:123479ms step_avg:61.46ms
step:2010/2330 train_time:123542ms step_avg:61.46ms
step:2011/2330 train_time:123602ms step_avg:61.46ms
step:2012/2330 train_time:123665ms step_avg:61.46ms
step:2013/2330 train_time:123724ms step_avg:61.46ms
step:2014/2330 train_time:123787ms step_avg:61.46ms
step:2015/2330 train_time:123847ms step_avg:61.46ms
step:2016/2330 train_time:123912ms step_avg:61.46ms
step:2017/2330 train_time:123975ms step_avg:61.46ms
step:2018/2330 train_time:124039ms step_avg:61.47ms
step:2019/2330 train_time:124102ms step_avg:61.47ms
step:2020/2330 train_time:124166ms step_avg:61.47ms
step:2021/2330 train_time:124227ms step_avg:61.47ms
step:2022/2330 train_time:124291ms step_avg:61.47ms
step:2023/2330 train_time:124352ms step_avg:61.47ms
step:2024/2330 train_time:124415ms step_avg:61.47ms
step:2025/2330 train_time:124475ms step_avg:61.47ms
step:2026/2330 train_time:124538ms step_avg:61.47ms
step:2027/2330 train_time:124599ms step_avg:61.47ms
step:2028/2330 train_time:124662ms step_avg:61.47ms
step:2029/2330 train_time:124721ms step_avg:61.47ms
step:2030/2330 train_time:124785ms step_avg:61.47ms
step:2031/2330 train_time:124846ms step_avg:61.47ms
step:2032/2330 train_time:124909ms step_avg:61.47ms
step:2033/2330 train_time:124970ms step_avg:61.47ms
step:2034/2330 train_time:125035ms step_avg:61.47ms
step:2035/2330 train_time:125096ms step_avg:61.47ms
step:2036/2330 train_time:125160ms step_avg:61.47ms
step:2037/2330 train_time:125221ms step_avg:61.47ms
step:2038/2330 train_time:125285ms step_avg:61.47ms
step:2039/2330 train_time:125346ms step_avg:61.47ms
step:2040/2330 train_time:125409ms step_avg:61.48ms
step:2041/2330 train_time:125470ms step_avg:61.47ms
step:2042/2330 train_time:125534ms step_avg:61.48ms
step:2043/2330 train_time:125594ms step_avg:61.48ms
step:2044/2330 train_time:125658ms step_avg:61.48ms
step:2045/2330 train_time:125718ms step_avg:61.48ms
step:2046/2330 train_time:125781ms step_avg:61.48ms
step:2047/2330 train_time:125841ms step_avg:61.48ms
step:2048/2330 train_time:125904ms step_avg:61.48ms
step:2049/2330 train_time:125965ms step_avg:61.48ms
step:2050/2330 train_time:126029ms step_avg:61.48ms
step:2051/2330 train_time:126090ms step_avg:61.48ms
step:2052/2330 train_time:126154ms step_avg:61.48ms
step:2053/2330 train_time:126215ms step_avg:61.48ms
step:2054/2330 train_time:126279ms step_avg:61.48ms
step:2055/2330 train_time:126339ms step_avg:61.48ms
step:2056/2330 train_time:126403ms step_avg:61.48ms
step:2057/2330 train_time:126464ms step_avg:61.48ms
step:2058/2330 train_time:126528ms step_avg:61.48ms
step:2059/2330 train_time:126588ms step_avg:61.48ms
step:2060/2330 train_time:126652ms step_avg:61.48ms
step:2061/2330 train_time:126712ms step_avg:61.48ms
step:2062/2330 train_time:126775ms step_avg:61.48ms
step:2063/2330 train_time:126836ms step_avg:61.48ms
step:2064/2330 train_time:126900ms step_avg:61.48ms
step:2065/2330 train_time:126960ms step_avg:61.48ms
step:2066/2330 train_time:127024ms step_avg:61.48ms
step:2067/2330 train_time:127086ms step_avg:61.48ms
step:2068/2330 train_time:127150ms step_avg:61.48ms
step:2069/2330 train_time:127210ms step_avg:61.48ms
step:2070/2330 train_time:127274ms step_avg:61.49ms
step:2071/2330 train_time:127335ms step_avg:61.48ms
step:2072/2330 train_time:127399ms step_avg:61.49ms
step:2073/2330 train_time:127459ms step_avg:61.49ms
step:2074/2330 train_time:127523ms step_avg:61.49ms
step:2075/2330 train_time:127584ms step_avg:61.49ms
step:2076/2330 train_time:127647ms step_avg:61.49ms
step:2077/2330 train_time:127708ms step_avg:61.49ms
step:2078/2330 train_time:127771ms step_avg:61.49ms
step:2079/2330 train_time:127832ms step_avg:61.49ms
step:2080/2330 train_time:127895ms step_avg:61.49ms
step:2081/2330 train_time:127957ms step_avg:61.49ms
step:2082/2330 train_time:128020ms step_avg:61.49ms
step:2083/2330 train_time:128081ms step_avg:61.49ms
step:2084/2330 train_time:128146ms step_avg:61.49ms
step:2085/2330 train_time:128206ms step_avg:61.49ms
step:2086/2330 train_time:128270ms step_avg:61.49ms
step:2087/2330 train_time:128330ms step_avg:61.49ms
step:2088/2330 train_time:128394ms step_avg:61.49ms
step:2089/2330 train_time:128454ms step_avg:61.49ms
step:2090/2330 train_time:128518ms step_avg:61.49ms
step:2091/2330 train_time:128579ms step_avg:61.49ms
step:2092/2330 train_time:128642ms step_avg:61.49ms
step:2093/2330 train_time:128703ms step_avg:61.49ms
step:2094/2330 train_time:128767ms step_avg:61.49ms
step:2095/2330 train_time:128828ms step_avg:61.49ms
step:2096/2330 train_time:128892ms step_avg:61.49ms
step:2097/2330 train_time:128953ms step_avg:61.49ms
step:2098/2330 train_time:129016ms step_avg:61.49ms
step:2099/2330 train_time:129078ms step_avg:61.49ms
step:2100/2330 train_time:129141ms step_avg:61.50ms
step:2101/2330 train_time:129201ms step_avg:61.50ms
step:2102/2330 train_time:129265ms step_avg:61.50ms
step:2103/2330 train_time:129326ms step_avg:61.50ms
step:2104/2330 train_time:129389ms step_avg:61.50ms
step:2105/2330 train_time:129450ms step_avg:61.50ms
step:2106/2330 train_time:129513ms step_avg:61.50ms
step:2107/2330 train_time:129573ms step_avg:61.50ms
step:2108/2330 train_time:129637ms step_avg:61.50ms
step:2109/2330 train_time:129698ms step_avg:61.50ms
step:2110/2330 train_time:129761ms step_avg:61.50ms
step:2111/2330 train_time:129822ms step_avg:61.50ms
step:2112/2330 train_time:129885ms step_avg:61.50ms
step:2113/2330 train_time:129945ms step_avg:61.50ms
step:2114/2330 train_time:130009ms step_avg:61.50ms
step:2115/2330 train_time:130070ms step_avg:61.50ms
step:2116/2330 train_time:130133ms step_avg:61.50ms
step:2117/2330 train_time:130193ms step_avg:61.50ms
step:2118/2330 train_time:130257ms step_avg:61.50ms
step:2119/2330 train_time:130318ms step_avg:61.50ms
step:2120/2330 train_time:130382ms step_avg:61.50ms
step:2121/2330 train_time:130442ms step_avg:61.50ms
step:2122/2330 train_time:130505ms step_avg:61.50ms
step:2123/2330 train_time:130566ms step_avg:61.50ms
step:2124/2330 train_time:130630ms step_avg:61.50ms
step:2125/2330 train_time:130690ms step_avg:61.50ms
step:2126/2330 train_time:130754ms step_avg:61.50ms
step:2127/2330 train_time:130814ms step_avg:61.50ms
step:2128/2330 train_time:130879ms step_avg:61.50ms
step:2129/2330 train_time:130940ms step_avg:61.50ms
step:2130/2330 train_time:131003ms step_avg:61.50ms
step:2131/2330 train_time:131063ms step_avg:61.50ms
step:2132/2330 train_time:131126ms step_avg:61.50ms
step:2133/2330 train_time:131187ms step_avg:61.50ms
step:2134/2330 train_time:131251ms step_avg:61.50ms
step:2135/2330 train_time:131311ms step_avg:61.50ms
step:2136/2330 train_time:131375ms step_avg:61.51ms
step:2137/2330 train_time:131435ms step_avg:61.50ms
step:2138/2330 train_time:131500ms step_avg:61.51ms
step:2139/2330 train_time:131560ms step_avg:61.51ms
step:2140/2330 train_time:131624ms step_avg:61.51ms
step:2141/2330 train_time:131685ms step_avg:61.51ms
step:2142/2330 train_time:131749ms step_avg:61.51ms
step:2143/2330 train_time:131810ms step_avg:61.51ms
step:2144/2330 train_time:131873ms step_avg:61.51ms
step:2145/2330 train_time:131934ms step_avg:61.51ms
step:2146/2330 train_time:131998ms step_avg:61.51ms
step:2147/2330 train_time:132059ms step_avg:61.51ms
step:2148/2330 train_time:132123ms step_avg:61.51ms
step:2149/2330 train_time:132184ms step_avg:61.51ms
step:2150/2330 train_time:132248ms step_avg:61.51ms
step:2151/2330 train_time:132309ms step_avg:61.51ms
step:2152/2330 train_time:132372ms step_avg:61.51ms
step:2153/2330 train_time:132432ms step_avg:61.51ms
step:2154/2330 train_time:132496ms step_avg:61.51ms
step:2155/2330 train_time:132556ms step_avg:61.51ms
step:2156/2330 train_time:132620ms step_avg:61.51ms
step:2157/2330 train_time:132681ms step_avg:61.51ms
step:2158/2330 train_time:132745ms step_avg:61.51ms
step:2159/2330 train_time:132805ms step_avg:61.51ms
step:2160/2330 train_time:132869ms step_avg:61.51ms
step:2161/2330 train_time:132929ms step_avg:61.51ms
step:2162/2330 train_time:132993ms step_avg:61.51ms
step:2163/2330 train_time:133053ms step_avg:61.51ms
step:2164/2330 train_time:133117ms step_avg:61.51ms
step:2165/2330 train_time:133178ms step_avg:61.51ms
step:2166/2330 train_time:133242ms step_avg:61.52ms
step:2167/2330 train_time:133303ms step_avg:61.51ms
step:2168/2330 train_time:133366ms step_avg:61.52ms
step:2169/2330 train_time:133426ms step_avg:61.52ms
step:2170/2330 train_time:133490ms step_avg:61.52ms
step:2171/2330 train_time:133551ms step_avg:61.52ms
step:2172/2330 train_time:133615ms step_avg:61.52ms
step:2173/2330 train_time:133676ms step_avg:61.52ms
step:2174/2330 train_time:133740ms step_avg:61.52ms
step:2175/2330 train_time:133801ms step_avg:61.52ms
step:2176/2330 train_time:133864ms step_avg:61.52ms
step:2177/2330 train_time:133925ms step_avg:61.52ms
step:2178/2330 train_time:133989ms step_avg:61.52ms
step:2179/2330 train_time:134050ms step_avg:61.52ms
step:2180/2330 train_time:134113ms step_avg:61.52ms
step:2181/2330 train_time:134174ms step_avg:61.52ms
step:2182/2330 train_time:134238ms step_avg:61.52ms
step:2183/2330 train_time:134299ms step_avg:61.52ms
step:2184/2330 train_time:134362ms step_avg:61.52ms
step:2185/2330 train_time:134424ms step_avg:61.52ms
step:2186/2330 train_time:134487ms step_avg:61.52ms
step:2187/2330 train_time:134548ms step_avg:61.52ms
step:2188/2330 train_time:134612ms step_avg:61.52ms
step:2189/2330 train_time:134672ms step_avg:61.52ms
step:2190/2330 train_time:134736ms step_avg:61.52ms
step:2191/2330 train_time:134797ms step_avg:61.52ms
step:2192/2330 train_time:134860ms step_avg:61.52ms
step:2193/2330 train_time:134920ms step_avg:61.52ms
step:2194/2330 train_time:134984ms step_avg:61.52ms
step:2195/2330 train_time:135045ms step_avg:61.52ms
step:2196/2330 train_time:135109ms step_avg:61.52ms
step:2197/2330 train_time:135169ms step_avg:61.52ms
step:2198/2330 train_time:135232ms step_avg:61.53ms
step:2199/2330 train_time:135294ms step_avg:61.53ms
step:2200/2330 train_time:135358ms step_avg:61.53ms
step:2201/2330 train_time:135418ms step_avg:61.53ms
step:2202/2330 train_time:135481ms step_avg:61.53ms
step:2203/2330 train_time:135542ms step_avg:61.53ms
step:2204/2330 train_time:135606ms step_avg:61.53ms
step:2205/2330 train_time:135666ms step_avg:61.53ms
step:2206/2330 train_time:135729ms step_avg:61.53ms
step:2207/2330 train_time:135790ms step_avg:61.53ms
step:2208/2330 train_time:135854ms step_avg:61.53ms
step:2209/2330 train_time:135915ms step_avg:61.53ms
step:2210/2330 train_time:135979ms step_avg:61.53ms
step:2211/2330 train_time:136039ms step_avg:61.53ms
step:2212/2330 train_time:136104ms step_avg:61.53ms
step:2213/2330 train_time:136164ms step_avg:61.53ms
step:2214/2330 train_time:136227ms step_avg:61.53ms
step:2215/2330 train_time:136288ms step_avg:61.53ms
step:2216/2330 train_time:136352ms step_avg:61.53ms
step:2217/2330 train_time:136413ms step_avg:61.53ms
step:2218/2330 train_time:136476ms step_avg:61.53ms
step:2219/2330 train_time:136537ms step_avg:61.53ms
step:2220/2330 train_time:136600ms step_avg:61.53ms
step:2221/2330 train_time:136660ms step_avg:61.53ms
step:2222/2330 train_time:136724ms step_avg:61.53ms
step:2223/2330 train_time:136784ms step_avg:61.53ms
step:2224/2330 train_time:136848ms step_avg:61.53ms
step:2225/2330 train_time:136909ms step_avg:61.53ms
step:2226/2330 train_time:136972ms step_avg:61.53ms
step:2227/2330 train_time:137034ms step_avg:61.53ms
step:2228/2330 train_time:137099ms step_avg:61.53ms
step:2229/2330 train_time:137159ms step_avg:61.53ms
step:2230/2330 train_time:137223ms step_avg:61.54ms
step:2231/2330 train_time:137283ms step_avg:61.53ms
step:2232/2330 train_time:137347ms step_avg:61.54ms
step:2233/2330 train_time:137407ms step_avg:61.53ms
step:2234/2330 train_time:137470ms step_avg:61.54ms
step:2235/2330 train_time:137531ms step_avg:61.54ms
step:2236/2330 train_time:137595ms step_avg:61.54ms
step:2237/2330 train_time:137656ms step_avg:61.54ms
step:2238/2330 train_time:137719ms step_avg:61.54ms
step:2239/2330 train_time:137780ms step_avg:61.54ms
step:2240/2330 train_time:137844ms step_avg:61.54ms
step:2241/2330 train_time:137904ms step_avg:61.54ms
step:2242/2330 train_time:137968ms step_avg:61.54ms
step:2243/2330 train_time:138028ms step_avg:61.54ms
step:2244/2330 train_time:138092ms step_avg:61.54ms
step:2245/2330 train_time:138153ms step_avg:61.54ms
step:2246/2330 train_time:138216ms step_avg:61.54ms
step:2247/2330 train_time:138278ms step_avg:61.54ms
step:2248/2330 train_time:138341ms step_avg:61.54ms
step:2249/2330 train_time:138402ms step_avg:61.54ms
step:2250/2330 train_time:138465ms step_avg:61.54ms
step:2250/2330 val_loss:3.4003 train_time:138530ms step_avg:61.57ms
step:2251/2330 train_time:138553ms step_avg:61.55ms
step:2252/2330 train_time:138593ms step_avg:61.54ms
step:2253/2330 train_time:138658ms step_avg:61.54ms
step:2254/2330 train_time:138722ms step_avg:61.55ms
step:2255/2330 train_time:138784ms step_avg:61.55ms
step:2256/2330 train_time:138848ms step_avg:61.55ms
step:2257/2330 train_time:138909ms step_avg:61.55ms
step:2258/2330 train_time:138972ms step_avg:61.55ms
step:2259/2330 train_time:139032ms step_avg:61.55ms
step:2260/2330 train_time:139094ms step_avg:61.55ms
step:2261/2330 train_time:139154ms step_avg:61.55ms
step:2262/2330 train_time:139216ms step_avg:61.55ms
step:2263/2330 train_time:139276ms step_avg:61.54ms
step:2264/2330 train_time:139339ms step_avg:61.55ms
step:2265/2330 train_time:139399ms step_avg:61.54ms
step:2266/2330 train_time:139463ms step_avg:61.55ms
step:2267/2330 train_time:139525ms step_avg:61.55ms
step:2268/2330 train_time:139590ms step_avg:61.55ms
step:2269/2330 train_time:139652ms step_avg:61.55ms
step:2270/2330 train_time:139716ms step_avg:61.55ms
step:2271/2330 train_time:139778ms step_avg:61.55ms
step:2272/2330 train_time:139842ms step_avg:61.55ms
step:2273/2330 train_time:139902ms step_avg:61.55ms
step:2274/2330 train_time:139966ms step_avg:61.55ms
step:2275/2330 train_time:140027ms step_avg:61.55ms
step:2276/2330 train_time:140090ms step_avg:61.55ms
step:2277/2330 train_time:140150ms step_avg:61.55ms
step:2278/2330 train_time:140212ms step_avg:61.55ms
step:2279/2330 train_time:140273ms step_avg:61.55ms
step:2280/2330 train_time:140335ms step_avg:61.55ms
step:2281/2330 train_time:140395ms step_avg:61.55ms
step:2282/2330 train_time:140458ms step_avg:61.55ms
step:2283/2330 train_time:140520ms step_avg:61.55ms
step:2284/2330 train_time:140583ms step_avg:61.55ms
step:2285/2330 train_time:140644ms step_avg:61.55ms
step:2286/2330 train_time:140708ms step_avg:61.55ms
step:2287/2330 train_time:140770ms step_avg:61.55ms
step:2288/2330 train_time:140834ms step_avg:61.55ms
step:2289/2330 train_time:140894ms step_avg:61.55ms
step:2290/2330 train_time:140958ms step_avg:61.55ms
step:2291/2330 train_time:141018ms step_avg:61.55ms
step:2292/2330 train_time:141081ms step_avg:61.55ms
step:2293/2330 train_time:141141ms step_avg:61.55ms
step:2294/2330 train_time:141206ms step_avg:61.55ms
step:2295/2330 train_time:141267ms step_avg:61.55ms
step:2296/2330 train_time:141330ms step_avg:61.55ms
step:2297/2330 train_time:141390ms step_avg:61.55ms
step:2298/2330 train_time:141453ms step_avg:61.55ms
step:2299/2330 train_time:141515ms step_avg:61.55ms
step:2300/2330 train_time:141578ms step_avg:61.56ms
step:2301/2330 train_time:141640ms step_avg:61.56ms
step:2302/2330 train_time:141704ms step_avg:61.56ms
step:2303/2330 train_time:141765ms step_avg:61.56ms
step:2304/2330 train_time:141829ms step_avg:61.56ms
step:2305/2330 train_time:141890ms step_avg:61.56ms
step:2306/2330 train_time:141953ms step_avg:61.56ms
step:2307/2330 train_time:142015ms step_avg:61.56ms
step:2308/2330 train_time:142078ms step_avg:61.56ms
step:2309/2330 train_time:142138ms step_avg:61.56ms
step:2310/2330 train_time:142202ms step_avg:61.56ms
step:2311/2330 train_time:142263ms step_avg:61.56ms
step:2312/2330 train_time:142326ms step_avg:61.56ms
step:2313/2330 train_time:142386ms step_avg:61.56ms
step:2314/2330 train_time:142450ms step_avg:61.56ms
step:2315/2330 train_time:142511ms step_avg:61.56ms
step:2316/2330 train_time:142574ms step_avg:61.56ms
step:2317/2330 train_time:142636ms step_avg:61.56ms
step:2318/2330 train_time:142699ms step_avg:61.56ms
step:2319/2330 train_time:142760ms step_avg:61.56ms
step:2320/2330 train_time:142824ms step_avg:61.56ms
step:2321/2330 train_time:142884ms step_avg:61.56ms
step:2322/2330 train_time:142949ms step_avg:61.56ms
step:2323/2330 train_time:143010ms step_avg:61.56ms
step:2324/2330 train_time:143074ms step_avg:61.56ms
step:2325/2330 train_time:143134ms step_avg:61.56ms
step:2326/2330 train_time:143198ms step_avg:61.56ms
step:2327/2330 train_time:143258ms step_avg:61.56ms
step:2328/2330 train_time:143322ms step_avg:61.56ms
step:2329/2330 train_time:143382ms step_avg:61.56ms
step:2330/2330 train_time:143446ms step_avg:61.56ms
step:2330/2330 val_loss:3.3899 train_time:143512ms step_avg:61.59ms
peak memory allocated: 29721 MiB reserved: 44076 MiB
