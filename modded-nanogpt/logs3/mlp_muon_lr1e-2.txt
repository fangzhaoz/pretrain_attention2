import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                # v_chunk = divide_by_norm(batched)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                # v_chunk = divide_by_norm(batched_update_grads)
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        # self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "mlp_muon_lr1e-2"
    os.makedirs("logs3", exist_ok=True)
    logfile = f"logs3/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
# embed_params = [p for n, p in model.named_parameters() if "embed" in n]
embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=1e-2, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    return 1.0

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum_min + 0.5 * (momentum_max - momentum_min)

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs3/{run_id}", exist_ok=True)
            torch.save(log, f"logs3/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 08:11:05 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   29C    P0             109W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   27C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   27C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   28C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   27C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   27C    P0             108W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:79ms step_avg:78.77ms
step:2/2330 train_time:141ms step_avg:70.35ms
step:3/2330 train_time:155ms step_avg:51.66ms
step:4/2330 train_time:169ms step_avg:42.21ms
step:5/2330 train_time:181ms step_avg:36.16ms
step:6/2330 train_time:214ms step_avg:35.62ms
step:7/2330 train_time:247ms step_avg:35.34ms
step:8/2330 train_time:291ms step_avg:36.41ms
step:9/2330 train_time:325ms step_avg:36.14ms
step:10/2330 train_time:369ms step_avg:36.91ms
step:11/2330 train_time:403ms step_avg:36.68ms
step:12/2330 train_time:448ms step_avg:37.35ms
step:13/2330 train_time:483ms step_avg:37.14ms
step:14/2330 train_time:527ms step_avg:37.64ms
step:15/2330 train_time:562ms step_avg:37.44ms
step:16/2330 train_time:606ms step_avg:37.86ms
step:17/2330 train_time:640ms step_avg:37.67ms
step:18/2330 train_time:685ms step_avg:38.04ms
step:19/2330 train_time:719ms step_avg:37.84ms
step:20/2330 train_time:763ms step_avg:38.15ms
step:21/2330 train_time:798ms step_avg:37.99ms
step:22/2330 train_time:841ms step_avg:38.23ms
step:23/2330 train_time:875ms step_avg:38.05ms
step:24/2330 train_time:919ms step_avg:38.29ms
step:25/2330 train_time:954ms step_avg:38.15ms
step:26/2330 train_time:1002ms step_avg:38.53ms
step:27/2330 train_time:1041ms step_avg:38.56ms
step:28/2330 train_time:1090ms step_avg:38.94ms
step:29/2330 train_time:1128ms step_avg:38.90ms
step:30/2330 train_time:1174ms step_avg:39.13ms
step:31/2330 train_time:1210ms step_avg:39.03ms
step:32/2330 train_time:1255ms step_avg:39.20ms
step:33/2330 train_time:1289ms step_avg:39.07ms
step:34/2330 train_time:1334ms step_avg:39.24ms
step:35/2330 train_time:1369ms step_avg:39.11ms
step:36/2330 train_time:1413ms step_avg:39.26ms
step:37/2330 train_time:1449ms step_avg:39.15ms
step:38/2330 train_time:1493ms step_avg:39.29ms
step:39/2330 train_time:1528ms step_avg:39.17ms
step:40/2330 train_time:1572ms step_avg:39.30ms
step:41/2330 train_time:1607ms step_avg:39.20ms
step:42/2330 train_time:1652ms step_avg:39.33ms
step:43/2330 train_time:1687ms step_avg:39.23ms
step:44/2330 train_time:1732ms step_avg:39.35ms
step:45/2330 train_time:1767ms step_avg:39.26ms
step:46/2330 train_time:1812ms step_avg:39.39ms
step:47/2330 train_time:1847ms step_avg:39.29ms
step:48/2330 train_time:1893ms step_avg:39.43ms
step:49/2330 train_time:1928ms step_avg:39.34ms
step:50/2330 train_time:1974ms step_avg:39.49ms
step:51/2330 train_time:2010ms step_avg:39.42ms
step:52/2330 train_time:2057ms step_avg:39.56ms
step:53/2330 train_time:2092ms step_avg:39.48ms
step:54/2330 train_time:2137ms step_avg:39.58ms
step:55/2330 train_time:2173ms step_avg:39.50ms
step:56/2330 train_time:2218ms step_avg:39.61ms
step:57/2330 train_time:2254ms step_avg:39.55ms
step:58/2330 train_time:2299ms step_avg:39.64ms
step:59/2330 train_time:2335ms step_avg:39.58ms
step:60/2330 train_time:2380ms step_avg:39.67ms
step:61/2330 train_time:2416ms step_avg:39.60ms
step:62/2330 train_time:2460ms step_avg:39.68ms
step:63/2330 train_time:2495ms step_avg:39.61ms
step:64/2330 train_time:2540ms step_avg:39.68ms
step:65/2330 train_time:2575ms step_avg:39.61ms
step:66/2330 train_time:2620ms step_avg:39.69ms
step:67/2330 train_time:2655ms step_avg:39.63ms
step:68/2330 train_time:2700ms step_avg:39.71ms
step:69/2330 train_time:2736ms step_avg:39.66ms
step:70/2330 train_time:2782ms step_avg:39.74ms
step:71/2330 train_time:2817ms step_avg:39.68ms
step:72/2330 train_time:2862ms step_avg:39.75ms
step:73/2330 train_time:2898ms step_avg:39.69ms
step:74/2330 train_time:2943ms step_avg:39.77ms
step:75/2330 train_time:2979ms step_avg:39.71ms
step:76/2330 train_time:3024ms step_avg:39.79ms
step:77/2330 train_time:3059ms step_avg:39.73ms
step:78/2330 train_time:3106ms step_avg:39.82ms
step:79/2330 train_time:3142ms step_avg:39.77ms
step:80/2330 train_time:3187ms step_avg:39.84ms
step:81/2330 train_time:3223ms step_avg:39.79ms
step:82/2330 train_time:3268ms step_avg:39.85ms
step:83/2330 train_time:3305ms step_avg:39.82ms
step:84/2330 train_time:3350ms step_avg:39.88ms
step:85/2330 train_time:3386ms step_avg:39.83ms
step:86/2330 train_time:3431ms step_avg:39.89ms
step:87/2330 train_time:3466ms step_avg:39.84ms
step:88/2330 train_time:3511ms step_avg:39.90ms
step:89/2330 train_time:3547ms step_avg:39.85ms
step:90/2330 train_time:3592ms step_avg:39.91ms
step:91/2330 train_time:3627ms step_avg:39.85ms
step:92/2330 train_time:3672ms step_avg:39.91ms
step:93/2330 train_time:3708ms step_avg:39.87ms
step:94/2330 train_time:3753ms step_avg:39.93ms
step:95/2330 train_time:3789ms step_avg:39.88ms
step:96/2330 train_time:3833ms step_avg:39.93ms
step:97/2330 train_time:3869ms step_avg:39.89ms
step:98/2330 train_time:3914ms step_avg:39.94ms
step:99/2330 train_time:3949ms step_avg:39.89ms
step:100/2330 train_time:3994ms step_avg:39.94ms
step:101/2330 train_time:4029ms step_avg:39.89ms
step:102/2330 train_time:4074ms step_avg:39.94ms
step:103/2330 train_time:4110ms step_avg:39.90ms
step:104/2330 train_time:4156ms step_avg:39.96ms
step:105/2330 train_time:4191ms step_avg:39.92ms
step:106/2330 train_time:4236ms step_avg:39.96ms
step:107/2330 train_time:4273ms step_avg:39.93ms
step:108/2330 train_time:4318ms step_avg:39.98ms
step:109/2330 train_time:4353ms step_avg:39.94ms
step:110/2330 train_time:4398ms step_avg:39.99ms
step:111/2330 train_time:4435ms step_avg:39.95ms
step:112/2330 train_time:4479ms step_avg:39.99ms
step:113/2330 train_time:4515ms step_avg:39.96ms
step:114/2330 train_time:4560ms step_avg:40.00ms
step:115/2330 train_time:4597ms step_avg:39.97ms
step:116/2330 train_time:4642ms step_avg:40.01ms
step:117/2330 train_time:4678ms step_avg:39.98ms
step:118/2330 train_time:4723ms step_avg:40.02ms
step:119/2330 train_time:4759ms step_avg:39.99ms
step:120/2330 train_time:4804ms step_avg:40.04ms
step:121/2330 train_time:4839ms step_avg:39.99ms
step:122/2330 train_time:4884ms step_avg:40.03ms
step:123/2330 train_time:4920ms step_avg:40.00ms
step:124/2330 train_time:4965ms step_avg:40.04ms
step:125/2330 train_time:5000ms step_avg:40.00ms
step:126/2330 train_time:5045ms step_avg:40.04ms
step:127/2330 train_time:5081ms step_avg:40.01ms
step:128/2330 train_time:5126ms step_avg:40.04ms
step:129/2330 train_time:5162ms step_avg:40.01ms
step:130/2330 train_time:5207ms step_avg:40.05ms
step:131/2330 train_time:5242ms step_avg:40.02ms
step:132/2330 train_time:5288ms step_avg:40.06ms
step:133/2330 train_time:5324ms step_avg:40.03ms
step:134/2330 train_time:5369ms step_avg:40.07ms
step:135/2330 train_time:5406ms step_avg:40.05ms
step:136/2330 train_time:5452ms step_avg:40.09ms
step:137/2330 train_time:5487ms step_avg:40.05ms
step:138/2330 train_time:5532ms step_avg:40.08ms
step:139/2330 train_time:5567ms step_avg:40.05ms
step:140/2330 train_time:5612ms step_avg:40.09ms
step:141/2330 train_time:5647ms step_avg:40.05ms
step:142/2330 train_time:5693ms step_avg:40.09ms
step:143/2330 train_time:5728ms step_avg:40.06ms
step:144/2330 train_time:5774ms step_avg:40.10ms
step:145/2330 train_time:5809ms step_avg:40.06ms
step:146/2330 train_time:5853ms step_avg:40.09ms
step:147/2330 train_time:5888ms step_avg:40.05ms
step:148/2330 train_time:5932ms step_avg:40.08ms
step:149/2330 train_time:5968ms step_avg:40.05ms
step:150/2330 train_time:6013ms step_avg:40.09ms
step:151/2330 train_time:6049ms step_avg:40.06ms
step:152/2330 train_time:6095ms step_avg:40.10ms
step:153/2330 train_time:6129ms step_avg:40.06ms
step:154/2330 train_time:6174ms step_avg:40.09ms
step:155/2330 train_time:6210ms step_avg:40.06ms
step:156/2330 train_time:6255ms step_avg:40.09ms
step:157/2330 train_time:6290ms step_avg:40.06ms
step:158/2330 train_time:6334ms step_avg:40.09ms
step:159/2330 train_time:6370ms step_avg:40.06ms
step:160/2330 train_time:6416ms step_avg:40.10ms
step:161/2330 train_time:6451ms step_avg:40.07ms
step:162/2330 train_time:6497ms step_avg:40.10ms
step:163/2330 train_time:6532ms step_avg:40.07ms
step:164/2330 train_time:6578ms step_avg:40.11ms
step:165/2330 train_time:6615ms step_avg:40.09ms
step:166/2330 train_time:6660ms step_avg:40.12ms
step:167/2330 train_time:6696ms step_avg:40.10ms
step:168/2330 train_time:6740ms step_avg:40.12ms
step:169/2330 train_time:6776ms step_avg:40.09ms
step:170/2330 train_time:6821ms step_avg:40.12ms
step:171/2330 train_time:6857ms step_avg:40.10ms
step:172/2330 train_time:6902ms step_avg:40.13ms
step:173/2330 train_time:6938ms step_avg:40.10ms
step:174/2330 train_time:6983ms step_avg:40.13ms
step:175/2330 train_time:7019ms step_avg:40.11ms
step:176/2330 train_time:7064ms step_avg:40.13ms
step:177/2330 train_time:7099ms step_avg:40.10ms
step:178/2330 train_time:7143ms step_avg:40.13ms
step:179/2330 train_time:7178ms step_avg:40.10ms
step:180/2330 train_time:7224ms step_avg:40.13ms
step:181/2330 train_time:7259ms step_avg:40.11ms
step:182/2330 train_time:7305ms step_avg:40.14ms
step:183/2330 train_time:7340ms step_avg:40.11ms
step:184/2330 train_time:7384ms step_avg:40.13ms
step:185/2330 train_time:7420ms step_avg:40.11ms
step:186/2330 train_time:7465ms step_avg:40.13ms
step:187/2330 train_time:7501ms step_avg:40.11ms
step:188/2330 train_time:7546ms step_avg:40.14ms
step:189/2330 train_time:7581ms step_avg:40.11ms
step:190/2330 train_time:7627ms step_avg:40.14ms
step:191/2330 train_time:7662ms step_avg:40.11ms
step:192/2330 train_time:7707ms step_avg:40.14ms
step:193/2330 train_time:7742ms step_avg:40.11ms
step:194/2330 train_time:7788ms step_avg:40.14ms
step:195/2330 train_time:7823ms step_avg:40.12ms
step:196/2330 train_time:7869ms step_avg:40.15ms
step:197/2330 train_time:7904ms step_avg:40.12ms
step:198/2330 train_time:7950ms step_avg:40.15ms
step:199/2330 train_time:7985ms step_avg:40.13ms
step:200/2330 train_time:8031ms step_avg:40.16ms
step:201/2330 train_time:8067ms step_avg:40.13ms
step:202/2330 train_time:8112ms step_avg:40.16ms
step:203/2330 train_time:8147ms step_avg:40.13ms
step:204/2330 train_time:8192ms step_avg:40.16ms
step:205/2330 train_time:8228ms step_avg:40.13ms
step:206/2330 train_time:8272ms step_avg:40.16ms
step:207/2330 train_time:8308ms step_avg:40.14ms
step:208/2330 train_time:8353ms step_avg:40.16ms
step:209/2330 train_time:8388ms step_avg:40.14ms
step:210/2330 train_time:8434ms step_avg:40.16ms
step:211/2330 train_time:8468ms step_avg:40.13ms
step:212/2330 train_time:8514ms step_avg:40.16ms
step:213/2330 train_time:8549ms step_avg:40.14ms
step:214/2330 train_time:8594ms step_avg:40.16ms
step:215/2330 train_time:8629ms step_avg:40.14ms
step:216/2330 train_time:8674ms step_avg:40.16ms
step:217/2330 train_time:8711ms step_avg:40.14ms
step:218/2330 train_time:8756ms step_avg:40.16ms
step:219/2330 train_time:8791ms step_avg:40.14ms
step:220/2330 train_time:8836ms step_avg:40.16ms
step:221/2330 train_time:8871ms step_avg:40.14ms
step:222/2330 train_time:8916ms step_avg:40.16ms
step:223/2330 train_time:8952ms step_avg:40.14ms
step:224/2330 train_time:8997ms step_avg:40.16ms
step:225/2330 train_time:9033ms step_avg:40.14ms
step:226/2330 train_time:9077ms step_avg:40.16ms
step:227/2330 train_time:9113ms step_avg:40.14ms
step:228/2330 train_time:9158ms step_avg:40.16ms
step:229/2330 train_time:9193ms step_avg:40.14ms
step:230/2330 train_time:9238ms step_avg:40.17ms
step:231/2330 train_time:9273ms step_avg:40.14ms
step:232/2330 train_time:9319ms step_avg:40.17ms
step:233/2330 train_time:9355ms step_avg:40.15ms
step:234/2330 train_time:9400ms step_avg:40.17ms
step:235/2330 train_time:9436ms step_avg:40.15ms
step:236/2330 train_time:9481ms step_avg:40.17ms
step:237/2330 train_time:9517ms step_avg:40.16ms
step:238/2330 train_time:9563ms step_avg:40.18ms
step:239/2330 train_time:9599ms step_avg:40.16ms
step:240/2330 train_time:9644ms step_avg:40.18ms
step:241/2330 train_time:9679ms step_avg:40.16ms
step:242/2330 train_time:9724ms step_avg:40.18ms
step:243/2330 train_time:9759ms step_avg:40.16ms
step:244/2330 train_time:9804ms step_avg:40.18ms
step:245/2330 train_time:9840ms step_avg:40.16ms
step:246/2330 train_time:9885ms step_avg:40.18ms
step:247/2330 train_time:9920ms step_avg:40.16ms
step:248/2330 train_time:9966ms step_avg:40.18ms
step:249/2330 train_time:10001ms step_avg:40.16ms
step:250/2330 train_time:10046ms step_avg:40.18ms
step:250/2330 val_loss:5.4064 train_time:10133ms step_avg:40.53ms
step:251/2330 train_time:10147ms step_avg:40.43ms
step:252/2330 train_time:10160ms step_avg:40.32ms
step:253/2330 train_time:10171ms step_avg:40.20ms
step:254/2330 train_time:10205ms step_avg:40.18ms
step:255/2330 train_time:10240ms step_avg:40.16ms
step:256/2330 train_time:10284ms step_avg:40.17ms
step:257/2330 train_time:10319ms step_avg:40.15ms
step:258/2330 train_time:10363ms step_avg:40.17ms
step:259/2330 train_time:10398ms step_avg:40.15ms
step:260/2330 train_time:10442ms step_avg:40.16ms
step:261/2330 train_time:10481ms step_avg:40.16ms
step:262/2330 train_time:10529ms step_avg:40.19ms
step:263/2330 train_time:10567ms step_avg:40.18ms
step:264/2330 train_time:10613ms step_avg:40.20ms
step:265/2330 train_time:10649ms step_avg:40.19ms
step:266/2330 train_time:10694ms step_avg:40.20ms
step:267/2330 train_time:10730ms step_avg:40.19ms
step:268/2330 train_time:10774ms step_avg:40.20ms
step:269/2330 train_time:10809ms step_avg:40.18ms
step:270/2330 train_time:10853ms step_avg:40.20ms
step:271/2330 train_time:10888ms step_avg:40.18ms
step:272/2330 train_time:10933ms step_avg:40.19ms
step:273/2330 train_time:10967ms step_avg:40.17ms
step:274/2330 train_time:11012ms step_avg:40.19ms
step:275/2330 train_time:11047ms step_avg:40.17ms
step:276/2330 train_time:11093ms step_avg:40.19ms
step:277/2330 train_time:11128ms step_avg:40.17ms
step:278/2330 train_time:11174ms step_avg:40.19ms
step:279/2330 train_time:11208ms step_avg:40.17ms
step:280/2330 train_time:11253ms step_avg:40.19ms
step:281/2330 train_time:11288ms step_avg:40.17ms
step:282/2330 train_time:11333ms step_avg:40.19ms
step:283/2330 train_time:11368ms step_avg:40.17ms
step:284/2330 train_time:11413ms step_avg:40.19ms
step:285/2330 train_time:11449ms step_avg:40.17ms
step:286/2330 train_time:11496ms step_avg:40.20ms
step:287/2330 train_time:11532ms step_avg:40.18ms
step:288/2330 train_time:11577ms step_avg:40.20ms
step:289/2330 train_time:11613ms step_avg:40.18ms
step:290/2330 train_time:11658ms step_avg:40.20ms
step:291/2330 train_time:11694ms step_avg:40.18ms
step:292/2330 train_time:11739ms step_avg:40.20ms
step:293/2330 train_time:11775ms step_avg:40.19ms
step:294/2330 train_time:11820ms step_avg:40.20ms
step:295/2330 train_time:11855ms step_avg:40.19ms
step:296/2330 train_time:11900ms step_avg:40.20ms
step:297/2330 train_time:11935ms step_avg:40.19ms
step:298/2330 train_time:11981ms step_avg:40.20ms
step:299/2330 train_time:12017ms step_avg:40.19ms
step:300/2330 train_time:12062ms step_avg:40.21ms
step:301/2330 train_time:12097ms step_avg:40.19ms
step:302/2330 train_time:12142ms step_avg:40.21ms
step:303/2330 train_time:12178ms step_avg:40.19ms
step:304/2330 train_time:12222ms step_avg:40.20ms
step:305/2330 train_time:12257ms step_avg:40.19ms
step:306/2330 train_time:12302ms step_avg:40.20ms
step:307/2330 train_time:12337ms step_avg:40.19ms
step:308/2330 train_time:12382ms step_avg:40.20ms
step:309/2330 train_time:12418ms step_avg:40.19ms
step:310/2330 train_time:12463ms step_avg:40.20ms
step:311/2330 train_time:12498ms step_avg:40.19ms
step:312/2330 train_time:12543ms step_avg:40.20ms
step:313/2330 train_time:12579ms step_avg:40.19ms
step:314/2330 train_time:12625ms step_avg:40.21ms
step:315/2330 train_time:12661ms step_avg:40.19ms
step:316/2330 train_time:12706ms step_avg:40.21ms
step:317/2330 train_time:12741ms step_avg:40.19ms
step:318/2330 train_time:12786ms step_avg:40.21ms
step:319/2330 train_time:12822ms step_avg:40.19ms
step:320/2330 train_time:12867ms step_avg:40.21ms
step:321/2330 train_time:12902ms step_avg:40.19ms
step:322/2330 train_time:12947ms step_avg:40.21ms
step:323/2330 train_time:12983ms step_avg:40.20ms
step:324/2330 train_time:13028ms step_avg:40.21ms
step:325/2330 train_time:13064ms step_avg:40.20ms
step:326/2330 train_time:13108ms step_avg:40.21ms
step:327/2330 train_time:13144ms step_avg:40.19ms
step:328/2330 train_time:13189ms step_avg:40.21ms
step:329/2330 train_time:13226ms step_avg:40.20ms
step:330/2330 train_time:13271ms step_avg:40.21ms
step:331/2330 train_time:13306ms step_avg:40.20ms
step:332/2330 train_time:13352ms step_avg:40.22ms
step:333/2330 train_time:13387ms step_avg:40.20ms
step:334/2330 train_time:13432ms step_avg:40.22ms
step:335/2330 train_time:13468ms step_avg:40.20ms
step:336/2330 train_time:13515ms step_avg:40.22ms
step:337/2330 train_time:13551ms step_avg:40.21ms
step:338/2330 train_time:13596ms step_avg:40.22ms
step:339/2330 train_time:13631ms step_avg:40.21ms
step:340/2330 train_time:13676ms step_avg:40.22ms
step:341/2330 train_time:13711ms step_avg:40.21ms
step:342/2330 train_time:13755ms step_avg:40.22ms
step:343/2330 train_time:13791ms step_avg:40.21ms
step:344/2330 train_time:13836ms step_avg:40.22ms
step:345/2330 train_time:13873ms step_avg:40.21ms
step:346/2330 train_time:13918ms step_avg:40.23ms
step:347/2330 train_time:13954ms step_avg:40.21ms
step:348/2330 train_time:14000ms step_avg:40.23ms
step:349/2330 train_time:14035ms step_avg:40.21ms
step:350/2330 train_time:14080ms step_avg:40.23ms
step:351/2330 train_time:14115ms step_avg:40.22ms
step:352/2330 train_time:14161ms step_avg:40.23ms
step:353/2330 train_time:14197ms step_avg:40.22ms
step:354/2330 train_time:14241ms step_avg:40.23ms
step:355/2330 train_time:14277ms step_avg:40.22ms
step:356/2330 train_time:14322ms step_avg:40.23ms
step:357/2330 train_time:14356ms step_avg:40.21ms
step:358/2330 train_time:14401ms step_avg:40.23ms
step:359/2330 train_time:14436ms step_avg:40.21ms
step:360/2330 train_time:14482ms step_avg:40.23ms
step:361/2330 train_time:14518ms step_avg:40.22ms
step:362/2330 train_time:14563ms step_avg:40.23ms
step:363/2330 train_time:14598ms step_avg:40.21ms
step:364/2330 train_time:14642ms step_avg:40.23ms
step:365/2330 train_time:14677ms step_avg:40.21ms
step:366/2330 train_time:14722ms step_avg:40.22ms
step:367/2330 train_time:14757ms step_avg:40.21ms
step:368/2330 train_time:14802ms step_avg:40.22ms
step:369/2330 train_time:14838ms step_avg:40.21ms
step:370/2330 train_time:14883ms step_avg:40.22ms
step:371/2330 train_time:14919ms step_avg:40.21ms
step:372/2330 train_time:14964ms step_avg:40.23ms
step:373/2330 train_time:14999ms step_avg:40.21ms
step:374/2330 train_time:15045ms step_avg:40.23ms
step:375/2330 train_time:15080ms step_avg:40.21ms
step:376/2330 train_time:15126ms step_avg:40.23ms
step:377/2330 train_time:15161ms step_avg:40.21ms
step:378/2330 train_time:15206ms step_avg:40.23ms
step:379/2330 train_time:15241ms step_avg:40.21ms
step:380/2330 train_time:15287ms step_avg:40.23ms
step:381/2330 train_time:15322ms step_avg:40.22ms
step:382/2330 train_time:15367ms step_avg:40.23ms
step:383/2330 train_time:15403ms step_avg:40.22ms
step:384/2330 train_time:15447ms step_avg:40.23ms
step:385/2330 train_time:15483ms step_avg:40.21ms
step:386/2330 train_time:15528ms step_avg:40.23ms
step:387/2330 train_time:15563ms step_avg:40.22ms
step:388/2330 train_time:15609ms step_avg:40.23ms
step:389/2330 train_time:15645ms step_avg:40.22ms
step:390/2330 train_time:15690ms step_avg:40.23ms
step:391/2330 train_time:15725ms step_avg:40.22ms
step:392/2330 train_time:15770ms step_avg:40.23ms
step:393/2330 train_time:15806ms step_avg:40.22ms
step:394/2330 train_time:15851ms step_avg:40.23ms
step:395/2330 train_time:15887ms step_avg:40.22ms
step:396/2330 train_time:15932ms step_avg:40.23ms
step:397/2330 train_time:15967ms step_avg:40.22ms
step:398/2330 train_time:16013ms step_avg:40.23ms
step:399/2330 train_time:16048ms step_avg:40.22ms
step:400/2330 train_time:16093ms step_avg:40.23ms
step:401/2330 train_time:16128ms step_avg:40.22ms
step:402/2330 train_time:16174ms step_avg:40.23ms
step:403/2330 train_time:16209ms step_avg:40.22ms
step:404/2330 train_time:16253ms step_avg:40.23ms
step:405/2330 train_time:16289ms step_avg:40.22ms
step:406/2330 train_time:16334ms step_avg:40.23ms
step:407/2330 train_time:16370ms step_avg:40.22ms
step:408/2330 train_time:16416ms step_avg:40.23ms
step:409/2330 train_time:16452ms step_avg:40.22ms
step:410/2330 train_time:16496ms step_avg:40.24ms
step:411/2330 train_time:16532ms step_avg:40.22ms
step:412/2330 train_time:16577ms step_avg:40.24ms
step:413/2330 train_time:16613ms step_avg:40.22ms
step:414/2330 train_time:16658ms step_avg:40.24ms
step:415/2330 train_time:16694ms step_avg:40.23ms
step:416/2330 train_time:16739ms step_avg:40.24ms
step:417/2330 train_time:16774ms step_avg:40.22ms
step:418/2330 train_time:16819ms step_avg:40.24ms
step:419/2330 train_time:16854ms step_avg:40.22ms
step:420/2330 train_time:16898ms step_avg:40.23ms
step:421/2330 train_time:16934ms step_avg:40.22ms
step:422/2330 train_time:16979ms step_avg:40.23ms
step:423/2330 train_time:17014ms step_avg:40.22ms
step:424/2330 train_time:17058ms step_avg:40.23ms
step:425/2330 train_time:17093ms step_avg:40.22ms
step:426/2330 train_time:17139ms step_avg:40.23ms
step:427/2330 train_time:17175ms step_avg:40.22ms
step:428/2330 train_time:17221ms step_avg:40.24ms
step:429/2330 train_time:17256ms step_avg:40.22ms
step:430/2330 train_time:17302ms step_avg:40.24ms
step:431/2330 train_time:17337ms step_avg:40.22ms
step:432/2330 train_time:17382ms step_avg:40.24ms
step:433/2330 train_time:17417ms step_avg:40.22ms
step:434/2330 train_time:17461ms step_avg:40.23ms
step:435/2330 train_time:17497ms step_avg:40.22ms
step:436/2330 train_time:17542ms step_avg:40.23ms
step:437/2330 train_time:17578ms step_avg:40.22ms
step:438/2330 train_time:17623ms step_avg:40.23ms
step:439/2330 train_time:17659ms step_avg:40.23ms
step:440/2330 train_time:17704ms step_avg:40.24ms
step:441/2330 train_time:17739ms step_avg:40.22ms
step:442/2330 train_time:17785ms step_avg:40.24ms
step:443/2330 train_time:17820ms step_avg:40.23ms
step:444/2330 train_time:17865ms step_avg:40.24ms
step:445/2330 train_time:17901ms step_avg:40.23ms
step:446/2330 train_time:17946ms step_avg:40.24ms
step:447/2330 train_time:17982ms step_avg:40.23ms
step:448/2330 train_time:18026ms step_avg:40.24ms
step:449/2330 train_time:18062ms step_avg:40.23ms
step:450/2330 train_time:18107ms step_avg:40.24ms
step:451/2330 train_time:18142ms step_avg:40.23ms
step:452/2330 train_time:18188ms step_avg:40.24ms
step:453/2330 train_time:18224ms step_avg:40.23ms
step:454/2330 train_time:18270ms step_avg:40.24ms
step:455/2330 train_time:18306ms step_avg:40.23ms
step:456/2330 train_time:18352ms step_avg:40.24ms
step:457/2330 train_time:18388ms step_avg:40.24ms
step:458/2330 train_time:18433ms step_avg:40.25ms
step:459/2330 train_time:18470ms step_avg:40.24ms
step:460/2330 train_time:18516ms step_avg:40.25ms
step:461/2330 train_time:18551ms step_avg:40.24ms
step:462/2330 train_time:18596ms step_avg:40.25ms
step:463/2330 train_time:18632ms step_avg:40.24ms
step:464/2330 train_time:18677ms step_avg:40.25ms
step:465/2330 train_time:18712ms step_avg:40.24ms
step:466/2330 train_time:18758ms step_avg:40.25ms
step:467/2330 train_time:18793ms step_avg:40.24ms
step:468/2330 train_time:18838ms step_avg:40.25ms
step:469/2330 train_time:18874ms step_avg:40.24ms
step:470/2330 train_time:18919ms step_avg:40.25ms
step:471/2330 train_time:18955ms step_avg:40.24ms
step:472/2330 train_time:19000ms step_avg:40.25ms
step:473/2330 train_time:19035ms step_avg:40.24ms
step:474/2330 train_time:19081ms step_avg:40.25ms
step:475/2330 train_time:19116ms step_avg:40.25ms
step:476/2330 train_time:19161ms step_avg:40.26ms
step:477/2330 train_time:19198ms step_avg:40.25ms
step:478/2330 train_time:19243ms step_avg:40.26ms
step:479/2330 train_time:19279ms step_avg:40.25ms
step:480/2330 train_time:19324ms step_avg:40.26ms
step:481/2330 train_time:19359ms step_avg:40.25ms
step:482/2330 train_time:19403ms step_avg:40.26ms
step:483/2330 train_time:19438ms step_avg:40.24ms
step:484/2330 train_time:19483ms step_avg:40.25ms
step:485/2330 train_time:19519ms step_avg:40.24ms
step:486/2330 train_time:19564ms step_avg:40.25ms
step:487/2330 train_time:19599ms step_avg:40.24ms
step:488/2330 train_time:19643ms step_avg:40.25ms
step:489/2330 train_time:19679ms step_avg:40.24ms
step:490/2330 train_time:19724ms step_avg:40.25ms
step:491/2330 train_time:19759ms step_avg:40.24ms
step:492/2330 train_time:19804ms step_avg:40.25ms
step:493/2330 train_time:19839ms step_avg:40.24ms
step:494/2330 train_time:19884ms step_avg:40.25ms
step:495/2330 train_time:19920ms step_avg:40.24ms
step:496/2330 train_time:19965ms step_avg:40.25ms
step:497/2330 train_time:20000ms step_avg:40.24ms
step:498/2330 train_time:20045ms step_avg:40.25ms
step:499/2330 train_time:20082ms step_avg:40.24ms
step:500/2330 train_time:20126ms step_avg:40.25ms
step:500/2330 val_loss:5.2840 train_time:20214ms step_avg:40.43ms
step:501/2330 train_time:20228ms step_avg:40.37ms
step:502/2330 train_time:20240ms step_avg:40.32ms
step:503/2330 train_time:20252ms step_avg:40.26ms
step:504/2330 train_time:20287ms step_avg:40.25ms
step:505/2330 train_time:20321ms step_avg:40.24ms
step:506/2330 train_time:20365ms step_avg:40.25ms
step:507/2330 train_time:20399ms step_avg:40.23ms
step:508/2330 train_time:20444ms step_avg:40.24ms
step:509/2330 train_time:20479ms step_avg:40.23ms
step:510/2330 train_time:20526ms step_avg:40.25ms
step:511/2330 train_time:20566ms step_avg:40.25ms
step:512/2330 train_time:20614ms step_avg:40.26ms
step:513/2330 train_time:20649ms step_avg:40.25ms
step:514/2330 train_time:20694ms step_avg:40.26ms
step:515/2330 train_time:20730ms step_avg:40.25ms
step:516/2330 train_time:20776ms step_avg:40.26ms
step:517/2330 train_time:20810ms step_avg:40.25ms
step:518/2330 train_time:20855ms step_avg:40.26ms
step:519/2330 train_time:20889ms step_avg:40.25ms
step:520/2330 train_time:20934ms step_avg:40.26ms
step:521/2330 train_time:20969ms step_avg:40.25ms
step:522/2330 train_time:21014ms step_avg:40.26ms
step:523/2330 train_time:21049ms step_avg:40.25ms
step:524/2330 train_time:21094ms step_avg:40.26ms
step:525/2330 train_time:21129ms step_avg:40.25ms
step:526/2330 train_time:21175ms step_avg:40.26ms
step:527/2330 train_time:21211ms step_avg:40.25ms
step:528/2330 train_time:21256ms step_avg:40.26ms
step:529/2330 train_time:21291ms step_avg:40.25ms
step:530/2330 train_time:21335ms step_avg:40.26ms
step:531/2330 train_time:21371ms step_avg:40.25ms
step:532/2330 train_time:21415ms step_avg:40.25ms
step:533/2330 train_time:21451ms step_avg:40.25ms
step:534/2330 train_time:21497ms step_avg:40.26ms
step:535/2330 train_time:21535ms step_avg:40.25ms
step:536/2330 train_time:21580ms step_avg:40.26ms
step:537/2330 train_time:21615ms step_avg:40.25ms
step:538/2330 train_time:21661ms step_avg:40.26ms
step:539/2330 train_time:21698ms step_avg:40.26ms
step:540/2330 train_time:21744ms step_avg:40.27ms
step:541/2330 train_time:21779ms step_avg:40.26ms
step:542/2330 train_time:21824ms step_avg:40.27ms
step:543/2330 train_time:21860ms step_avg:40.26ms
step:544/2330 train_time:21904ms step_avg:40.27ms
step:545/2330 train_time:21940ms step_avg:40.26ms
step:546/2330 train_time:21986ms step_avg:40.27ms
step:547/2330 train_time:22021ms step_avg:40.26ms
step:548/2330 train_time:22065ms step_avg:40.26ms
step:549/2330 train_time:22100ms step_avg:40.26ms
step:550/2330 train_time:22146ms step_avg:40.26ms
step:551/2330 train_time:22181ms step_avg:40.26ms
step:552/2330 train_time:22225ms step_avg:40.26ms
step:553/2330 train_time:22260ms step_avg:40.25ms
step:554/2330 train_time:22305ms step_avg:40.26ms
step:555/2330 train_time:22340ms step_avg:40.25ms
step:556/2330 train_time:22385ms step_avg:40.26ms
step:557/2330 train_time:22421ms step_avg:40.25ms
step:558/2330 train_time:22466ms step_avg:40.26ms
step:559/2330 train_time:22502ms step_avg:40.25ms
step:560/2330 train_time:22547ms step_avg:40.26ms
step:561/2330 train_time:22582ms step_avg:40.25ms
step:562/2330 train_time:22627ms step_avg:40.26ms
step:563/2330 train_time:22663ms step_avg:40.25ms
step:564/2330 train_time:22708ms step_avg:40.26ms
step:565/2330 train_time:22743ms step_avg:40.25ms
step:566/2330 train_time:22789ms step_avg:40.26ms
step:567/2330 train_time:22824ms step_avg:40.25ms
step:568/2330 train_time:22870ms step_avg:40.26ms
step:569/2330 train_time:22905ms step_avg:40.25ms
step:570/2330 train_time:22950ms step_avg:40.26ms
step:571/2330 train_time:22985ms step_avg:40.25ms
step:572/2330 train_time:23030ms step_avg:40.26ms
step:573/2330 train_time:23066ms step_avg:40.25ms
step:574/2330 train_time:23110ms step_avg:40.26ms
step:575/2330 train_time:23145ms step_avg:40.25ms
step:576/2330 train_time:23190ms step_avg:40.26ms
step:577/2330 train_time:23225ms step_avg:40.25ms
step:578/2330 train_time:23270ms step_avg:40.26ms
step:579/2330 train_time:23305ms step_avg:40.25ms
step:580/2330 train_time:23350ms step_avg:40.26ms
step:581/2330 train_time:23386ms step_avg:40.25ms
step:582/2330 train_time:23431ms step_avg:40.26ms
step:583/2330 train_time:23467ms step_avg:40.25ms
step:584/2330 train_time:23512ms step_avg:40.26ms
step:585/2330 train_time:23548ms step_avg:40.25ms
step:586/2330 train_time:23592ms step_avg:40.26ms
step:587/2330 train_time:23627ms step_avg:40.25ms
step:588/2330 train_time:23673ms step_avg:40.26ms
step:589/2330 train_time:23709ms step_avg:40.25ms
step:590/2330 train_time:23754ms step_avg:40.26ms
step:591/2330 train_time:23791ms step_avg:40.25ms
step:592/2330 train_time:23836ms step_avg:40.26ms
step:593/2330 train_time:23871ms step_avg:40.26ms
step:594/2330 train_time:23917ms step_avg:40.26ms
step:595/2330 train_time:23952ms step_avg:40.26ms
step:596/2330 train_time:23997ms step_avg:40.26ms
step:597/2330 train_time:24032ms step_avg:40.25ms
step:598/2330 train_time:24077ms step_avg:40.26ms
step:599/2330 train_time:24112ms step_avg:40.25ms
step:600/2330 train_time:24156ms step_avg:40.26ms
step:601/2330 train_time:24192ms step_avg:40.25ms
step:602/2330 train_time:24236ms step_avg:40.26ms
step:603/2330 train_time:24271ms step_avg:40.25ms
step:604/2330 train_time:24316ms step_avg:40.26ms
step:605/2330 train_time:24352ms step_avg:40.25ms
step:606/2330 train_time:24398ms step_avg:40.26ms
step:607/2330 train_time:24434ms step_avg:40.25ms
step:608/2330 train_time:24479ms step_avg:40.26ms
step:609/2330 train_time:24515ms step_avg:40.25ms
step:610/2330 train_time:24559ms step_avg:40.26ms
step:611/2330 train_time:24595ms step_avg:40.25ms
step:612/2330 train_time:24641ms step_avg:40.26ms
step:613/2330 train_time:24676ms step_avg:40.25ms
step:614/2330 train_time:24721ms step_avg:40.26ms
step:615/2330 train_time:24755ms step_avg:40.25ms
step:616/2330 train_time:24800ms step_avg:40.26ms
step:617/2330 train_time:24836ms step_avg:40.25ms
step:618/2330 train_time:24881ms step_avg:40.26ms
step:619/2330 train_time:24917ms step_avg:40.25ms
step:620/2330 train_time:24962ms step_avg:40.26ms
step:621/2330 train_time:24997ms step_avg:40.25ms
step:622/2330 train_time:25042ms step_avg:40.26ms
step:623/2330 train_time:25077ms step_avg:40.25ms
step:624/2330 train_time:25122ms step_avg:40.26ms
step:625/2330 train_time:25157ms step_avg:40.25ms
step:626/2330 train_time:25201ms step_avg:40.26ms
step:627/2330 train_time:25237ms step_avg:40.25ms
step:628/2330 train_time:25282ms step_avg:40.26ms
step:629/2330 train_time:25318ms step_avg:40.25ms
step:630/2330 train_time:25363ms step_avg:40.26ms
step:631/2330 train_time:25399ms step_avg:40.25ms
step:632/2330 train_time:25444ms step_avg:40.26ms
step:633/2330 train_time:25479ms step_avg:40.25ms
step:634/2330 train_time:25525ms step_avg:40.26ms
step:635/2330 train_time:25560ms step_avg:40.25ms
step:636/2330 train_time:25605ms step_avg:40.26ms
step:637/2330 train_time:25641ms step_avg:40.25ms
step:638/2330 train_time:25686ms step_avg:40.26ms
step:639/2330 train_time:25722ms step_avg:40.25ms
step:640/2330 train_time:25767ms step_avg:40.26ms
step:641/2330 train_time:25802ms step_avg:40.25ms
step:642/2330 train_time:25847ms step_avg:40.26ms
step:643/2330 train_time:25883ms step_avg:40.25ms
step:644/2330 train_time:25928ms step_avg:40.26ms
step:645/2330 train_time:25964ms step_avg:40.25ms
step:646/2330 train_time:26009ms step_avg:40.26ms
step:647/2330 train_time:26044ms step_avg:40.25ms
step:648/2330 train_time:26089ms step_avg:40.26ms
step:649/2330 train_time:26125ms step_avg:40.25ms
step:650/2330 train_time:26170ms step_avg:40.26ms
step:651/2330 train_time:26205ms step_avg:40.25ms
step:652/2330 train_time:26249ms step_avg:40.26ms
step:653/2330 train_time:26285ms step_avg:40.25ms
step:654/2330 train_time:26331ms step_avg:40.26ms
step:655/2330 train_time:26366ms step_avg:40.25ms
step:656/2330 train_time:26411ms step_avg:40.26ms
step:657/2330 train_time:26447ms step_avg:40.25ms
step:658/2330 train_time:26491ms step_avg:40.26ms
step:659/2330 train_time:26527ms step_avg:40.25ms
step:660/2330 train_time:26573ms step_avg:40.26ms
step:661/2330 train_time:26609ms step_avg:40.26ms
step:662/2330 train_time:26653ms step_avg:40.26ms
step:663/2330 train_time:26689ms step_avg:40.25ms
step:664/2330 train_time:26733ms step_avg:40.26ms
step:665/2330 train_time:26769ms step_avg:40.25ms
step:666/2330 train_time:26814ms step_avg:40.26ms
step:667/2330 train_time:26849ms step_avg:40.25ms
step:668/2330 train_time:26894ms step_avg:40.26ms
step:669/2330 train_time:26929ms step_avg:40.25ms
step:670/2330 train_time:26974ms step_avg:40.26ms
step:671/2330 train_time:27009ms step_avg:40.25ms
step:672/2330 train_time:27054ms step_avg:40.26ms
step:673/2330 train_time:27090ms step_avg:40.25ms
step:674/2330 train_time:27135ms step_avg:40.26ms
step:675/2330 train_time:27170ms step_avg:40.25ms
step:676/2330 train_time:27215ms step_avg:40.26ms
step:677/2330 train_time:27250ms step_avg:40.25ms
step:678/2330 train_time:27295ms step_avg:40.26ms
step:679/2330 train_time:27331ms step_avg:40.25ms
step:680/2330 train_time:27377ms step_avg:40.26ms
step:681/2330 train_time:27412ms step_avg:40.25ms
step:682/2330 train_time:27457ms step_avg:40.26ms
step:683/2330 train_time:27493ms step_avg:40.25ms
step:684/2330 train_time:27537ms step_avg:40.26ms
step:685/2330 train_time:27573ms step_avg:40.25ms
step:686/2330 train_time:27618ms step_avg:40.26ms
step:687/2330 train_time:27653ms step_avg:40.25ms
step:688/2330 train_time:27698ms step_avg:40.26ms
step:689/2330 train_time:27733ms step_avg:40.25ms
step:690/2330 train_time:27778ms step_avg:40.26ms
step:691/2330 train_time:27813ms step_avg:40.25ms
step:692/2330 train_time:27857ms step_avg:40.26ms
step:693/2330 train_time:27893ms step_avg:40.25ms
step:694/2330 train_time:27938ms step_avg:40.26ms
step:695/2330 train_time:27973ms step_avg:40.25ms
step:696/2330 train_time:28018ms step_avg:40.26ms
step:697/2330 train_time:28052ms step_avg:40.25ms
step:698/2330 train_time:28098ms step_avg:40.25ms
step:699/2330 train_time:28133ms step_avg:40.25ms
step:700/2330 train_time:28177ms step_avg:40.25ms
step:701/2330 train_time:28213ms step_avg:40.25ms
step:702/2330 train_time:28258ms step_avg:40.25ms
step:703/2330 train_time:28293ms step_avg:40.25ms
step:704/2330 train_time:28339ms step_avg:40.25ms
step:705/2330 train_time:28375ms step_avg:40.25ms
step:706/2330 train_time:28419ms step_avg:40.25ms
step:707/2330 train_time:28455ms step_avg:40.25ms
step:708/2330 train_time:28499ms step_avg:40.25ms
step:709/2330 train_time:28535ms step_avg:40.25ms
step:710/2330 train_time:28580ms step_avg:40.25ms
step:711/2330 train_time:28615ms step_avg:40.25ms
step:712/2330 train_time:28660ms step_avg:40.25ms
step:713/2330 train_time:28696ms step_avg:40.25ms
step:714/2330 train_time:28741ms step_avg:40.25ms
step:715/2330 train_time:28776ms step_avg:40.25ms
step:716/2330 train_time:28821ms step_avg:40.25ms
step:717/2330 train_time:28856ms step_avg:40.25ms
step:718/2330 train_time:28900ms step_avg:40.25ms
step:719/2330 train_time:28936ms step_avg:40.25ms
step:720/2330 train_time:28981ms step_avg:40.25ms
step:721/2330 train_time:29017ms step_avg:40.25ms
step:722/2330 train_time:29062ms step_avg:40.25ms
step:723/2330 train_time:29098ms step_avg:40.25ms
step:724/2330 train_time:29142ms step_avg:40.25ms
step:725/2330 train_time:29178ms step_avg:40.25ms
step:726/2330 train_time:29223ms step_avg:40.25ms
step:727/2330 train_time:29259ms step_avg:40.25ms
step:728/2330 train_time:29304ms step_avg:40.25ms
step:729/2330 train_time:29339ms step_avg:40.25ms
step:730/2330 train_time:29384ms step_avg:40.25ms
step:731/2330 train_time:29420ms step_avg:40.25ms
step:732/2330 train_time:29465ms step_avg:40.25ms
step:733/2330 train_time:29501ms step_avg:40.25ms
step:734/2330 train_time:29546ms step_avg:40.25ms
step:735/2330 train_time:29581ms step_avg:40.25ms
step:736/2330 train_time:29626ms step_avg:40.25ms
step:737/2330 train_time:29661ms step_avg:40.25ms
step:738/2330 train_time:29707ms step_avg:40.25ms
step:739/2330 train_time:29742ms step_avg:40.25ms
step:740/2330 train_time:29787ms step_avg:40.25ms
step:741/2330 train_time:29822ms step_avg:40.25ms
step:742/2330 train_time:29866ms step_avg:40.25ms
step:743/2330 train_time:29902ms step_avg:40.24ms
step:744/2330 train_time:29946ms step_avg:40.25ms
step:745/2330 train_time:29982ms step_avg:40.24ms
step:746/2330 train_time:30027ms step_avg:40.25ms
step:747/2330 train_time:30062ms step_avg:40.24ms
step:748/2330 train_time:30107ms step_avg:40.25ms
step:749/2330 train_time:30142ms step_avg:40.24ms
step:750/2330 train_time:30187ms step_avg:40.25ms
step:750/2330 val_loss:5.2232 train_time:30274ms step_avg:40.36ms
step:751/2330 train_time:30287ms step_avg:40.33ms
step:752/2330 train_time:30300ms step_avg:40.29ms
step:753/2330 train_time:30312ms step_avg:40.25ms
step:754/2330 train_time:30347ms step_avg:40.25ms
step:755/2330 train_time:30381ms step_avg:40.24ms
step:756/2330 train_time:30424ms step_avg:40.24ms
step:757/2330 train_time:30459ms step_avg:40.24ms
step:758/2330 train_time:30502ms step_avg:40.24ms
step:759/2330 train_time:30537ms step_avg:40.23ms
step:760/2330 train_time:30582ms step_avg:40.24ms
step:761/2330 train_time:30622ms step_avg:40.24ms
step:762/2330 train_time:30671ms step_avg:40.25ms
step:763/2330 train_time:30707ms step_avg:40.24ms
step:764/2330 train_time:30754ms step_avg:40.25ms
step:765/2330 train_time:30790ms step_avg:40.25ms
step:766/2330 train_time:30834ms step_avg:40.25ms
step:767/2330 train_time:30869ms step_avg:40.25ms
step:768/2330 train_time:30913ms step_avg:40.25ms
step:769/2330 train_time:30948ms step_avg:40.24ms
step:770/2330 train_time:30992ms step_avg:40.25ms
step:771/2330 train_time:31027ms step_avg:40.24ms
step:772/2330 train_time:31072ms step_avg:40.25ms
step:773/2330 train_time:31107ms step_avg:40.24ms
step:774/2330 train_time:31151ms step_avg:40.25ms
step:775/2330 train_time:31186ms step_avg:40.24ms
step:776/2330 train_time:31232ms step_avg:40.25ms
step:777/2330 train_time:31269ms step_avg:40.24ms
step:778/2330 train_time:31314ms step_avg:40.25ms
step:779/2330 train_time:31350ms step_avg:40.24ms
step:780/2330 train_time:31395ms step_avg:40.25ms
step:781/2330 train_time:31429ms step_avg:40.24ms
step:782/2330 train_time:31475ms step_avg:40.25ms
step:783/2330 train_time:31510ms step_avg:40.24ms
step:784/2330 train_time:31556ms step_avg:40.25ms
step:785/2330 train_time:31591ms step_avg:40.24ms
step:786/2330 train_time:31636ms step_avg:40.25ms
step:787/2330 train_time:31672ms step_avg:40.24ms
step:788/2330 train_time:31718ms step_avg:40.25ms
step:789/2330 train_time:31754ms step_avg:40.25ms
step:790/2330 train_time:31799ms step_avg:40.25ms
step:791/2330 train_time:31834ms step_avg:40.25ms
step:792/2330 train_time:31878ms step_avg:40.25ms
step:793/2330 train_time:31914ms step_avg:40.24ms
step:794/2330 train_time:31959ms step_avg:40.25ms
step:795/2330 train_time:31994ms step_avg:40.24ms
step:796/2330 train_time:32038ms step_avg:40.25ms
step:797/2330 train_time:32074ms step_avg:40.24ms
step:798/2330 train_time:32118ms step_avg:40.25ms
step:799/2330 train_time:32154ms step_avg:40.24ms
step:800/2330 train_time:32199ms step_avg:40.25ms
step:801/2330 train_time:32235ms step_avg:40.24ms
step:802/2330 train_time:32280ms step_avg:40.25ms
step:803/2330 train_time:32317ms step_avg:40.24ms
step:804/2330 train_time:32361ms step_avg:40.25ms
step:805/2330 train_time:32397ms step_avg:40.24ms
step:806/2330 train_time:32441ms step_avg:40.25ms
step:807/2330 train_time:32477ms step_avg:40.24ms
step:808/2330 train_time:32521ms step_avg:40.25ms
step:809/2330 train_time:32557ms step_avg:40.24ms
step:810/2330 train_time:32603ms step_avg:40.25ms
step:811/2330 train_time:32638ms step_avg:40.24ms
step:812/2330 train_time:32683ms step_avg:40.25ms
step:813/2330 train_time:32719ms step_avg:40.24ms
step:814/2330 train_time:32764ms step_avg:40.25ms
step:815/2330 train_time:32800ms step_avg:40.25ms
step:816/2330 train_time:32845ms step_avg:40.25ms
step:817/2330 train_time:32881ms step_avg:40.25ms
step:818/2330 train_time:32925ms step_avg:40.25ms
step:819/2330 train_time:32961ms step_avg:40.25ms
step:820/2330 train_time:33006ms step_avg:40.25ms
step:821/2330 train_time:33041ms step_avg:40.24ms
step:822/2330 train_time:33085ms step_avg:40.25ms
step:823/2330 train_time:33120ms step_avg:40.24ms
step:824/2330 train_time:33167ms step_avg:40.25ms
step:825/2330 train_time:33202ms step_avg:40.24ms
step:826/2330 train_time:33247ms step_avg:40.25ms
step:827/2330 train_time:33282ms step_avg:40.24ms
step:828/2330 train_time:33327ms step_avg:40.25ms
step:829/2330 train_time:33362ms step_avg:40.24ms
step:830/2330 train_time:33407ms step_avg:40.25ms
step:831/2330 train_time:33442ms step_avg:40.24ms
step:832/2330 train_time:33488ms step_avg:40.25ms
step:833/2330 train_time:33524ms step_avg:40.25ms
step:834/2330 train_time:33569ms step_avg:40.25ms
step:835/2330 train_time:33605ms step_avg:40.25ms
step:836/2330 train_time:33651ms step_avg:40.25ms
step:837/2330 train_time:33687ms step_avg:40.25ms
step:838/2330 train_time:33732ms step_avg:40.25ms
step:839/2330 train_time:33768ms step_avg:40.25ms
step:840/2330 train_time:33813ms step_avg:40.25ms
step:841/2330 train_time:33848ms step_avg:40.25ms
step:842/2330 train_time:33893ms step_avg:40.25ms
step:843/2330 train_time:33928ms step_avg:40.25ms
step:844/2330 train_time:33973ms step_avg:40.25ms
step:845/2330 train_time:34009ms step_avg:40.25ms
step:846/2330 train_time:34053ms step_avg:40.25ms
step:847/2330 train_time:34088ms step_avg:40.25ms
step:848/2330 train_time:34133ms step_avg:40.25ms
step:849/2330 train_time:34168ms step_avg:40.25ms
step:850/2330 train_time:34214ms step_avg:40.25ms
step:851/2330 train_time:34249ms step_avg:40.25ms
step:852/2330 train_time:34293ms step_avg:40.25ms
step:853/2330 train_time:34329ms step_avg:40.25ms
step:854/2330 train_time:34375ms step_avg:40.25ms
step:855/2330 train_time:34410ms step_avg:40.25ms
step:856/2330 train_time:34455ms step_avg:40.25ms
step:857/2330 train_time:34491ms step_avg:40.25ms
step:858/2330 train_time:34536ms step_avg:40.25ms
step:859/2330 train_time:34571ms step_avg:40.25ms
step:860/2330 train_time:34616ms step_avg:40.25ms
step:861/2330 train_time:34652ms step_avg:40.25ms
step:862/2330 train_time:34696ms step_avg:40.25ms
step:863/2330 train_time:34732ms step_avg:40.25ms
step:864/2330 train_time:34776ms step_avg:40.25ms
step:865/2330 train_time:34811ms step_avg:40.24ms
step:866/2330 train_time:34856ms step_avg:40.25ms
step:867/2330 train_time:34891ms step_avg:40.24ms
step:868/2330 train_time:34935ms step_avg:40.25ms
step:869/2330 train_time:34971ms step_avg:40.24ms
step:870/2330 train_time:35016ms step_avg:40.25ms
step:871/2330 train_time:35051ms step_avg:40.24ms
step:872/2330 train_time:35096ms step_avg:40.25ms
step:873/2330 train_time:35131ms step_avg:40.24ms
step:874/2330 train_time:35176ms step_avg:40.25ms
step:875/2330 train_time:35211ms step_avg:40.24ms
step:876/2330 train_time:35256ms step_avg:40.25ms
step:877/2330 train_time:35291ms step_avg:40.24ms
step:878/2330 train_time:35336ms step_avg:40.25ms
step:879/2330 train_time:35371ms step_avg:40.24ms
step:880/2330 train_time:35416ms step_avg:40.25ms
step:881/2330 train_time:35452ms step_avg:40.24ms
step:882/2330 train_time:35497ms step_avg:40.25ms
step:883/2330 train_time:35532ms step_avg:40.24ms
step:884/2330 train_time:35577ms step_avg:40.25ms
step:885/2330 train_time:35613ms step_avg:40.24ms
step:886/2330 train_time:35658ms step_avg:40.25ms
step:887/2330 train_time:35694ms step_avg:40.24ms
step:888/2330 train_time:35739ms step_avg:40.25ms
step:889/2330 train_time:35775ms step_avg:40.24ms
step:890/2330 train_time:35820ms step_avg:40.25ms
step:891/2330 train_time:35855ms step_avg:40.24ms
step:892/2330 train_time:35900ms step_avg:40.25ms
step:893/2330 train_time:35935ms step_avg:40.24ms
step:894/2330 train_time:35981ms step_avg:40.25ms
step:895/2330 train_time:36016ms step_avg:40.24ms
step:896/2330 train_time:36060ms step_avg:40.25ms
step:897/2330 train_time:36096ms step_avg:40.24ms
step:898/2330 train_time:36141ms step_avg:40.25ms
step:899/2330 train_time:36177ms step_avg:40.24ms
step:900/2330 train_time:36222ms step_avg:40.25ms
step:901/2330 train_time:36258ms step_avg:40.24ms
step:902/2330 train_time:36302ms step_avg:40.25ms
step:903/2330 train_time:36338ms step_avg:40.24ms
step:904/2330 train_time:36383ms step_avg:40.25ms
step:905/2330 train_time:36419ms step_avg:40.24ms
step:906/2330 train_time:36464ms step_avg:40.25ms
step:907/2330 train_time:36499ms step_avg:40.24ms
step:908/2330 train_time:36543ms step_avg:40.25ms
step:909/2330 train_time:36578ms step_avg:40.24ms
step:910/2330 train_time:36624ms step_avg:40.25ms
step:911/2330 train_time:36659ms step_avg:40.24ms
step:912/2330 train_time:36704ms step_avg:40.25ms
step:913/2330 train_time:36740ms step_avg:40.24ms
step:914/2330 train_time:36784ms step_avg:40.25ms
step:915/2330 train_time:36819ms step_avg:40.24ms
step:916/2330 train_time:36864ms step_avg:40.24ms
step:917/2330 train_time:36901ms step_avg:40.24ms
step:918/2330 train_time:36946ms step_avg:40.25ms
step:919/2330 train_time:36982ms step_avg:40.24ms
step:920/2330 train_time:37026ms step_avg:40.25ms
step:921/2330 train_time:37062ms step_avg:40.24ms
step:922/2330 train_time:37106ms step_avg:40.25ms
step:923/2330 train_time:37142ms step_avg:40.24ms
step:924/2330 train_time:37187ms step_avg:40.25ms
step:925/2330 train_time:37222ms step_avg:40.24ms
step:926/2330 train_time:37267ms step_avg:40.24ms
step:927/2330 train_time:37302ms step_avg:40.24ms
step:928/2330 train_time:37347ms step_avg:40.24ms
step:929/2330 train_time:37382ms step_avg:40.24ms
step:930/2330 train_time:37427ms step_avg:40.24ms
step:931/2330 train_time:37462ms step_avg:40.24ms
step:932/2330 train_time:37507ms step_avg:40.24ms
step:933/2330 train_time:37542ms step_avg:40.24ms
step:934/2330 train_time:37586ms step_avg:40.24ms
step:935/2330 train_time:37622ms step_avg:40.24ms
step:936/2330 train_time:37667ms step_avg:40.24ms
step:937/2330 train_time:37703ms step_avg:40.24ms
step:938/2330 train_time:37747ms step_avg:40.24ms
step:939/2330 train_time:37783ms step_avg:40.24ms
step:940/2330 train_time:37828ms step_avg:40.24ms
step:941/2330 train_time:37863ms step_avg:40.24ms
step:942/2330 train_time:37908ms step_avg:40.24ms
step:943/2330 train_time:37943ms step_avg:40.24ms
step:944/2330 train_time:37988ms step_avg:40.24ms
step:945/2330 train_time:38023ms step_avg:40.24ms
step:946/2330 train_time:38067ms step_avg:40.24ms
step:947/2330 train_time:38103ms step_avg:40.24ms
step:948/2330 train_time:38148ms step_avg:40.24ms
step:949/2330 train_time:38183ms step_avg:40.24ms
step:950/2330 train_time:38229ms step_avg:40.24ms
step:951/2330 train_time:38264ms step_avg:40.24ms
step:952/2330 train_time:38308ms step_avg:40.24ms
step:953/2330 train_time:38344ms step_avg:40.24ms
step:954/2330 train_time:38389ms step_avg:40.24ms
step:955/2330 train_time:38424ms step_avg:40.23ms
step:956/2330 train_time:38470ms step_avg:40.24ms
step:957/2330 train_time:38506ms step_avg:40.24ms
step:958/2330 train_time:38552ms step_avg:40.24ms
step:959/2330 train_time:38587ms step_avg:40.24ms
step:960/2330 train_time:38632ms step_avg:40.24ms
step:961/2330 train_time:38668ms step_avg:40.24ms
step:962/2330 train_time:38712ms step_avg:40.24ms
step:963/2330 train_time:38748ms step_avg:40.24ms
step:964/2330 train_time:38794ms step_avg:40.24ms
step:965/2330 train_time:38830ms step_avg:40.24ms
step:966/2330 train_time:38875ms step_avg:40.24ms
step:967/2330 train_time:38910ms step_avg:40.24ms
step:968/2330 train_time:38955ms step_avg:40.24ms
step:969/2330 train_time:38990ms step_avg:40.24ms
step:970/2330 train_time:39034ms step_avg:40.24ms
step:971/2330 train_time:39069ms step_avg:40.24ms
step:972/2330 train_time:39115ms step_avg:40.24ms
step:973/2330 train_time:39149ms step_avg:40.24ms
step:974/2330 train_time:39194ms step_avg:40.24ms
step:975/2330 train_time:39229ms step_avg:40.24ms
step:976/2330 train_time:39274ms step_avg:40.24ms
step:977/2330 train_time:39310ms step_avg:40.24ms
step:978/2330 train_time:39355ms step_avg:40.24ms
step:979/2330 train_time:39391ms step_avg:40.24ms
step:980/2330 train_time:39435ms step_avg:40.24ms
step:981/2330 train_time:39471ms step_avg:40.24ms
step:982/2330 train_time:39516ms step_avg:40.24ms
step:983/2330 train_time:39551ms step_avg:40.24ms
step:984/2330 train_time:39596ms step_avg:40.24ms
step:985/2330 train_time:39631ms step_avg:40.23ms
step:986/2330 train_time:39675ms step_avg:40.24ms
step:987/2330 train_time:39711ms step_avg:40.23ms
step:988/2330 train_time:39756ms step_avg:40.24ms
step:989/2330 train_time:39792ms step_avg:40.23ms
step:990/2330 train_time:39837ms step_avg:40.24ms
step:991/2330 train_time:39873ms step_avg:40.23ms
step:992/2330 train_time:39917ms step_avg:40.24ms
step:993/2330 train_time:39953ms step_avg:40.23ms
step:994/2330 train_time:39997ms step_avg:40.24ms
step:995/2330 train_time:40032ms step_avg:40.23ms
step:996/2330 train_time:40077ms step_avg:40.24ms
step:997/2330 train_time:40113ms step_avg:40.23ms
step:998/2330 train_time:40157ms step_avg:40.24ms
step:999/2330 train_time:40193ms step_avg:40.23ms
step:1000/2330 train_time:40237ms step_avg:40.24ms
step:1000/2330 val_loss:5.1916 train_time:40327ms step_avg:40.33ms
step:1001/2330 train_time:40341ms step_avg:40.30ms
step:1002/2330 train_time:40354ms step_avg:40.27ms
step:1003/2330 train_time:40366ms step_avg:40.25ms
step:1004/2330 train_time:40401ms step_avg:40.24ms
step:1005/2330 train_time:40435ms step_avg:40.23ms
step:1006/2330 train_time:40479ms step_avg:40.24ms
step:1007/2330 train_time:40513ms step_avg:40.23ms
step:1008/2330 train_time:40557ms step_avg:40.24ms
step:1009/2330 train_time:40593ms step_avg:40.23ms
step:1010/2330 train_time:40642ms step_avg:40.24ms
step:1011/2330 train_time:40681ms step_avg:40.24ms
step:1012/2330 train_time:40727ms step_avg:40.24ms
step:1013/2330 train_time:40763ms step_avg:40.24ms
step:1014/2330 train_time:40807ms step_avg:40.24ms
step:1015/2330 train_time:40842ms step_avg:40.24ms
step:1016/2330 train_time:40887ms step_avg:40.24ms
step:1017/2330 train_time:40922ms step_avg:40.24ms
step:1018/2330 train_time:40966ms step_avg:40.24ms
step:1019/2330 train_time:41001ms step_avg:40.24ms
step:1020/2330 train_time:41045ms step_avg:40.24ms
step:1021/2330 train_time:41080ms step_avg:40.24ms
step:1022/2330 train_time:41125ms step_avg:40.24ms
step:1023/2330 train_time:41160ms step_avg:40.23ms
step:1024/2330 train_time:41203ms step_avg:40.24ms
step:1025/2330 train_time:41240ms step_avg:40.23ms
step:1026/2330 train_time:41285ms step_avg:40.24ms
step:1027/2330 train_time:41322ms step_avg:40.24ms
step:1028/2330 train_time:41368ms step_avg:40.24ms
step:1029/2330 train_time:41404ms step_avg:40.24ms
step:1030/2330 train_time:41449ms step_avg:40.24ms
step:1031/2330 train_time:41485ms step_avg:40.24ms
step:1032/2330 train_time:41530ms step_avg:40.24ms
step:1033/2330 train_time:41566ms step_avg:40.24ms
step:1034/2330 train_time:41611ms step_avg:40.24ms
step:1035/2330 train_time:41649ms step_avg:40.24ms
step:1036/2330 train_time:41696ms step_avg:40.25ms
step:1037/2330 train_time:41732ms step_avg:40.24ms
step:1038/2330 train_time:41777ms step_avg:40.25ms
step:1039/2330 train_time:41812ms step_avg:40.24ms
step:1040/2330 train_time:41857ms step_avg:40.25ms
step:1041/2330 train_time:41892ms step_avg:40.24ms
step:1042/2330 train_time:41936ms step_avg:40.25ms
step:1043/2330 train_time:41972ms step_avg:40.24ms
step:1044/2330 train_time:42016ms step_avg:40.25ms
step:1045/2330 train_time:42052ms step_avg:40.24ms
step:1046/2330 train_time:42096ms step_avg:40.24ms
step:1047/2330 train_time:42132ms step_avg:40.24ms
step:1048/2330 train_time:42176ms step_avg:40.24ms
step:1049/2330 train_time:42212ms step_avg:40.24ms
step:1050/2330 train_time:42257ms step_avg:40.24ms
step:1051/2330 train_time:42293ms step_avg:40.24ms
step:1052/2330 train_time:42338ms step_avg:40.25ms
step:1053/2330 train_time:42374ms step_avg:40.24ms
step:1054/2330 train_time:42419ms step_avg:40.25ms
step:1055/2330 train_time:42455ms step_avg:40.24ms
step:1056/2330 train_time:42500ms step_avg:40.25ms
step:1057/2330 train_time:42536ms step_avg:40.24ms
step:1058/2330 train_time:42581ms step_avg:40.25ms
step:1059/2330 train_time:42616ms step_avg:40.24ms
step:1060/2330 train_time:42661ms step_avg:40.25ms
step:1061/2330 train_time:42696ms step_avg:40.24ms
step:1062/2330 train_time:42741ms step_avg:40.25ms
step:1063/2330 train_time:42777ms step_avg:40.24ms
step:1064/2330 train_time:42822ms step_avg:40.25ms
step:1065/2330 train_time:42857ms step_avg:40.24ms
step:1066/2330 train_time:42902ms step_avg:40.25ms
step:1067/2330 train_time:42937ms step_avg:40.24ms
step:1068/2330 train_time:42982ms step_avg:40.25ms
step:1069/2330 train_time:43018ms step_avg:40.24ms
step:1070/2330 train_time:43062ms step_avg:40.24ms
step:1071/2330 train_time:43097ms step_avg:40.24ms
step:1072/2330 train_time:43141ms step_avg:40.24ms
step:1073/2330 train_time:43177ms step_avg:40.24ms
step:1074/2330 train_time:43222ms step_avg:40.24ms
step:1075/2330 train_time:43258ms step_avg:40.24ms
step:1076/2330 train_time:43304ms step_avg:40.25ms
step:1077/2330 train_time:43339ms step_avg:40.24ms
step:1078/2330 train_time:43384ms step_avg:40.24ms
step:1079/2330 train_time:43419ms step_avg:40.24ms
step:1080/2330 train_time:43464ms step_avg:40.24ms
step:1081/2330 train_time:43499ms step_avg:40.24ms
step:1082/2330 train_time:43544ms step_avg:40.24ms
step:1083/2330 train_time:43579ms step_avg:40.24ms
step:1084/2330 train_time:43625ms step_avg:40.24ms
step:1085/2330 train_time:43660ms step_avg:40.24ms
step:1086/2330 train_time:43705ms step_avg:40.24ms
step:1087/2330 train_time:43741ms step_avg:40.24ms
step:1088/2330 train_time:43786ms step_avg:40.24ms
step:1089/2330 train_time:43821ms step_avg:40.24ms
step:1090/2330 train_time:43865ms step_avg:40.24ms
step:1091/2330 train_time:43900ms step_avg:40.24ms
step:1092/2330 train_time:43944ms step_avg:40.24ms
step:1093/2330 train_time:43980ms step_avg:40.24ms
step:1094/2330 train_time:44024ms step_avg:40.24ms
step:1095/2330 train_time:44059ms step_avg:40.24ms
step:1096/2330 train_time:44104ms step_avg:40.24ms
step:1097/2330 train_time:44139ms step_avg:40.24ms
step:1098/2330 train_time:44184ms step_avg:40.24ms
step:1099/2330 train_time:44219ms step_avg:40.24ms
step:1100/2330 train_time:44264ms step_avg:40.24ms
step:1101/2330 train_time:44300ms step_avg:40.24ms
step:1102/2330 train_time:44345ms step_avg:40.24ms
step:1103/2330 train_time:44380ms step_avg:40.24ms
step:1104/2330 train_time:44424ms step_avg:40.24ms
step:1105/2330 train_time:44461ms step_avg:40.24ms
step:1106/2330 train_time:44506ms step_avg:40.24ms
step:1107/2330 train_time:44542ms step_avg:40.24ms
step:1108/2330 train_time:44586ms step_avg:40.24ms
step:1109/2330 train_time:44621ms step_avg:40.24ms
step:1110/2330 train_time:44666ms step_avg:40.24ms
step:1111/2330 train_time:44702ms step_avg:40.24ms
step:1112/2330 train_time:44746ms step_avg:40.24ms
step:1113/2330 train_time:44782ms step_avg:40.24ms
step:1114/2330 train_time:44827ms step_avg:40.24ms
step:1115/2330 train_time:44862ms step_avg:40.23ms
step:1116/2330 train_time:44906ms step_avg:40.24ms
step:1117/2330 train_time:44942ms step_avg:40.23ms
step:1118/2330 train_time:44986ms step_avg:40.24ms
step:1119/2330 train_time:45021ms step_avg:40.23ms
step:1120/2330 train_time:45066ms step_avg:40.24ms
step:1121/2330 train_time:45102ms step_avg:40.23ms
step:1122/2330 train_time:45148ms step_avg:40.24ms
step:1123/2330 train_time:45184ms step_avg:40.23ms
step:1124/2330 train_time:45228ms step_avg:40.24ms
step:1125/2330 train_time:45264ms step_avg:40.23ms
step:1126/2330 train_time:45308ms step_avg:40.24ms
step:1127/2330 train_time:45344ms step_avg:40.23ms
step:1128/2330 train_time:45388ms step_avg:40.24ms
step:1129/2330 train_time:45424ms step_avg:40.23ms
step:1130/2330 train_time:45469ms step_avg:40.24ms
step:1131/2330 train_time:45504ms step_avg:40.23ms
step:1132/2330 train_time:45550ms step_avg:40.24ms
step:1133/2330 train_time:45585ms step_avg:40.23ms
step:1134/2330 train_time:45629ms step_avg:40.24ms
step:1135/2330 train_time:45664ms step_avg:40.23ms
step:1136/2330 train_time:45709ms step_avg:40.24ms
step:1137/2330 train_time:45744ms step_avg:40.23ms
step:1138/2330 train_time:45789ms step_avg:40.24ms
step:1139/2330 train_time:45824ms step_avg:40.23ms
step:1140/2330 train_time:45869ms step_avg:40.24ms
step:1141/2330 train_time:45904ms step_avg:40.23ms
step:1142/2330 train_time:45949ms step_avg:40.24ms
step:1143/2330 train_time:45985ms step_avg:40.23ms
step:1144/2330 train_time:46029ms step_avg:40.24ms
step:1145/2330 train_time:46065ms step_avg:40.23ms
step:1146/2330 train_time:46110ms step_avg:40.24ms
step:1147/2330 train_time:46146ms step_avg:40.23ms
step:1148/2330 train_time:46192ms step_avg:40.24ms
step:1149/2330 train_time:46228ms step_avg:40.23ms
step:1150/2330 train_time:46274ms step_avg:40.24ms
step:1151/2330 train_time:46310ms step_avg:40.23ms
step:1152/2330 train_time:46355ms step_avg:40.24ms
step:1153/2330 train_time:46390ms step_avg:40.23ms
step:1154/2330 train_time:46435ms step_avg:40.24ms
step:1155/2330 train_time:46470ms step_avg:40.23ms
step:1156/2330 train_time:46514ms step_avg:40.24ms
step:1157/2330 train_time:46549ms step_avg:40.23ms
step:1158/2330 train_time:46593ms step_avg:40.24ms
step:1159/2330 train_time:46628ms step_avg:40.23ms
step:1160/2330 train_time:46672ms step_avg:40.23ms
step:1161/2330 train_time:46707ms step_avg:40.23ms
step:1162/2330 train_time:46752ms step_avg:40.23ms
step:1163/2330 train_time:46787ms step_avg:40.23ms
step:1164/2330 train_time:46832ms step_avg:40.23ms
step:1165/2330 train_time:46867ms step_avg:40.23ms
step:1166/2330 train_time:46912ms step_avg:40.23ms
step:1167/2330 train_time:46948ms step_avg:40.23ms
step:1168/2330 train_time:46992ms step_avg:40.23ms
step:1169/2330 train_time:47028ms step_avg:40.23ms
step:1170/2330 train_time:47073ms step_avg:40.23ms
step:1171/2330 train_time:47109ms step_avg:40.23ms
step:1172/2330 train_time:47155ms step_avg:40.23ms
step:1173/2330 train_time:47191ms step_avg:40.23ms
step:1174/2330 train_time:47236ms step_avg:40.23ms
step:1175/2330 train_time:47271ms step_avg:40.23ms
step:1176/2330 train_time:47316ms step_avg:40.23ms
step:1177/2330 train_time:47351ms step_avg:40.23ms
step:1178/2330 train_time:47396ms step_avg:40.23ms
step:1179/2330 train_time:47431ms step_avg:40.23ms
step:1180/2330 train_time:47475ms step_avg:40.23ms
step:1181/2330 train_time:47510ms step_avg:40.23ms
step:1182/2330 train_time:47556ms step_avg:40.23ms
step:1183/2330 train_time:47590ms step_avg:40.23ms
step:1184/2330 train_time:47635ms step_avg:40.23ms
step:1185/2330 train_time:47670ms step_avg:40.23ms
step:1186/2330 train_time:47715ms step_avg:40.23ms
step:1187/2330 train_time:47750ms step_avg:40.23ms
step:1188/2330 train_time:47795ms step_avg:40.23ms
step:1189/2330 train_time:47830ms step_avg:40.23ms
step:1190/2330 train_time:47875ms step_avg:40.23ms
step:1191/2330 train_time:47910ms step_avg:40.23ms
step:1192/2330 train_time:47955ms step_avg:40.23ms
step:1193/2330 train_time:47990ms step_avg:40.23ms
step:1194/2330 train_time:48035ms step_avg:40.23ms
step:1195/2330 train_time:48070ms step_avg:40.23ms
step:1196/2330 train_time:48115ms step_avg:40.23ms
step:1197/2330 train_time:48152ms step_avg:40.23ms
step:1198/2330 train_time:48196ms step_avg:40.23ms
step:1199/2330 train_time:48231ms step_avg:40.23ms
step:1200/2330 train_time:48275ms step_avg:40.23ms
step:1201/2330 train_time:48312ms step_avg:40.23ms
step:1202/2330 train_time:48356ms step_avg:40.23ms
step:1203/2330 train_time:48392ms step_avg:40.23ms
step:1204/2330 train_time:48438ms step_avg:40.23ms
step:1205/2330 train_time:48474ms step_avg:40.23ms
step:1206/2330 train_time:48519ms step_avg:40.23ms
step:1207/2330 train_time:48554ms step_avg:40.23ms
step:1208/2330 train_time:48599ms step_avg:40.23ms
step:1209/2330 train_time:48635ms step_avg:40.23ms
step:1210/2330 train_time:48681ms step_avg:40.23ms
step:1211/2330 train_time:48716ms step_avg:40.23ms
step:1212/2330 train_time:48761ms step_avg:40.23ms
step:1213/2330 train_time:48797ms step_avg:40.23ms
step:1214/2330 train_time:48842ms step_avg:40.23ms
step:1215/2330 train_time:48877ms step_avg:40.23ms
step:1216/2330 train_time:48923ms step_avg:40.23ms
step:1217/2330 train_time:48958ms step_avg:40.23ms
step:1218/2330 train_time:49003ms step_avg:40.23ms
step:1219/2330 train_time:49039ms step_avg:40.23ms
step:1220/2330 train_time:49083ms step_avg:40.23ms
step:1221/2330 train_time:49118ms step_avg:40.23ms
step:1222/2330 train_time:49163ms step_avg:40.23ms
step:1223/2330 train_time:49198ms step_avg:40.23ms
step:1224/2330 train_time:49243ms step_avg:40.23ms
step:1225/2330 train_time:49278ms step_avg:40.23ms
step:1226/2330 train_time:49323ms step_avg:40.23ms
step:1227/2330 train_time:49359ms step_avg:40.23ms
step:1228/2330 train_time:49404ms step_avg:40.23ms
step:1229/2330 train_time:49440ms step_avg:40.23ms
step:1230/2330 train_time:49485ms step_avg:40.23ms
step:1231/2330 train_time:49520ms step_avg:40.23ms
step:1232/2330 train_time:49564ms step_avg:40.23ms
step:1233/2330 train_time:49600ms step_avg:40.23ms
step:1234/2330 train_time:49645ms step_avg:40.23ms
step:1235/2330 train_time:49680ms step_avg:40.23ms
step:1236/2330 train_time:49724ms step_avg:40.23ms
step:1237/2330 train_time:49760ms step_avg:40.23ms
step:1238/2330 train_time:49805ms step_avg:40.23ms
step:1239/2330 train_time:49840ms step_avg:40.23ms
step:1240/2330 train_time:49885ms step_avg:40.23ms
step:1241/2330 train_time:49921ms step_avg:40.23ms
step:1242/2330 train_time:49965ms step_avg:40.23ms
step:1243/2330 train_time:50000ms step_avg:40.23ms
step:1244/2330 train_time:50045ms step_avg:40.23ms
step:1245/2330 train_time:50081ms step_avg:40.23ms
step:1246/2330 train_time:50125ms step_avg:40.23ms
step:1247/2330 train_time:50160ms step_avg:40.22ms
step:1248/2330 train_time:50205ms step_avg:40.23ms
step:1249/2330 train_time:50240ms step_avg:40.22ms
step:1250/2330 train_time:50285ms step_avg:40.23ms
step:1250/2330 val_loss:5.1619 train_time:50374ms step_avg:40.30ms
step:1251/2330 train_time:50388ms step_avg:40.28ms
step:1252/2330 train_time:50400ms step_avg:40.26ms
step:1253/2330 train_time:50411ms step_avg:40.23ms
step:1254/2330 train_time:50447ms step_avg:40.23ms
step:1255/2330 train_time:50481ms step_avg:40.22ms
step:1256/2330 train_time:50526ms step_avg:40.23ms
step:1257/2330 train_time:50562ms step_avg:40.22ms
step:1258/2330 train_time:50607ms step_avg:40.23ms
step:1259/2330 train_time:50642ms step_avg:40.22ms
step:1260/2330 train_time:50692ms step_avg:40.23ms
step:1261/2330 train_time:50731ms step_avg:40.23ms
step:1262/2330 train_time:50778ms step_avg:40.24ms
step:1263/2330 train_time:50814ms step_avg:40.23ms
step:1264/2330 train_time:50859ms step_avg:40.24ms
step:1265/2330 train_time:50895ms step_avg:40.23ms
step:1266/2330 train_time:50939ms step_avg:40.24ms
step:1267/2330 train_time:50975ms step_avg:40.23ms
step:1268/2330 train_time:51020ms step_avg:40.24ms
step:1269/2330 train_time:51055ms step_avg:40.23ms
step:1270/2330 train_time:51099ms step_avg:40.24ms
step:1271/2330 train_time:51347ms step_avg:40.40ms
step:1272/2330 train_time:51390ms step_avg:40.40ms
step:1273/2330 train_time:51424ms step_avg:40.40ms
step:1274/2330 train_time:51468ms step_avg:40.40ms
step:1275/2330 train_time:51502ms step_avg:40.39ms
step:1276/2330 train_time:51546ms step_avg:40.40ms
step:1277/2330 train_time:51581ms step_avg:40.39ms
step:1278/2330 train_time:51624ms step_avg:40.39ms
step:1279/2330 train_time:51659ms step_avg:40.39ms
step:1280/2330 train_time:51702ms step_avg:40.39ms
step:1281/2330 train_time:51737ms step_avg:40.39ms
step:1282/2330 train_time:51780ms step_avg:40.39ms
step:1283/2330 train_time:51814ms step_avg:40.39ms
step:1284/2330 train_time:51857ms step_avg:40.39ms
step:1285/2330 train_time:51892ms step_avg:40.38ms
step:1286/2330 train_time:51936ms step_avg:40.39ms
step:1287/2330 train_time:51971ms step_avg:40.38ms
step:1288/2330 train_time:52015ms step_avg:40.38ms
step:1289/2330 train_time:52049ms step_avg:40.38ms
step:1290/2330 train_time:52093ms step_avg:40.38ms
step:1291/2330 train_time:52128ms step_avg:40.38ms
step:1292/2330 train_time:52173ms step_avg:40.38ms
step:1293/2330 train_time:52213ms step_avg:40.38ms
step:1294/2330 train_time:52262ms step_avg:40.39ms
step:1295/2330 train_time:52300ms step_avg:40.39ms
step:1296/2330 train_time:52347ms step_avg:40.39ms
step:1297/2330 train_time:52383ms step_avg:40.39ms
step:1298/2330 train_time:52429ms step_avg:40.39ms
step:1299/2330 train_time:52465ms step_avg:40.39ms
step:1300/2330 train_time:52509ms step_avg:40.39ms
step:1301/2330 train_time:52545ms step_avg:40.39ms
step:1302/2330 train_time:52589ms step_avg:40.39ms
step:1303/2330 train_time:52625ms step_avg:40.39ms
step:1304/2330 train_time:52669ms step_avg:40.39ms
step:1305/2330 train_time:52703ms step_avg:40.39ms
step:1306/2330 train_time:52748ms step_avg:40.39ms
step:1307/2330 train_time:52783ms step_avg:40.38ms
step:1308/2330 train_time:52827ms step_avg:40.39ms
step:1309/2330 train_time:52862ms step_avg:40.38ms
step:1310/2330 train_time:52907ms step_avg:40.39ms
step:1311/2330 train_time:52941ms step_avg:40.38ms
step:1312/2330 train_time:52986ms step_avg:40.39ms
step:1313/2330 train_time:53020ms step_avg:40.38ms
step:1314/2330 train_time:53065ms step_avg:40.38ms
step:1315/2330 train_time:53100ms step_avg:40.38ms
step:1316/2330 train_time:53146ms step_avg:40.38ms
step:1317/2330 train_time:53182ms step_avg:40.38ms
step:1318/2330 train_time:53229ms step_avg:40.39ms
step:1319/2330 train_time:53265ms step_avg:40.38ms
step:1320/2330 train_time:53311ms step_avg:40.39ms
step:1321/2330 train_time:53347ms step_avg:40.38ms
step:1322/2330 train_time:53393ms step_avg:40.39ms
step:1323/2330 train_time:53429ms step_avg:40.38ms
step:1324/2330 train_time:53474ms step_avg:40.39ms
step:1325/2330 train_time:53509ms step_avg:40.38ms
step:1326/2330 train_time:53554ms step_avg:40.39ms
step:1327/2330 train_time:53589ms step_avg:40.38ms
step:1328/2330 train_time:53634ms step_avg:40.39ms
step:1329/2330 train_time:53669ms step_avg:40.38ms
step:1330/2330 train_time:53713ms step_avg:40.39ms
step:1331/2330 train_time:53747ms step_avg:40.38ms
step:1332/2330 train_time:53792ms step_avg:40.38ms
step:1333/2330 train_time:53827ms step_avg:40.38ms
step:1334/2330 train_time:53871ms step_avg:40.38ms
step:1335/2330 train_time:53906ms step_avg:40.38ms
step:1336/2330 train_time:53951ms step_avg:40.38ms
step:1337/2330 train_time:53985ms step_avg:40.38ms
step:1338/2330 train_time:54031ms step_avg:40.38ms
step:1339/2330 train_time:54066ms step_avg:40.38ms
step:1340/2330 train_time:54112ms step_avg:40.38ms
step:1341/2330 train_time:54148ms step_avg:40.38ms
step:1342/2330 train_time:54194ms step_avg:40.38ms
step:1343/2330 train_time:54229ms step_avg:40.38ms
step:1344/2330 train_time:54275ms step_avg:40.38ms
step:1345/2330 train_time:54310ms step_avg:40.38ms
step:1346/2330 train_time:54355ms step_avg:40.38ms
step:1347/2330 train_time:54390ms step_avg:40.38ms
step:1348/2330 train_time:54436ms step_avg:40.38ms
step:1349/2330 train_time:54471ms step_avg:40.38ms
step:1350/2330 train_time:54516ms step_avg:40.38ms
step:1351/2330 train_time:54551ms step_avg:40.38ms
step:1352/2330 train_time:54595ms step_avg:40.38ms
step:1353/2330 train_time:54631ms step_avg:40.38ms
step:1354/2330 train_time:54675ms step_avg:40.38ms
step:1355/2330 train_time:54710ms step_avg:40.38ms
step:1356/2330 train_time:54754ms step_avg:40.38ms
step:1357/2330 train_time:54790ms step_avg:40.38ms
step:1358/2330 train_time:54834ms step_avg:40.38ms
step:1359/2330 train_time:54869ms step_avg:40.37ms
step:1360/2330 train_time:54914ms step_avg:40.38ms
step:1361/2330 train_time:54949ms step_avg:40.37ms
step:1362/2330 train_time:54994ms step_avg:40.38ms
step:1363/2330 train_time:55029ms step_avg:40.37ms
step:1364/2330 train_time:55073ms step_avg:40.38ms
step:1365/2330 train_time:55109ms step_avg:40.37ms
step:1366/2330 train_time:55153ms step_avg:40.38ms
step:1367/2330 train_time:55189ms step_avg:40.37ms
step:1368/2330 train_time:55234ms step_avg:40.38ms
step:1369/2330 train_time:55270ms step_avg:40.37ms
step:1370/2330 train_time:55315ms step_avg:40.38ms
step:1371/2330 train_time:55350ms step_avg:40.37ms
step:1372/2330 train_time:55395ms step_avg:40.38ms
step:1373/2330 train_time:55432ms step_avg:40.37ms
step:1374/2330 train_time:55477ms step_avg:40.38ms
step:1375/2330 train_time:55512ms step_avg:40.37ms
step:1376/2330 train_time:55557ms step_avg:40.38ms
step:1377/2330 train_time:55593ms step_avg:40.37ms
step:1378/2330 train_time:55637ms step_avg:40.38ms
step:1379/2330 train_time:55672ms step_avg:40.37ms
step:1380/2330 train_time:55717ms step_avg:40.37ms
step:1381/2330 train_time:55752ms step_avg:40.37ms
step:1382/2330 train_time:55797ms step_avg:40.37ms
step:1383/2330 train_time:55832ms step_avg:40.37ms
step:1384/2330 train_time:55877ms step_avg:40.37ms
step:1385/2330 train_time:55913ms step_avg:40.37ms
step:1386/2330 train_time:55958ms step_avg:40.37ms
step:1387/2330 train_time:55993ms step_avg:40.37ms
step:1388/2330 train_time:56038ms step_avg:40.37ms
step:1389/2330 train_time:56073ms step_avg:40.37ms
step:1390/2330 train_time:56118ms step_avg:40.37ms
step:1391/2330 train_time:56154ms step_avg:40.37ms
step:1392/2330 train_time:56199ms step_avg:40.37ms
step:1393/2330 train_time:56235ms step_avg:40.37ms
step:1394/2330 train_time:56279ms step_avg:40.37ms
step:1395/2330 train_time:56315ms step_avg:40.37ms
step:1396/2330 train_time:56359ms step_avg:40.37ms
step:1397/2330 train_time:56395ms step_avg:40.37ms
step:1398/2330 train_time:56440ms step_avg:40.37ms
step:1399/2330 train_time:56476ms step_avg:40.37ms
step:1400/2330 train_time:56520ms step_avg:40.37ms
step:1401/2330 train_time:56555ms step_avg:40.37ms
step:1402/2330 train_time:56600ms step_avg:40.37ms
step:1403/2330 train_time:56635ms step_avg:40.37ms
step:1404/2330 train_time:56680ms step_avg:40.37ms
step:1405/2330 train_time:56715ms step_avg:40.37ms
step:1406/2330 train_time:56760ms step_avg:40.37ms
step:1407/2330 train_time:56795ms step_avg:40.37ms
step:1408/2330 train_time:56840ms step_avg:40.37ms
step:1409/2330 train_time:56876ms step_avg:40.37ms
step:1410/2330 train_time:56920ms step_avg:40.37ms
step:1411/2330 train_time:56956ms step_avg:40.37ms
step:1412/2330 train_time:57002ms step_avg:40.37ms
step:1413/2330 train_time:57037ms step_avg:40.37ms
step:1414/2330 train_time:57082ms step_avg:40.37ms
step:1415/2330 train_time:57117ms step_avg:40.37ms
step:1416/2330 train_time:57163ms step_avg:40.37ms
step:1417/2330 train_time:57198ms step_avg:40.37ms
step:1418/2330 train_time:57242ms step_avg:40.37ms
step:1419/2330 train_time:57278ms step_avg:40.36ms
step:1420/2330 train_time:57324ms step_avg:40.37ms
step:1421/2330 train_time:57359ms step_avg:40.37ms
step:1422/2330 train_time:57403ms step_avg:40.37ms
step:1423/2330 train_time:57438ms step_avg:40.36ms
step:1424/2330 train_time:57483ms step_avg:40.37ms
step:1425/2330 train_time:57518ms step_avg:40.36ms
step:1426/2330 train_time:57562ms step_avg:40.37ms
step:1427/2330 train_time:57597ms step_avg:40.36ms
step:1428/2330 train_time:57642ms step_avg:40.37ms
step:1429/2330 train_time:57678ms step_avg:40.36ms
step:1430/2330 train_time:57723ms step_avg:40.37ms
step:1431/2330 train_time:57759ms step_avg:40.36ms
step:1432/2330 train_time:57803ms step_avg:40.37ms
step:1433/2330 train_time:57838ms step_avg:40.36ms
step:1434/2330 train_time:57884ms step_avg:40.37ms
step:1435/2330 train_time:57919ms step_avg:40.36ms
step:1436/2330 train_time:57963ms step_avg:40.36ms
step:1437/2330 train_time:57999ms step_avg:40.36ms
step:1438/2330 train_time:58043ms step_avg:40.36ms
step:1439/2330 train_time:58078ms step_avg:40.36ms
step:1440/2330 train_time:58123ms step_avg:40.36ms
step:1441/2330 train_time:58159ms step_avg:40.36ms
step:1442/2330 train_time:58203ms step_avg:40.36ms
step:1443/2330 train_time:58238ms step_avg:40.36ms
step:1444/2330 train_time:58283ms step_avg:40.36ms
step:1445/2330 train_time:58317ms step_avg:40.36ms
step:1446/2330 train_time:58362ms step_avg:40.36ms
step:1447/2330 train_time:58397ms step_avg:40.36ms
step:1448/2330 train_time:58442ms step_avg:40.36ms
step:1449/2330 train_time:58477ms step_avg:40.36ms
step:1450/2330 train_time:58522ms step_avg:40.36ms
step:1451/2330 train_time:58558ms step_avg:40.36ms
step:1452/2330 train_time:58602ms step_avg:40.36ms
step:1453/2330 train_time:58637ms step_avg:40.36ms
step:1454/2330 train_time:58682ms step_avg:40.36ms
step:1455/2330 train_time:58717ms step_avg:40.36ms
step:1456/2330 train_time:58762ms step_avg:40.36ms
step:1457/2330 train_time:58798ms step_avg:40.36ms
step:1458/2330 train_time:58842ms step_avg:40.36ms
step:1459/2330 train_time:58878ms step_avg:40.36ms
step:1460/2330 train_time:58923ms step_avg:40.36ms
step:1461/2330 train_time:58958ms step_avg:40.35ms
step:1462/2330 train_time:59003ms step_avg:40.36ms
step:1463/2330 train_time:59038ms step_avg:40.35ms
step:1464/2330 train_time:59083ms step_avg:40.36ms
step:1465/2330 train_time:59117ms step_avg:40.35ms
step:1466/2330 train_time:59162ms step_avg:40.36ms
step:1467/2330 train_time:59197ms step_avg:40.35ms
step:1468/2330 train_time:59241ms step_avg:40.35ms
step:1469/2330 train_time:59276ms step_avg:40.35ms
step:1470/2330 train_time:59320ms step_avg:40.35ms
step:1471/2330 train_time:59356ms step_avg:40.35ms
step:1472/2330 train_time:59401ms step_avg:40.35ms
step:1473/2330 train_time:59436ms step_avg:40.35ms
step:1474/2330 train_time:59481ms step_avg:40.35ms
step:1475/2330 train_time:59517ms step_avg:40.35ms
step:1476/2330 train_time:59562ms step_avg:40.35ms
step:1477/2330 train_time:59597ms step_avg:40.35ms
step:1478/2330 train_time:59642ms step_avg:40.35ms
step:1479/2330 train_time:59677ms step_avg:40.35ms
step:1480/2330 train_time:59722ms step_avg:40.35ms
step:1481/2330 train_time:59758ms step_avg:40.35ms
step:1482/2330 train_time:59803ms step_avg:40.35ms
step:1483/2330 train_time:59838ms step_avg:40.35ms
step:1484/2330 train_time:59883ms step_avg:40.35ms
step:1485/2330 train_time:59918ms step_avg:40.35ms
step:1486/2330 train_time:59962ms step_avg:40.35ms
step:1487/2330 train_time:59997ms step_avg:40.35ms
step:1488/2330 train_time:60042ms step_avg:40.35ms
step:1489/2330 train_time:60077ms step_avg:40.35ms
step:1490/2330 train_time:60123ms step_avg:40.35ms
step:1491/2330 train_time:60158ms step_avg:40.35ms
step:1492/2330 train_time:60203ms step_avg:40.35ms
step:1493/2330 train_time:60238ms step_avg:40.35ms
step:1494/2330 train_time:60283ms step_avg:40.35ms
step:1495/2330 train_time:60318ms step_avg:40.35ms
step:1496/2330 train_time:60363ms step_avg:40.35ms
step:1497/2330 train_time:60397ms step_avg:40.35ms
step:1498/2330 train_time:60441ms step_avg:40.35ms
step:1499/2330 train_time:60476ms step_avg:40.34ms
step:1500/2330 train_time:60521ms step_avg:40.35ms
step:1500/2330 val_loss:5.1433 train_time:60610ms step_avg:40.41ms
step:1501/2330 train_time:60624ms step_avg:40.39ms
step:1502/2330 train_time:60636ms step_avg:40.37ms
step:1503/2330 train_time:60647ms step_avg:40.35ms
step:1504/2330 train_time:60683ms step_avg:40.35ms
step:1505/2330 train_time:60717ms step_avg:40.34ms
step:1506/2330 train_time:60761ms step_avg:40.35ms
step:1507/2330 train_time:60885ms step_avg:40.40ms
step:1508/2330 train_time:60928ms step_avg:40.40ms
step:1509/2330 train_time:61018ms step_avg:40.44ms
step:1510/2330 train_time:61084ms step_avg:40.45ms
step:1511/2330 train_time:61117ms step_avg:40.45ms
step:1512/2330 train_time:61160ms step_avg:40.45ms
step:1513/2330 train_time:61195ms step_avg:40.45ms
step:1514/2330 train_time:61239ms step_avg:40.45ms
step:1515/2330 train_time:61273ms step_avg:40.44ms
step:1516/2330 train_time:61317ms step_avg:40.45ms
step:1517/2330 train_time:61351ms step_avg:40.44ms
step:1518/2330 train_time:61395ms step_avg:40.44ms
step:1519/2330 train_time:61429ms step_avg:40.44ms
step:1520/2330 train_time:61473ms step_avg:40.44ms
step:1521/2330 train_time:61508ms step_avg:40.44ms
step:1522/2330 train_time:61552ms step_avg:40.44ms
step:1523/2330 train_time:61587ms step_avg:40.44ms
step:1524/2330 train_time:61631ms step_avg:40.44ms
step:1525/2330 train_time:61665ms step_avg:40.44ms
step:1526/2330 train_time:61709ms step_avg:40.44ms
step:1527/2330 train_time:61743ms step_avg:40.43ms
step:1528/2330 train_time:61787ms step_avg:40.44ms
step:1529/2330 train_time:61823ms step_avg:40.43ms
step:1530/2330 train_time:61867ms step_avg:40.44ms
step:1531/2330 train_time:61905ms step_avg:40.43ms
step:1532/2330 train_time:61955ms step_avg:40.44ms
step:1533/2330 train_time:61994ms step_avg:40.44ms
step:1534/2330 train_time:62042ms step_avg:40.44ms
step:1535/2330 train_time:62078ms step_avg:40.44ms
step:1536/2330 train_time:62122ms step_avg:40.44ms
step:1537/2330 train_time:62157ms step_avg:40.44ms
step:1538/2330 train_time:62202ms step_avg:40.44ms
step:1539/2330 train_time:62237ms step_avg:40.44ms
step:1540/2330 train_time:62280ms step_avg:40.44ms
step:1541/2330 train_time:62315ms step_avg:40.44ms
step:1542/2330 train_time:62360ms step_avg:40.44ms
step:1543/2330 train_time:62395ms step_avg:40.44ms
step:1544/2330 train_time:62439ms step_avg:40.44ms
step:1545/2330 train_time:62474ms step_avg:40.44ms
step:1546/2330 train_time:62519ms step_avg:40.44ms
step:1547/2330 train_time:62553ms step_avg:40.44ms
step:1548/2330 train_time:62597ms step_avg:40.44ms
step:1549/2330 train_time:62632ms step_avg:40.43ms
step:1550/2330 train_time:62676ms step_avg:40.44ms
step:1551/2330 train_time:62711ms step_avg:40.43ms
step:1552/2330 train_time:62755ms step_avg:40.44ms
step:1553/2330 train_time:62790ms step_avg:40.43ms
step:1554/2330 train_time:62834ms step_avg:40.43ms
step:1555/2330 train_time:62871ms step_avg:40.43ms
step:1556/2330 train_time:62918ms step_avg:40.44ms
step:1557/2330 train_time:62955ms step_avg:40.43ms
step:1558/2330 train_time:63003ms step_avg:40.44ms
step:1559/2330 train_time:63039ms step_avg:40.44ms
step:1560/2330 train_time:63084ms step_avg:40.44ms
step:1561/2330 train_time:63120ms step_avg:40.44ms
step:1562/2330 train_time:63166ms step_avg:40.44ms
step:1563/2330 train_time:63202ms step_avg:40.44ms
step:1564/2330 train_time:63247ms step_avg:40.44ms
step:1565/2330 train_time:63283ms step_avg:40.44ms
step:1566/2330 train_time:63327ms step_avg:40.44ms
step:1567/2330 train_time:63362ms step_avg:40.44ms
step:1568/2330 train_time:63406ms step_avg:40.44ms
step:1569/2330 train_time:63441ms step_avg:40.43ms
step:1570/2330 train_time:63485ms step_avg:40.44ms
step:1571/2330 train_time:63520ms step_avg:40.43ms
step:1572/2330 train_time:63564ms step_avg:40.43ms
step:1573/2330 train_time:63599ms step_avg:40.43ms
step:1574/2330 train_time:63643ms step_avg:40.43ms
step:1575/2330 train_time:63678ms step_avg:40.43ms
step:1576/2330 train_time:63723ms step_avg:40.43ms
step:1577/2330 train_time:63758ms step_avg:40.43ms
step:1578/2330 train_time:63803ms step_avg:40.43ms
step:1579/2330 train_time:63839ms step_avg:40.43ms
step:1580/2330 train_time:63884ms step_avg:40.43ms
step:1581/2330 train_time:63920ms step_avg:40.43ms
step:1582/2330 train_time:63965ms step_avg:40.43ms
step:1583/2330 train_time:64001ms step_avg:40.43ms
step:1584/2330 train_time:64048ms step_avg:40.43ms
step:1585/2330 train_time:64084ms step_avg:40.43ms
step:1586/2330 train_time:64129ms step_avg:40.43ms
step:1587/2330 train_time:64164ms step_avg:40.43ms
step:1588/2330 train_time:64210ms step_avg:40.43ms
step:1589/2330 train_time:64245ms step_avg:40.43ms
step:1590/2330 train_time:64290ms step_avg:40.43ms
step:1591/2330 train_time:64325ms step_avg:40.43ms
step:1592/2330 train_time:64369ms step_avg:40.43ms
step:1593/2330 train_time:64405ms step_avg:40.43ms
step:1594/2330 train_time:64449ms step_avg:40.43ms
step:1595/2330 train_time:64484ms step_avg:40.43ms
step:1596/2330 train_time:64529ms step_avg:40.43ms
step:1597/2330 train_time:64564ms step_avg:40.43ms
step:1598/2330 train_time:64608ms step_avg:40.43ms
step:1599/2330 train_time:64643ms step_avg:40.43ms
step:1600/2330 train_time:64688ms step_avg:40.43ms
step:1601/2330 train_time:64722ms step_avg:40.43ms
step:1602/2330 train_time:64767ms step_avg:40.43ms
step:1603/2330 train_time:64802ms step_avg:40.43ms
step:1604/2330 train_time:64847ms step_avg:40.43ms
step:1605/2330 train_time:64883ms step_avg:40.43ms
step:1606/2330 train_time:64928ms step_avg:40.43ms
step:1607/2330 train_time:64963ms step_avg:40.43ms
step:1608/2330 train_time:65008ms step_avg:40.43ms
step:1609/2330 train_time:65043ms step_avg:40.42ms
step:1610/2330 train_time:65090ms step_avg:40.43ms
step:1611/2330 train_time:65125ms step_avg:40.43ms
step:1612/2330 train_time:65171ms step_avg:40.43ms
step:1613/2330 train_time:65206ms step_avg:40.43ms
step:1614/2330 train_time:65251ms step_avg:40.43ms
step:1615/2330 train_time:65287ms step_avg:40.43ms
step:1616/2330 train_time:65332ms step_avg:40.43ms
step:1617/2330 train_time:65367ms step_avg:40.42ms
step:1618/2330 train_time:65412ms step_avg:40.43ms
step:1619/2330 train_time:65447ms step_avg:40.42ms
step:1620/2330 train_time:65492ms step_avg:40.43ms
step:1621/2330 train_time:65527ms step_avg:40.42ms
step:1622/2330 train_time:65572ms step_avg:40.43ms
step:1623/2330 train_time:65607ms step_avg:40.42ms
step:1624/2330 train_time:65652ms step_avg:40.43ms
step:1625/2330 train_time:65688ms step_avg:40.42ms
step:1626/2330 train_time:65733ms step_avg:40.43ms
step:1627/2330 train_time:65769ms step_avg:40.42ms
step:1628/2330 train_time:65815ms step_avg:40.43ms
step:1629/2330 train_time:65850ms step_avg:40.42ms
step:1630/2330 train_time:65895ms step_avg:40.43ms
step:1631/2330 train_time:65930ms step_avg:40.42ms
step:1632/2330 train_time:65975ms step_avg:40.43ms
step:1633/2330 train_time:66010ms step_avg:40.42ms
step:1634/2330 train_time:66056ms step_avg:40.43ms
step:1635/2330 train_time:66091ms step_avg:40.42ms
step:1636/2330 train_time:66136ms step_avg:40.43ms
step:1637/2330 train_time:66171ms step_avg:40.42ms
step:1638/2330 train_time:66216ms step_avg:40.42ms
step:1639/2330 train_time:66251ms step_avg:40.42ms
step:1640/2330 train_time:66296ms step_avg:40.42ms
step:1641/2330 train_time:66332ms step_avg:40.42ms
step:1642/2330 train_time:66376ms step_avg:40.42ms
step:1643/2330 train_time:66411ms step_avg:40.42ms
step:1644/2330 train_time:66455ms step_avg:40.42ms
step:1645/2330 train_time:66491ms step_avg:40.42ms
step:1646/2330 train_time:66536ms step_avg:40.42ms
step:1647/2330 train_time:66571ms step_avg:40.42ms
step:1648/2330 train_time:66616ms step_avg:40.42ms
step:1649/2330 train_time:66651ms step_avg:40.42ms
step:1650/2330 train_time:66696ms step_avg:40.42ms
step:1651/2330 train_time:66731ms step_avg:40.42ms
step:1652/2330 train_time:66776ms step_avg:40.42ms
step:1653/2330 train_time:66812ms step_avg:40.42ms
step:1654/2330 train_time:66856ms step_avg:40.42ms
step:1655/2330 train_time:66892ms step_avg:40.42ms
step:1656/2330 train_time:66937ms step_avg:40.42ms
step:1657/2330 train_time:66972ms step_avg:40.42ms
step:1658/2330 train_time:67016ms step_avg:40.42ms
step:1659/2330 train_time:67051ms step_avg:40.42ms
step:1660/2330 train_time:67096ms step_avg:40.42ms
step:1661/2330 train_time:67131ms step_avg:40.42ms
step:1662/2330 train_time:67177ms step_avg:40.42ms
step:1663/2330 train_time:67212ms step_avg:40.42ms
step:1664/2330 train_time:67257ms step_avg:40.42ms
step:1665/2330 train_time:67292ms step_avg:40.42ms
step:1666/2330 train_time:67338ms step_avg:40.42ms
step:1667/2330 train_time:67373ms step_avg:40.42ms
step:1668/2330 train_time:67417ms step_avg:40.42ms
step:1669/2330 train_time:67453ms step_avg:40.42ms
step:1670/2330 train_time:67498ms step_avg:40.42ms
step:1671/2330 train_time:67533ms step_avg:40.41ms
step:1672/2330 train_time:67578ms step_avg:40.42ms
step:1673/2330 train_time:67613ms step_avg:40.41ms
step:1674/2330 train_time:67657ms step_avg:40.42ms
step:1675/2330 train_time:67692ms step_avg:40.41ms
step:1676/2330 train_time:67737ms step_avg:40.42ms
step:1677/2330 train_time:67773ms step_avg:40.41ms
step:1678/2330 train_time:67817ms step_avg:40.42ms
step:1679/2330 train_time:67853ms step_avg:40.41ms
step:1680/2330 train_time:67898ms step_avg:40.42ms
step:1681/2330 train_time:67933ms step_avg:40.41ms
step:1682/2330 train_time:67978ms step_avg:40.41ms
step:1683/2330 train_time:68012ms step_avg:40.41ms
step:1684/2330 train_time:68058ms step_avg:40.41ms
step:1685/2330 train_time:68093ms step_avg:40.41ms
step:1686/2330 train_time:68138ms step_avg:40.41ms
step:1687/2330 train_time:68173ms step_avg:40.41ms
step:1688/2330 train_time:68218ms step_avg:40.41ms
step:1689/2330 train_time:68253ms step_avg:40.41ms
step:1690/2330 train_time:68297ms step_avg:40.41ms
step:1691/2330 train_time:68333ms step_avg:40.41ms
step:1692/2330 train_time:68378ms step_avg:40.41ms
step:1693/2330 train_time:68414ms step_avg:40.41ms
step:1694/2330 train_time:68458ms step_avg:40.41ms
step:1695/2330 train_time:68494ms step_avg:40.41ms
step:1696/2330 train_time:68538ms step_avg:40.41ms
step:1697/2330 train_time:68574ms step_avg:40.41ms
step:1698/2330 train_time:68618ms step_avg:40.41ms
step:1699/2330 train_time:68654ms step_avg:40.41ms
step:1700/2330 train_time:68698ms step_avg:40.41ms
step:1701/2330 train_time:68733ms step_avg:40.41ms
step:1702/2330 train_time:68778ms step_avg:40.41ms
step:1703/2330 train_time:68814ms step_avg:40.41ms
step:1704/2330 train_time:68858ms step_avg:40.41ms
step:1705/2330 train_time:68894ms step_avg:40.41ms
step:1706/2330 train_time:68938ms step_avg:40.41ms
step:1707/2330 train_time:68973ms step_avg:40.41ms
step:1708/2330 train_time:69018ms step_avg:40.41ms
step:1709/2330 train_time:69053ms step_avg:40.41ms
step:1710/2330 train_time:69098ms step_avg:40.41ms
step:1711/2330 train_time:69133ms step_avg:40.40ms
step:1712/2330 train_time:69178ms step_avg:40.41ms
step:1713/2330 train_time:69213ms step_avg:40.40ms
step:1714/2330 train_time:69257ms step_avg:40.41ms
step:1715/2330 train_time:69293ms step_avg:40.40ms
step:1716/2330 train_time:69338ms step_avg:40.41ms
step:1717/2330 train_time:69373ms step_avg:40.40ms
step:1718/2330 train_time:69418ms step_avg:40.41ms
step:1719/2330 train_time:69452ms step_avg:40.40ms
step:1720/2330 train_time:69497ms step_avg:40.41ms
step:1721/2330 train_time:69533ms step_avg:40.40ms
step:1722/2330 train_time:69577ms step_avg:40.40ms
step:1723/2330 train_time:69612ms step_avg:40.40ms
step:1724/2330 train_time:69657ms step_avg:40.40ms
step:1725/2330 train_time:69692ms step_avg:40.40ms
step:1726/2330 train_time:69737ms step_avg:40.40ms
step:1727/2330 train_time:69772ms step_avg:40.40ms
step:1728/2330 train_time:69817ms step_avg:40.40ms
step:1729/2330 train_time:69852ms step_avg:40.40ms
step:1730/2330 train_time:69897ms step_avg:40.40ms
step:1731/2330 train_time:69932ms step_avg:40.40ms
step:1732/2330 train_time:69977ms step_avg:40.40ms
step:1733/2330 train_time:70012ms step_avg:40.40ms
step:1734/2330 train_time:70057ms step_avg:40.40ms
step:1735/2330 train_time:70092ms step_avg:40.40ms
step:1736/2330 train_time:70137ms step_avg:40.40ms
step:1737/2330 train_time:70172ms step_avg:40.40ms
step:1738/2330 train_time:70216ms step_avg:40.40ms
step:1739/2330 train_time:70252ms step_avg:40.40ms
step:1740/2330 train_time:70298ms step_avg:40.40ms
step:1741/2330 train_time:70333ms step_avg:40.40ms
step:1742/2330 train_time:70378ms step_avg:40.40ms
step:1743/2330 train_time:70413ms step_avg:40.40ms
step:1744/2330 train_time:70458ms step_avg:40.40ms
step:1745/2330 train_time:70494ms step_avg:40.40ms
step:1746/2330 train_time:70538ms step_avg:40.40ms
step:1747/2330 train_time:70573ms step_avg:40.40ms
step:1748/2330 train_time:70618ms step_avg:40.40ms
step:1749/2330 train_time:70654ms step_avg:40.40ms
step:1750/2330 train_time:70699ms step_avg:40.40ms
step:1750/2330 val_loss:5.1285 train_time:70787ms step_avg:40.45ms
step:1751/2330 train_time:70800ms step_avg:40.43ms
step:1752/2330 train_time:70812ms step_avg:40.42ms
step:1753/2330 train_time:70822ms step_avg:40.40ms
step:1754/2330 train_time:70859ms step_avg:40.40ms
step:1755/2330 train_time:70893ms step_avg:40.40ms
step:1756/2330 train_time:70937ms step_avg:40.40ms
step:1757/2330 train_time:70972ms step_avg:40.39ms
step:1758/2330 train_time:71016ms step_avg:40.40ms
step:1759/2330 train_time:71050ms step_avg:40.39ms
step:1760/2330 train_time:71094ms step_avg:40.39ms
step:1761/2330 train_time:71129ms step_avg:40.39ms
step:1762/2330 train_time:71176ms step_avg:40.40ms
step:1763/2330 train_time:71214ms step_avg:40.39ms
step:1764/2330 train_time:71259ms step_avg:40.40ms
step:1765/2330 train_time:71294ms step_avg:40.39ms
step:1766/2330 train_time:71338ms step_avg:40.40ms
step:1767/2330 train_time:71372ms step_avg:40.39ms
step:1768/2330 train_time:71416ms step_avg:40.39ms
step:1769/2330 train_time:71450ms step_avg:40.39ms
step:1770/2330 train_time:71494ms step_avg:40.39ms
step:1771/2330 train_time:71529ms step_avg:40.39ms
step:1772/2330 train_time:71574ms step_avg:40.39ms
step:1773/2330 train_time:71608ms step_avg:40.39ms
step:1774/2330 train_time:71656ms step_avg:40.39ms
step:1775/2330 train_time:71695ms step_avg:40.39ms
step:1776/2330 train_time:71744ms step_avg:40.40ms
step:1777/2330 train_time:71781ms step_avg:40.39ms
step:1778/2330 train_time:71827ms step_avg:40.40ms
step:1779/2330 train_time:71863ms step_avg:40.39ms
step:1780/2330 train_time:71907ms step_avg:40.40ms
step:1781/2330 train_time:71943ms step_avg:40.39ms
step:1782/2330 train_time:71988ms step_avg:40.40ms
step:1783/2330 train_time:72023ms step_avg:40.39ms
step:1784/2330 train_time:72068ms step_avg:40.40ms
step:1785/2330 train_time:72103ms step_avg:40.39ms
step:1786/2330 train_time:72148ms step_avg:40.40ms
step:1787/2330 train_time:72183ms step_avg:40.39ms
step:1788/2330 train_time:72229ms step_avg:40.40ms
step:1789/2330 train_time:72265ms step_avg:40.39ms
step:1790/2330 train_time:72310ms step_avg:40.40ms
step:1791/2330 train_time:72345ms step_avg:40.39ms
step:1792/2330 train_time:72389ms step_avg:40.40ms
step:1793/2330 train_time:72425ms step_avg:40.39ms
step:1794/2330 train_time:72469ms step_avg:40.40ms
step:1795/2330 train_time:72503ms step_avg:40.39ms
step:1796/2330 train_time:72548ms step_avg:40.39ms
step:1797/2330 train_time:72583ms step_avg:40.39ms
step:1798/2330 train_time:72628ms step_avg:40.39ms
step:1799/2330 train_time:72664ms step_avg:40.39ms
step:1800/2330 train_time:72710ms step_avg:40.39ms
step:1801/2330 train_time:72746ms step_avg:40.39ms
step:1802/2330 train_time:72793ms step_avg:40.40ms
step:1803/2330 train_time:72830ms step_avg:40.39ms
step:1804/2330 train_time:72876ms step_avg:40.40ms
step:1805/2330 train_time:72912ms step_avg:40.39ms
step:1806/2330 train_time:72958ms step_avg:40.40ms
step:1807/2330 train_time:72993ms step_avg:40.39ms
step:1808/2330 train_time:73039ms step_avg:40.40ms
step:1809/2330 train_time:73075ms step_avg:40.40ms
step:1810/2330 train_time:73120ms step_avg:40.40ms
step:1811/2330 train_time:73155ms step_avg:40.39ms
step:1812/2330 train_time:73199ms step_avg:40.40ms
step:1813/2330 train_time:73235ms step_avg:40.39ms
step:1814/2330 train_time:73280ms step_avg:40.40ms
step:1815/2330 train_time:73315ms step_avg:40.39ms
step:1816/2330 train_time:73360ms step_avg:40.40ms
step:1817/2330 train_time:73396ms step_avg:40.39ms
step:1818/2330 train_time:73441ms step_avg:40.40ms
step:1819/2330 train_time:73476ms step_avg:40.39ms
step:1820/2330 train_time:73521ms step_avg:40.40ms
step:1821/2330 train_time:73556ms step_avg:40.39ms
step:1822/2330 train_time:73600ms step_avg:40.40ms
step:1823/2330 train_time:73636ms step_avg:40.39ms
step:1824/2330 train_time:73681ms step_avg:40.40ms
step:1825/2330 train_time:73716ms step_avg:40.39ms
step:1826/2330 train_time:73762ms step_avg:40.40ms
step:1827/2330 train_time:73797ms step_avg:40.39ms
step:1828/2330 train_time:73842ms step_avg:40.39ms
step:1829/2330 train_time:73878ms step_avg:40.39ms
step:1830/2330 train_time:73923ms step_avg:40.39ms
step:1831/2330 train_time:73958ms step_avg:40.39ms
step:1832/2330 train_time:74003ms step_avg:40.39ms
step:1833/2330 train_time:74039ms step_avg:40.39ms
step:1834/2330 train_time:74084ms step_avg:40.39ms
step:1835/2330 train_time:74118ms step_avg:40.39ms
step:1836/2330 train_time:74163ms step_avg:40.39ms
step:1837/2330 train_time:74199ms step_avg:40.39ms
step:1838/2330 train_time:74244ms step_avg:40.39ms
step:1839/2330 train_time:74279ms step_avg:40.39ms
step:1840/2330 train_time:74324ms step_avg:40.39ms
step:1841/2330 train_time:74359ms step_avg:40.39ms
step:1842/2330 train_time:74404ms step_avg:40.39ms
step:1843/2330 train_time:74439ms step_avg:40.39ms
step:1844/2330 train_time:74484ms step_avg:40.39ms
step:1845/2330 train_time:74519ms step_avg:40.39ms
step:1846/2330 train_time:74564ms step_avg:40.39ms
step:1847/2330 train_time:74600ms step_avg:40.39ms
step:1848/2330 train_time:74645ms step_avg:40.39ms
step:1849/2330 train_time:74679ms step_avg:40.39ms
step:1850/2330 train_time:74724ms step_avg:40.39ms
step:1851/2330 train_time:74760ms step_avg:40.39ms
step:1852/2330 train_time:74805ms step_avg:40.39ms
step:1853/2330 train_time:74840ms step_avg:40.39ms
step:1854/2330 train_time:74885ms step_avg:40.39ms
step:1855/2330 train_time:74920ms step_avg:40.39ms
step:1856/2330 train_time:74965ms step_avg:40.39ms
step:1857/2330 train_time:75001ms step_avg:40.39ms
step:1858/2330 train_time:75045ms step_avg:40.39ms
step:1859/2330 train_time:75081ms step_avg:40.39ms
step:1860/2330 train_time:75126ms step_avg:40.39ms
step:1861/2330 train_time:75162ms step_avg:40.39ms
step:1862/2330 train_time:75206ms step_avg:40.39ms
step:1863/2330 train_time:75241ms step_avg:40.39ms
step:1864/2330 train_time:75286ms step_avg:40.39ms
step:1865/2330 train_time:75321ms step_avg:40.39ms
step:1866/2330 train_time:75366ms step_avg:40.39ms
step:1867/2330 train_time:75401ms step_avg:40.39ms
step:1868/2330 train_time:75445ms step_avg:40.39ms
step:1869/2330 train_time:75481ms step_avg:40.39ms
step:1870/2330 train_time:75525ms step_avg:40.39ms
step:1871/2330 train_time:75561ms step_avg:40.39ms
step:1872/2330 train_time:75606ms step_avg:40.39ms
step:1873/2330 train_time:75641ms step_avg:40.38ms
step:1874/2330 train_time:75686ms step_avg:40.39ms
step:1875/2330 train_time:75722ms step_avg:40.38ms
step:1876/2330 train_time:75766ms step_avg:40.39ms
step:1877/2330 train_time:75802ms step_avg:40.38ms
step:1878/2330 train_time:75847ms step_avg:40.39ms
step:1879/2330 train_time:75882ms step_avg:40.38ms
step:1880/2330 train_time:75927ms step_avg:40.39ms
step:1881/2330 train_time:75963ms step_avg:40.38ms
step:1882/2330 train_time:76008ms step_avg:40.39ms
step:1883/2330 train_time:76044ms step_avg:40.38ms
step:1884/2330 train_time:76088ms step_avg:40.39ms
step:1885/2330 train_time:76124ms step_avg:40.38ms
step:1886/2330 train_time:76168ms step_avg:40.39ms
step:1887/2330 train_time:76203ms step_avg:40.38ms
step:1888/2330 train_time:76247ms step_avg:40.39ms
step:1889/2330 train_time:76283ms step_avg:40.38ms
step:1890/2330 train_time:76328ms step_avg:40.39ms
step:1891/2330 train_time:76364ms step_avg:40.38ms
step:1892/2330 train_time:76409ms step_avg:40.39ms
step:1893/2330 train_time:76445ms step_avg:40.38ms
step:1894/2330 train_time:76490ms step_avg:40.39ms
step:1895/2330 train_time:76525ms step_avg:40.38ms
step:1896/2330 train_time:76569ms step_avg:40.38ms
step:1897/2330 train_time:76605ms step_avg:40.38ms
step:1898/2330 train_time:76650ms step_avg:40.38ms
step:1899/2330 train_time:76686ms step_avg:40.38ms
step:1900/2330 train_time:76730ms step_avg:40.38ms
step:1901/2330 train_time:76766ms step_avg:40.38ms
step:1902/2330 train_time:76811ms step_avg:40.38ms
step:1903/2330 train_time:76846ms step_avg:40.38ms
step:1904/2330 train_time:76891ms step_avg:40.38ms
step:1905/2330 train_time:76925ms step_avg:40.38ms
step:1906/2330 train_time:76970ms step_avg:40.38ms
step:1907/2330 train_time:77005ms step_avg:40.38ms
step:1908/2330 train_time:77050ms step_avg:40.38ms
step:1909/2330 train_time:77086ms step_avg:40.38ms
step:1910/2330 train_time:77131ms step_avg:40.38ms
step:1911/2330 train_time:77166ms step_avg:40.38ms
step:1912/2330 train_time:77211ms step_avg:40.38ms
step:1913/2330 train_time:77247ms step_avg:40.38ms
step:1914/2330 train_time:77292ms step_avg:40.38ms
step:1915/2330 train_time:77327ms step_avg:40.38ms
step:1916/2330 train_time:77371ms step_avg:40.38ms
step:1917/2330 train_time:77406ms step_avg:40.38ms
step:1918/2330 train_time:77451ms step_avg:40.38ms
step:1919/2330 train_time:77486ms step_avg:40.38ms
step:1920/2330 train_time:77530ms step_avg:40.38ms
step:1921/2330 train_time:77565ms step_avg:40.38ms
step:1922/2330 train_time:77610ms step_avg:40.38ms
step:1923/2330 train_time:77645ms step_avg:40.38ms
step:1924/2330 train_time:77690ms step_avg:40.38ms
step:1925/2330 train_time:77725ms step_avg:40.38ms
step:1926/2330 train_time:77769ms step_avg:40.38ms
step:1927/2330 train_time:77805ms step_avg:40.38ms
step:1928/2330 train_time:77849ms step_avg:40.38ms
step:1929/2330 train_time:77884ms step_avg:40.38ms
step:1930/2330 train_time:77929ms step_avg:40.38ms
step:1931/2330 train_time:77965ms step_avg:40.38ms
step:1932/2330 train_time:78010ms step_avg:40.38ms
step:1933/2330 train_time:78045ms step_avg:40.38ms
step:1934/2330 train_time:78090ms step_avg:40.38ms
step:1935/2330 train_time:78126ms step_avg:40.38ms
step:1936/2330 train_time:78171ms step_avg:40.38ms
step:1937/2330 train_time:78207ms step_avg:40.38ms
step:1938/2330 train_time:78252ms step_avg:40.38ms
step:1939/2330 train_time:78288ms step_avg:40.38ms
step:1940/2330 train_time:78333ms step_avg:40.38ms
step:1941/2330 train_time:78368ms step_avg:40.38ms
step:1942/2330 train_time:78413ms step_avg:40.38ms
step:1943/2330 train_time:78448ms step_avg:40.37ms
step:1944/2330 train_time:78493ms step_avg:40.38ms
step:1945/2330 train_time:78528ms step_avg:40.37ms
step:1946/2330 train_time:78571ms step_avg:40.38ms
step:1947/2330 train_time:78607ms step_avg:40.37ms
step:1948/2330 train_time:78652ms step_avg:40.38ms
step:1949/2330 train_time:78687ms step_avg:40.37ms
step:1950/2330 train_time:78732ms step_avg:40.38ms
step:1951/2330 train_time:78766ms step_avg:40.37ms
step:1952/2330 train_time:78812ms step_avg:40.37ms
step:1953/2330 train_time:78847ms step_avg:40.37ms
step:1954/2330 train_time:78892ms step_avg:40.37ms
step:1955/2330 train_time:78927ms step_avg:40.37ms
step:1956/2330 train_time:78971ms step_avg:40.37ms
step:1957/2330 train_time:79006ms step_avg:40.37ms
step:1958/2330 train_time:79050ms step_avg:40.37ms
step:1959/2330 train_time:79085ms step_avg:40.37ms
step:1960/2330 train_time:79130ms step_avg:40.37ms
step:1961/2330 train_time:79165ms step_avg:40.37ms
step:1962/2330 train_time:79210ms step_avg:40.37ms
step:1963/2330 train_time:79245ms step_avg:40.37ms
step:1964/2330 train_time:79290ms step_avg:40.37ms
step:1965/2330 train_time:79326ms step_avg:40.37ms
step:1966/2330 train_time:79371ms step_avg:40.37ms
step:1967/2330 train_time:79407ms step_avg:40.37ms
step:1968/2330 train_time:79452ms step_avg:40.37ms
step:1969/2330 train_time:79487ms step_avg:40.37ms
step:1970/2330 train_time:79532ms step_avg:40.37ms
step:1971/2330 train_time:79567ms step_avg:40.37ms
step:1972/2330 train_time:79612ms step_avg:40.37ms
step:1973/2330 train_time:79647ms step_avg:40.37ms
step:1974/2330 train_time:79692ms step_avg:40.37ms
step:1975/2330 train_time:79726ms step_avg:40.37ms
step:1976/2330 train_time:79771ms step_avg:40.37ms
step:1977/2330 train_time:79806ms step_avg:40.37ms
step:1978/2330 train_time:79851ms step_avg:40.37ms
step:1979/2330 train_time:79886ms step_avg:40.37ms
step:1980/2330 train_time:79930ms step_avg:40.37ms
step:1981/2330 train_time:79965ms step_avg:40.37ms
step:1982/2330 train_time:80010ms step_avg:40.37ms
step:1983/2330 train_time:80045ms step_avg:40.37ms
step:1984/2330 train_time:80089ms step_avg:40.37ms
step:1985/2330 train_time:80125ms step_avg:40.37ms
step:1986/2330 train_time:80170ms step_avg:40.37ms
step:1987/2330 train_time:80205ms step_avg:40.37ms
step:1988/2330 train_time:80251ms step_avg:40.37ms
step:1989/2330 train_time:80286ms step_avg:40.37ms
step:1990/2330 train_time:80331ms step_avg:40.37ms
step:1991/2330 train_time:80367ms step_avg:40.37ms
step:1992/2330 train_time:80412ms step_avg:40.37ms
step:1993/2330 train_time:80448ms step_avg:40.37ms
step:1994/2330 train_time:80492ms step_avg:40.37ms
step:1995/2330 train_time:80527ms step_avg:40.36ms
step:1996/2330 train_time:80572ms step_avg:40.37ms
step:1997/2330 train_time:80607ms step_avg:40.36ms
step:1998/2330 train_time:80652ms step_avg:40.37ms
step:1999/2330 train_time:80687ms step_avg:40.36ms
step:2000/2330 train_time:80732ms step_avg:40.37ms
step:2000/2330 val_loss:5.1165 train_time:80818ms step_avg:40.41ms
step:2001/2330 train_time:80831ms step_avg:40.40ms
step:2002/2330 train_time:80844ms step_avg:40.38ms
step:2003/2330 train_time:80855ms step_avg:40.37ms
step:2004/2330 train_time:80892ms step_avg:40.37ms
step:2005/2330 train_time:80926ms step_avg:40.36ms
step:2006/2330 train_time:80969ms step_avg:40.36ms
step:2007/2330 train_time:81004ms step_avg:40.36ms
step:2008/2330 train_time:81048ms step_avg:40.36ms
step:2009/2330 train_time:81082ms step_avg:40.36ms
step:2010/2330 train_time:81127ms step_avg:40.36ms
step:2011/2330 train_time:81166ms step_avg:40.36ms
step:2012/2330 train_time:81214ms step_avg:40.36ms
step:2013/2330 train_time:81252ms step_avg:40.36ms
step:2014/2330 train_time:81297ms step_avg:40.37ms
step:2015/2330 train_time:81332ms step_avg:40.36ms
step:2016/2330 train_time:81376ms step_avg:40.37ms
step:2017/2330 train_time:81412ms step_avg:40.36ms
step:2018/2330 train_time:81456ms step_avg:40.36ms
step:2019/2330 train_time:81491ms step_avg:40.36ms
step:2020/2330 train_time:81535ms step_avg:40.36ms
step:2021/2330 train_time:81570ms step_avg:40.36ms
step:2022/2330 train_time:81615ms step_avg:40.36ms
step:2023/2330 train_time:81649ms step_avg:40.36ms
step:2024/2330 train_time:81693ms step_avg:40.36ms
step:2025/2330 train_time:81728ms step_avg:40.36ms
step:2026/2330 train_time:81774ms step_avg:40.36ms
step:2027/2330 train_time:81810ms step_avg:40.36ms
step:2028/2330 train_time:81855ms step_avg:40.36ms
step:2029/2330 train_time:81890ms step_avg:40.36ms
step:2030/2330 train_time:81935ms step_avg:40.36ms
step:2031/2330 train_time:81969ms step_avg:40.36ms
step:2032/2330 train_time:82013ms step_avg:40.36ms
step:2033/2330 train_time:82048ms step_avg:40.36ms
step:2034/2330 train_time:82093ms step_avg:40.36ms
step:2035/2330 train_time:82128ms step_avg:40.36ms
step:2036/2330 train_time:82174ms step_avg:40.36ms
step:2037/2330 train_time:82211ms step_avg:40.36ms
step:2038/2330 train_time:82256ms step_avg:40.36ms
step:2039/2330 train_time:82291ms step_avg:40.36ms
step:2040/2330 train_time:82336ms step_avg:40.36ms
step:2041/2330 train_time:82372ms step_avg:40.36ms
step:2042/2330 train_time:82417ms step_avg:40.36ms
step:2043/2330 train_time:82452ms step_avg:40.36ms
step:2044/2330 train_time:82496ms step_avg:40.36ms
step:2045/2330 train_time:82531ms step_avg:40.36ms
step:2046/2330 train_time:82575ms step_avg:40.36ms
step:2047/2330 train_time:82612ms step_avg:40.36ms
step:2048/2330 train_time:82657ms step_avg:40.36ms
step:2049/2330 train_time:82692ms step_avg:40.36ms
step:2050/2330 train_time:82737ms step_avg:40.36ms
step:2051/2330 train_time:82772ms step_avg:40.36ms
step:2052/2330 train_time:82818ms step_avg:40.36ms
step:2053/2330 train_time:82853ms step_avg:40.36ms
step:2054/2330 train_time:82898ms step_avg:40.36ms
step:2055/2330 train_time:82934ms step_avg:40.36ms
step:2056/2330 train_time:82978ms step_avg:40.36ms
step:2057/2330 train_time:83014ms step_avg:40.36ms
step:2058/2330 train_time:83059ms step_avg:40.36ms
step:2059/2330 train_time:83094ms step_avg:40.36ms
step:2060/2330 train_time:83138ms step_avg:40.36ms
step:2061/2330 train_time:83174ms step_avg:40.36ms
step:2062/2330 train_time:83219ms step_avg:40.36ms
step:2063/2330 train_time:83254ms step_avg:40.36ms
step:2064/2330 train_time:83298ms step_avg:40.36ms
step:2065/2330 train_time:83333ms step_avg:40.35ms
step:2066/2330 train_time:83378ms step_avg:40.36ms
step:2067/2330 train_time:83413ms step_avg:40.35ms
step:2068/2330 train_time:83457ms step_avg:40.36ms
step:2069/2330 train_time:83492ms step_avg:40.35ms
step:2070/2330 train_time:83536ms step_avg:40.36ms
step:2071/2330 train_time:83571ms step_avg:40.35ms
step:2072/2330 train_time:83616ms step_avg:40.36ms
step:2073/2330 train_time:83651ms step_avg:40.35ms
step:2074/2330 train_time:83696ms step_avg:40.35ms
step:2075/2330 train_time:83731ms step_avg:40.35ms
step:2076/2330 train_time:83776ms step_avg:40.35ms
step:2077/2330 train_time:83811ms step_avg:40.35ms
step:2078/2330 train_time:83856ms step_avg:40.35ms
step:2079/2330 train_time:83891ms step_avg:40.35ms
step:2080/2330 train_time:83936ms step_avg:40.35ms
step:2081/2330 train_time:83971ms step_avg:40.35ms
step:2082/2330 train_time:84016ms step_avg:40.35ms
step:2083/2330 train_time:84052ms step_avg:40.35ms
step:2084/2330 train_time:84096ms step_avg:40.35ms
step:2085/2330 train_time:84132ms step_avg:40.35ms
step:2086/2330 train_time:84178ms step_avg:40.35ms
step:2087/2330 train_time:84213ms step_avg:40.35ms
step:2088/2330 train_time:84258ms step_avg:40.35ms
step:2089/2330 train_time:84292ms step_avg:40.35ms
step:2090/2330 train_time:84337ms step_avg:40.35ms
step:2091/2330 train_time:84372ms step_avg:40.35ms
step:2092/2330 train_time:84416ms step_avg:40.35ms
step:2093/2330 train_time:84452ms step_avg:40.35ms
step:2094/2330 train_time:84496ms step_avg:40.35ms
step:2095/2330 train_time:84531ms step_avg:40.35ms
step:2096/2330 train_time:84576ms step_avg:40.35ms
step:2097/2330 train_time:84611ms step_avg:40.35ms
step:2098/2330 train_time:84655ms step_avg:40.35ms
step:2099/2330 train_time:84690ms step_avg:40.35ms
step:2100/2330 train_time:84735ms step_avg:40.35ms
step:2101/2330 train_time:84771ms step_avg:40.35ms
step:2102/2330 train_time:84816ms step_avg:40.35ms
step:2103/2330 train_time:84851ms step_avg:40.35ms
step:2104/2330 train_time:84895ms step_avg:40.35ms
step:2105/2330 train_time:84931ms step_avg:40.35ms
step:2106/2330 train_time:84975ms step_avg:40.35ms
step:2107/2330 train_time:85012ms step_avg:40.35ms
step:2108/2330 train_time:85056ms step_avg:40.35ms
step:2109/2330 train_time:85092ms step_avg:40.35ms
step:2110/2330 train_time:85136ms step_avg:40.35ms
step:2111/2330 train_time:85172ms step_avg:40.35ms
step:2112/2330 train_time:85217ms step_avg:40.35ms
step:2113/2330 train_time:85252ms step_avg:40.35ms
step:2114/2330 train_time:85296ms step_avg:40.35ms
step:2115/2330 train_time:85331ms step_avg:40.35ms
step:2116/2330 train_time:85376ms step_avg:40.35ms
step:2117/2330 train_time:85412ms step_avg:40.35ms
step:2118/2330 train_time:85457ms step_avg:40.35ms
step:2119/2330 train_time:85493ms step_avg:40.35ms
step:2120/2330 train_time:85537ms step_avg:40.35ms
step:2121/2330 train_time:85572ms step_avg:40.35ms
step:2122/2330 train_time:85617ms step_avg:40.35ms
step:2123/2330 train_time:85652ms step_avg:40.34ms
step:2124/2330 train_time:85696ms step_avg:40.35ms
step:2125/2330 train_time:85732ms step_avg:40.34ms
step:2126/2330 train_time:85776ms step_avg:40.35ms
step:2127/2330 train_time:85812ms step_avg:40.34ms
step:2128/2330 train_time:85856ms step_avg:40.35ms
step:2129/2330 train_time:85891ms step_avg:40.34ms
step:2130/2330 train_time:85936ms step_avg:40.35ms
step:2131/2330 train_time:85971ms step_avg:40.34ms
step:2132/2330 train_time:86016ms step_avg:40.35ms
step:2133/2330 train_time:86052ms step_avg:40.34ms
step:2134/2330 train_time:86097ms step_avg:40.35ms
step:2135/2330 train_time:86132ms step_avg:40.34ms
step:2136/2330 train_time:86177ms step_avg:40.35ms
step:2137/2330 train_time:86212ms step_avg:40.34ms
step:2138/2330 train_time:86256ms step_avg:40.34ms
step:2139/2330 train_time:86291ms step_avg:40.34ms
step:2140/2330 train_time:86336ms step_avg:40.34ms
step:2141/2330 train_time:86371ms step_avg:40.34ms
step:2142/2330 train_time:86416ms step_avg:40.34ms
step:2143/2330 train_time:86452ms step_avg:40.34ms
step:2144/2330 train_time:86496ms step_avg:40.34ms
step:2145/2330 train_time:86531ms step_avg:40.34ms
step:2146/2330 train_time:86576ms step_avg:40.34ms
step:2147/2330 train_time:86611ms step_avg:40.34ms
step:2148/2330 train_time:86656ms step_avg:40.34ms
step:2149/2330 train_time:86691ms step_avg:40.34ms
step:2150/2330 train_time:86735ms step_avg:40.34ms
step:2151/2330 train_time:86770ms step_avg:40.34ms
step:2152/2330 train_time:86815ms step_avg:40.34ms
step:2153/2330 train_time:86851ms step_avg:40.34ms
step:2154/2330 train_time:86895ms step_avg:40.34ms
step:2155/2330 train_time:86931ms step_avg:40.34ms
step:2156/2330 train_time:86975ms step_avg:40.34ms
step:2157/2330 train_time:87011ms step_avg:40.34ms
step:2158/2330 train_time:87055ms step_avg:40.34ms
step:2159/2330 train_time:87091ms step_avg:40.34ms
step:2160/2330 train_time:87135ms step_avg:40.34ms
step:2161/2330 train_time:87171ms step_avg:40.34ms
step:2162/2330 train_time:87215ms step_avg:40.34ms
step:2163/2330 train_time:87250ms step_avg:40.34ms
step:2164/2330 train_time:87295ms step_avg:40.34ms
step:2165/2330 train_time:87330ms step_avg:40.34ms
step:2166/2330 train_time:87375ms step_avg:40.34ms
step:2167/2330 train_time:87410ms step_avg:40.34ms
step:2168/2330 train_time:87454ms step_avg:40.34ms
step:2169/2330 train_time:87490ms step_avg:40.34ms
step:2170/2330 train_time:87534ms step_avg:40.34ms
step:2171/2330 train_time:87570ms step_avg:40.34ms
step:2172/2330 train_time:87614ms step_avg:40.34ms
step:2173/2330 train_time:87649ms step_avg:40.34ms
step:2174/2330 train_time:87693ms step_avg:40.34ms
step:2175/2330 train_time:87728ms step_avg:40.33ms
step:2176/2330 train_time:87772ms step_avg:40.34ms
step:2177/2330 train_time:87807ms step_avg:40.33ms
step:2178/2330 train_time:87852ms step_avg:40.34ms
step:2179/2330 train_time:87887ms step_avg:40.33ms
step:2180/2330 train_time:87932ms step_avg:40.34ms
step:2181/2330 train_time:87967ms step_avg:40.33ms
step:2182/2330 train_time:88012ms step_avg:40.34ms
step:2183/2330 train_time:88047ms step_avg:40.33ms
step:2184/2330 train_time:88091ms step_avg:40.33ms
step:2185/2330 train_time:88126ms step_avg:40.33ms
step:2186/2330 train_time:88170ms step_avg:40.33ms
step:2187/2330 train_time:88206ms step_avg:40.33ms
step:2188/2330 train_time:88250ms step_avg:40.33ms
step:2189/2330 train_time:88286ms step_avg:40.33ms
step:2190/2330 train_time:88330ms step_avg:40.33ms
step:2191/2330 train_time:88366ms step_avg:40.33ms
step:2192/2330 train_time:88411ms step_avg:40.33ms
step:2193/2330 train_time:88446ms step_avg:40.33ms
step:2194/2330 train_time:88490ms step_avg:40.33ms
step:2195/2330 train_time:88525ms step_avg:40.33ms
step:2196/2330 train_time:88570ms step_avg:40.33ms
step:2197/2330 train_time:88605ms step_avg:40.33ms
step:2198/2330 train_time:88650ms step_avg:40.33ms
step:2199/2330 train_time:88685ms step_avg:40.33ms
step:2200/2330 train_time:88730ms step_avg:40.33ms
step:2201/2330 train_time:88765ms step_avg:40.33ms
step:2202/2330 train_time:88810ms step_avg:40.33ms
step:2203/2330 train_time:88845ms step_avg:40.33ms
step:2204/2330 train_time:88890ms step_avg:40.33ms
step:2205/2330 train_time:88925ms step_avg:40.33ms
step:2206/2330 train_time:88970ms step_avg:40.33ms
step:2207/2330 train_time:89006ms step_avg:40.33ms
step:2208/2330 train_time:89050ms step_avg:40.33ms
step:2209/2330 train_time:89085ms step_avg:40.33ms
step:2210/2330 train_time:89130ms step_avg:40.33ms
step:2211/2330 train_time:89165ms step_avg:40.33ms
step:2212/2330 train_time:89210ms step_avg:40.33ms
step:2213/2330 train_time:89245ms step_avg:40.33ms
step:2214/2330 train_time:89290ms step_avg:40.33ms
step:2215/2330 train_time:89325ms step_avg:40.33ms
step:2216/2330 train_time:89370ms step_avg:40.33ms
step:2217/2330 train_time:89406ms step_avg:40.33ms
step:2218/2330 train_time:89450ms step_avg:40.33ms
step:2219/2330 train_time:89486ms step_avg:40.33ms
step:2220/2330 train_time:89530ms step_avg:40.33ms
step:2221/2330 train_time:89566ms step_avg:40.33ms
step:2222/2330 train_time:89611ms step_avg:40.33ms
step:2223/2330 train_time:89646ms step_avg:40.33ms
step:2224/2330 train_time:89690ms step_avg:40.33ms
step:2225/2330 train_time:89726ms step_avg:40.33ms
step:2226/2330 train_time:89771ms step_avg:40.33ms
step:2227/2330 train_time:89806ms step_avg:40.33ms
step:2228/2330 train_time:89851ms step_avg:40.33ms
step:2229/2330 train_time:89886ms step_avg:40.33ms
step:2230/2330 train_time:89931ms step_avg:40.33ms
step:2231/2330 train_time:89966ms step_avg:40.33ms
step:2232/2330 train_time:90010ms step_avg:40.33ms
step:2233/2330 train_time:90046ms step_avg:40.33ms
step:2234/2330 train_time:90091ms step_avg:40.33ms
step:2235/2330 train_time:90126ms step_avg:40.32ms
step:2236/2330 train_time:90170ms step_avg:40.33ms
step:2237/2330 train_time:90206ms step_avg:40.32ms
step:2238/2330 train_time:90250ms step_avg:40.33ms
step:2239/2330 train_time:90286ms step_avg:40.32ms
step:2240/2330 train_time:90330ms step_avg:40.33ms
step:2241/2330 train_time:90366ms step_avg:40.32ms
step:2242/2330 train_time:90411ms step_avg:40.33ms
step:2243/2330 train_time:90446ms step_avg:40.32ms
step:2244/2330 train_time:90490ms step_avg:40.33ms
step:2245/2330 train_time:90526ms step_avg:40.32ms
step:2246/2330 train_time:90571ms step_avg:40.33ms
step:2247/2330 train_time:90605ms step_avg:40.32ms
step:2248/2330 train_time:90650ms step_avg:40.32ms
step:2249/2330 train_time:90686ms step_avg:40.32ms
step:2250/2330 train_time:90730ms step_avg:40.32ms
step:2250/2330 val_loss:5.1084 train_time:90819ms step_avg:40.36ms
step:2251/2330 train_time:90832ms step_avg:40.35ms
step:2252/2330 train_time:90844ms step_avg:40.34ms
step:2253/2330 train_time:90855ms step_avg:40.33ms
step:2254/2330 train_time:90892ms step_avg:40.32ms
step:2255/2330 train_time:90926ms step_avg:40.32ms
step:2256/2330 train_time:90971ms step_avg:40.32ms
step:2257/2330 train_time:91005ms step_avg:40.32ms
step:2258/2330 train_time:91049ms step_avg:40.32ms
step:2259/2330 train_time:91084ms step_avg:40.32ms
step:2260/2330 train_time:91132ms step_avg:40.32ms
step:2261/2330 train_time:91171ms step_avg:40.32ms
step:2262/2330 train_time:91218ms step_avg:40.33ms
step:2263/2330 train_time:91254ms step_avg:40.32ms
step:2264/2330 train_time:91299ms step_avg:40.33ms
step:2265/2330 train_time:91334ms step_avg:40.32ms
step:2266/2330 train_time:91379ms step_avg:40.33ms
step:2267/2330 train_time:91414ms step_avg:40.32ms
step:2268/2330 train_time:91457ms step_avg:40.33ms
step:2269/2330 train_time:91492ms step_avg:40.32ms
step:2270/2330 train_time:91537ms step_avg:40.32ms
step:2271/2330 train_time:91572ms step_avg:40.32ms
step:2272/2330 train_time:91616ms step_avg:40.32ms
step:2273/2330 train_time:91651ms step_avg:40.32ms
step:2274/2330 train_time:91695ms step_avg:40.32ms
step:2275/2330 train_time:91731ms step_avg:40.32ms
step:2276/2330 train_time:91777ms step_avg:40.32ms
step:2277/2330 train_time:91812ms step_avg:40.32ms
step:2278/2330 train_time:91856ms step_avg:40.32ms
step:2279/2330 train_time:91891ms step_avg:40.32ms
step:2280/2330 train_time:91935ms step_avg:40.32ms
step:2281/2330 train_time:91970ms step_avg:40.32ms
step:2282/2330 train_time:92014ms step_avg:40.32ms
step:2283/2330 train_time:92050ms step_avg:40.32ms
step:2284/2330 train_time:92096ms step_avg:40.32ms
step:2285/2330 train_time:92133ms step_avg:40.32ms
step:2286/2330 train_time:92179ms step_avg:40.32ms
step:2287/2330 train_time:92215ms step_avg:40.32ms
step:2288/2330 train_time:92260ms step_avg:40.32ms
step:2289/2330 train_time:92296ms step_avg:40.32ms
step:2290/2330 train_time:92341ms step_avg:40.32ms
step:2291/2330 train_time:92376ms step_avg:40.32ms
step:2292/2330 train_time:92420ms step_avg:40.32ms
step:2293/2330 train_time:92455ms step_avg:40.32ms
step:2294/2330 train_time:92499ms step_avg:40.32ms
step:2295/2330 train_time:92534ms step_avg:40.32ms
step:2296/2330 train_time:92579ms step_avg:40.32ms
step:2297/2330 train_time:92614ms step_avg:40.32ms
step:2298/2330 train_time:92659ms step_avg:40.32ms
step:2299/2330 train_time:92695ms step_avg:40.32ms
step:2300/2330 train_time:92740ms step_avg:40.32ms
step:2301/2330 train_time:92775ms step_avg:40.32ms
step:2302/2330 train_time:92820ms step_avg:40.32ms
step:2303/2330 train_time:92855ms step_avg:40.32ms
step:2304/2330 train_time:92899ms step_avg:40.32ms
step:2305/2330 train_time:92934ms step_avg:40.32ms
step:2306/2330 train_time:92979ms step_avg:40.32ms
step:2307/2330 train_time:93015ms step_avg:40.32ms
step:2308/2330 train_time:93060ms step_avg:40.32ms
step:2309/2330 train_time:93096ms step_avg:40.32ms
step:2310/2330 train_time:93141ms step_avg:40.32ms
step:2311/2330 train_time:93177ms step_avg:40.32ms
step:2312/2330 train_time:93222ms step_avg:40.32ms
step:2313/2330 train_time:93257ms step_avg:40.32ms
step:2314/2330 train_time:93302ms step_avg:40.32ms
step:2315/2330 train_time:93337ms step_avg:40.32ms
step:2316/2330 train_time:93382ms step_avg:40.32ms
step:2317/2330 train_time:93417ms step_avg:40.32ms
step:2318/2330 train_time:93461ms step_avg:40.32ms
step:2319/2330 train_time:93496ms step_avg:40.32ms
step:2320/2330 train_time:93541ms step_avg:40.32ms
step:2321/2330 train_time:93576ms step_avg:40.32ms
step:2322/2330 train_time:93621ms step_avg:40.32ms
step:2323/2330 train_time:93656ms step_avg:40.32ms
step:2324/2330 train_time:93701ms step_avg:40.32ms
step:2325/2330 train_time:93736ms step_avg:40.32ms
step:2326/2330 train_time:93782ms step_avg:40.32ms
step:2327/2330 train_time:93817ms step_avg:40.32ms
step:2328/2330 train_time:93861ms step_avg:40.32ms
step:2329/2330 train_time:93896ms step_avg:40.32ms
step:2330/2330 train_time:93941ms step_avg:40.32ms
step:2330/2330 val_loss:5.1036 train_time:94031ms step_avg:40.36ms
peak memory allocated: 29226 MiB reserved: 38888 MiB
