import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = divide_by_norm(batched)
                # v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = divide_by_norm(batched_update_grads)
                # v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "gd_lr1e-1"
    os.makedirs("logs3", exist_ok=True)
    logfile = f"logs3/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
# embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=1e-1, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    return 1.0

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum_min + 0.5 * (momentum_max - momentum_min)

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs3/{run_id}", exist_ok=True)
            torch.save(log, f"logs3/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 07:49:28 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   32C    P0             112W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   30C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   28C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   28C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   30C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   28C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   31C    P0             114W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:73ms step_avg:72.77ms
step:2/2330 train_time:191ms step_avg:95.42ms
step:3/2330 train_time:213ms step_avg:71.00ms
step:4/2330 train_time:241ms step_avg:60.28ms
step:5/2330 train_time:297ms step_avg:59.47ms
step:6/2330 train_time:358ms step_avg:59.63ms
step:7/2330 train_time:416ms step_avg:59.36ms
step:8/2330 train_time:477ms step_avg:59.64ms
step:9/2330 train_time:535ms step_avg:59.40ms
step:10/2330 train_time:596ms step_avg:59.61ms
step:11/2330 train_time:653ms step_avg:59.41ms
step:12/2330 train_time:715ms step_avg:59.57ms
step:13/2330 train_time:772ms step_avg:59.42ms
step:14/2330 train_time:834ms step_avg:59.55ms
step:15/2330 train_time:891ms step_avg:59.43ms
step:16/2330 train_time:952ms step_avg:59.50ms
step:17/2330 train_time:1010ms step_avg:59.43ms
step:18/2330 train_time:1073ms step_avg:59.59ms
step:19/2330 train_time:1134ms step_avg:59.69ms
step:20/2330 train_time:1199ms step_avg:59.93ms
step:21/2330 train_time:1258ms step_avg:59.92ms
step:22/2330 train_time:1321ms step_avg:60.06ms
step:23/2330 train_time:1380ms step_avg:59.99ms
step:24/2330 train_time:1442ms step_avg:60.07ms
step:25/2330 train_time:1500ms step_avg:59.98ms
step:26/2330 train_time:1561ms step_avg:60.04ms
step:27/2330 train_time:1619ms step_avg:59.95ms
step:28/2330 train_time:1680ms step_avg:60.00ms
step:29/2330 train_time:1737ms step_avg:59.90ms
step:30/2330 train_time:1800ms step_avg:59.98ms
step:31/2330 train_time:1857ms step_avg:59.91ms
step:32/2330 train_time:1919ms step_avg:59.97ms
step:33/2330 train_time:1977ms step_avg:59.90ms
step:34/2330 train_time:2039ms step_avg:59.97ms
step:35/2330 train_time:2099ms step_avg:59.96ms
step:36/2330 train_time:2162ms step_avg:60.05ms
step:37/2330 train_time:2222ms step_avg:60.04ms
step:38/2330 train_time:2284ms step_avg:60.11ms
step:39/2330 train_time:2342ms step_avg:60.06ms
step:40/2330 train_time:2404ms step_avg:60.11ms
step:41/2330 train_time:2462ms step_avg:60.05ms
step:42/2330 train_time:2525ms step_avg:60.11ms
step:43/2330 train_time:2583ms step_avg:60.06ms
step:44/2330 train_time:2644ms step_avg:60.10ms
step:45/2330 train_time:2703ms step_avg:60.06ms
step:46/2330 train_time:2765ms step_avg:60.12ms
step:47/2330 train_time:2823ms step_avg:60.07ms
step:48/2330 train_time:2885ms step_avg:60.11ms
step:49/2330 train_time:2944ms step_avg:60.07ms
step:50/2330 train_time:3005ms step_avg:60.11ms
step:51/2330 train_time:3064ms step_avg:60.07ms
step:52/2330 train_time:3126ms step_avg:60.11ms
step:53/2330 train_time:3185ms step_avg:60.09ms
step:54/2330 train_time:3247ms step_avg:60.12ms
step:55/2330 train_time:3305ms step_avg:60.08ms
step:56/2330 train_time:3367ms step_avg:60.12ms
step:57/2330 train_time:3426ms step_avg:60.11ms
step:58/2330 train_time:3487ms step_avg:60.12ms
step:59/2330 train_time:3545ms step_avg:60.08ms
step:60/2330 train_time:3606ms step_avg:60.11ms
step:61/2330 train_time:3665ms step_avg:60.08ms
step:62/2330 train_time:3727ms step_avg:60.11ms
step:63/2330 train_time:3784ms step_avg:60.07ms
step:64/2330 train_time:3846ms step_avg:60.09ms
step:65/2330 train_time:3904ms step_avg:60.05ms
step:66/2330 train_time:3965ms step_avg:60.08ms
step:67/2330 train_time:4024ms step_avg:60.06ms
step:68/2330 train_time:4086ms step_avg:60.09ms
step:69/2330 train_time:4144ms step_avg:60.06ms
step:70/2330 train_time:4207ms step_avg:60.09ms
step:71/2330 train_time:4265ms step_avg:60.07ms
step:72/2330 train_time:4326ms step_avg:60.09ms
step:73/2330 train_time:4385ms step_avg:60.07ms
step:74/2330 train_time:4447ms step_avg:60.10ms
step:75/2330 train_time:4506ms step_avg:60.07ms
step:76/2330 train_time:4567ms step_avg:60.09ms
step:77/2330 train_time:4626ms step_avg:60.07ms
step:78/2330 train_time:4687ms step_avg:60.09ms
step:79/2330 train_time:4746ms step_avg:60.08ms
step:80/2330 train_time:4808ms step_avg:60.09ms
step:81/2330 train_time:4866ms step_avg:60.08ms
step:82/2330 train_time:4928ms step_avg:60.10ms
step:83/2330 train_time:4986ms step_avg:60.07ms
step:84/2330 train_time:5047ms step_avg:60.09ms
step:85/2330 train_time:5106ms step_avg:60.07ms
step:86/2330 train_time:5168ms step_avg:60.09ms
step:87/2330 train_time:5227ms step_avg:60.08ms
step:88/2330 train_time:5288ms step_avg:60.10ms
step:89/2330 train_time:5347ms step_avg:60.07ms
step:90/2330 train_time:5408ms step_avg:60.09ms
step:91/2330 train_time:5467ms step_avg:60.08ms
step:92/2330 train_time:5529ms step_avg:60.09ms
step:93/2330 train_time:5588ms step_avg:60.08ms
step:94/2330 train_time:5649ms step_avg:60.10ms
step:95/2330 train_time:5707ms step_avg:60.08ms
step:96/2330 train_time:5770ms step_avg:60.10ms
step:97/2330 train_time:5828ms step_avg:60.08ms
step:98/2330 train_time:5890ms step_avg:60.11ms
step:99/2330 train_time:5949ms step_avg:60.09ms
step:100/2330 train_time:6011ms step_avg:60.11ms
step:101/2330 train_time:6069ms step_avg:60.09ms
step:102/2330 train_time:6131ms step_avg:60.11ms
step:103/2330 train_time:6189ms step_avg:60.09ms
step:104/2330 train_time:6251ms step_avg:60.11ms
step:105/2330 train_time:6310ms step_avg:60.09ms
step:106/2330 train_time:6371ms step_avg:60.11ms
step:107/2330 train_time:6430ms step_avg:60.09ms
step:108/2330 train_time:6491ms step_avg:60.11ms
step:109/2330 train_time:6551ms step_avg:60.10ms
step:110/2330 train_time:6612ms step_avg:60.11ms
step:111/2330 train_time:6670ms step_avg:60.09ms
step:112/2330 train_time:6732ms step_avg:60.11ms
step:113/2330 train_time:6790ms step_avg:60.09ms
step:114/2330 train_time:6852ms step_avg:60.11ms
step:115/2330 train_time:6911ms step_avg:60.10ms
step:116/2330 train_time:6973ms step_avg:60.12ms
step:117/2330 train_time:7032ms step_avg:60.10ms
step:118/2330 train_time:7093ms step_avg:60.11ms
step:119/2330 train_time:7151ms step_avg:60.10ms
step:120/2330 train_time:7214ms step_avg:60.12ms
step:121/2330 train_time:7272ms step_avg:60.10ms
step:122/2330 train_time:7334ms step_avg:60.11ms
step:123/2330 train_time:7392ms step_avg:60.10ms
step:124/2330 train_time:7454ms step_avg:60.11ms
step:125/2330 train_time:7513ms step_avg:60.10ms
step:126/2330 train_time:7574ms step_avg:60.11ms
step:127/2330 train_time:7633ms step_avg:60.10ms
step:128/2330 train_time:7694ms step_avg:60.11ms
step:129/2330 train_time:7752ms step_avg:60.10ms
step:130/2330 train_time:7815ms step_avg:60.11ms
step:131/2330 train_time:7873ms step_avg:60.10ms
step:132/2330 train_time:7934ms step_avg:60.11ms
step:133/2330 train_time:7992ms step_avg:60.09ms
step:134/2330 train_time:8054ms step_avg:60.11ms
step:135/2330 train_time:8113ms step_avg:60.10ms
step:136/2330 train_time:8175ms step_avg:60.11ms
step:137/2330 train_time:8233ms step_avg:60.10ms
step:138/2330 train_time:8295ms step_avg:60.11ms
step:139/2330 train_time:8354ms step_avg:60.10ms
step:140/2330 train_time:8416ms step_avg:60.11ms
step:141/2330 train_time:8473ms step_avg:60.09ms
step:142/2330 train_time:8536ms step_avg:60.11ms
step:143/2330 train_time:8594ms step_avg:60.10ms
step:144/2330 train_time:8656ms step_avg:60.11ms
step:145/2330 train_time:8715ms step_avg:60.10ms
step:146/2330 train_time:8776ms step_avg:60.11ms
step:147/2330 train_time:8835ms step_avg:60.10ms
step:148/2330 train_time:8897ms step_avg:60.11ms
step:149/2330 train_time:8955ms step_avg:60.10ms
step:150/2330 train_time:9018ms step_avg:60.12ms
step:151/2330 train_time:9076ms step_avg:60.11ms
step:152/2330 train_time:9139ms step_avg:60.12ms
step:153/2330 train_time:9198ms step_avg:60.12ms
step:154/2330 train_time:9260ms step_avg:60.13ms
step:155/2330 train_time:9319ms step_avg:60.12ms
step:156/2330 train_time:9381ms step_avg:60.14ms
step:157/2330 train_time:9440ms step_avg:60.12ms
step:158/2330 train_time:9502ms step_avg:60.14ms
step:159/2330 train_time:9560ms step_avg:60.13ms
step:160/2330 train_time:9622ms step_avg:60.14ms
step:161/2330 train_time:9681ms step_avg:60.13ms
step:162/2330 train_time:9744ms step_avg:60.15ms
step:163/2330 train_time:9801ms step_avg:60.13ms
step:164/2330 train_time:9863ms step_avg:60.14ms
step:165/2330 train_time:9921ms step_avg:60.13ms
step:166/2330 train_time:9983ms step_avg:60.14ms
step:167/2330 train_time:10042ms step_avg:60.13ms
step:168/2330 train_time:10104ms step_avg:60.15ms
step:169/2330 train_time:10163ms step_avg:60.14ms
step:170/2330 train_time:10225ms step_avg:60.15ms
step:171/2330 train_time:10284ms step_avg:60.14ms
step:172/2330 train_time:10346ms step_avg:60.15ms
step:173/2330 train_time:10405ms step_avg:60.15ms
step:174/2330 train_time:10468ms step_avg:60.16ms
step:175/2330 train_time:10526ms step_avg:60.15ms
step:176/2330 train_time:10588ms step_avg:60.16ms
step:177/2330 train_time:10647ms step_avg:60.15ms
step:178/2330 train_time:10709ms step_avg:60.17ms
step:179/2330 train_time:10768ms step_avg:60.16ms
step:180/2330 train_time:10829ms step_avg:60.16ms
step:181/2330 train_time:10888ms step_avg:60.15ms
step:182/2330 train_time:10951ms step_avg:60.17ms
step:183/2330 train_time:11010ms step_avg:60.16ms
step:184/2330 train_time:11072ms step_avg:60.17ms
step:185/2330 train_time:11131ms step_avg:60.17ms
step:186/2330 train_time:11192ms step_avg:60.17ms
step:187/2330 train_time:11251ms step_avg:60.17ms
step:188/2330 train_time:11313ms step_avg:60.18ms
step:189/2330 train_time:11370ms step_avg:60.16ms
step:190/2330 train_time:11433ms step_avg:60.17ms
step:191/2330 train_time:11491ms step_avg:60.16ms
step:192/2330 train_time:11553ms step_avg:60.17ms
step:193/2330 train_time:11613ms step_avg:60.17ms
step:194/2330 train_time:11674ms step_avg:60.18ms
step:195/2330 train_time:11732ms step_avg:60.16ms
step:196/2330 train_time:11794ms step_avg:60.17ms
step:197/2330 train_time:11852ms step_avg:60.16ms
step:198/2330 train_time:11914ms step_avg:60.17ms
step:199/2330 train_time:11973ms step_avg:60.16ms
step:200/2330 train_time:12034ms step_avg:60.17ms
step:201/2330 train_time:12092ms step_avg:60.16ms
step:202/2330 train_time:12155ms step_avg:60.17ms
step:203/2330 train_time:12213ms step_avg:60.16ms
step:204/2330 train_time:12274ms step_avg:60.17ms
step:205/2330 train_time:12332ms step_avg:60.16ms
step:206/2330 train_time:12394ms step_avg:60.17ms
step:207/2330 train_time:12452ms step_avg:60.15ms
step:208/2330 train_time:12515ms step_avg:60.17ms
step:209/2330 train_time:12573ms step_avg:60.16ms
step:210/2330 train_time:12635ms step_avg:60.17ms
step:211/2330 train_time:12694ms step_avg:60.16ms
step:212/2330 train_time:12755ms step_avg:60.17ms
step:213/2330 train_time:12813ms step_avg:60.16ms
step:214/2330 train_time:12876ms step_avg:60.17ms
step:215/2330 train_time:12934ms step_avg:60.16ms
step:216/2330 train_time:12995ms step_avg:60.16ms
step:217/2330 train_time:13054ms step_avg:60.16ms
step:218/2330 train_time:13116ms step_avg:60.17ms
step:219/2330 train_time:13174ms step_avg:60.16ms
step:220/2330 train_time:13236ms step_avg:60.17ms
step:221/2330 train_time:13294ms step_avg:60.16ms
step:222/2330 train_time:13357ms step_avg:60.17ms
step:223/2330 train_time:13416ms step_avg:60.16ms
step:224/2330 train_time:13478ms step_avg:60.17ms
step:225/2330 train_time:13536ms step_avg:60.16ms
step:226/2330 train_time:13598ms step_avg:60.17ms
step:227/2330 train_time:13656ms step_avg:60.16ms
step:228/2330 train_time:13718ms step_avg:60.17ms
step:229/2330 train_time:13777ms step_avg:60.16ms
step:230/2330 train_time:13839ms step_avg:60.17ms
step:231/2330 train_time:13897ms step_avg:60.16ms
step:232/2330 train_time:13959ms step_avg:60.17ms
step:233/2330 train_time:14018ms step_avg:60.16ms
step:234/2330 train_time:14080ms step_avg:60.17ms
step:235/2330 train_time:14138ms step_avg:60.16ms
step:236/2330 train_time:14199ms step_avg:60.17ms
step:237/2330 train_time:14257ms step_avg:60.16ms
step:238/2330 train_time:14320ms step_avg:60.17ms
step:239/2330 train_time:14378ms step_avg:60.16ms
step:240/2330 train_time:14441ms step_avg:60.17ms
step:241/2330 train_time:14498ms step_avg:60.16ms
step:242/2330 train_time:14561ms step_avg:60.17ms
step:243/2330 train_time:14620ms step_avg:60.16ms
step:244/2330 train_time:14681ms step_avg:60.17ms
step:245/2330 train_time:14740ms step_avg:60.16ms
step:246/2330 train_time:14803ms step_avg:60.17ms
step:247/2330 train_time:14861ms step_avg:60.16ms
step:248/2330 train_time:14923ms step_avg:60.17ms
step:249/2330 train_time:14981ms step_avg:60.16ms
step:250/2330 train_time:15043ms step_avg:60.17ms
step:250/2330 val_loss:4.7103 train_time:15114ms step_avg:60.46ms
step:251/2330 train_time:15136ms step_avg:60.30ms
step:252/2330 train_time:15165ms step_avg:60.18ms
step:253/2330 train_time:15225ms step_avg:60.18ms
step:254/2330 train_time:15295ms step_avg:60.22ms
step:255/2330 train_time:15358ms step_avg:60.23ms
step:256/2330 train_time:15422ms step_avg:60.24ms
step:257/2330 train_time:15481ms step_avg:60.24ms
step:258/2330 train_time:15543ms step_avg:60.24ms
step:259/2330 train_time:15601ms step_avg:60.23ms
step:260/2330 train_time:15662ms step_avg:60.24ms
step:261/2330 train_time:15720ms step_avg:60.23ms
step:262/2330 train_time:15782ms step_avg:60.23ms
step:263/2330 train_time:15839ms step_avg:60.23ms
step:264/2330 train_time:15900ms step_avg:60.23ms
step:265/2330 train_time:15958ms step_avg:60.22ms
step:266/2330 train_time:16020ms step_avg:60.22ms
step:267/2330 train_time:16078ms step_avg:60.22ms
step:268/2330 train_time:16143ms step_avg:60.23ms
step:269/2330 train_time:16203ms step_avg:60.23ms
step:270/2330 train_time:16266ms step_avg:60.24ms
step:271/2330 train_time:16327ms step_avg:60.25ms
step:272/2330 train_time:16391ms step_avg:60.26ms
step:273/2330 train_time:16450ms step_avg:60.26ms
step:274/2330 train_time:16512ms step_avg:60.26ms
step:275/2330 train_time:16570ms step_avg:60.25ms
step:276/2330 train_time:16632ms step_avg:60.26ms
step:277/2330 train_time:16692ms step_avg:60.26ms
step:278/2330 train_time:16753ms step_avg:60.26ms
step:279/2330 train_time:16811ms step_avg:60.26ms
step:280/2330 train_time:16873ms step_avg:60.26ms
step:281/2330 train_time:16931ms step_avg:60.25ms
step:282/2330 train_time:16993ms step_avg:60.26ms
step:283/2330 train_time:17051ms step_avg:60.25ms
step:284/2330 train_time:17113ms step_avg:60.26ms
step:285/2330 train_time:17171ms step_avg:60.25ms
step:286/2330 train_time:17235ms step_avg:60.26ms
step:287/2330 train_time:17294ms step_avg:60.26ms
step:288/2330 train_time:17357ms step_avg:60.27ms
step:289/2330 train_time:17416ms step_avg:60.26ms
step:290/2330 train_time:17478ms step_avg:60.27ms
step:291/2330 train_time:17537ms step_avg:60.26ms
step:292/2330 train_time:17599ms step_avg:60.27ms
step:293/2330 train_time:17657ms step_avg:60.26ms
step:294/2330 train_time:17719ms step_avg:60.27ms
step:295/2330 train_time:17777ms step_avg:60.26ms
step:296/2330 train_time:17839ms step_avg:60.27ms
step:297/2330 train_time:17897ms step_avg:60.26ms
step:298/2330 train_time:17959ms step_avg:60.26ms
step:299/2330 train_time:18016ms step_avg:60.25ms
step:300/2330 train_time:18078ms step_avg:60.26ms
step:301/2330 train_time:18136ms step_avg:60.25ms
step:302/2330 train_time:18200ms step_avg:60.27ms
step:303/2330 train_time:18259ms step_avg:60.26ms
step:304/2330 train_time:18321ms step_avg:60.27ms
step:305/2330 train_time:18380ms step_avg:60.26ms
step:306/2330 train_time:18442ms step_avg:60.27ms
step:307/2330 train_time:18501ms step_avg:60.26ms
step:308/2330 train_time:18563ms step_avg:60.27ms
step:309/2330 train_time:18622ms step_avg:60.26ms
step:310/2330 train_time:18684ms step_avg:60.27ms
step:311/2330 train_time:18743ms step_avg:60.27ms
step:312/2330 train_time:18806ms step_avg:60.28ms
step:313/2330 train_time:18865ms step_avg:60.27ms
step:314/2330 train_time:18928ms step_avg:60.28ms
step:315/2330 train_time:18986ms step_avg:60.27ms
step:316/2330 train_time:19049ms step_avg:60.28ms
step:317/2330 train_time:19107ms step_avg:60.27ms
step:318/2330 train_time:19169ms step_avg:60.28ms
step:319/2330 train_time:19227ms step_avg:60.27ms
step:320/2330 train_time:19289ms step_avg:60.28ms
step:321/2330 train_time:19348ms step_avg:60.27ms
step:322/2330 train_time:19410ms step_avg:60.28ms
step:323/2330 train_time:19469ms step_avg:60.28ms
step:324/2330 train_time:19532ms step_avg:60.28ms
step:325/2330 train_time:19590ms step_avg:60.28ms
step:326/2330 train_time:19653ms step_avg:60.28ms
step:327/2330 train_time:19711ms step_avg:60.28ms
step:328/2330 train_time:19773ms step_avg:60.28ms
step:329/2330 train_time:19832ms step_avg:60.28ms
step:330/2330 train_time:19894ms step_avg:60.28ms
step:331/2330 train_time:19952ms step_avg:60.28ms
step:332/2330 train_time:20015ms step_avg:60.29ms
step:333/2330 train_time:20073ms step_avg:60.28ms
step:334/2330 train_time:20135ms step_avg:60.28ms
step:335/2330 train_time:20194ms step_avg:60.28ms
step:336/2330 train_time:20255ms step_avg:60.28ms
step:337/2330 train_time:20314ms step_avg:60.28ms
step:338/2330 train_time:20376ms step_avg:60.28ms
step:339/2330 train_time:20434ms step_avg:60.28ms
step:340/2330 train_time:20497ms step_avg:60.29ms
step:341/2330 train_time:20555ms step_avg:60.28ms
step:342/2330 train_time:20617ms step_avg:60.28ms
step:343/2330 train_time:20675ms step_avg:60.28ms
step:344/2330 train_time:20738ms step_avg:60.28ms
step:345/2330 train_time:20797ms step_avg:60.28ms
step:346/2330 train_time:20858ms step_avg:60.28ms
step:347/2330 train_time:20917ms step_avg:60.28ms
step:348/2330 train_time:20979ms step_avg:60.28ms
step:349/2330 train_time:21037ms step_avg:60.28ms
step:350/2330 train_time:21099ms step_avg:60.28ms
step:351/2330 train_time:21157ms step_avg:60.28ms
step:352/2330 train_time:21220ms step_avg:60.28ms
step:353/2330 train_time:21278ms step_avg:60.28ms
step:354/2330 train_time:21340ms step_avg:60.28ms
step:355/2330 train_time:21399ms step_avg:60.28ms
step:356/2330 train_time:21461ms step_avg:60.28ms
step:357/2330 train_time:21519ms step_avg:60.28ms
step:358/2330 train_time:21582ms step_avg:60.28ms
step:359/2330 train_time:21640ms step_avg:60.28ms
step:360/2330 train_time:21703ms step_avg:60.29ms
step:361/2330 train_time:21762ms step_avg:60.28ms
step:362/2330 train_time:21824ms step_avg:60.29ms
step:363/2330 train_time:21883ms step_avg:60.28ms
step:364/2330 train_time:21945ms step_avg:60.29ms
step:365/2330 train_time:22004ms step_avg:60.29ms
step:366/2330 train_time:22066ms step_avg:60.29ms
step:367/2330 train_time:22126ms step_avg:60.29ms
step:368/2330 train_time:22188ms step_avg:60.29ms
step:369/2330 train_time:22247ms step_avg:60.29ms
step:370/2330 train_time:22309ms step_avg:60.30ms
step:371/2330 train_time:22368ms step_avg:60.29ms
step:372/2330 train_time:22431ms step_avg:60.30ms
step:373/2330 train_time:22489ms step_avg:60.29ms
step:374/2330 train_time:22551ms step_avg:60.30ms
step:375/2330 train_time:22610ms step_avg:60.29ms
step:376/2330 train_time:22671ms step_avg:60.29ms
step:377/2330 train_time:22729ms step_avg:60.29ms
step:378/2330 train_time:22791ms step_avg:60.29ms
step:379/2330 train_time:22850ms step_avg:60.29ms
step:380/2330 train_time:22912ms step_avg:60.29ms
step:381/2330 train_time:22970ms step_avg:60.29ms
step:382/2330 train_time:23032ms step_avg:60.29ms
step:383/2330 train_time:23092ms step_avg:60.29ms
step:384/2330 train_time:23153ms step_avg:60.29ms
step:385/2330 train_time:23211ms step_avg:60.29ms
step:386/2330 train_time:23273ms step_avg:60.29ms
step:387/2330 train_time:23332ms step_avg:60.29ms
step:388/2330 train_time:23394ms step_avg:60.29ms
step:389/2330 train_time:23452ms step_avg:60.29ms
step:390/2330 train_time:23514ms step_avg:60.29ms
step:391/2330 train_time:23573ms step_avg:60.29ms
step:392/2330 train_time:23635ms step_avg:60.29ms
step:393/2330 train_time:23694ms step_avg:60.29ms
step:394/2330 train_time:23756ms step_avg:60.29ms
step:395/2330 train_time:23814ms step_avg:60.29ms
step:396/2330 train_time:23876ms step_avg:60.29ms
step:397/2330 train_time:23935ms step_avg:60.29ms
step:398/2330 train_time:23998ms step_avg:60.30ms
step:399/2330 train_time:24056ms step_avg:60.29ms
step:400/2330 train_time:24119ms step_avg:60.30ms
step:401/2330 train_time:24177ms step_avg:60.29ms
step:402/2330 train_time:24239ms step_avg:60.30ms
step:403/2330 train_time:24298ms step_avg:60.29ms
step:404/2330 train_time:24359ms step_avg:60.30ms
step:405/2330 train_time:24417ms step_avg:60.29ms
step:406/2330 train_time:24479ms step_avg:60.29ms
step:407/2330 train_time:24538ms step_avg:60.29ms
step:408/2330 train_time:24601ms step_avg:60.30ms
step:409/2330 train_time:24659ms step_avg:60.29ms
step:410/2330 train_time:24721ms step_avg:60.30ms
step:411/2330 train_time:24780ms step_avg:60.29ms
step:412/2330 train_time:24843ms step_avg:60.30ms
step:413/2330 train_time:24901ms step_avg:60.29ms
step:414/2330 train_time:24963ms step_avg:60.30ms
step:415/2330 train_time:25022ms step_avg:60.29ms
step:416/2330 train_time:25085ms step_avg:60.30ms
step:417/2330 train_time:25145ms step_avg:60.30ms
step:418/2330 train_time:25207ms step_avg:60.30ms
step:419/2330 train_time:25265ms step_avg:60.30ms
step:420/2330 train_time:25328ms step_avg:60.30ms
step:421/2330 train_time:25387ms step_avg:60.30ms
step:422/2330 train_time:25449ms step_avg:60.31ms
step:423/2330 train_time:25507ms step_avg:60.30ms
step:424/2330 train_time:25569ms step_avg:60.30ms
step:425/2330 train_time:25628ms step_avg:60.30ms
step:426/2330 train_time:25690ms step_avg:60.31ms
step:427/2330 train_time:25748ms step_avg:60.30ms
step:428/2330 train_time:25810ms step_avg:60.30ms
step:429/2330 train_time:25868ms step_avg:60.30ms
step:430/2330 train_time:25931ms step_avg:60.30ms
step:431/2330 train_time:25989ms step_avg:60.30ms
step:432/2330 train_time:26051ms step_avg:60.30ms
step:433/2330 train_time:26109ms step_avg:60.30ms
step:434/2330 train_time:26172ms step_avg:60.30ms
step:435/2330 train_time:26231ms step_avg:60.30ms
step:436/2330 train_time:26294ms step_avg:60.31ms
step:437/2330 train_time:26352ms step_avg:60.30ms
step:438/2330 train_time:26414ms step_avg:60.31ms
step:439/2330 train_time:26472ms step_avg:60.30ms
step:440/2330 train_time:26534ms step_avg:60.30ms
step:441/2330 train_time:26593ms step_avg:60.30ms
step:442/2330 train_time:26654ms step_avg:60.30ms
step:443/2330 train_time:26713ms step_avg:60.30ms
step:444/2330 train_time:26776ms step_avg:60.31ms
step:445/2330 train_time:26835ms step_avg:60.30ms
step:446/2330 train_time:26897ms step_avg:60.31ms
step:447/2330 train_time:26956ms step_avg:60.30ms
step:448/2330 train_time:27018ms step_avg:60.31ms
step:449/2330 train_time:27076ms step_avg:60.30ms
step:450/2330 train_time:27138ms step_avg:60.31ms
step:451/2330 train_time:27197ms step_avg:60.30ms
step:452/2330 train_time:27260ms step_avg:60.31ms
step:453/2330 train_time:27318ms step_avg:60.31ms
step:454/2330 train_time:27381ms step_avg:60.31ms
step:455/2330 train_time:27439ms step_avg:60.31ms
step:456/2330 train_time:27501ms step_avg:60.31ms
step:457/2330 train_time:27559ms step_avg:60.30ms
step:458/2330 train_time:27621ms step_avg:60.31ms
step:459/2330 train_time:27680ms step_avg:60.30ms
step:460/2330 train_time:27742ms step_avg:60.31ms
step:461/2330 train_time:27801ms step_avg:60.31ms
step:462/2330 train_time:27864ms step_avg:60.31ms
step:463/2330 train_time:27923ms step_avg:60.31ms
step:464/2330 train_time:27985ms step_avg:60.31ms
step:465/2330 train_time:28045ms step_avg:60.31ms
step:466/2330 train_time:28107ms step_avg:60.32ms
step:467/2330 train_time:28167ms step_avg:60.31ms
step:468/2330 train_time:28229ms step_avg:60.32ms
step:469/2330 train_time:28289ms step_avg:60.32ms
step:470/2330 train_time:28351ms step_avg:60.32ms
step:471/2330 train_time:28409ms step_avg:60.32ms
step:472/2330 train_time:28470ms step_avg:60.32ms
step:473/2330 train_time:28528ms step_avg:60.31ms
step:474/2330 train_time:28591ms step_avg:60.32ms
step:475/2330 train_time:28650ms step_avg:60.32ms
step:476/2330 train_time:28711ms step_avg:60.32ms
step:477/2330 train_time:28770ms step_avg:60.31ms
step:478/2330 train_time:28833ms step_avg:60.32ms
step:479/2330 train_time:28892ms step_avg:60.32ms
step:480/2330 train_time:28953ms step_avg:60.32ms
step:481/2330 train_time:29011ms step_avg:60.31ms
step:482/2330 train_time:29072ms step_avg:60.32ms
step:483/2330 train_time:29131ms step_avg:60.31ms
step:484/2330 train_time:29193ms step_avg:60.32ms
step:485/2330 train_time:29251ms step_avg:60.31ms
step:486/2330 train_time:29313ms step_avg:60.32ms
step:487/2330 train_time:29372ms step_avg:60.31ms
step:488/2330 train_time:29435ms step_avg:60.32ms
step:489/2330 train_time:29494ms step_avg:60.31ms
step:490/2330 train_time:29556ms step_avg:60.32ms
step:491/2330 train_time:29615ms step_avg:60.32ms
step:492/2330 train_time:29678ms step_avg:60.32ms
step:493/2330 train_time:29736ms step_avg:60.32ms
step:494/2330 train_time:29799ms step_avg:60.32ms
step:495/2330 train_time:29856ms step_avg:60.32ms
step:496/2330 train_time:29919ms step_avg:60.32ms
step:497/2330 train_time:29977ms step_avg:60.32ms
step:498/2330 train_time:30039ms step_avg:60.32ms
step:499/2330 train_time:30098ms step_avg:60.32ms
step:500/2330 train_time:30160ms step_avg:60.32ms
step:500/2330 val_loss:4.2153 train_time:30231ms step_avg:60.46ms
step:501/2330 train_time:30253ms step_avg:60.38ms
step:502/2330 train_time:30283ms step_avg:60.32ms
step:503/2330 train_time:30345ms step_avg:60.33ms
step:504/2330 train_time:30411ms step_avg:60.34ms
step:505/2330 train_time:30470ms step_avg:60.34ms
step:506/2330 train_time:30533ms step_avg:60.34ms
step:507/2330 train_time:30593ms step_avg:60.34ms
step:508/2330 train_time:30655ms step_avg:60.34ms
step:509/2330 train_time:30713ms step_avg:60.34ms
step:510/2330 train_time:30775ms step_avg:60.34ms
step:511/2330 train_time:30833ms step_avg:60.34ms
step:512/2330 train_time:30894ms step_avg:60.34ms
step:513/2330 train_time:30952ms step_avg:60.33ms
step:514/2330 train_time:31013ms step_avg:60.34ms
step:515/2330 train_time:31070ms step_avg:60.33ms
step:516/2330 train_time:31132ms step_avg:60.33ms
step:517/2330 train_time:31190ms step_avg:60.33ms
step:518/2330 train_time:31254ms step_avg:60.34ms
step:519/2330 train_time:31315ms step_avg:60.34ms
step:520/2330 train_time:31379ms step_avg:60.34ms
step:521/2330 train_time:31438ms step_avg:60.34ms
step:522/2330 train_time:31500ms step_avg:60.35ms
step:523/2330 train_time:31559ms step_avg:60.34ms
step:524/2330 train_time:31622ms step_avg:60.35ms
step:525/2330 train_time:31681ms step_avg:60.34ms
step:526/2330 train_time:31744ms step_avg:60.35ms
step:527/2330 train_time:31802ms step_avg:60.35ms
step:528/2330 train_time:31865ms step_avg:60.35ms
step:529/2330 train_time:31924ms step_avg:60.35ms
step:530/2330 train_time:31986ms step_avg:60.35ms
step:531/2330 train_time:32045ms step_avg:60.35ms
step:532/2330 train_time:32107ms step_avg:60.35ms
step:533/2330 train_time:32166ms step_avg:60.35ms
step:534/2330 train_time:32228ms step_avg:60.35ms
step:535/2330 train_time:32287ms step_avg:60.35ms
step:536/2330 train_time:32349ms step_avg:60.35ms
step:537/2330 train_time:32408ms step_avg:60.35ms
step:538/2330 train_time:32469ms step_avg:60.35ms
step:539/2330 train_time:32528ms step_avg:60.35ms
step:540/2330 train_time:32591ms step_avg:60.35ms
step:541/2330 train_time:32650ms step_avg:60.35ms
step:542/2330 train_time:32713ms step_avg:60.36ms
step:543/2330 train_time:32771ms step_avg:60.35ms
step:544/2330 train_time:32833ms step_avg:60.36ms
step:545/2330 train_time:32892ms step_avg:60.35ms
step:546/2330 train_time:32955ms step_avg:60.36ms
step:547/2330 train_time:33014ms step_avg:60.35ms
step:548/2330 train_time:33076ms step_avg:60.36ms
step:549/2330 train_time:33135ms step_avg:60.35ms
step:550/2330 train_time:33197ms step_avg:60.36ms
step:551/2330 train_time:33255ms step_avg:60.35ms
step:552/2330 train_time:33318ms step_avg:60.36ms
step:553/2330 train_time:33376ms step_avg:60.35ms
step:554/2330 train_time:33439ms step_avg:60.36ms
step:555/2330 train_time:33497ms step_avg:60.36ms
step:556/2330 train_time:33560ms step_avg:60.36ms
step:557/2330 train_time:33619ms step_avg:60.36ms
step:558/2330 train_time:33682ms step_avg:60.36ms
step:559/2330 train_time:33742ms step_avg:60.36ms
step:560/2330 train_time:33804ms step_avg:60.36ms
step:561/2330 train_time:33862ms step_avg:60.36ms
step:562/2330 train_time:33925ms step_avg:60.37ms
step:563/2330 train_time:33984ms step_avg:60.36ms
step:564/2330 train_time:34046ms step_avg:60.37ms
step:565/2330 train_time:34105ms step_avg:60.36ms
step:566/2330 train_time:34167ms step_avg:60.37ms
step:567/2330 train_time:34225ms step_avg:60.36ms
step:568/2330 train_time:34287ms step_avg:60.36ms
step:569/2330 train_time:34346ms step_avg:60.36ms
step:570/2330 train_time:34408ms step_avg:60.37ms
step:571/2330 train_time:34467ms step_avg:60.36ms
step:572/2330 train_time:34530ms step_avg:60.37ms
step:573/2330 train_time:34588ms step_avg:60.36ms
step:574/2330 train_time:34650ms step_avg:60.37ms
step:575/2330 train_time:34709ms step_avg:60.36ms
step:576/2330 train_time:34771ms step_avg:60.37ms
step:577/2330 train_time:34830ms step_avg:60.36ms
step:578/2330 train_time:34893ms step_avg:60.37ms
step:579/2330 train_time:34952ms step_avg:60.37ms
step:580/2330 train_time:35015ms step_avg:60.37ms
step:581/2330 train_time:35073ms step_avg:60.37ms
step:582/2330 train_time:35135ms step_avg:60.37ms
step:583/2330 train_time:35193ms step_avg:60.37ms
step:584/2330 train_time:35256ms step_avg:60.37ms
step:585/2330 train_time:35314ms step_avg:60.37ms
step:586/2330 train_time:35376ms step_avg:60.37ms
step:587/2330 train_time:35436ms step_avg:60.37ms
step:588/2330 train_time:35498ms step_avg:60.37ms
step:589/2330 train_time:35555ms step_avg:60.37ms
step:590/2330 train_time:35617ms step_avg:60.37ms
step:591/2330 train_time:35676ms step_avg:60.37ms
step:592/2330 train_time:35738ms step_avg:60.37ms
step:593/2330 train_time:35798ms step_avg:60.37ms
step:594/2330 train_time:35860ms step_avg:60.37ms
step:595/2330 train_time:35919ms step_avg:60.37ms
step:596/2330 train_time:35982ms step_avg:60.37ms
step:597/2330 train_time:36041ms step_avg:60.37ms
step:598/2330 train_time:36104ms step_avg:60.37ms
step:599/2330 train_time:36164ms step_avg:60.37ms
step:600/2330 train_time:36226ms step_avg:60.38ms
step:601/2330 train_time:36284ms step_avg:60.37ms
step:602/2330 train_time:36346ms step_avg:60.38ms
step:603/2330 train_time:36404ms step_avg:60.37ms
step:604/2330 train_time:36467ms step_avg:60.38ms
step:605/2330 train_time:36525ms step_avg:60.37ms
step:606/2330 train_time:36587ms step_avg:60.37ms
step:607/2330 train_time:36646ms step_avg:60.37ms
step:608/2330 train_time:36708ms step_avg:60.37ms
step:609/2330 train_time:36766ms step_avg:60.37ms
step:610/2330 train_time:36828ms step_avg:60.37ms
step:611/2330 train_time:36887ms step_avg:60.37ms
step:612/2330 train_time:36949ms step_avg:60.37ms
step:613/2330 train_time:37007ms step_avg:60.37ms
step:614/2330 train_time:37069ms step_avg:60.37ms
step:615/2330 train_time:37129ms step_avg:60.37ms
step:616/2330 train_time:37191ms step_avg:60.37ms
step:617/2330 train_time:37250ms step_avg:60.37ms
step:618/2330 train_time:37312ms step_avg:60.38ms
step:619/2330 train_time:37370ms step_avg:60.37ms
step:620/2330 train_time:37433ms step_avg:60.38ms
step:621/2330 train_time:37491ms step_avg:60.37ms
step:622/2330 train_time:37554ms step_avg:60.38ms
step:623/2330 train_time:37612ms step_avg:60.37ms
step:624/2330 train_time:37675ms step_avg:60.38ms
step:625/2330 train_time:37734ms step_avg:60.37ms
step:626/2330 train_time:37796ms step_avg:60.38ms
step:627/2330 train_time:37854ms step_avg:60.37ms
step:628/2330 train_time:37915ms step_avg:60.37ms
step:629/2330 train_time:37974ms step_avg:60.37ms
step:630/2330 train_time:38036ms step_avg:60.37ms
step:631/2330 train_time:38094ms step_avg:60.37ms
step:632/2330 train_time:38157ms step_avg:60.37ms
step:633/2330 train_time:38216ms step_avg:60.37ms
step:634/2330 train_time:38279ms step_avg:60.38ms
step:635/2330 train_time:38337ms step_avg:60.37ms
step:636/2330 train_time:38400ms step_avg:60.38ms
step:637/2330 train_time:38458ms step_avg:60.37ms
step:638/2330 train_time:38520ms step_avg:60.38ms
step:639/2330 train_time:38580ms step_avg:60.38ms
step:640/2330 train_time:38643ms step_avg:60.38ms
step:641/2330 train_time:38701ms step_avg:60.38ms
step:642/2330 train_time:38764ms step_avg:60.38ms
step:643/2330 train_time:38823ms step_avg:60.38ms
step:644/2330 train_time:38885ms step_avg:60.38ms
step:645/2330 train_time:38943ms step_avg:60.38ms
step:646/2330 train_time:39005ms step_avg:60.38ms
step:647/2330 train_time:39063ms step_avg:60.38ms
step:648/2330 train_time:39125ms step_avg:60.38ms
step:649/2330 train_time:39184ms step_avg:60.38ms
step:650/2330 train_time:39246ms step_avg:60.38ms
step:651/2330 train_time:39304ms step_avg:60.38ms
step:652/2330 train_time:39367ms step_avg:60.38ms
step:653/2330 train_time:39425ms step_avg:60.38ms
step:654/2330 train_time:39487ms step_avg:60.38ms
step:655/2330 train_time:39546ms step_avg:60.38ms
step:656/2330 train_time:39608ms step_avg:60.38ms
step:657/2330 train_time:39666ms step_avg:60.38ms
step:658/2330 train_time:39728ms step_avg:60.38ms
step:659/2330 train_time:39787ms step_avg:60.38ms
step:660/2330 train_time:39849ms step_avg:60.38ms
step:661/2330 train_time:39907ms step_avg:60.37ms
step:662/2330 train_time:39969ms step_avg:60.38ms
step:663/2330 train_time:40028ms step_avg:60.37ms
step:664/2330 train_time:40090ms step_avg:60.38ms
step:665/2330 train_time:40149ms step_avg:60.37ms
step:666/2330 train_time:40211ms step_avg:60.38ms
step:667/2330 train_time:40270ms step_avg:60.37ms
step:668/2330 train_time:40332ms step_avg:60.38ms
step:669/2330 train_time:40390ms step_avg:60.37ms
step:670/2330 train_time:40453ms step_avg:60.38ms
step:671/2330 train_time:40511ms step_avg:60.37ms
step:672/2330 train_time:40573ms step_avg:60.38ms
step:673/2330 train_time:40632ms step_avg:60.37ms
step:674/2330 train_time:40694ms step_avg:60.38ms
step:675/2330 train_time:40753ms step_avg:60.38ms
step:676/2330 train_time:40816ms step_avg:60.38ms
step:677/2330 train_time:40874ms step_avg:60.38ms
step:678/2330 train_time:40936ms step_avg:60.38ms
step:679/2330 train_time:40994ms step_avg:60.37ms
step:680/2330 train_time:41056ms step_avg:60.38ms
step:681/2330 train_time:41114ms step_avg:60.37ms
step:682/2330 train_time:41176ms step_avg:60.38ms
step:683/2330 train_time:41235ms step_avg:60.37ms
step:684/2330 train_time:41297ms step_avg:60.38ms
step:685/2330 train_time:41355ms step_avg:60.37ms
step:686/2330 train_time:41418ms step_avg:60.38ms
step:687/2330 train_time:41477ms step_avg:60.37ms
step:688/2330 train_time:41540ms step_avg:60.38ms
step:689/2330 train_time:41599ms step_avg:60.38ms
step:690/2330 train_time:41662ms step_avg:60.38ms
step:691/2330 train_time:41720ms step_avg:60.38ms
step:692/2330 train_time:41782ms step_avg:60.38ms
step:693/2330 train_time:41841ms step_avg:60.38ms
step:694/2330 train_time:41903ms step_avg:60.38ms
step:695/2330 train_time:41961ms step_avg:60.38ms
step:696/2330 train_time:42023ms step_avg:60.38ms
step:697/2330 train_time:42083ms step_avg:60.38ms
step:698/2330 train_time:42145ms step_avg:60.38ms
step:699/2330 train_time:42204ms step_avg:60.38ms
step:700/2330 train_time:42265ms step_avg:60.38ms
step:701/2330 train_time:42325ms step_avg:60.38ms
step:702/2330 train_time:42388ms step_avg:60.38ms
step:703/2330 train_time:42446ms step_avg:60.38ms
step:704/2330 train_time:42508ms step_avg:60.38ms
step:705/2330 train_time:42566ms step_avg:60.38ms
step:706/2330 train_time:42629ms step_avg:60.38ms
step:707/2330 train_time:42687ms step_avg:60.38ms
step:708/2330 train_time:42749ms step_avg:60.38ms
step:709/2330 train_time:42807ms step_avg:60.38ms
step:710/2330 train_time:42870ms step_avg:60.38ms
step:711/2330 train_time:42928ms step_avg:60.38ms
step:712/2330 train_time:42990ms step_avg:60.38ms
step:713/2330 train_time:43049ms step_avg:60.38ms
step:714/2330 train_time:43111ms step_avg:60.38ms
step:715/2330 train_time:43170ms step_avg:60.38ms
step:716/2330 train_time:43233ms step_avg:60.38ms
step:717/2330 train_time:43291ms step_avg:60.38ms
step:718/2330 train_time:43354ms step_avg:60.38ms
step:719/2330 train_time:43413ms step_avg:60.38ms
step:720/2330 train_time:43475ms step_avg:60.38ms
step:721/2330 train_time:43534ms step_avg:60.38ms
step:722/2330 train_time:43596ms step_avg:60.38ms
step:723/2330 train_time:43654ms step_avg:60.38ms
step:724/2330 train_time:43717ms step_avg:60.38ms
step:725/2330 train_time:43775ms step_avg:60.38ms
step:726/2330 train_time:43837ms step_avg:60.38ms
step:727/2330 train_time:43895ms step_avg:60.38ms
step:728/2330 train_time:43958ms step_avg:60.38ms
step:729/2330 train_time:44016ms step_avg:60.38ms
step:730/2330 train_time:44079ms step_avg:60.38ms
step:731/2330 train_time:44137ms step_avg:60.38ms
step:732/2330 train_time:44200ms step_avg:60.38ms
step:733/2330 train_time:44259ms step_avg:60.38ms
step:734/2330 train_time:44322ms step_avg:60.38ms
step:735/2330 train_time:44382ms step_avg:60.38ms
step:736/2330 train_time:44445ms step_avg:60.39ms
step:737/2330 train_time:44504ms step_avg:60.39ms
step:738/2330 train_time:44567ms step_avg:60.39ms
step:739/2330 train_time:44626ms step_avg:60.39ms
step:740/2330 train_time:44687ms step_avg:60.39ms
step:741/2330 train_time:44746ms step_avg:60.39ms
step:742/2330 train_time:44808ms step_avg:60.39ms
step:743/2330 train_time:44866ms step_avg:60.39ms
step:744/2330 train_time:44928ms step_avg:60.39ms
step:745/2330 train_time:44987ms step_avg:60.39ms
step:746/2330 train_time:45049ms step_avg:60.39ms
step:747/2330 train_time:45107ms step_avg:60.38ms
step:748/2330 train_time:45169ms step_avg:60.39ms
step:749/2330 train_time:45227ms step_avg:60.38ms
step:750/2330 train_time:45290ms step_avg:60.39ms
step:750/2330 val_loss:4.0043 train_time:45361ms step_avg:60.48ms
step:751/2330 train_time:45383ms step_avg:60.43ms
step:752/2330 train_time:45414ms step_avg:60.39ms
step:753/2330 train_time:45474ms step_avg:60.39ms
step:754/2330 train_time:45540ms step_avg:60.40ms
step:755/2330 train_time:45602ms step_avg:60.40ms
step:756/2330 train_time:45665ms step_avg:60.40ms
step:757/2330 train_time:45724ms step_avg:60.40ms
step:758/2330 train_time:45785ms step_avg:60.40ms
step:759/2330 train_time:45844ms step_avg:60.40ms
step:760/2330 train_time:45905ms step_avg:60.40ms
step:761/2330 train_time:45964ms step_avg:60.40ms
step:762/2330 train_time:46025ms step_avg:60.40ms
step:763/2330 train_time:46082ms step_avg:60.40ms
step:764/2330 train_time:46143ms step_avg:60.40ms
step:765/2330 train_time:46202ms step_avg:60.39ms
step:766/2330 train_time:46264ms step_avg:60.40ms
step:767/2330 train_time:46323ms step_avg:60.40ms
step:768/2330 train_time:46387ms step_avg:60.40ms
step:769/2330 train_time:46449ms step_avg:60.40ms
step:770/2330 train_time:46512ms step_avg:60.40ms
step:771/2330 train_time:46572ms step_avg:60.40ms
step:772/2330 train_time:46636ms step_avg:60.41ms
step:773/2330 train_time:46695ms step_avg:60.41ms
step:774/2330 train_time:46756ms step_avg:60.41ms
step:775/2330 train_time:46815ms step_avg:60.41ms
step:776/2330 train_time:46879ms step_avg:60.41ms
step:777/2330 train_time:46937ms step_avg:60.41ms
step:778/2330 train_time:47000ms step_avg:60.41ms
step:779/2330 train_time:47060ms step_avg:60.41ms
step:780/2330 train_time:47122ms step_avg:60.41ms
step:781/2330 train_time:47181ms step_avg:60.41ms
step:782/2330 train_time:47244ms step_avg:60.41ms
step:783/2330 train_time:47303ms step_avg:60.41ms
step:784/2330 train_time:47367ms step_avg:60.42ms
step:785/2330 train_time:47428ms step_avg:60.42ms
step:786/2330 train_time:47491ms step_avg:60.42ms
step:787/2330 train_time:47550ms step_avg:60.42ms
step:788/2330 train_time:47613ms step_avg:60.42ms
step:789/2330 train_time:47673ms step_avg:60.42ms
step:790/2330 train_time:47735ms step_avg:60.42ms
step:791/2330 train_time:47794ms step_avg:60.42ms
step:792/2330 train_time:47856ms step_avg:60.42ms
step:793/2330 train_time:47915ms step_avg:60.42ms
step:794/2330 train_time:47979ms step_avg:60.43ms
step:795/2330 train_time:48037ms step_avg:60.42ms
step:796/2330 train_time:48099ms step_avg:60.43ms
step:797/2330 train_time:48158ms step_avg:60.42ms
step:798/2330 train_time:48221ms step_avg:60.43ms
step:799/2330 train_time:48280ms step_avg:60.43ms
step:800/2330 train_time:48344ms step_avg:60.43ms
step:801/2330 train_time:48405ms step_avg:60.43ms
step:802/2330 train_time:48469ms step_avg:60.44ms
step:803/2330 train_time:48529ms step_avg:60.43ms
step:804/2330 train_time:48592ms step_avg:60.44ms
step:805/2330 train_time:48651ms step_avg:60.44ms
step:806/2330 train_time:48715ms step_avg:60.44ms
step:807/2330 train_time:48775ms step_avg:60.44ms
step:808/2330 train_time:48837ms step_avg:60.44ms
step:809/2330 train_time:48896ms step_avg:60.44ms
step:810/2330 train_time:48959ms step_avg:60.44ms
step:811/2330 train_time:49018ms step_avg:60.44ms
step:812/2330 train_time:49080ms step_avg:60.44ms
step:813/2330 train_time:49139ms step_avg:60.44ms
step:814/2330 train_time:49202ms step_avg:60.44ms
step:815/2330 train_time:49260ms step_avg:60.44ms
step:816/2330 train_time:49324ms step_avg:60.45ms
step:817/2330 train_time:49384ms step_avg:60.45ms
step:818/2330 train_time:49448ms step_avg:60.45ms
step:819/2330 train_time:49508ms step_avg:60.45ms
step:820/2330 train_time:49571ms step_avg:60.45ms
step:821/2330 train_time:49630ms step_avg:60.45ms
step:822/2330 train_time:49694ms step_avg:60.45ms
step:823/2330 train_time:49753ms step_avg:60.45ms
step:824/2330 train_time:49815ms step_avg:60.46ms
step:825/2330 train_time:49874ms step_avg:60.45ms
step:826/2330 train_time:49936ms step_avg:60.46ms
step:827/2330 train_time:49996ms step_avg:60.45ms
step:828/2330 train_time:50058ms step_avg:60.46ms
step:829/2330 train_time:50116ms step_avg:60.45ms
step:830/2330 train_time:50180ms step_avg:60.46ms
step:831/2330 train_time:50238ms step_avg:60.45ms
step:832/2330 train_time:50300ms step_avg:60.46ms
step:833/2330 train_time:50361ms step_avg:60.46ms
step:834/2330 train_time:50425ms step_avg:60.46ms
step:835/2330 train_time:50486ms step_avg:60.46ms
step:836/2330 train_time:50549ms step_avg:60.47ms
step:837/2330 train_time:50609ms step_avg:60.46ms
step:838/2330 train_time:50673ms step_avg:60.47ms
step:839/2330 train_time:50732ms step_avg:60.47ms
step:840/2330 train_time:50794ms step_avg:60.47ms
step:841/2330 train_time:50854ms step_avg:60.47ms
step:842/2330 train_time:50915ms step_avg:60.47ms
step:843/2330 train_time:50975ms step_avg:60.47ms
step:844/2330 train_time:51037ms step_avg:60.47ms
step:845/2330 train_time:51096ms step_avg:60.47ms
step:846/2330 train_time:51159ms step_avg:60.47ms
step:847/2330 train_time:51219ms step_avg:60.47ms
step:848/2330 train_time:51282ms step_avg:60.47ms
step:849/2330 train_time:51342ms step_avg:60.47ms
step:850/2330 train_time:51405ms step_avg:60.48ms
step:851/2330 train_time:51466ms step_avg:60.48ms
step:852/2330 train_time:51529ms step_avg:60.48ms
step:853/2330 train_time:51588ms step_avg:60.48ms
step:854/2330 train_time:51651ms step_avg:60.48ms
step:855/2330 train_time:51709ms step_avg:60.48ms
step:856/2330 train_time:51773ms step_avg:60.48ms
step:857/2330 train_time:51832ms step_avg:60.48ms
step:858/2330 train_time:51895ms step_avg:60.48ms
step:859/2330 train_time:51953ms step_avg:60.48ms
step:860/2330 train_time:52017ms step_avg:60.48ms
step:861/2330 train_time:52076ms step_avg:60.48ms
step:862/2330 train_time:52139ms step_avg:60.49ms
step:863/2330 train_time:52198ms step_avg:60.48ms
step:864/2330 train_time:52260ms step_avg:60.49ms
step:865/2330 train_time:52320ms step_avg:60.49ms
step:866/2330 train_time:52384ms step_avg:60.49ms
step:867/2330 train_time:52444ms step_avg:60.49ms
step:868/2330 train_time:52507ms step_avg:60.49ms
step:869/2330 train_time:52568ms step_avg:60.49ms
step:870/2330 train_time:52631ms step_avg:60.49ms
step:871/2330 train_time:52689ms step_avg:60.49ms
step:872/2330 train_time:52752ms step_avg:60.49ms
step:873/2330 train_time:52811ms step_avg:60.49ms
step:874/2330 train_time:52874ms step_avg:60.50ms
step:875/2330 train_time:52933ms step_avg:60.49ms
step:876/2330 train_time:52996ms step_avg:60.50ms
step:877/2330 train_time:53055ms step_avg:60.50ms
step:878/2330 train_time:53118ms step_avg:60.50ms
step:879/2330 train_time:53178ms step_avg:60.50ms
step:880/2330 train_time:53240ms step_avg:60.50ms
step:881/2330 train_time:53300ms step_avg:60.50ms
step:882/2330 train_time:53363ms step_avg:60.50ms
step:883/2330 train_time:53424ms step_avg:60.50ms
step:884/2330 train_time:53487ms step_avg:60.51ms
step:885/2330 train_time:53547ms step_avg:60.51ms
step:886/2330 train_time:53610ms step_avg:60.51ms
step:887/2330 train_time:53670ms step_avg:60.51ms
step:888/2330 train_time:53733ms step_avg:60.51ms
step:889/2330 train_time:53792ms step_avg:60.51ms
step:890/2330 train_time:53855ms step_avg:60.51ms
step:891/2330 train_time:53914ms step_avg:60.51ms
step:892/2330 train_time:53977ms step_avg:60.51ms
step:893/2330 train_time:54036ms step_avg:60.51ms
step:894/2330 train_time:54098ms step_avg:60.51ms
step:895/2330 train_time:54157ms step_avg:60.51ms
step:896/2330 train_time:54220ms step_avg:60.51ms
step:897/2330 train_time:54280ms step_avg:60.51ms
step:898/2330 train_time:54342ms step_avg:60.51ms
step:899/2330 train_time:54401ms step_avg:60.51ms
step:900/2330 train_time:54465ms step_avg:60.52ms
step:901/2330 train_time:54525ms step_avg:60.52ms
step:902/2330 train_time:54588ms step_avg:60.52ms
step:903/2330 train_time:54649ms step_avg:60.52ms
step:904/2330 train_time:54711ms step_avg:60.52ms
step:905/2330 train_time:54770ms step_avg:60.52ms
step:906/2330 train_time:54833ms step_avg:60.52ms
step:907/2330 train_time:54892ms step_avg:60.52ms
step:908/2330 train_time:54955ms step_avg:60.52ms
step:909/2330 train_time:55014ms step_avg:60.52ms
step:910/2330 train_time:55077ms step_avg:60.52ms
step:911/2330 train_time:55135ms step_avg:60.52ms
step:912/2330 train_time:55199ms step_avg:60.53ms
step:913/2330 train_time:55258ms step_avg:60.52ms
step:914/2330 train_time:55321ms step_avg:60.53ms
step:915/2330 train_time:55381ms step_avg:60.53ms
step:916/2330 train_time:55444ms step_avg:60.53ms
step:917/2330 train_time:55504ms step_avg:60.53ms
step:918/2330 train_time:55568ms step_avg:60.53ms
step:919/2330 train_time:55628ms step_avg:60.53ms
step:920/2330 train_time:55690ms step_avg:60.53ms
step:921/2330 train_time:55750ms step_avg:60.53ms
step:922/2330 train_time:55812ms step_avg:60.53ms
step:923/2330 train_time:55872ms step_avg:60.53ms
step:924/2330 train_time:55934ms step_avg:60.53ms
step:925/2330 train_time:55993ms step_avg:60.53ms
step:926/2330 train_time:56056ms step_avg:60.54ms
step:927/2330 train_time:56115ms step_avg:60.53ms
step:928/2330 train_time:56179ms step_avg:60.54ms
step:929/2330 train_time:56237ms step_avg:60.54ms
step:930/2330 train_time:56299ms step_avg:60.54ms
step:931/2330 train_time:56359ms step_avg:60.54ms
step:932/2330 train_time:56422ms step_avg:60.54ms
step:933/2330 train_time:56481ms step_avg:60.54ms
step:934/2330 train_time:56545ms step_avg:60.54ms
step:935/2330 train_time:56605ms step_avg:60.54ms
step:936/2330 train_time:56669ms step_avg:60.54ms
step:937/2330 train_time:56729ms step_avg:60.54ms
step:938/2330 train_time:56791ms step_avg:60.54ms
step:939/2330 train_time:56850ms step_avg:60.54ms
step:940/2330 train_time:56914ms step_avg:60.55ms
step:941/2330 train_time:56973ms step_avg:60.55ms
step:942/2330 train_time:57035ms step_avg:60.55ms
step:943/2330 train_time:57094ms step_avg:60.54ms
step:944/2330 train_time:57157ms step_avg:60.55ms
step:945/2330 train_time:57215ms step_avg:60.55ms
step:946/2330 train_time:57278ms step_avg:60.55ms
step:947/2330 train_time:57337ms step_avg:60.55ms
step:948/2330 train_time:57400ms step_avg:60.55ms
step:949/2330 train_time:57460ms step_avg:60.55ms
step:950/2330 train_time:57524ms step_avg:60.55ms
step:951/2330 train_time:57584ms step_avg:60.55ms
step:952/2330 train_time:57647ms step_avg:60.55ms
step:953/2330 train_time:57707ms step_avg:60.55ms
step:954/2330 train_time:57769ms step_avg:60.56ms
step:955/2330 train_time:57829ms step_avg:60.55ms
step:956/2330 train_time:57892ms step_avg:60.56ms
step:957/2330 train_time:57951ms step_avg:60.56ms
step:958/2330 train_time:58014ms step_avg:60.56ms
step:959/2330 train_time:58073ms step_avg:60.56ms
step:960/2330 train_time:58135ms step_avg:60.56ms
step:961/2330 train_time:58193ms step_avg:60.55ms
step:962/2330 train_time:58257ms step_avg:60.56ms
step:963/2330 train_time:58316ms step_avg:60.56ms
step:964/2330 train_time:58379ms step_avg:60.56ms
step:965/2330 train_time:58438ms step_avg:60.56ms
step:966/2330 train_time:58501ms step_avg:60.56ms
step:967/2330 train_time:58562ms step_avg:60.56ms
step:968/2330 train_time:58627ms step_avg:60.56ms
step:969/2330 train_time:58686ms step_avg:60.56ms
step:970/2330 train_time:58750ms step_avg:60.57ms
step:971/2330 train_time:58809ms step_avg:60.57ms
step:972/2330 train_time:58873ms step_avg:60.57ms
step:973/2330 train_time:58932ms step_avg:60.57ms
step:974/2330 train_time:58995ms step_avg:60.57ms
step:975/2330 train_time:59053ms step_avg:60.57ms
step:976/2330 train_time:59116ms step_avg:60.57ms
step:977/2330 train_time:59175ms step_avg:60.57ms
step:978/2330 train_time:59237ms step_avg:60.57ms
step:979/2330 train_time:59296ms step_avg:60.57ms
step:980/2330 train_time:59359ms step_avg:60.57ms
step:981/2330 train_time:59418ms step_avg:60.57ms
step:982/2330 train_time:59482ms step_avg:60.57ms
step:983/2330 train_time:59541ms step_avg:60.57ms
step:984/2330 train_time:59605ms step_avg:60.57ms
step:985/2330 train_time:59665ms step_avg:60.57ms
step:986/2330 train_time:59728ms step_avg:60.58ms
step:987/2330 train_time:59787ms step_avg:60.57ms
step:988/2330 train_time:59850ms step_avg:60.58ms
step:989/2330 train_time:59910ms step_avg:60.58ms
step:990/2330 train_time:59973ms step_avg:60.58ms
step:991/2330 train_time:60032ms step_avg:60.58ms
step:992/2330 train_time:60094ms step_avg:60.58ms
step:993/2330 train_time:60153ms step_avg:60.58ms
step:994/2330 train_time:60216ms step_avg:60.58ms
step:995/2330 train_time:60276ms step_avg:60.58ms
step:996/2330 train_time:60339ms step_avg:60.58ms
step:997/2330 train_time:60398ms step_avg:60.58ms
step:998/2330 train_time:60460ms step_avg:60.58ms
step:999/2330 train_time:60520ms step_avg:60.58ms
step:1000/2330 train_time:60583ms step_avg:60.58ms
step:1000/2330 val_loss:3.8532 train_time:60656ms step_avg:60.66ms
step:1001/2330 train_time:60679ms step_avg:60.62ms
step:1002/2330 train_time:60710ms step_avg:60.59ms
step:1003/2330 train_time:60776ms step_avg:60.59ms
step:1004/2330 train_time:60842ms step_avg:60.60ms
step:1005/2330 train_time:60901ms step_avg:60.60ms
step:1006/2330 train_time:60964ms step_avg:60.60ms
step:1007/2330 train_time:61022ms step_avg:60.60ms
step:1008/2330 train_time:61084ms step_avg:60.60ms
step:1009/2330 train_time:61142ms step_avg:60.60ms
step:1010/2330 train_time:61204ms step_avg:60.60ms
step:1011/2330 train_time:61263ms step_avg:60.60ms
step:1012/2330 train_time:61325ms step_avg:60.60ms
step:1013/2330 train_time:61383ms step_avg:60.60ms
step:1014/2330 train_time:61445ms step_avg:60.60ms
step:1015/2330 train_time:61503ms step_avg:60.59ms
step:1016/2330 train_time:61565ms step_avg:60.60ms
step:1017/2330 train_time:61627ms step_avg:60.60ms
step:1018/2330 train_time:61693ms step_avg:60.60ms
step:1019/2330 train_time:61754ms step_avg:60.60ms
step:1020/2330 train_time:61818ms step_avg:60.61ms
step:1021/2330 train_time:61879ms step_avg:60.61ms
step:1022/2330 train_time:61941ms step_avg:60.61ms
step:1023/2330 train_time:62000ms step_avg:60.61ms
step:1024/2330 train_time:62063ms step_avg:60.61ms
step:1025/2330 train_time:62121ms step_avg:60.61ms
step:1026/2330 train_time:62183ms step_avg:60.61ms
step:1027/2330 train_time:62242ms step_avg:60.61ms
step:1028/2330 train_time:62304ms step_avg:60.61ms
step:1029/2330 train_time:62363ms step_avg:60.61ms
step:1030/2330 train_time:62425ms step_avg:60.61ms
step:1031/2330 train_time:62483ms step_avg:60.60ms
step:1032/2330 train_time:62546ms step_avg:60.61ms
step:1033/2330 train_time:62605ms step_avg:60.61ms
step:1034/2330 train_time:62671ms step_avg:60.61ms
step:1035/2330 train_time:62731ms step_avg:60.61ms
step:1036/2330 train_time:62794ms step_avg:60.61ms
step:1037/2330 train_time:62855ms step_avg:60.61ms
step:1038/2330 train_time:62918ms step_avg:60.61ms
step:1039/2330 train_time:62977ms step_avg:60.61ms
step:1040/2330 train_time:63041ms step_avg:60.62ms
step:1041/2330 train_time:63100ms step_avg:60.61ms
step:1042/2330 train_time:63163ms step_avg:60.62ms
step:1043/2330 train_time:63221ms step_avg:60.61ms
step:1044/2330 train_time:63284ms step_avg:60.62ms
step:1045/2330 train_time:63343ms step_avg:60.61ms
step:1046/2330 train_time:63405ms step_avg:60.62ms
step:1047/2330 train_time:63464ms step_avg:60.62ms
step:1048/2330 train_time:63527ms step_avg:60.62ms
step:1049/2330 train_time:63586ms step_avg:60.62ms
step:1050/2330 train_time:63649ms step_avg:60.62ms
step:1051/2330 train_time:63709ms step_avg:60.62ms
step:1052/2330 train_time:63773ms step_avg:60.62ms
step:1053/2330 train_time:63832ms step_avg:60.62ms
step:1054/2330 train_time:63896ms step_avg:60.62ms
step:1055/2330 train_time:63956ms step_avg:60.62ms
step:1056/2330 train_time:64020ms step_avg:60.63ms
step:1057/2330 train_time:64079ms step_avg:60.62ms
step:1058/2330 train_time:64142ms step_avg:60.63ms
step:1059/2330 train_time:64201ms step_avg:60.62ms
step:1060/2330 train_time:64263ms step_avg:60.63ms
step:1061/2330 train_time:64322ms step_avg:60.62ms
step:1062/2330 train_time:64384ms step_avg:60.63ms
step:1063/2330 train_time:64444ms step_avg:60.62ms
step:1064/2330 train_time:64506ms step_avg:60.63ms
step:1065/2330 train_time:64565ms step_avg:60.62ms
step:1066/2330 train_time:64628ms step_avg:60.63ms
step:1067/2330 train_time:64687ms step_avg:60.63ms
step:1068/2330 train_time:64752ms step_avg:60.63ms
step:1069/2330 train_time:64811ms step_avg:60.63ms
step:1070/2330 train_time:64875ms step_avg:60.63ms
step:1071/2330 train_time:64934ms step_avg:60.63ms
step:1072/2330 train_time:64997ms step_avg:60.63ms
step:1073/2330 train_time:65059ms step_avg:60.63ms
step:1074/2330 train_time:65121ms step_avg:60.63ms
step:1075/2330 train_time:65180ms step_avg:60.63ms
step:1076/2330 train_time:65243ms step_avg:60.63ms
step:1077/2330 train_time:65302ms step_avg:60.63ms
step:1078/2330 train_time:65364ms step_avg:60.63ms
step:1079/2330 train_time:65423ms step_avg:60.63ms
step:1080/2330 train_time:65486ms step_avg:60.63ms
step:1081/2330 train_time:65545ms step_avg:60.63ms
step:1082/2330 train_time:65608ms step_avg:60.64ms
step:1083/2330 train_time:65668ms step_avg:60.63ms
step:1084/2330 train_time:65730ms step_avg:60.64ms
step:1085/2330 train_time:65790ms step_avg:60.64ms
step:1086/2330 train_time:65853ms step_avg:60.64ms
step:1087/2330 train_time:65912ms step_avg:60.64ms
step:1088/2330 train_time:65976ms step_avg:60.64ms
step:1089/2330 train_time:66036ms step_avg:60.64ms
step:1090/2330 train_time:66099ms step_avg:60.64ms
step:1091/2330 train_time:66158ms step_avg:60.64ms
step:1092/2330 train_time:66221ms step_avg:60.64ms
step:1093/2330 train_time:66280ms step_avg:60.64ms
step:1094/2330 train_time:66343ms step_avg:60.64ms
step:1095/2330 train_time:66402ms step_avg:60.64ms
step:1096/2330 train_time:66464ms step_avg:60.64ms
step:1097/2330 train_time:66523ms step_avg:60.64ms
step:1098/2330 train_time:66587ms step_avg:60.64ms
step:1099/2330 train_time:66646ms step_avg:60.64ms
step:1100/2330 train_time:66710ms step_avg:60.65ms
step:1101/2330 train_time:66770ms step_avg:60.64ms
step:1102/2330 train_time:66833ms step_avg:60.65ms
step:1103/2330 train_time:66892ms step_avg:60.65ms
step:1104/2330 train_time:66955ms step_avg:60.65ms
step:1105/2330 train_time:67015ms step_avg:60.65ms
step:1106/2330 train_time:67077ms step_avg:60.65ms
step:1107/2330 train_time:67138ms step_avg:60.65ms
step:1108/2330 train_time:67200ms step_avg:60.65ms
step:1109/2330 train_time:67260ms step_avg:60.65ms
step:1110/2330 train_time:67323ms step_avg:60.65ms
step:1111/2330 train_time:67382ms step_avg:60.65ms
step:1112/2330 train_time:67444ms step_avg:60.65ms
step:1113/2330 train_time:67503ms step_avg:60.65ms
step:1114/2330 train_time:67567ms step_avg:60.65ms
step:1115/2330 train_time:67626ms step_avg:60.65ms
step:1116/2330 train_time:67688ms step_avg:60.65ms
step:1117/2330 train_time:67747ms step_avg:60.65ms
step:1118/2330 train_time:67810ms step_avg:60.65ms
step:1119/2330 train_time:67870ms step_avg:60.65ms
step:1120/2330 train_time:67932ms step_avg:60.65ms
step:1121/2330 train_time:67992ms step_avg:60.65ms
step:1122/2330 train_time:68055ms step_avg:60.66ms
step:1123/2330 train_time:68115ms step_avg:60.65ms
step:1124/2330 train_time:68179ms step_avg:60.66ms
step:1125/2330 train_time:68238ms step_avg:60.66ms
step:1126/2330 train_time:68302ms step_avg:60.66ms
step:1127/2330 train_time:68362ms step_avg:60.66ms
step:1128/2330 train_time:68424ms step_avg:60.66ms
step:1129/2330 train_time:68483ms step_avg:60.66ms
step:1130/2330 train_time:68546ms step_avg:60.66ms
step:1131/2330 train_time:68604ms step_avg:60.66ms
step:1132/2330 train_time:68668ms step_avg:60.66ms
step:1133/2330 train_time:68727ms step_avg:60.66ms
step:1134/2330 train_time:68790ms step_avg:60.66ms
step:1135/2330 train_time:68849ms step_avg:60.66ms
step:1136/2330 train_time:68912ms step_avg:60.66ms
step:1137/2330 train_time:68972ms step_avg:60.66ms
step:1138/2330 train_time:69035ms step_avg:60.66ms
step:1139/2330 train_time:69094ms step_avg:60.66ms
step:1140/2330 train_time:69158ms step_avg:60.66ms
step:1141/2330 train_time:69217ms step_avg:60.66ms
step:1142/2330 train_time:69281ms step_avg:60.67ms
step:1143/2330 train_time:69340ms step_avg:60.66ms
step:1144/2330 train_time:69403ms step_avg:60.67ms
step:1145/2330 train_time:69463ms step_avg:60.67ms
step:1146/2330 train_time:69525ms step_avg:60.67ms
step:1147/2330 train_time:69584ms step_avg:60.67ms
step:1148/2330 train_time:69647ms step_avg:60.67ms
step:1149/2330 train_time:69706ms step_avg:60.67ms
step:1150/2330 train_time:69770ms step_avg:60.67ms
step:1151/2330 train_time:69828ms step_avg:60.67ms
step:1152/2330 train_time:69892ms step_avg:60.67ms
step:1153/2330 train_time:69952ms step_avg:60.67ms
step:1154/2330 train_time:70014ms step_avg:60.67ms
step:1155/2330 train_time:70074ms step_avg:60.67ms
step:1156/2330 train_time:70137ms step_avg:60.67ms
step:1157/2330 train_time:70196ms step_avg:60.67ms
step:1158/2330 train_time:70260ms step_avg:60.67ms
step:1159/2330 train_time:70321ms step_avg:60.67ms
step:1160/2330 train_time:70383ms step_avg:60.67ms
step:1161/2330 train_time:70443ms step_avg:60.67ms
step:1162/2330 train_time:70506ms step_avg:60.68ms
step:1163/2330 train_time:70565ms step_avg:60.67ms
step:1164/2330 train_time:70628ms step_avg:60.68ms
step:1165/2330 train_time:70687ms step_avg:60.68ms
step:1166/2330 train_time:70750ms step_avg:60.68ms
step:1167/2330 train_time:70809ms step_avg:60.68ms
step:1168/2330 train_time:70873ms step_avg:60.68ms
step:1169/2330 train_time:70932ms step_avg:60.68ms
step:1170/2330 train_time:70994ms step_avg:60.68ms
step:1171/2330 train_time:71054ms step_avg:60.68ms
step:1172/2330 train_time:71117ms step_avg:60.68ms
step:1173/2330 train_time:71176ms step_avg:60.68ms
step:1174/2330 train_time:71240ms step_avg:60.68ms
step:1175/2330 train_time:71301ms step_avg:60.68ms
step:1176/2330 train_time:71363ms step_avg:60.68ms
step:1177/2330 train_time:71422ms step_avg:60.68ms
step:1178/2330 train_time:71486ms step_avg:60.68ms
step:1179/2330 train_time:71545ms step_avg:60.68ms
step:1180/2330 train_time:71607ms step_avg:60.68ms
step:1181/2330 train_time:71667ms step_avg:60.68ms
step:1182/2330 train_time:71728ms step_avg:60.68ms
step:1183/2330 train_time:71787ms step_avg:60.68ms
step:1184/2330 train_time:71851ms step_avg:60.68ms
step:1185/2330 train_time:71910ms step_avg:60.68ms
step:1186/2330 train_time:71973ms step_avg:60.69ms
step:1187/2330 train_time:72033ms step_avg:60.68ms
step:1188/2330 train_time:72095ms step_avg:60.69ms
step:1189/2330 train_time:72155ms step_avg:60.69ms
step:1190/2330 train_time:72220ms step_avg:60.69ms
step:1191/2330 train_time:72280ms step_avg:60.69ms
step:1192/2330 train_time:72343ms step_avg:60.69ms
step:1193/2330 train_time:72402ms step_avg:60.69ms
step:1194/2330 train_time:72465ms step_avg:60.69ms
step:1195/2330 train_time:72523ms step_avg:60.69ms
step:1196/2330 train_time:72586ms step_avg:60.69ms
step:1197/2330 train_time:72645ms step_avg:60.69ms
step:1198/2330 train_time:72708ms step_avg:60.69ms
step:1199/2330 train_time:72767ms step_avg:60.69ms
step:1200/2330 train_time:72830ms step_avg:60.69ms
step:1201/2330 train_time:72889ms step_avg:60.69ms
step:1202/2330 train_time:72953ms step_avg:60.69ms
step:1203/2330 train_time:73012ms step_avg:60.69ms
step:1204/2330 train_time:73075ms step_avg:60.69ms
step:1205/2330 train_time:73134ms step_avg:60.69ms
step:1206/2330 train_time:73198ms step_avg:60.70ms
step:1207/2330 train_time:73258ms step_avg:60.69ms
step:1208/2330 train_time:73322ms step_avg:60.70ms
step:1209/2330 train_time:73381ms step_avg:60.70ms
step:1210/2330 train_time:73444ms step_avg:60.70ms
step:1211/2330 train_time:73503ms step_avg:60.70ms
step:1212/2330 train_time:73565ms step_avg:60.70ms
step:1213/2330 train_time:73624ms step_avg:60.70ms
step:1214/2330 train_time:73687ms step_avg:60.70ms
step:1215/2330 train_time:73746ms step_avg:60.70ms
step:1216/2330 train_time:73809ms step_avg:60.70ms
step:1217/2330 train_time:73869ms step_avg:60.70ms
step:1218/2330 train_time:73932ms step_avg:60.70ms
step:1219/2330 train_time:73991ms step_avg:60.70ms
step:1220/2330 train_time:74055ms step_avg:60.70ms
step:1221/2330 train_time:74115ms step_avg:60.70ms
step:1222/2330 train_time:74179ms step_avg:60.70ms
step:1223/2330 train_time:74238ms step_avg:60.70ms
step:1224/2330 train_time:74301ms step_avg:60.70ms
step:1225/2330 train_time:74360ms step_avg:60.70ms
step:1226/2330 train_time:74423ms step_avg:60.70ms
step:1227/2330 train_time:74483ms step_avg:60.70ms
step:1228/2330 train_time:74546ms step_avg:60.71ms
step:1229/2330 train_time:74605ms step_avg:60.70ms
step:1230/2330 train_time:74667ms step_avg:60.71ms
step:1231/2330 train_time:74727ms step_avg:60.70ms
step:1232/2330 train_time:74790ms step_avg:60.71ms
step:1233/2330 train_time:74849ms step_avg:60.70ms
step:1234/2330 train_time:74912ms step_avg:60.71ms
step:1235/2330 train_time:74972ms step_avg:60.71ms
step:1236/2330 train_time:75035ms step_avg:60.71ms
step:1237/2330 train_time:75094ms step_avg:60.71ms
step:1238/2330 train_time:75158ms step_avg:60.71ms
step:1239/2330 train_time:75217ms step_avg:60.71ms
step:1240/2330 train_time:75280ms step_avg:60.71ms
step:1241/2330 train_time:75339ms step_avg:60.71ms
step:1242/2330 train_time:75402ms step_avg:60.71ms
step:1243/2330 train_time:75462ms step_avg:60.71ms
step:1244/2330 train_time:75524ms step_avg:60.71ms
step:1245/2330 train_time:75583ms step_avg:60.71ms
step:1246/2330 train_time:75647ms step_avg:60.71ms
step:1247/2330 train_time:75707ms step_avg:60.71ms
step:1248/2330 train_time:75769ms step_avg:60.71ms
step:1249/2330 train_time:75828ms step_avg:60.71ms
step:1250/2330 train_time:75891ms step_avg:60.71ms
step:1250/2330 val_loss:3.7679 train_time:75962ms step_avg:60.77ms
step:1251/2330 train_time:75985ms step_avg:60.74ms
step:1252/2330 train_time:76017ms step_avg:60.72ms
step:1253/2330 train_time:76077ms step_avg:60.72ms
step:1254/2330 train_time:76145ms step_avg:60.72ms
step:1255/2330 train_time:76205ms step_avg:60.72ms
step:1256/2330 train_time:76267ms step_avg:60.72ms
step:1257/2330 train_time:76326ms step_avg:60.72ms
step:1258/2330 train_time:76389ms step_avg:60.72ms
step:1259/2330 train_time:76448ms step_avg:60.72ms
step:1260/2330 train_time:76510ms step_avg:60.72ms
step:1261/2330 train_time:76569ms step_avg:60.72ms
step:1262/2330 train_time:76632ms step_avg:60.72ms
step:1263/2330 train_time:76692ms step_avg:60.72ms
step:1264/2330 train_time:76754ms step_avg:60.72ms
step:1265/2330 train_time:76811ms step_avg:60.72ms
step:1266/2330 train_time:76874ms step_avg:60.72ms
step:1267/2330 train_time:76934ms step_avg:60.72ms
step:1268/2330 train_time:76998ms step_avg:60.72ms
step:1269/2330 train_time:77058ms step_avg:60.72ms
step:1270/2330 train_time:77122ms step_avg:60.73ms
step:1271/2330 train_time:77182ms step_avg:60.73ms
step:1272/2330 train_time:77245ms step_avg:60.73ms
step:1273/2330 train_time:77305ms step_avg:60.73ms
step:1274/2330 train_time:77366ms step_avg:60.73ms
step:1275/2330 train_time:77426ms step_avg:60.73ms
step:1276/2330 train_time:77489ms step_avg:60.73ms
step:1277/2330 train_time:77549ms step_avg:60.73ms
step:1278/2330 train_time:77612ms step_avg:60.73ms
step:1279/2330 train_time:77670ms step_avg:60.73ms
step:1280/2330 train_time:77733ms step_avg:60.73ms
step:1281/2330 train_time:77792ms step_avg:60.73ms
step:1282/2330 train_time:77855ms step_avg:60.73ms
step:1283/2330 train_time:77914ms step_avg:60.73ms
step:1284/2330 train_time:77977ms step_avg:60.73ms
step:1285/2330 train_time:78037ms step_avg:60.73ms
step:1286/2330 train_time:78102ms step_avg:60.73ms
step:1287/2330 train_time:78161ms step_avg:60.73ms
step:1288/2330 train_time:78224ms step_avg:60.73ms
step:1289/2330 train_time:78283ms step_avg:60.73ms
step:1290/2330 train_time:78346ms step_avg:60.73ms
step:1291/2330 train_time:78406ms step_avg:60.73ms
step:1292/2330 train_time:78469ms step_avg:60.73ms
step:1293/2330 train_time:78528ms step_avg:60.73ms
step:1294/2330 train_time:78591ms step_avg:60.74ms
step:1295/2330 train_time:78650ms step_avg:60.73ms
step:1296/2330 train_time:78713ms step_avg:60.74ms
step:1297/2330 train_time:78772ms step_avg:60.73ms
step:1298/2330 train_time:78834ms step_avg:60.74ms
step:1299/2330 train_time:78894ms step_avg:60.73ms
step:1300/2330 train_time:78957ms step_avg:60.74ms
step:1301/2330 train_time:79016ms step_avg:60.74ms
step:1302/2330 train_time:79080ms step_avg:60.74ms
step:1303/2330 train_time:79140ms step_avg:60.74ms
step:1304/2330 train_time:79204ms step_avg:60.74ms
step:1305/2330 train_time:79263ms step_avg:60.74ms
step:1306/2330 train_time:79325ms step_avg:60.74ms
step:1307/2330 train_time:79385ms step_avg:60.74ms
step:1308/2330 train_time:79448ms step_avg:60.74ms
step:1309/2330 train_time:79508ms step_avg:60.74ms
step:1310/2330 train_time:79571ms step_avg:60.74ms
step:1311/2330 train_time:79631ms step_avg:60.74ms
step:1312/2330 train_time:79694ms step_avg:60.74ms
step:1313/2330 train_time:79753ms step_avg:60.74ms
step:1314/2330 train_time:79815ms step_avg:60.74ms
step:1315/2330 train_time:79874ms step_avg:60.74ms
step:1316/2330 train_time:79938ms step_avg:60.74ms
step:1317/2330 train_time:79997ms step_avg:60.74ms
step:1318/2330 train_time:80060ms step_avg:60.74ms
step:1319/2330 train_time:80119ms step_avg:60.74ms
step:1320/2330 train_time:80182ms step_avg:60.74ms
step:1321/2330 train_time:80242ms step_avg:60.74ms
step:1322/2330 train_time:80305ms step_avg:60.75ms
step:1323/2330 train_time:80365ms step_avg:60.74ms
step:1324/2330 train_time:80427ms step_avg:60.75ms
step:1325/2330 train_time:80488ms step_avg:60.75ms
step:1326/2330 train_time:80551ms step_avg:60.75ms
step:1327/2330 train_time:80610ms step_avg:60.75ms
step:1328/2330 train_time:80673ms step_avg:60.75ms
step:1329/2330 train_time:80732ms step_avg:60.75ms
step:1330/2330 train_time:80794ms step_avg:60.75ms
step:1331/2330 train_time:80854ms step_avg:60.75ms
step:1332/2330 train_time:80917ms step_avg:60.75ms
step:1333/2330 train_time:80976ms step_avg:60.75ms
step:1334/2330 train_time:81040ms step_avg:60.75ms
step:1335/2330 train_time:81100ms step_avg:60.75ms
step:1336/2330 train_time:81162ms step_avg:60.75ms
step:1337/2330 train_time:81221ms step_avg:60.75ms
step:1338/2330 train_time:81285ms step_avg:60.75ms
step:1339/2330 train_time:81344ms step_avg:60.75ms
step:1340/2330 train_time:81407ms step_avg:60.75ms
step:1341/2330 train_time:81466ms step_avg:60.75ms
step:1342/2330 train_time:81530ms step_avg:60.75ms
step:1343/2330 train_time:81589ms step_avg:60.75ms
step:1344/2330 train_time:81652ms step_avg:60.75ms
step:1345/2330 train_time:81711ms step_avg:60.75ms
step:1346/2330 train_time:81774ms step_avg:60.75ms
step:1347/2330 train_time:81833ms step_avg:60.75ms
step:1348/2330 train_time:81895ms step_avg:60.75ms
step:1349/2330 train_time:81955ms step_avg:60.75ms
step:1350/2330 train_time:82018ms step_avg:60.75ms
step:1351/2330 train_time:82077ms step_avg:60.75ms
step:1352/2330 train_time:82140ms step_avg:60.75ms
step:1353/2330 train_time:82200ms step_avg:60.75ms
step:1354/2330 train_time:82263ms step_avg:60.76ms
step:1355/2330 train_time:82323ms step_avg:60.75ms
step:1356/2330 train_time:82386ms step_avg:60.76ms
step:1357/2330 train_time:82446ms step_avg:60.76ms
step:1358/2330 train_time:82509ms step_avg:60.76ms
step:1359/2330 train_time:82569ms step_avg:60.76ms
step:1360/2330 train_time:82632ms step_avg:60.76ms
step:1361/2330 train_time:82691ms step_avg:60.76ms
step:1362/2330 train_time:82753ms step_avg:60.76ms
step:1363/2330 train_time:82812ms step_avg:60.76ms
step:1364/2330 train_time:82875ms step_avg:60.76ms
step:1365/2330 train_time:82935ms step_avg:60.76ms
step:1366/2330 train_time:82998ms step_avg:60.76ms
step:1367/2330 train_time:83058ms step_avg:60.76ms
step:1368/2330 train_time:83120ms step_avg:60.76ms
step:1369/2330 train_time:83180ms step_avg:60.76ms
step:1370/2330 train_time:83244ms step_avg:60.76ms
step:1371/2330 train_time:83303ms step_avg:60.76ms
step:1372/2330 train_time:83366ms step_avg:60.76ms
step:1373/2330 train_time:83425ms step_avg:60.76ms
step:1374/2330 train_time:83488ms step_avg:60.76ms
step:1375/2330 train_time:83548ms step_avg:60.76ms
step:1376/2330 train_time:83611ms step_avg:60.76ms
step:1377/2330 train_time:83671ms step_avg:60.76ms
step:1378/2330 train_time:83734ms step_avg:60.76ms
step:1379/2330 train_time:83793ms step_avg:60.76ms
step:1380/2330 train_time:83855ms step_avg:60.76ms
step:1381/2330 train_time:83914ms step_avg:60.76ms
step:1382/2330 train_time:83977ms step_avg:60.76ms
step:1383/2330 train_time:84036ms step_avg:60.76ms
step:1384/2330 train_time:84099ms step_avg:60.77ms
step:1385/2330 train_time:84159ms step_avg:60.76ms
step:1386/2330 train_time:84222ms step_avg:60.77ms
step:1387/2330 train_time:84281ms step_avg:60.77ms
step:1388/2330 train_time:84344ms step_avg:60.77ms
step:1389/2330 train_time:84405ms step_avg:60.77ms
step:1390/2330 train_time:84468ms step_avg:60.77ms
step:1391/2330 train_time:84527ms step_avg:60.77ms
step:1392/2330 train_time:84591ms step_avg:60.77ms
step:1393/2330 train_time:84650ms step_avg:60.77ms
step:1394/2330 train_time:84713ms step_avg:60.77ms
step:1395/2330 train_time:84772ms step_avg:60.77ms
step:1396/2330 train_time:84836ms step_avg:60.77ms
step:1397/2330 train_time:84896ms step_avg:60.77ms
step:1398/2330 train_time:84958ms step_avg:60.77ms
step:1399/2330 train_time:85017ms step_avg:60.77ms
step:1400/2330 train_time:85080ms step_avg:60.77ms
step:1401/2330 train_time:85140ms step_avg:60.77ms
step:1402/2330 train_time:85204ms step_avg:60.77ms
step:1403/2330 train_time:85264ms step_avg:60.77ms
step:1404/2330 train_time:85326ms step_avg:60.77ms
step:1405/2330 train_time:85385ms step_avg:60.77ms
step:1406/2330 train_time:85448ms step_avg:60.77ms
step:1407/2330 train_time:85508ms step_avg:60.77ms
step:1408/2330 train_time:85570ms step_avg:60.77ms
step:1409/2330 train_time:85630ms step_avg:60.77ms
step:1410/2330 train_time:85693ms step_avg:60.78ms
step:1411/2330 train_time:85752ms step_avg:60.77ms
step:1412/2330 train_time:85816ms step_avg:60.78ms
step:1413/2330 train_time:85876ms step_avg:60.78ms
step:1414/2330 train_time:85939ms step_avg:60.78ms
step:1415/2330 train_time:85998ms step_avg:60.78ms
step:1416/2330 train_time:86061ms step_avg:60.78ms
step:1417/2330 train_time:86119ms step_avg:60.78ms
step:1418/2330 train_time:86183ms step_avg:60.78ms
step:1419/2330 train_time:86242ms step_avg:60.78ms
step:1420/2330 train_time:86305ms step_avg:60.78ms
step:1421/2330 train_time:86364ms step_avg:60.78ms
step:1422/2330 train_time:86427ms step_avg:60.78ms
step:1423/2330 train_time:86486ms step_avg:60.78ms
step:1424/2330 train_time:86549ms step_avg:60.78ms
step:1425/2330 train_time:86608ms step_avg:60.78ms
step:1426/2330 train_time:86671ms step_avg:60.78ms
step:1427/2330 train_time:86730ms step_avg:60.78ms
step:1428/2330 train_time:86795ms step_avg:60.78ms
step:1429/2330 train_time:86853ms step_avg:60.78ms
step:1430/2330 train_time:86916ms step_avg:60.78ms
step:1431/2330 train_time:86975ms step_avg:60.78ms
step:1432/2330 train_time:87038ms step_avg:60.78ms
step:1433/2330 train_time:87098ms step_avg:60.78ms
step:1434/2330 train_time:87160ms step_avg:60.78ms
step:1435/2330 train_time:87220ms step_avg:60.78ms
step:1436/2330 train_time:87283ms step_avg:60.78ms
step:1437/2330 train_time:87343ms step_avg:60.78ms
step:1438/2330 train_time:87407ms step_avg:60.78ms
step:1439/2330 train_time:87466ms step_avg:60.78ms
step:1440/2330 train_time:87529ms step_avg:60.78ms
step:1441/2330 train_time:87589ms step_avg:60.78ms
step:1442/2330 train_time:87652ms step_avg:60.78ms
step:1443/2330 train_time:87712ms step_avg:60.78ms
step:1444/2330 train_time:87774ms step_avg:60.79ms
step:1445/2330 train_time:87833ms step_avg:60.78ms
step:1446/2330 train_time:87897ms step_avg:60.79ms
step:1447/2330 train_time:87955ms step_avg:60.78ms
step:1448/2330 train_time:88019ms step_avg:60.79ms
step:1449/2330 train_time:88078ms step_avg:60.79ms
step:1450/2330 train_time:88141ms step_avg:60.79ms
step:1451/2330 train_time:88200ms step_avg:60.79ms
step:1452/2330 train_time:88263ms step_avg:60.79ms
step:1453/2330 train_time:88323ms step_avg:60.79ms
step:1454/2330 train_time:88387ms step_avg:60.79ms
step:1455/2330 train_time:88446ms step_avg:60.79ms
step:1456/2330 train_time:88509ms step_avg:60.79ms
step:1457/2330 train_time:88568ms step_avg:60.79ms
step:1458/2330 train_time:88632ms step_avg:60.79ms
step:1459/2330 train_time:88692ms step_avg:60.79ms
step:1460/2330 train_time:88754ms step_avg:60.79ms
step:1461/2330 train_time:88813ms step_avg:60.79ms
step:1462/2330 train_time:88876ms step_avg:60.79ms
step:1463/2330 train_time:88936ms step_avg:60.79ms
step:1464/2330 train_time:88998ms step_avg:60.79ms
step:1465/2330 train_time:89057ms step_avg:60.79ms
step:1466/2330 train_time:89120ms step_avg:60.79ms
step:1467/2330 train_time:89178ms step_avg:60.79ms
step:1468/2330 train_time:89242ms step_avg:60.79ms
step:1469/2330 train_time:89302ms step_avg:60.79ms
step:1470/2330 train_time:89364ms step_avg:60.79ms
step:1471/2330 train_time:89423ms step_avg:60.79ms
step:1472/2330 train_time:89487ms step_avg:60.79ms
step:1473/2330 train_time:89546ms step_avg:60.79ms
step:1474/2330 train_time:89609ms step_avg:60.79ms
step:1475/2330 train_time:89669ms step_avg:60.79ms
step:1476/2330 train_time:89732ms step_avg:60.79ms
step:1477/2330 train_time:89791ms step_avg:60.79ms
step:1478/2330 train_time:89855ms step_avg:60.80ms
step:1479/2330 train_time:89914ms step_avg:60.79ms
step:1480/2330 train_time:89978ms step_avg:60.80ms
step:1481/2330 train_time:90036ms step_avg:60.79ms
step:1482/2330 train_time:90100ms step_avg:60.80ms
step:1483/2330 train_time:90159ms step_avg:60.79ms
step:1484/2330 train_time:90221ms step_avg:60.80ms
step:1485/2330 train_time:90280ms step_avg:60.79ms
step:1486/2330 train_time:90344ms step_avg:60.80ms
step:1487/2330 train_time:90404ms step_avg:60.80ms
step:1488/2330 train_time:90467ms step_avg:60.80ms
step:1489/2330 train_time:90526ms step_avg:60.80ms
step:1490/2330 train_time:90589ms step_avg:60.80ms
step:1491/2330 train_time:90649ms step_avg:60.80ms
step:1492/2330 train_time:90712ms step_avg:60.80ms
step:1493/2330 train_time:90772ms step_avg:60.80ms
step:1494/2330 train_time:90834ms step_avg:60.80ms
step:1495/2330 train_time:90894ms step_avg:60.80ms
step:1496/2330 train_time:90958ms step_avg:60.80ms
step:1497/2330 train_time:91017ms step_avg:60.80ms
step:1498/2330 train_time:91080ms step_avg:60.80ms
step:1499/2330 train_time:91139ms step_avg:60.80ms
step:1500/2330 train_time:91202ms step_avg:60.80ms
step:1500/2330 val_loss:3.7018 train_time:91273ms step_avg:60.85ms
step:1501/2330 train_time:91295ms step_avg:60.82ms
step:1502/2330 train_time:91326ms step_avg:60.80ms
step:1503/2330 train_time:91386ms step_avg:60.80ms
step:1504/2330 train_time:91452ms step_avg:60.81ms
step:1505/2330 train_time:91514ms step_avg:60.81ms
step:1506/2330 train_time:91578ms step_avg:60.81ms
step:1507/2330 train_time:91637ms step_avg:60.81ms
step:1508/2330 train_time:91699ms step_avg:60.81ms
step:1509/2330 train_time:91758ms step_avg:60.81ms
step:1510/2330 train_time:91821ms step_avg:60.81ms
step:1511/2330 train_time:91879ms step_avg:60.81ms
step:1512/2330 train_time:91943ms step_avg:60.81ms
step:1513/2330 train_time:92001ms step_avg:60.81ms
step:1514/2330 train_time:92063ms step_avg:60.81ms
step:1515/2330 train_time:92122ms step_avg:60.81ms
step:1516/2330 train_time:92185ms step_avg:60.81ms
step:1517/2330 train_time:92246ms step_avg:60.81ms
step:1518/2330 train_time:92309ms step_avg:60.81ms
step:1519/2330 train_time:92369ms step_avg:60.81ms
step:1520/2330 train_time:92434ms step_avg:60.81ms
step:1521/2330 train_time:92493ms step_avg:60.81ms
step:1522/2330 train_time:92557ms step_avg:60.81ms
step:1523/2330 train_time:92616ms step_avg:60.81ms
step:1524/2330 train_time:92680ms step_avg:60.81ms
step:1525/2330 train_time:92739ms step_avg:60.81ms
step:1526/2330 train_time:92802ms step_avg:60.81ms
step:1527/2330 train_time:92861ms step_avg:60.81ms
step:1528/2330 train_time:92924ms step_avg:60.81ms
step:1529/2330 train_time:92983ms step_avg:60.81ms
step:1530/2330 train_time:93045ms step_avg:60.81ms
step:1531/2330 train_time:93104ms step_avg:60.81ms
step:1532/2330 train_time:93167ms step_avg:60.81ms
step:1533/2330 train_time:93228ms step_avg:60.81ms
step:1534/2330 train_time:93292ms step_avg:60.82ms
step:1535/2330 train_time:93351ms step_avg:60.82ms
step:1536/2330 train_time:93415ms step_avg:60.82ms
step:1537/2330 train_time:93476ms step_avg:60.82ms
step:1538/2330 train_time:93540ms step_avg:60.82ms
step:1539/2330 train_time:93600ms step_avg:60.82ms
step:1540/2330 train_time:93664ms step_avg:60.82ms
step:1541/2330 train_time:93724ms step_avg:60.82ms
step:1542/2330 train_time:93787ms step_avg:60.82ms
step:1543/2330 train_time:93847ms step_avg:60.82ms
step:1544/2330 train_time:93910ms step_avg:60.82ms
step:1545/2330 train_time:93969ms step_avg:60.82ms
step:1546/2330 train_time:94033ms step_avg:60.82ms
step:1547/2330 train_time:94091ms step_avg:60.82ms
step:1548/2330 train_time:94154ms step_avg:60.82ms
step:1549/2330 train_time:94214ms step_avg:60.82ms
step:1550/2330 train_time:94278ms step_avg:60.82ms
step:1551/2330 train_time:94339ms step_avg:60.82ms
step:1552/2330 train_time:94402ms step_avg:60.83ms
step:1553/2330 train_time:94462ms step_avg:60.83ms
step:1554/2330 train_time:94526ms step_avg:60.83ms
step:1555/2330 train_time:94585ms step_avg:60.83ms
step:1556/2330 train_time:94649ms step_avg:60.83ms
step:1557/2330 train_time:94709ms step_avg:60.83ms
step:1558/2330 train_time:94773ms step_avg:60.83ms
step:1559/2330 train_time:94833ms step_avg:60.83ms
step:1560/2330 train_time:94897ms step_avg:60.83ms
step:1561/2330 train_time:94957ms step_avg:60.83ms
step:1562/2330 train_time:95021ms step_avg:60.83ms
step:1563/2330 train_time:95080ms step_avg:60.83ms
step:1564/2330 train_time:95144ms step_avg:60.83ms
step:1565/2330 train_time:95204ms step_avg:60.83ms
step:1566/2330 train_time:95267ms step_avg:60.83ms
step:1567/2330 train_time:95327ms step_avg:60.83ms
step:1568/2330 train_time:95391ms step_avg:60.84ms
step:1569/2330 train_time:95450ms step_avg:60.84ms
step:1570/2330 train_time:95514ms step_avg:60.84ms
step:1571/2330 train_time:95573ms step_avg:60.84ms
step:1572/2330 train_time:95637ms step_avg:60.84ms
step:1573/2330 train_time:95698ms step_avg:60.84ms
step:1574/2330 train_time:95761ms step_avg:60.84ms
step:1575/2330 train_time:95821ms step_avg:60.84ms
step:1576/2330 train_time:95885ms step_avg:60.84ms
step:1577/2330 train_time:95944ms step_avg:60.84ms
step:1578/2330 train_time:96008ms step_avg:60.84ms
step:1579/2330 train_time:96068ms step_avg:60.84ms
step:1580/2330 train_time:96131ms step_avg:60.84ms
step:1581/2330 train_time:96190ms step_avg:60.84ms
step:1582/2330 train_time:96253ms step_avg:60.84ms
step:1583/2330 train_time:96314ms step_avg:60.84ms
step:1584/2330 train_time:96378ms step_avg:60.84ms
step:1585/2330 train_time:96438ms step_avg:60.84ms
step:1586/2330 train_time:96501ms step_avg:60.85ms
step:1587/2330 train_time:96561ms step_avg:60.85ms
step:1588/2330 train_time:96624ms step_avg:60.85ms
step:1589/2330 train_time:96684ms step_avg:60.85ms
step:1590/2330 train_time:96748ms step_avg:60.85ms
step:1591/2330 train_time:96808ms step_avg:60.85ms
step:1592/2330 train_time:96871ms step_avg:60.85ms
step:1593/2330 train_time:96932ms step_avg:60.85ms
step:1594/2330 train_time:96995ms step_avg:60.85ms
step:1595/2330 train_time:97055ms step_avg:60.85ms
step:1596/2330 train_time:97119ms step_avg:60.85ms
step:1597/2330 train_time:97179ms step_avg:60.85ms
step:1598/2330 train_time:97242ms step_avg:60.85ms
step:1599/2330 train_time:97302ms step_avg:60.85ms
step:1600/2330 train_time:97366ms step_avg:60.85ms
step:1601/2330 train_time:97426ms step_avg:60.85ms
step:1602/2330 train_time:97490ms step_avg:60.86ms
step:1603/2330 train_time:97549ms step_avg:60.85ms
step:1604/2330 train_time:97613ms step_avg:60.86ms
step:1605/2330 train_time:97673ms step_avg:60.86ms
step:1606/2330 train_time:97736ms step_avg:60.86ms
step:1607/2330 train_time:97797ms step_avg:60.86ms
step:1608/2330 train_time:97861ms step_avg:60.86ms
step:1609/2330 train_time:97921ms step_avg:60.86ms
step:1610/2330 train_time:97984ms step_avg:60.86ms
step:1611/2330 train_time:98043ms step_avg:60.86ms
step:1612/2330 train_time:98107ms step_avg:60.86ms
step:1613/2330 train_time:98167ms step_avg:60.86ms
step:1614/2330 train_time:98230ms step_avg:60.86ms
step:1615/2330 train_time:98289ms step_avg:60.86ms
step:1616/2330 train_time:98353ms step_avg:60.86ms
step:1617/2330 train_time:98412ms step_avg:60.86ms
step:1618/2330 train_time:98476ms step_avg:60.86ms
step:1619/2330 train_time:98537ms step_avg:60.86ms
step:1620/2330 train_time:98601ms step_avg:60.86ms
step:1621/2330 train_time:98660ms step_avg:60.86ms
step:1622/2330 train_time:98724ms step_avg:60.87ms
step:1623/2330 train_time:98785ms step_avg:60.87ms
step:1624/2330 train_time:98848ms step_avg:60.87ms
step:1625/2330 train_time:98908ms step_avg:60.87ms
step:1626/2330 train_time:98972ms step_avg:60.87ms
step:1627/2330 train_time:99031ms step_avg:60.87ms
step:1628/2330 train_time:99095ms step_avg:60.87ms
step:1629/2330 train_time:99156ms step_avg:60.87ms
step:1630/2330 train_time:99220ms step_avg:60.87ms
step:1631/2330 train_time:99280ms step_avg:60.87ms
step:1632/2330 train_time:99343ms step_avg:60.87ms
step:1633/2330 train_time:99403ms step_avg:60.87ms
step:1634/2330 train_time:99467ms step_avg:60.87ms
step:1635/2330 train_time:99527ms step_avg:60.87ms
step:1636/2330 train_time:99590ms step_avg:60.87ms
step:1637/2330 train_time:99649ms step_avg:60.87ms
step:1638/2330 train_time:99712ms step_avg:60.87ms
step:1639/2330 train_time:99772ms step_avg:60.87ms
step:1640/2330 train_time:99836ms step_avg:60.88ms
step:1641/2330 train_time:99896ms step_avg:60.88ms
step:1642/2330 train_time:99959ms step_avg:60.88ms
step:1643/2330 train_time:100019ms step_avg:60.88ms
step:1644/2330 train_time:100083ms step_avg:60.88ms
step:1645/2330 train_time:100142ms step_avg:60.88ms
step:1646/2330 train_time:100206ms step_avg:60.88ms
step:1647/2330 train_time:100265ms step_avg:60.88ms
step:1648/2330 train_time:100328ms step_avg:60.88ms
step:1649/2330 train_time:100388ms step_avg:60.88ms
step:1650/2330 train_time:100452ms step_avg:60.88ms
step:1651/2330 train_time:100511ms step_avg:60.88ms
step:1652/2330 train_time:100575ms step_avg:60.88ms
step:1653/2330 train_time:100636ms step_avg:60.88ms
step:1654/2330 train_time:100700ms step_avg:60.88ms
step:1655/2330 train_time:100759ms step_avg:60.88ms
step:1656/2330 train_time:100823ms step_avg:60.88ms
step:1657/2330 train_time:100882ms step_avg:60.88ms
step:1658/2330 train_time:100945ms step_avg:60.88ms
step:1659/2330 train_time:101004ms step_avg:60.88ms
step:1660/2330 train_time:101067ms step_avg:60.88ms
step:1661/2330 train_time:101128ms step_avg:60.88ms
step:1662/2330 train_time:101191ms step_avg:60.88ms
step:1663/2330 train_time:101250ms step_avg:60.88ms
step:1664/2330 train_time:101313ms step_avg:60.89ms
step:1665/2330 train_time:101373ms step_avg:60.88ms
step:1666/2330 train_time:101437ms step_avg:60.89ms
step:1667/2330 train_time:101497ms step_avg:60.89ms
step:1668/2330 train_time:101561ms step_avg:60.89ms
step:1669/2330 train_time:101621ms step_avg:60.89ms
step:1670/2330 train_time:101685ms step_avg:60.89ms
step:1671/2330 train_time:101744ms step_avg:60.89ms
step:1672/2330 train_time:101808ms step_avg:60.89ms
step:1673/2330 train_time:101867ms step_avg:60.89ms
step:1674/2330 train_time:101931ms step_avg:60.89ms
step:1675/2330 train_time:101990ms step_avg:60.89ms
step:1676/2330 train_time:102053ms step_avg:60.89ms
step:1677/2330 train_time:102112ms step_avg:60.89ms
step:1678/2330 train_time:102175ms step_avg:60.89ms
step:1679/2330 train_time:102236ms step_avg:60.89ms
step:1680/2330 train_time:102300ms step_avg:60.89ms
step:1681/2330 train_time:102361ms step_avg:60.89ms
step:1682/2330 train_time:102424ms step_avg:60.89ms
step:1683/2330 train_time:102483ms step_avg:60.89ms
step:1684/2330 train_time:102547ms step_avg:60.90ms
step:1685/2330 train_time:102608ms step_avg:60.89ms
step:1686/2330 train_time:102671ms step_avg:60.90ms
step:1687/2330 train_time:102731ms step_avg:60.90ms
step:1688/2330 train_time:102795ms step_avg:60.90ms
step:1689/2330 train_time:102855ms step_avg:60.90ms
step:1690/2330 train_time:102920ms step_avg:60.90ms
step:1691/2330 train_time:102981ms step_avg:60.90ms
step:1692/2330 train_time:103044ms step_avg:60.90ms
step:1693/2330 train_time:103103ms step_avg:60.90ms
step:1694/2330 train_time:103166ms step_avg:60.90ms
step:1695/2330 train_time:103225ms step_avg:60.90ms
step:1696/2330 train_time:103289ms step_avg:60.90ms
step:1697/2330 train_time:103349ms step_avg:60.90ms
step:1698/2330 train_time:103412ms step_avg:60.90ms
step:1699/2330 train_time:103472ms step_avg:60.90ms
step:1700/2330 train_time:103535ms step_avg:60.90ms
step:1701/2330 train_time:103595ms step_avg:60.90ms
step:1702/2330 train_time:103658ms step_avg:60.90ms
step:1703/2330 train_time:103718ms step_avg:60.90ms
step:1704/2330 train_time:103780ms step_avg:60.90ms
step:1705/2330 train_time:103840ms step_avg:60.90ms
step:1706/2330 train_time:103904ms step_avg:60.90ms
step:1707/2330 train_time:103964ms step_avg:60.90ms
step:1708/2330 train_time:104028ms step_avg:60.91ms
step:1709/2330 train_time:104087ms step_avg:60.91ms
step:1710/2330 train_time:104150ms step_avg:60.91ms
step:1711/2330 train_time:104210ms step_avg:60.91ms
step:1712/2330 train_time:104273ms step_avg:60.91ms
step:1713/2330 train_time:104332ms step_avg:60.91ms
step:1714/2330 train_time:104396ms step_avg:60.91ms
step:1715/2330 train_time:104456ms step_avg:60.91ms
step:1716/2330 train_time:104519ms step_avg:60.91ms
step:1717/2330 train_time:104579ms step_avg:60.91ms
step:1718/2330 train_time:104642ms step_avg:60.91ms
step:1719/2330 train_time:104702ms step_avg:60.91ms
step:1720/2330 train_time:104765ms step_avg:60.91ms
step:1721/2330 train_time:104824ms step_avg:60.91ms
step:1722/2330 train_time:104887ms step_avg:60.91ms
step:1723/2330 train_time:104947ms step_avg:60.91ms
step:1724/2330 train_time:105011ms step_avg:60.91ms
step:1725/2330 train_time:105071ms step_avg:60.91ms
step:1726/2330 train_time:105135ms step_avg:60.91ms
step:1727/2330 train_time:105194ms step_avg:60.91ms
step:1728/2330 train_time:105258ms step_avg:60.91ms
step:1729/2330 train_time:105318ms step_avg:60.91ms
step:1730/2330 train_time:105382ms step_avg:60.91ms
step:1731/2330 train_time:105442ms step_avg:60.91ms
step:1732/2330 train_time:105505ms step_avg:60.92ms
step:1733/2330 train_time:105564ms step_avg:60.91ms
step:1734/2330 train_time:105628ms step_avg:60.92ms
step:1735/2330 train_time:105687ms step_avg:60.91ms
step:1736/2330 train_time:105751ms step_avg:60.92ms
step:1737/2330 train_time:105811ms step_avg:60.92ms
step:1738/2330 train_time:105874ms step_avg:60.92ms
step:1739/2330 train_time:105934ms step_avg:60.92ms
step:1740/2330 train_time:105997ms step_avg:60.92ms
step:1741/2330 train_time:106059ms step_avg:60.92ms
step:1742/2330 train_time:106123ms step_avg:60.92ms
step:1743/2330 train_time:106183ms step_avg:60.92ms
step:1744/2330 train_time:106246ms step_avg:60.92ms
step:1745/2330 train_time:106305ms step_avg:60.92ms
step:1746/2330 train_time:106369ms step_avg:60.92ms
step:1747/2330 train_time:106429ms step_avg:60.92ms
step:1748/2330 train_time:106492ms step_avg:60.92ms
step:1749/2330 train_time:106551ms step_avg:60.92ms
step:1750/2330 train_time:106615ms step_avg:60.92ms
step:1750/2330 val_loss:3.6533 train_time:106687ms step_avg:60.96ms
step:1751/2330 train_time:106709ms step_avg:60.94ms
step:1752/2330 train_time:106740ms step_avg:60.92ms
step:1753/2330 train_time:106800ms step_avg:60.92ms
step:1754/2330 train_time:106869ms step_avg:60.93ms
step:1755/2330 train_time:106930ms step_avg:60.93ms
step:1756/2330 train_time:106993ms step_avg:60.93ms
step:1757/2330 train_time:107052ms step_avg:60.93ms
step:1758/2330 train_time:107114ms step_avg:60.93ms
step:1759/2330 train_time:107173ms step_avg:60.93ms
step:1760/2330 train_time:107236ms step_avg:60.93ms
step:1761/2330 train_time:107294ms step_avg:60.93ms
step:1762/2330 train_time:107358ms step_avg:60.93ms
step:1763/2330 train_time:107416ms step_avg:60.93ms
step:1764/2330 train_time:107479ms step_avg:60.93ms
step:1765/2330 train_time:107538ms step_avg:60.93ms
step:1766/2330 train_time:107600ms step_avg:60.93ms
step:1767/2330 train_time:107662ms step_avg:60.93ms
step:1768/2330 train_time:107728ms step_avg:60.93ms
step:1769/2330 train_time:107789ms step_avg:60.93ms
step:1770/2330 train_time:107853ms step_avg:60.93ms
step:1771/2330 train_time:107913ms step_avg:60.93ms
step:1772/2330 train_time:107976ms step_avg:60.93ms
step:1773/2330 train_time:108036ms step_avg:60.93ms
step:1774/2330 train_time:108099ms step_avg:60.94ms
step:1775/2330 train_time:108158ms step_avg:60.93ms
step:1776/2330 train_time:108221ms step_avg:60.93ms
step:1777/2330 train_time:108281ms step_avg:60.93ms
step:1778/2330 train_time:108344ms step_avg:60.94ms
step:1779/2330 train_time:108403ms step_avg:60.93ms
step:1780/2330 train_time:108466ms step_avg:60.94ms
step:1781/2330 train_time:108525ms step_avg:60.93ms
step:1782/2330 train_time:108587ms step_avg:60.94ms
step:1783/2330 train_time:108647ms step_avg:60.94ms
step:1784/2330 train_time:108711ms step_avg:60.94ms
step:1785/2330 train_time:108771ms step_avg:60.94ms
step:1786/2330 train_time:108835ms step_avg:60.94ms
step:1787/2330 train_time:108895ms step_avg:60.94ms
step:1788/2330 train_time:108958ms step_avg:60.94ms
step:1789/2330 train_time:109017ms step_avg:60.94ms
step:1790/2330 train_time:109081ms step_avg:60.94ms
step:1791/2330 train_time:109141ms step_avg:60.94ms
step:1792/2330 train_time:109203ms step_avg:60.94ms
step:1793/2330 train_time:109263ms step_avg:60.94ms
step:1794/2330 train_time:109325ms step_avg:60.94ms
step:1795/2330 train_time:109384ms step_avg:60.94ms
step:1796/2330 train_time:109447ms step_avg:60.94ms
step:1797/2330 train_time:109506ms step_avg:60.94ms
step:1798/2330 train_time:109569ms step_avg:60.94ms
step:1799/2330 train_time:109629ms step_avg:60.94ms
step:1800/2330 train_time:109692ms step_avg:60.94ms
step:1801/2330 train_time:109752ms step_avg:60.94ms
step:1802/2330 train_time:109816ms step_avg:60.94ms
step:1803/2330 train_time:109876ms step_avg:60.94ms
step:1804/2330 train_time:109939ms step_avg:60.94ms
step:1805/2330 train_time:109998ms step_avg:60.94ms
step:1806/2330 train_time:110061ms step_avg:60.94ms
step:1807/2330 train_time:110121ms step_avg:60.94ms
step:1808/2330 train_time:110184ms step_avg:60.94ms
step:1809/2330 train_time:110243ms step_avg:60.94ms
step:1810/2330 train_time:110306ms step_avg:60.94ms
step:1811/2330 train_time:110366ms step_avg:60.94ms
step:1812/2330 train_time:110430ms step_avg:60.94ms
step:1813/2330 train_time:110489ms step_avg:60.94ms
step:1814/2330 train_time:110551ms step_avg:60.94ms
step:1815/2330 train_time:110610ms step_avg:60.94ms
step:1816/2330 train_time:110673ms step_avg:60.94ms
step:1817/2330 train_time:110734ms step_avg:60.94ms
step:1818/2330 train_time:110797ms step_avg:60.94ms
step:1819/2330 train_time:110857ms step_avg:60.94ms
step:1820/2330 train_time:110920ms step_avg:60.95ms
step:1821/2330 train_time:110981ms step_avg:60.94ms
step:1822/2330 train_time:111043ms step_avg:60.95ms
step:1823/2330 train_time:111103ms step_avg:60.95ms
step:1824/2330 train_time:111165ms step_avg:60.95ms
step:1825/2330 train_time:111224ms step_avg:60.94ms
step:1826/2330 train_time:111287ms step_avg:60.95ms
step:1827/2330 train_time:111346ms step_avg:60.94ms
step:1828/2330 train_time:111409ms step_avg:60.95ms
step:1829/2330 train_time:111470ms step_avg:60.95ms
step:1830/2330 train_time:111532ms step_avg:60.95ms
step:1831/2330 train_time:111591ms step_avg:60.95ms
step:1832/2330 train_time:111655ms step_avg:60.95ms
step:1833/2330 train_time:111715ms step_avg:60.95ms
step:1834/2330 train_time:111777ms step_avg:60.95ms
step:1835/2330 train_time:111837ms step_avg:60.95ms
step:1836/2330 train_time:111900ms step_avg:60.95ms
step:1837/2330 train_time:111960ms step_avg:60.95ms
step:1838/2330 train_time:112023ms step_avg:60.95ms
step:1839/2330 train_time:112083ms step_avg:60.95ms
step:1840/2330 train_time:112146ms step_avg:60.95ms
step:1841/2330 train_time:112206ms step_avg:60.95ms
step:1842/2330 train_time:112269ms step_avg:60.95ms
step:1843/2330 train_time:112328ms step_avg:60.95ms
step:1844/2330 train_time:112391ms step_avg:60.95ms
step:1845/2330 train_time:112450ms step_avg:60.95ms
step:1846/2330 train_time:112512ms step_avg:60.95ms
step:1847/2330 train_time:112571ms step_avg:60.95ms
step:1848/2330 train_time:112635ms step_avg:60.95ms
step:1849/2330 train_time:112694ms step_avg:60.95ms
step:1850/2330 train_time:112757ms step_avg:60.95ms
step:1851/2330 train_time:112817ms step_avg:60.95ms
step:1852/2330 train_time:112880ms step_avg:60.95ms
step:1853/2330 train_time:112940ms step_avg:60.95ms
step:1854/2330 train_time:113003ms step_avg:60.95ms
step:1855/2330 train_time:113063ms step_avg:60.95ms
step:1856/2330 train_time:113127ms step_avg:60.95ms
step:1857/2330 train_time:113186ms step_avg:60.95ms
step:1858/2330 train_time:113249ms step_avg:60.95ms
step:1859/2330 train_time:113309ms step_avg:60.95ms
step:1860/2330 train_time:113371ms step_avg:60.95ms
step:1861/2330 train_time:113431ms step_avg:60.95ms
step:1862/2330 train_time:113494ms step_avg:60.95ms
step:1863/2330 train_time:113553ms step_avg:60.95ms
step:1864/2330 train_time:113616ms step_avg:60.95ms
step:1865/2330 train_time:113676ms step_avg:60.95ms
step:1866/2330 train_time:113740ms step_avg:60.95ms
step:1867/2330 train_time:113799ms step_avg:60.95ms
step:1868/2330 train_time:113863ms step_avg:60.95ms
step:1869/2330 train_time:113923ms step_avg:60.95ms
step:1870/2330 train_time:113986ms step_avg:60.96ms
step:1871/2330 train_time:114046ms step_avg:60.95ms
step:1872/2330 train_time:114109ms step_avg:60.96ms
step:1873/2330 train_time:114169ms step_avg:60.96ms
step:1874/2330 train_time:114232ms step_avg:60.96ms
step:1875/2330 train_time:114291ms step_avg:60.96ms
step:1876/2330 train_time:114354ms step_avg:60.96ms
step:1877/2330 train_time:114413ms step_avg:60.96ms
step:1878/2330 train_time:114477ms step_avg:60.96ms
step:1879/2330 train_time:114537ms step_avg:60.96ms
step:1880/2330 train_time:114600ms step_avg:60.96ms
step:1881/2330 train_time:114659ms step_avg:60.96ms
step:1882/2330 train_time:114723ms step_avg:60.96ms
step:1883/2330 train_time:114783ms step_avg:60.96ms
step:1884/2330 train_time:114846ms step_avg:60.96ms
step:1885/2330 train_time:114907ms step_avg:60.96ms
step:1886/2330 train_time:114969ms step_avg:60.96ms
step:1887/2330 train_time:115029ms step_avg:60.96ms
step:1888/2330 train_time:115092ms step_avg:60.96ms
step:1889/2330 train_time:115152ms step_avg:60.96ms
step:1890/2330 train_time:115215ms step_avg:60.96ms
step:1891/2330 train_time:115275ms step_avg:60.96ms
step:1892/2330 train_time:115338ms step_avg:60.96ms
step:1893/2330 train_time:115397ms step_avg:60.96ms
step:1894/2330 train_time:115460ms step_avg:60.96ms
step:1895/2330 train_time:115521ms step_avg:60.96ms
step:1896/2330 train_time:115584ms step_avg:60.96ms
step:1897/2330 train_time:115644ms step_avg:60.96ms
step:1898/2330 train_time:115707ms step_avg:60.96ms
step:1899/2330 train_time:115766ms step_avg:60.96ms
step:1900/2330 train_time:115830ms step_avg:60.96ms
step:1901/2330 train_time:115890ms step_avg:60.96ms
step:1902/2330 train_time:115953ms step_avg:60.96ms
step:1903/2330 train_time:116011ms step_avg:60.96ms
step:1904/2330 train_time:116076ms step_avg:60.96ms
step:1905/2330 train_time:116135ms step_avg:60.96ms
step:1906/2330 train_time:116199ms step_avg:60.96ms
step:1907/2330 train_time:116258ms step_avg:60.96ms
step:1908/2330 train_time:116321ms step_avg:60.96ms
step:1909/2330 train_time:116379ms step_avg:60.96ms
step:1910/2330 train_time:116443ms step_avg:60.97ms
step:1911/2330 train_time:116503ms step_avg:60.96ms
step:1912/2330 train_time:116566ms step_avg:60.97ms
step:1913/2330 train_time:116627ms step_avg:60.97ms
step:1914/2330 train_time:116690ms step_avg:60.97ms
step:1915/2330 train_time:116749ms step_avg:60.97ms
step:1916/2330 train_time:116812ms step_avg:60.97ms
step:1917/2330 train_time:116871ms step_avg:60.97ms
step:1918/2330 train_time:116934ms step_avg:60.97ms
step:1919/2330 train_time:116993ms step_avg:60.97ms
step:1920/2330 train_time:117057ms step_avg:60.97ms
step:1921/2330 train_time:117116ms step_avg:60.97ms
step:1922/2330 train_time:117180ms step_avg:60.97ms
step:1923/2330 train_time:117240ms step_avg:60.97ms
step:1924/2330 train_time:117303ms step_avg:60.97ms
step:1925/2330 train_time:117362ms step_avg:60.97ms
step:1926/2330 train_time:117426ms step_avg:60.97ms
step:1927/2330 train_time:117486ms step_avg:60.97ms
step:1928/2330 train_time:117549ms step_avg:60.97ms
step:1929/2330 train_time:117608ms step_avg:60.97ms
step:1930/2330 train_time:117671ms step_avg:60.97ms
step:1931/2330 train_time:117730ms step_avg:60.97ms
step:1932/2330 train_time:117793ms step_avg:60.97ms
step:1933/2330 train_time:117852ms step_avg:60.97ms
step:1934/2330 train_time:117915ms step_avg:60.97ms
step:1935/2330 train_time:117975ms step_avg:60.97ms
step:1936/2330 train_time:118040ms step_avg:60.97ms
step:1937/2330 train_time:118098ms step_avg:60.97ms
step:1938/2330 train_time:118161ms step_avg:60.97ms
step:1939/2330 train_time:118220ms step_avg:60.97ms
step:1940/2330 train_time:118284ms step_avg:60.97ms
step:1941/2330 train_time:118345ms step_avg:60.97ms
step:1942/2330 train_time:118407ms step_avg:60.97ms
step:1943/2330 train_time:118467ms step_avg:60.97ms
step:1944/2330 train_time:118530ms step_avg:60.97ms
step:1945/2330 train_time:118589ms step_avg:60.97ms
step:1946/2330 train_time:118653ms step_avg:60.97ms
step:1947/2330 train_time:118711ms step_avg:60.97ms
step:1948/2330 train_time:118775ms step_avg:60.97ms
step:1949/2330 train_time:118834ms step_avg:60.97ms
step:1950/2330 train_time:118897ms step_avg:60.97ms
step:1951/2330 train_time:118957ms step_avg:60.97ms
step:1952/2330 train_time:119020ms step_avg:60.97ms
step:1953/2330 train_time:119079ms step_avg:60.97ms
step:1954/2330 train_time:119142ms step_avg:60.97ms
step:1955/2330 train_time:119202ms step_avg:60.97ms
step:1956/2330 train_time:119266ms step_avg:60.97ms
step:1957/2330 train_time:119325ms step_avg:60.97ms
step:1958/2330 train_time:119389ms step_avg:60.97ms
step:1959/2330 train_time:119448ms step_avg:60.97ms
step:1960/2330 train_time:119511ms step_avg:60.98ms
step:1961/2330 train_time:119570ms step_avg:60.97ms
step:1962/2330 train_time:119633ms step_avg:60.98ms
step:1963/2330 train_time:119692ms step_avg:60.97ms
step:1964/2330 train_time:119756ms step_avg:60.98ms
step:1965/2330 train_time:119815ms step_avg:60.97ms
step:1966/2330 train_time:119877ms step_avg:60.98ms
step:1967/2330 train_time:119937ms step_avg:60.97ms
step:1968/2330 train_time:120000ms step_avg:60.98ms
step:1969/2330 train_time:120059ms step_avg:60.97ms
step:1970/2330 train_time:120122ms step_avg:60.98ms
step:1971/2330 train_time:120182ms step_avg:60.98ms
step:1972/2330 train_time:120245ms step_avg:60.98ms
step:1973/2330 train_time:120306ms step_avg:60.98ms
step:1974/2330 train_time:120369ms step_avg:60.98ms
step:1975/2330 train_time:120429ms step_avg:60.98ms
step:1976/2330 train_time:120491ms step_avg:60.98ms
step:1977/2330 train_time:120550ms step_avg:60.98ms
step:1978/2330 train_time:120612ms step_avg:60.98ms
step:1979/2330 train_time:120672ms step_avg:60.98ms
step:1980/2330 train_time:120735ms step_avg:60.98ms
step:1981/2330 train_time:120794ms step_avg:60.98ms
step:1982/2330 train_time:120858ms step_avg:60.98ms
step:1983/2330 train_time:120917ms step_avg:60.98ms
step:1984/2330 train_time:120980ms step_avg:60.98ms
step:1985/2330 train_time:121039ms step_avg:60.98ms
step:1986/2330 train_time:121102ms step_avg:60.98ms
step:1987/2330 train_time:121162ms step_avg:60.98ms
step:1988/2330 train_time:121226ms step_avg:60.98ms
step:1989/2330 train_time:121286ms step_avg:60.98ms
step:1990/2330 train_time:121349ms step_avg:60.98ms
step:1991/2330 train_time:121408ms step_avg:60.98ms
step:1992/2330 train_time:121472ms step_avg:60.98ms
step:1993/2330 train_time:121532ms step_avg:60.98ms
step:1994/2330 train_time:121594ms step_avg:60.98ms
step:1995/2330 train_time:121653ms step_avg:60.98ms
step:1996/2330 train_time:121716ms step_avg:60.98ms
step:1997/2330 train_time:121776ms step_avg:60.98ms
step:1998/2330 train_time:121839ms step_avg:60.98ms
step:1999/2330 train_time:121898ms step_avg:60.98ms
step:2000/2330 train_time:121961ms step_avg:60.98ms
step:2000/2330 val_loss:3.6073 train_time:122033ms step_avg:61.02ms
step:2001/2330 train_time:122056ms step_avg:61.00ms
step:2002/2330 train_time:122088ms step_avg:60.98ms
step:2003/2330 train_time:122152ms step_avg:60.98ms
step:2004/2330 train_time:122217ms step_avg:60.99ms
step:2005/2330 train_time:122277ms step_avg:60.99ms
step:2006/2330 train_time:122340ms step_avg:60.99ms
step:2007/2330 train_time:122399ms step_avg:60.99ms
step:2008/2330 train_time:122462ms step_avg:60.99ms
step:2009/2330 train_time:122521ms step_avg:60.99ms
step:2010/2330 train_time:122584ms step_avg:60.99ms
step:2011/2330 train_time:122644ms step_avg:60.99ms
step:2012/2330 train_time:122707ms step_avg:60.99ms
step:2013/2330 train_time:122766ms step_avg:60.99ms
step:2014/2330 train_time:122830ms step_avg:60.99ms
step:2015/2330 train_time:122888ms step_avg:60.99ms
step:2016/2330 train_time:122951ms step_avg:60.99ms
step:2017/2330 train_time:123011ms step_avg:60.99ms
step:2018/2330 train_time:123075ms step_avg:60.99ms
step:2019/2330 train_time:123135ms step_avg:60.99ms
step:2020/2330 train_time:123198ms step_avg:60.99ms
step:2021/2330 train_time:123259ms step_avg:60.99ms
step:2022/2330 train_time:123323ms step_avg:60.99ms
step:2023/2330 train_time:123383ms step_avg:60.99ms
step:2024/2330 train_time:123446ms step_avg:60.99ms
step:2025/2330 train_time:123505ms step_avg:60.99ms
step:2026/2330 train_time:123568ms step_avg:60.99ms
step:2027/2330 train_time:123627ms step_avg:60.99ms
step:2028/2330 train_time:123690ms step_avg:60.99ms
step:2029/2330 train_time:123748ms step_avg:60.99ms
step:2030/2330 train_time:123811ms step_avg:60.99ms
step:2031/2330 train_time:123870ms step_avg:60.99ms
step:2032/2330 train_time:123933ms step_avg:60.99ms
step:2033/2330 train_time:123993ms step_avg:60.99ms
step:2034/2330 train_time:124057ms step_avg:60.99ms
step:2035/2330 train_time:124116ms step_avg:60.99ms
step:2036/2330 train_time:124180ms step_avg:60.99ms
step:2037/2330 train_time:124239ms step_avg:60.99ms
step:2038/2330 train_time:124303ms step_avg:60.99ms
step:2039/2330 train_time:124363ms step_avg:60.99ms
step:2040/2330 train_time:124427ms step_avg:60.99ms
step:2041/2330 train_time:124486ms step_avg:60.99ms
step:2042/2330 train_time:124550ms step_avg:60.99ms
step:2043/2330 train_time:124609ms step_avg:60.99ms
step:2044/2330 train_time:124672ms step_avg:60.99ms
step:2045/2330 train_time:124731ms step_avg:60.99ms
step:2046/2330 train_time:124794ms step_avg:60.99ms
step:2047/2330 train_time:124853ms step_avg:60.99ms
step:2048/2330 train_time:124916ms step_avg:60.99ms
step:2049/2330 train_time:124975ms step_avg:60.99ms
step:2050/2330 train_time:125038ms step_avg:60.99ms
step:2051/2330 train_time:125098ms step_avg:60.99ms
step:2052/2330 train_time:125162ms step_avg:61.00ms
step:2053/2330 train_time:125221ms step_avg:60.99ms
step:2054/2330 train_time:125285ms step_avg:61.00ms
step:2055/2330 train_time:125344ms step_avg:60.99ms
step:2056/2330 train_time:125408ms step_avg:61.00ms
step:2057/2330 train_time:125468ms step_avg:61.00ms
step:2058/2330 train_time:125531ms step_avg:61.00ms
step:2059/2330 train_time:125591ms step_avg:61.00ms
step:2060/2330 train_time:125654ms step_avg:61.00ms
step:2061/2330 train_time:125713ms step_avg:61.00ms
step:2062/2330 train_time:125777ms step_avg:61.00ms
step:2063/2330 train_time:125836ms step_avg:61.00ms
step:2064/2330 train_time:125899ms step_avg:61.00ms
step:2065/2330 train_time:125958ms step_avg:61.00ms
step:2066/2330 train_time:126021ms step_avg:61.00ms
step:2067/2330 train_time:126081ms step_avg:61.00ms
step:2068/2330 train_time:126144ms step_avg:61.00ms
step:2069/2330 train_time:126205ms step_avg:61.00ms
step:2070/2330 train_time:126268ms step_avg:61.00ms
step:2071/2330 train_time:126328ms step_avg:61.00ms
step:2072/2330 train_time:126391ms step_avg:61.00ms
step:2073/2330 train_time:126451ms step_avg:61.00ms
step:2074/2330 train_time:126513ms step_avg:61.00ms
step:2075/2330 train_time:126574ms step_avg:61.00ms
step:2076/2330 train_time:126636ms step_avg:61.00ms
step:2077/2330 train_time:126695ms step_avg:61.00ms
step:2078/2330 train_time:126759ms step_avg:61.00ms
step:2079/2330 train_time:126818ms step_avg:61.00ms
step:2080/2330 train_time:126880ms step_avg:61.00ms
step:2081/2330 train_time:126940ms step_avg:61.00ms
step:2082/2330 train_time:127003ms step_avg:61.00ms
step:2083/2330 train_time:127063ms step_avg:61.00ms
step:2084/2330 train_time:127126ms step_avg:61.00ms
step:2085/2330 train_time:127186ms step_avg:61.00ms
step:2086/2330 train_time:127250ms step_avg:61.00ms
step:2087/2330 train_time:127309ms step_avg:61.00ms
step:2088/2330 train_time:127372ms step_avg:61.00ms
step:2089/2330 train_time:127432ms step_avg:61.00ms
step:2090/2330 train_time:127495ms step_avg:61.00ms
step:2091/2330 train_time:127555ms step_avg:61.00ms
step:2092/2330 train_time:127618ms step_avg:61.00ms
step:2093/2330 train_time:127678ms step_avg:61.00ms
step:2094/2330 train_time:127741ms step_avg:61.00ms
step:2095/2330 train_time:127799ms step_avg:61.00ms
step:2096/2330 train_time:127863ms step_avg:61.00ms
step:2097/2330 train_time:127922ms step_avg:61.00ms
step:2098/2330 train_time:127985ms step_avg:61.00ms
step:2099/2330 train_time:128045ms step_avg:61.00ms
step:2100/2330 train_time:128109ms step_avg:61.00ms
step:2101/2330 train_time:128168ms step_avg:61.00ms
step:2102/2330 train_time:128232ms step_avg:61.00ms
step:2103/2330 train_time:128291ms step_avg:61.00ms
step:2104/2330 train_time:128354ms step_avg:61.00ms
step:2105/2330 train_time:128414ms step_avg:61.00ms
step:2106/2330 train_time:128477ms step_avg:61.01ms
step:2107/2330 train_time:128537ms step_avg:61.00ms
step:2108/2330 train_time:128601ms step_avg:61.01ms
step:2109/2330 train_time:128661ms step_avg:61.01ms
step:2110/2330 train_time:128725ms step_avg:61.01ms
step:2111/2330 train_time:128784ms step_avg:61.01ms
step:2112/2330 train_time:128848ms step_avg:61.01ms
step:2113/2330 train_time:128907ms step_avg:61.01ms
step:2114/2330 train_time:128970ms step_avg:61.01ms
step:2115/2330 train_time:129030ms step_avg:61.01ms
step:2116/2330 train_time:129092ms step_avg:61.01ms
step:2117/2330 train_time:129152ms step_avg:61.01ms
step:2118/2330 train_time:129216ms step_avg:61.01ms
step:2119/2330 train_time:129275ms step_avg:61.01ms
step:2120/2330 train_time:129338ms step_avg:61.01ms
step:2121/2330 train_time:129398ms step_avg:61.01ms
step:2122/2330 train_time:129463ms step_avg:61.01ms
step:2123/2330 train_time:129522ms step_avg:61.01ms
step:2124/2330 train_time:129586ms step_avg:61.01ms
step:2125/2330 train_time:129645ms step_avg:61.01ms
step:2126/2330 train_time:129707ms step_avg:61.01ms
step:2127/2330 train_time:129767ms step_avg:61.01ms
step:2128/2330 train_time:129830ms step_avg:61.01ms
step:2129/2330 train_time:129889ms step_avg:61.01ms
step:2130/2330 train_time:129953ms step_avg:61.01ms
step:2131/2330 train_time:130013ms step_avg:61.01ms
step:2132/2330 train_time:130076ms step_avg:61.01ms
step:2133/2330 train_time:130135ms step_avg:61.01ms
step:2134/2330 train_time:130199ms step_avg:61.01ms
step:2135/2330 train_time:130259ms step_avg:61.01ms
step:2136/2330 train_time:130322ms step_avg:61.01ms
step:2137/2330 train_time:130382ms step_avg:61.01ms
step:2138/2330 train_time:130446ms step_avg:61.01ms
step:2139/2330 train_time:130506ms step_avg:61.01ms
step:2140/2330 train_time:130568ms step_avg:61.01ms
step:2141/2330 train_time:130627ms step_avg:61.01ms
step:2142/2330 train_time:130690ms step_avg:61.01ms
step:2143/2330 train_time:130750ms step_avg:61.01ms
step:2144/2330 train_time:130813ms step_avg:61.01ms
step:2145/2330 train_time:130871ms step_avg:61.01ms
step:2146/2330 train_time:130934ms step_avg:61.01ms
step:2147/2330 train_time:130994ms step_avg:61.01ms
step:2148/2330 train_time:131058ms step_avg:61.01ms
step:2149/2330 train_time:131116ms step_avg:61.01ms
step:2150/2330 train_time:131180ms step_avg:61.01ms
step:2151/2330 train_time:131239ms step_avg:61.01ms
step:2152/2330 train_time:131302ms step_avg:61.01ms
step:2153/2330 train_time:131362ms step_avg:61.01ms
step:2154/2330 train_time:131425ms step_avg:61.01ms
step:2155/2330 train_time:131485ms step_avg:61.01ms
step:2156/2330 train_time:131550ms step_avg:61.02ms
step:2157/2330 train_time:131609ms step_avg:61.01ms
step:2158/2330 train_time:131672ms step_avg:61.02ms
step:2159/2330 train_time:131731ms step_avg:61.02ms
step:2160/2330 train_time:131795ms step_avg:61.02ms
step:2161/2330 train_time:131855ms step_avg:61.02ms
step:2162/2330 train_time:131918ms step_avg:61.02ms
step:2163/2330 train_time:131977ms step_avg:61.02ms
step:2164/2330 train_time:132041ms step_avg:61.02ms
step:2165/2330 train_time:132101ms step_avg:61.02ms
step:2166/2330 train_time:132165ms step_avg:61.02ms
step:2167/2330 train_time:132224ms step_avg:61.02ms
step:2168/2330 train_time:132286ms step_avg:61.02ms
step:2169/2330 train_time:132346ms step_avg:61.02ms
step:2170/2330 train_time:132410ms step_avg:61.02ms
step:2171/2330 train_time:132470ms step_avg:61.02ms
step:2172/2330 train_time:132533ms step_avg:61.02ms
step:2173/2330 train_time:132592ms step_avg:61.02ms
step:2174/2330 train_time:132656ms step_avg:61.02ms
step:2175/2330 train_time:132716ms step_avg:61.02ms
step:2176/2330 train_time:132779ms step_avg:61.02ms
step:2177/2330 train_time:132838ms step_avg:61.02ms
step:2178/2330 train_time:132901ms step_avg:61.02ms
step:2179/2330 train_time:132961ms step_avg:61.02ms
step:2180/2330 train_time:133025ms step_avg:61.02ms
step:2181/2330 train_time:133085ms step_avg:61.02ms
step:2182/2330 train_time:133149ms step_avg:61.02ms
step:2183/2330 train_time:133209ms step_avg:61.02ms
step:2184/2330 train_time:133272ms step_avg:61.02ms
step:2185/2330 train_time:133331ms step_avg:61.02ms
step:2186/2330 train_time:133395ms step_avg:61.02ms
step:2187/2330 train_time:133456ms step_avg:61.02ms
step:2188/2330 train_time:133518ms step_avg:61.02ms
step:2189/2330 train_time:133577ms step_avg:61.02ms
step:2190/2330 train_time:133640ms step_avg:61.02ms
step:2191/2330 train_time:133700ms step_avg:61.02ms
step:2192/2330 train_time:133763ms step_avg:61.02ms
step:2193/2330 train_time:133823ms step_avg:61.02ms
step:2194/2330 train_time:133886ms step_avg:61.02ms
step:2195/2330 train_time:133946ms step_avg:61.02ms
step:2196/2330 train_time:134009ms step_avg:61.02ms
step:2197/2330 train_time:134069ms step_avg:61.02ms
step:2198/2330 train_time:134132ms step_avg:61.02ms
step:2199/2330 train_time:134192ms step_avg:61.02ms
step:2200/2330 train_time:134255ms step_avg:61.03ms
step:2201/2330 train_time:134315ms step_avg:61.02ms
step:2202/2330 train_time:134379ms step_avg:61.03ms
step:2203/2330 train_time:134439ms step_avg:61.03ms
step:2204/2330 train_time:134503ms step_avg:61.03ms
step:2205/2330 train_time:134563ms step_avg:61.03ms
step:2206/2330 train_time:134626ms step_avg:61.03ms
step:2207/2330 train_time:134685ms step_avg:61.03ms
step:2208/2330 train_time:134749ms step_avg:61.03ms
step:2209/2330 train_time:134808ms step_avg:61.03ms
step:2210/2330 train_time:134871ms step_avg:61.03ms
step:2211/2330 train_time:134930ms step_avg:61.03ms
step:2212/2330 train_time:134994ms step_avg:61.03ms
step:2213/2330 train_time:135054ms step_avg:61.03ms
step:2214/2330 train_time:135117ms step_avg:61.03ms
step:2215/2330 train_time:135176ms step_avg:61.03ms
step:2216/2330 train_time:135239ms step_avg:61.03ms
step:2217/2330 train_time:135299ms step_avg:61.03ms
step:2218/2330 train_time:135363ms step_avg:61.03ms
step:2219/2330 train_time:135423ms step_avg:61.03ms
step:2220/2330 train_time:135486ms step_avg:61.03ms
step:2221/2330 train_time:135546ms step_avg:61.03ms
step:2222/2330 train_time:135609ms step_avg:61.03ms
step:2223/2330 train_time:135668ms step_avg:61.03ms
step:2224/2330 train_time:135732ms step_avg:61.03ms
step:2225/2330 train_time:135790ms step_avg:61.03ms
step:2226/2330 train_time:135854ms step_avg:61.03ms
step:2227/2330 train_time:135914ms step_avg:61.03ms
step:2228/2330 train_time:135978ms step_avg:61.03ms
step:2229/2330 train_time:136037ms step_avg:61.03ms
step:2230/2330 train_time:136100ms step_avg:61.03ms
step:2231/2330 train_time:136160ms step_avg:61.03ms
step:2232/2330 train_time:136223ms step_avg:61.03ms
step:2233/2330 train_time:136283ms step_avg:61.03ms
step:2234/2330 train_time:136346ms step_avg:61.03ms
step:2235/2330 train_time:136405ms step_avg:61.03ms
step:2236/2330 train_time:136469ms step_avg:61.03ms
step:2237/2330 train_time:136529ms step_avg:61.03ms
step:2238/2330 train_time:136592ms step_avg:61.03ms
step:2239/2330 train_time:136651ms step_avg:61.03ms
step:2240/2330 train_time:136714ms step_avg:61.03ms
step:2241/2330 train_time:136774ms step_avg:61.03ms
step:2242/2330 train_time:136837ms step_avg:61.03ms
step:2243/2330 train_time:136896ms step_avg:61.03ms
step:2244/2330 train_time:136960ms step_avg:61.03ms
step:2245/2330 train_time:137019ms step_avg:61.03ms
step:2246/2330 train_time:137082ms step_avg:61.03ms
step:2247/2330 train_time:137143ms step_avg:61.03ms
step:2248/2330 train_time:137206ms step_avg:61.03ms
step:2249/2330 train_time:137266ms step_avg:61.03ms
step:2250/2330 train_time:137330ms step_avg:61.04ms
step:2250/2330 val_loss:3.6033 train_time:137402ms step_avg:61.07ms
step:2251/2330 train_time:137424ms step_avg:61.05ms
step:2252/2330 train_time:137456ms step_avg:61.04ms
step:2253/2330 train_time:137518ms step_avg:61.04ms
step:2254/2330 train_time:137585ms step_avg:61.04ms
step:2255/2330 train_time:137646ms step_avg:61.04ms
step:2256/2330 train_time:137709ms step_avg:61.04ms
step:2257/2330 train_time:137768ms step_avg:61.04ms
step:2258/2330 train_time:137831ms step_avg:61.04ms
step:2259/2330 train_time:137889ms step_avg:61.04ms
step:2260/2330 train_time:137952ms step_avg:61.04ms
step:2261/2330 train_time:138011ms step_avg:61.04ms
step:2262/2330 train_time:138074ms step_avg:61.04ms
step:2263/2330 train_time:138133ms step_avg:61.04ms
step:2264/2330 train_time:138195ms step_avg:61.04ms
step:2265/2330 train_time:138254ms step_avg:61.04ms
step:2266/2330 train_time:138316ms step_avg:61.04ms
step:2267/2330 train_time:138376ms step_avg:61.04ms
step:2268/2330 train_time:138440ms step_avg:61.04ms
step:2269/2330 train_time:138502ms step_avg:61.04ms
step:2270/2330 train_time:138566ms step_avg:61.04ms
step:2271/2330 train_time:138627ms step_avg:61.04ms
step:2272/2330 train_time:138690ms step_avg:61.04ms
step:2273/2330 train_time:138749ms step_avg:61.04ms
step:2274/2330 train_time:138813ms step_avg:61.04ms
step:2275/2330 train_time:138872ms step_avg:61.04ms
step:2276/2330 train_time:138935ms step_avg:61.04ms
step:2277/2330 train_time:138993ms step_avg:61.04ms
step:2278/2330 train_time:139056ms step_avg:61.04ms
step:2279/2330 train_time:139115ms step_avg:61.04ms
step:2280/2330 train_time:139178ms step_avg:61.04ms
step:2281/2330 train_time:139237ms step_avg:61.04ms
step:2282/2330 train_time:139300ms step_avg:61.04ms
step:2283/2330 train_time:139359ms step_avg:61.04ms
step:2284/2330 train_time:139423ms step_avg:61.04ms
step:2285/2330 train_time:139483ms step_avg:61.04ms
step:2286/2330 train_time:139547ms step_avg:61.04ms
step:2287/2330 train_time:139606ms step_avg:61.04ms
step:2288/2330 train_time:139670ms step_avg:61.04ms
step:2289/2330 train_time:139730ms step_avg:61.04ms
step:2290/2330 train_time:139793ms step_avg:61.04ms
step:2291/2330 train_time:139852ms step_avg:61.04ms
step:2292/2330 train_time:139915ms step_avg:61.04ms
step:2293/2330 train_time:139974ms step_avg:61.04ms
step:2294/2330 train_time:140037ms step_avg:61.04ms
step:2295/2330 train_time:140096ms step_avg:61.04ms
step:2296/2330 train_time:140159ms step_avg:61.04ms
step:2297/2330 train_time:140219ms step_avg:61.04ms
step:2298/2330 train_time:140282ms step_avg:61.05ms
step:2299/2330 train_time:140342ms step_avg:61.04ms
step:2300/2330 train_time:140405ms step_avg:61.05ms
step:2301/2330 train_time:140465ms step_avg:61.05ms
step:2302/2330 train_time:140528ms step_avg:61.05ms
step:2303/2330 train_time:140588ms step_avg:61.05ms
step:2304/2330 train_time:140651ms step_avg:61.05ms
step:2305/2330 train_time:140711ms step_avg:61.05ms
step:2306/2330 train_time:140774ms step_avg:61.05ms
step:2307/2330 train_time:140834ms step_avg:61.05ms
step:2308/2330 train_time:140897ms step_avg:61.05ms
step:2309/2330 train_time:140956ms step_avg:61.05ms
step:2310/2330 train_time:141018ms step_avg:61.05ms
step:2311/2330 train_time:141078ms step_avg:61.05ms
step:2312/2330 train_time:141140ms step_avg:61.05ms
step:2313/2330 train_time:141199ms step_avg:61.05ms
step:2314/2330 train_time:141263ms step_avg:61.05ms
step:2315/2330 train_time:141322ms step_avg:61.05ms
step:2316/2330 train_time:141386ms step_avg:61.05ms
step:2317/2330 train_time:141446ms step_avg:61.05ms
step:2318/2330 train_time:141509ms step_avg:61.05ms
step:2319/2330 train_time:141568ms step_avg:61.05ms
step:2320/2330 train_time:141633ms step_avg:61.05ms
step:2321/2330 train_time:141693ms step_avg:61.05ms
step:2322/2330 train_time:141757ms step_avg:61.05ms
step:2323/2330 train_time:141817ms step_avg:61.05ms
step:2324/2330 train_time:141880ms step_avg:61.05ms
step:2325/2330 train_time:141939ms step_avg:61.05ms
step:2326/2330 train_time:142002ms step_avg:61.05ms
step:2327/2330 train_time:142061ms step_avg:61.05ms
step:2328/2330 train_time:142124ms step_avg:61.05ms
step:2329/2330 train_time:142183ms step_avg:61.05ms
step:2330/2330 train_time:142246ms step_avg:61.05ms
step:2330/2330 val_loss:3.5561 train_time:142318ms step_avg:61.08ms
peak memory allocated: 29998 MiB reserved: 44076 MiB
