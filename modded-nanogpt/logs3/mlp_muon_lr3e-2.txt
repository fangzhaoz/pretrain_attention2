import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                # v_chunk = divide_by_norm(batched)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                # v_chunk = divide_by_norm(batched_update_grads)
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        # self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "mlp_muon_lr3e-2"
    os.makedirs("logs3", exist_ok=True)
    logfile = f"logs3/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
# embed_params = [p for n, p in model.named_parameters() if "embed" in n]
embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=3e-2, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    return 1.0

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum_min + 0.5 * (momentum_max - momentum_min)

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs3/{run_id}", exist_ok=True)
            torch.save(log, f"logs3/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 08:06:22 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   34C    P0             112W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   32C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   32C    P0             111W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             116W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:70ms step_avg:70.17ms
step:2/2330 train_time:165ms step_avg:82.74ms
step:3/2330 train_time:179ms step_avg:59.63ms
step:4/2330 train_time:192ms step_avg:48.03ms
step:5/2330 train_time:203ms step_avg:40.69ms
step:6/2330 train_time:274ms step_avg:45.67ms
step:7/2330 train_time:307ms step_avg:43.91ms
step:8/2330 train_time:351ms step_avg:43.82ms
step:9/2330 train_time:385ms step_avg:42.76ms
step:10/2330 train_time:429ms step_avg:42.87ms
step:11/2330 train_time:464ms step_avg:42.16ms
step:12/2330 train_time:508ms step_avg:42.36ms
step:13/2330 train_time:542ms step_avg:41.72ms
step:14/2330 train_time:587ms step_avg:41.90ms
step:15/2330 train_time:621ms step_avg:41.41ms
step:16/2330 train_time:665ms step_avg:41.57ms
step:17/2330 train_time:699ms step_avg:41.14ms
step:18/2330 train_time:743ms step_avg:41.30ms
step:19/2330 train_time:778ms step_avg:40.96ms
step:20/2330 train_time:822ms step_avg:41.12ms
step:21/2330 train_time:857ms step_avg:40.82ms
step:22/2330 train_time:901ms step_avg:40.97ms
step:23/2330 train_time:936ms step_avg:40.71ms
step:24/2330 train_time:980ms step_avg:40.85ms
step:25/2330 train_time:1015ms step_avg:40.60ms
step:26/2330 train_time:1060ms step_avg:40.75ms
step:27/2330 train_time:1096ms step_avg:40.59ms
step:28/2330 train_time:1146ms step_avg:40.92ms
step:29/2330 train_time:1184ms step_avg:40.83ms
step:30/2330 train_time:1231ms step_avg:41.04ms
step:31/2330 train_time:1267ms step_avg:40.87ms
step:32/2330 train_time:1312ms step_avg:41.00ms
step:33/2330 train_time:1347ms step_avg:40.83ms
step:34/2330 train_time:1392ms step_avg:40.94ms
step:35/2330 train_time:1427ms step_avg:40.78ms
step:36/2330 train_time:1472ms step_avg:40.89ms
step:37/2330 train_time:1507ms step_avg:40.73ms
step:38/2330 train_time:1551ms step_avg:40.83ms
step:39/2330 train_time:1587ms step_avg:40.69ms
step:40/2330 train_time:1632ms step_avg:40.80ms
step:41/2330 train_time:1668ms step_avg:40.67ms
step:42/2330 train_time:1712ms step_avg:40.76ms
step:43/2330 train_time:1747ms step_avg:40.62ms
step:44/2330 train_time:1791ms step_avg:40.71ms
step:45/2330 train_time:1826ms step_avg:40.58ms
step:46/2330 train_time:1870ms step_avg:40.65ms
step:47/2330 train_time:1905ms step_avg:40.53ms
step:48/2330 train_time:1949ms step_avg:40.61ms
step:49/2330 train_time:1984ms step_avg:40.49ms
step:50/2330 train_time:2029ms step_avg:40.57ms
step:51/2330 train_time:2065ms step_avg:40.50ms
step:52/2330 train_time:2111ms step_avg:40.60ms
step:53/2330 train_time:2149ms step_avg:40.55ms
step:54/2330 train_time:2195ms step_avg:40.65ms
step:55/2330 train_time:2232ms step_avg:40.57ms
step:56/2330 train_time:2277ms step_avg:40.67ms
step:57/2330 train_time:2314ms step_avg:40.59ms
step:58/2330 train_time:2358ms step_avg:40.66ms
step:59/2330 train_time:2394ms step_avg:40.58ms
step:60/2330 train_time:2440ms step_avg:40.67ms
step:61/2330 train_time:2476ms step_avg:40.59ms
step:62/2330 train_time:2520ms step_avg:40.65ms
step:63/2330 train_time:2555ms step_avg:40.56ms
step:64/2330 train_time:2600ms step_avg:40.62ms
step:65/2330 train_time:2636ms step_avg:40.55ms
step:66/2330 train_time:2681ms step_avg:40.63ms
step:67/2330 train_time:2716ms step_avg:40.54ms
step:68/2330 train_time:2760ms step_avg:40.59ms
step:69/2330 train_time:2796ms step_avg:40.52ms
step:70/2330 train_time:2841ms step_avg:40.58ms
step:71/2330 train_time:2875ms step_avg:40.49ms
step:72/2330 train_time:2919ms step_avg:40.54ms
step:73/2330 train_time:2954ms step_avg:40.46ms
step:74/2330 train_time:3000ms step_avg:40.53ms
step:75/2330 train_time:3035ms step_avg:40.47ms
step:76/2330 train_time:3081ms step_avg:40.54ms
step:77/2330 train_time:3116ms step_avg:40.47ms
step:78/2330 train_time:3161ms step_avg:40.53ms
step:79/2330 train_time:3196ms step_avg:40.46ms
step:80/2330 train_time:3242ms step_avg:40.52ms
step:81/2330 train_time:3278ms step_avg:40.46ms
step:82/2330 train_time:3322ms step_avg:40.51ms
step:83/2330 train_time:3358ms step_avg:40.46ms
step:84/2330 train_time:3403ms step_avg:40.52ms
step:85/2330 train_time:3438ms step_avg:40.45ms
step:86/2330 train_time:3483ms step_avg:40.50ms
step:87/2330 train_time:3519ms step_avg:40.44ms
step:88/2330 train_time:3563ms step_avg:40.49ms
step:89/2330 train_time:3599ms step_avg:40.43ms
step:90/2330 train_time:3644ms step_avg:40.48ms
step:91/2330 train_time:3678ms step_avg:40.42ms
step:92/2330 train_time:3724ms step_avg:40.48ms
step:93/2330 train_time:3759ms step_avg:40.42ms
step:94/2330 train_time:3804ms step_avg:40.47ms
step:95/2330 train_time:3838ms step_avg:40.41ms
step:96/2330 train_time:3884ms step_avg:40.45ms
step:97/2330 train_time:3919ms step_avg:40.40ms
step:98/2330 train_time:3964ms step_avg:40.45ms
step:99/2330 train_time:3999ms step_avg:40.39ms
step:100/2330 train_time:4044ms step_avg:40.44ms
step:101/2330 train_time:4079ms step_avg:40.39ms
step:102/2330 train_time:4124ms step_avg:40.43ms
step:103/2330 train_time:4159ms step_avg:40.38ms
step:104/2330 train_time:4205ms step_avg:40.43ms
step:105/2330 train_time:4240ms step_avg:40.38ms
step:106/2330 train_time:4285ms step_avg:40.42ms
step:107/2330 train_time:4320ms step_avg:40.38ms
step:108/2330 train_time:4366ms step_avg:40.42ms
step:109/2330 train_time:4401ms step_avg:40.37ms
step:110/2330 train_time:4446ms step_avg:40.41ms
step:111/2330 train_time:4481ms step_avg:40.37ms
step:112/2330 train_time:4526ms step_avg:40.41ms
step:113/2330 train_time:4561ms step_avg:40.36ms
step:114/2330 train_time:4605ms step_avg:40.40ms
step:115/2330 train_time:4641ms step_avg:40.35ms
step:116/2330 train_time:4686ms step_avg:40.39ms
step:117/2330 train_time:4720ms step_avg:40.34ms
step:118/2330 train_time:4764ms step_avg:40.38ms
step:119/2330 train_time:4799ms step_avg:40.33ms
step:120/2330 train_time:4844ms step_avg:40.36ms
step:121/2330 train_time:4879ms step_avg:40.32ms
step:122/2330 train_time:4924ms step_avg:40.36ms
step:123/2330 train_time:4959ms step_avg:40.32ms
step:124/2330 train_time:5004ms step_avg:40.36ms
step:125/2330 train_time:5039ms step_avg:40.31ms
step:126/2330 train_time:5085ms step_avg:40.35ms
step:127/2330 train_time:5119ms step_avg:40.31ms
step:128/2330 train_time:5164ms step_avg:40.35ms
step:129/2330 train_time:5200ms step_avg:40.31ms
step:130/2330 train_time:5244ms step_avg:40.34ms
step:131/2330 train_time:5280ms step_avg:40.31ms
step:132/2330 train_time:5325ms step_avg:40.34ms
step:133/2330 train_time:5361ms step_avg:40.31ms
step:134/2330 train_time:5406ms step_avg:40.34ms
step:135/2330 train_time:5442ms step_avg:40.31ms
step:136/2330 train_time:5487ms step_avg:40.35ms
step:137/2330 train_time:5523ms step_avg:40.31ms
step:138/2330 train_time:5568ms step_avg:40.35ms
step:139/2330 train_time:5603ms step_avg:40.31ms
step:140/2330 train_time:5647ms step_avg:40.34ms
step:141/2330 train_time:5682ms step_avg:40.30ms
step:142/2330 train_time:5727ms step_avg:40.33ms
step:143/2330 train_time:5762ms step_avg:40.30ms
step:144/2330 train_time:5806ms step_avg:40.32ms
step:145/2330 train_time:5843ms step_avg:40.29ms
step:146/2330 train_time:5886ms step_avg:40.32ms
step:147/2330 train_time:5922ms step_avg:40.28ms
step:148/2330 train_time:5966ms step_avg:40.31ms
step:149/2330 train_time:6001ms step_avg:40.27ms
step:150/2330 train_time:6045ms step_avg:40.30ms
step:151/2330 train_time:6080ms step_avg:40.27ms
step:152/2330 train_time:6125ms step_avg:40.30ms
step:153/2330 train_time:6160ms step_avg:40.26ms
step:154/2330 train_time:6205ms step_avg:40.29ms
step:155/2330 train_time:6241ms step_avg:40.27ms
step:156/2330 train_time:6286ms step_avg:40.30ms
step:157/2330 train_time:6321ms step_avg:40.26ms
step:158/2330 train_time:6365ms step_avg:40.29ms
step:159/2330 train_time:6401ms step_avg:40.26ms
step:160/2330 train_time:6447ms step_avg:40.29ms
step:161/2330 train_time:6481ms step_avg:40.26ms
step:162/2330 train_time:6526ms step_avg:40.28ms
step:163/2330 train_time:6561ms step_avg:40.25ms
step:164/2330 train_time:6606ms step_avg:40.28ms
step:165/2330 train_time:6642ms step_avg:40.25ms
step:166/2330 train_time:6686ms step_avg:40.28ms
step:167/2330 train_time:6722ms step_avg:40.25ms
step:168/2330 train_time:6766ms step_avg:40.27ms
step:169/2330 train_time:6801ms step_avg:40.24ms
step:170/2330 train_time:6846ms step_avg:40.27ms
step:171/2330 train_time:6881ms step_avg:40.24ms
step:172/2330 train_time:6925ms step_avg:40.26ms
step:173/2330 train_time:6960ms step_avg:40.23ms
step:174/2330 train_time:7004ms step_avg:40.26ms
step:175/2330 train_time:7040ms step_avg:40.23ms
step:176/2330 train_time:7085ms step_avg:40.26ms
step:177/2330 train_time:7120ms step_avg:40.23ms
step:178/2330 train_time:7164ms step_avg:40.25ms
step:179/2330 train_time:7199ms step_avg:40.22ms
step:180/2330 train_time:7244ms step_avg:40.24ms
step:181/2330 train_time:7280ms step_avg:40.22ms
step:182/2330 train_time:7325ms step_avg:40.25ms
step:183/2330 train_time:7360ms step_avg:40.22ms
step:184/2330 train_time:7405ms step_avg:40.25ms
step:185/2330 train_time:7441ms step_avg:40.22ms
step:186/2330 train_time:7486ms step_avg:40.25ms
step:187/2330 train_time:7520ms step_avg:40.21ms
step:188/2330 train_time:7565ms step_avg:40.24ms
step:189/2330 train_time:7600ms step_avg:40.21ms
step:190/2330 train_time:7645ms step_avg:40.23ms
step:191/2330 train_time:7680ms step_avg:40.21ms
step:192/2330 train_time:7725ms step_avg:40.24ms
step:193/2330 train_time:7760ms step_avg:40.21ms
step:194/2330 train_time:7805ms step_avg:40.23ms
step:195/2330 train_time:7840ms step_avg:40.20ms
step:196/2330 train_time:7885ms step_avg:40.23ms
step:197/2330 train_time:7919ms step_avg:40.20ms
step:198/2330 train_time:7964ms step_avg:40.22ms
step:199/2330 train_time:7999ms step_avg:40.19ms
step:200/2330 train_time:8044ms step_avg:40.22ms
step:201/2330 train_time:8079ms step_avg:40.19ms
step:202/2330 train_time:8123ms step_avg:40.21ms
step:203/2330 train_time:8158ms step_avg:40.19ms
step:204/2330 train_time:8203ms step_avg:40.21ms
step:205/2330 train_time:8238ms step_avg:40.19ms
step:206/2330 train_time:8284ms step_avg:40.21ms
step:207/2330 train_time:8319ms step_avg:40.19ms
step:208/2330 train_time:8364ms step_avg:40.21ms
step:209/2330 train_time:8400ms step_avg:40.19ms
step:210/2330 train_time:8444ms step_avg:40.21ms
step:211/2330 train_time:8480ms step_avg:40.19ms
step:212/2330 train_time:8526ms step_avg:40.22ms
step:213/2330 train_time:8561ms step_avg:40.19ms
step:214/2330 train_time:8606ms step_avg:40.22ms
step:215/2330 train_time:8642ms step_avg:40.20ms
step:216/2330 train_time:8687ms step_avg:40.22ms
step:217/2330 train_time:8722ms step_avg:40.19ms
step:218/2330 train_time:8767ms step_avg:40.22ms
step:219/2330 train_time:8803ms step_avg:40.19ms
step:220/2330 train_time:8847ms step_avg:40.21ms
step:221/2330 train_time:8882ms step_avg:40.19ms
step:222/2330 train_time:8926ms step_avg:40.21ms
step:223/2330 train_time:8962ms step_avg:40.19ms
step:224/2330 train_time:9006ms step_avg:40.21ms
step:225/2330 train_time:9041ms step_avg:40.18ms
step:226/2330 train_time:9086ms step_avg:40.20ms
step:227/2330 train_time:9121ms step_avg:40.18ms
step:228/2330 train_time:9166ms step_avg:40.20ms
step:229/2330 train_time:9201ms step_avg:40.18ms
step:230/2330 train_time:9245ms step_avg:40.20ms
step:231/2330 train_time:9281ms step_avg:40.18ms
step:232/2330 train_time:9326ms step_avg:40.20ms
step:233/2330 train_time:9362ms step_avg:40.18ms
step:234/2330 train_time:9406ms step_avg:40.20ms
step:235/2330 train_time:9441ms step_avg:40.18ms
step:236/2330 train_time:9486ms step_avg:40.19ms
step:237/2330 train_time:9522ms step_avg:40.18ms
step:238/2330 train_time:9566ms step_avg:40.19ms
step:239/2330 train_time:9602ms step_avg:40.18ms
step:240/2330 train_time:9646ms step_avg:40.19ms
step:241/2330 train_time:9682ms step_avg:40.17ms
step:242/2330 train_time:9727ms step_avg:40.19ms
step:243/2330 train_time:9762ms step_avg:40.17ms
step:244/2330 train_time:9806ms step_avg:40.19ms
step:245/2330 train_time:9841ms step_avg:40.17ms
step:246/2330 train_time:9885ms step_avg:40.18ms
step:247/2330 train_time:9920ms step_avg:40.16ms
step:248/2330 train_time:9964ms step_avg:40.18ms
step:249/2330 train_time:9999ms step_avg:40.16ms
step:250/2330 train_time:10044ms step_avg:40.17ms
step:250/2330 val_loss:5.3952 train_time:10129ms step_avg:40.52ms
step:251/2330 train_time:10143ms step_avg:40.41ms
step:252/2330 train_time:10155ms step_avg:40.30ms
step:253/2330 train_time:10166ms step_avg:40.18ms
step:254/2330 train_time:10201ms step_avg:40.16ms
step:255/2330 train_time:10236ms step_avg:40.14ms
step:256/2330 train_time:10279ms step_avg:40.15ms
step:257/2330 train_time:10314ms step_avg:40.13ms
step:258/2330 train_time:10358ms step_avg:40.15ms
step:259/2330 train_time:10392ms step_avg:40.12ms
step:260/2330 train_time:10436ms step_avg:40.14ms
step:261/2330 train_time:10475ms step_avg:40.13ms
step:262/2330 train_time:10524ms step_avg:40.17ms
step:263/2330 train_time:10561ms step_avg:40.16ms
step:264/2330 train_time:10607ms step_avg:40.18ms
step:265/2330 train_time:10642ms step_avg:40.16ms
step:266/2330 train_time:10687ms step_avg:40.18ms
step:267/2330 train_time:10721ms step_avg:40.15ms
step:268/2330 train_time:10765ms step_avg:40.17ms
step:269/2330 train_time:10800ms step_avg:40.15ms
step:270/2330 train_time:10844ms step_avg:40.16ms
step:271/2330 train_time:10879ms step_avg:40.14ms
step:272/2330 train_time:10923ms step_avg:40.16ms
step:273/2330 train_time:10958ms step_avg:40.14ms
step:274/2330 train_time:11002ms step_avg:40.15ms
step:275/2330 train_time:11040ms step_avg:40.14ms
step:276/2330 train_time:11087ms step_avg:40.17ms
step:277/2330 train_time:11124ms step_avg:40.16ms
step:278/2330 train_time:11169ms step_avg:40.18ms
step:279/2330 train_time:11205ms step_avg:40.16ms
step:280/2330 train_time:11250ms step_avg:40.18ms
step:281/2330 train_time:11286ms step_avg:40.16ms
step:282/2330 train_time:11330ms step_avg:40.18ms
step:283/2330 train_time:11365ms step_avg:40.16ms
step:284/2330 train_time:11411ms step_avg:40.18ms
step:285/2330 train_time:11447ms step_avg:40.17ms
step:286/2330 train_time:11492ms step_avg:40.18ms
step:287/2330 train_time:11528ms step_avg:40.17ms
step:288/2330 train_time:11573ms step_avg:40.18ms
step:289/2330 train_time:11608ms step_avg:40.17ms
step:290/2330 train_time:11653ms step_avg:40.18ms
step:291/2330 train_time:11688ms step_avg:40.17ms
step:292/2330 train_time:11732ms step_avg:40.18ms
step:293/2330 train_time:11767ms step_avg:40.16ms
step:294/2330 train_time:11811ms step_avg:40.17ms
step:295/2330 train_time:11845ms step_avg:40.15ms
step:296/2330 train_time:11889ms step_avg:40.17ms
step:297/2330 train_time:11924ms step_avg:40.15ms
step:298/2330 train_time:11969ms step_avg:40.16ms
step:299/2330 train_time:12004ms step_avg:40.15ms
step:300/2330 train_time:12050ms step_avg:40.17ms
step:301/2330 train_time:12085ms step_avg:40.15ms
step:302/2330 train_time:12130ms step_avg:40.17ms
step:303/2330 train_time:12166ms step_avg:40.15ms
step:304/2330 train_time:12211ms step_avg:40.17ms
step:305/2330 train_time:12247ms step_avg:40.15ms
step:306/2330 train_time:12292ms step_avg:40.17ms
step:307/2330 train_time:12328ms step_avg:40.16ms
step:308/2330 train_time:12373ms step_avg:40.17ms
step:309/2330 train_time:12408ms step_avg:40.16ms
step:310/2330 train_time:12453ms step_avg:40.17ms
step:311/2330 train_time:12489ms step_avg:40.16ms
step:312/2330 train_time:12533ms step_avg:40.17ms
step:313/2330 train_time:12568ms step_avg:40.15ms
step:314/2330 train_time:12613ms step_avg:40.17ms
step:315/2330 train_time:12648ms step_avg:40.15ms
step:316/2330 train_time:12692ms step_avg:40.16ms
step:317/2330 train_time:12727ms step_avg:40.15ms
step:318/2330 train_time:12771ms step_avg:40.16ms
step:319/2330 train_time:12806ms step_avg:40.14ms
step:320/2330 train_time:12851ms step_avg:40.16ms
step:321/2330 train_time:12886ms step_avg:40.14ms
step:322/2330 train_time:12930ms step_avg:40.16ms
step:323/2330 train_time:12965ms step_avg:40.14ms
step:324/2330 train_time:13010ms step_avg:40.15ms
step:325/2330 train_time:13045ms step_avg:40.14ms
step:326/2330 train_time:13090ms step_avg:40.15ms
step:327/2330 train_time:13126ms step_avg:40.14ms
step:328/2330 train_time:13171ms step_avg:40.16ms
step:329/2330 train_time:13207ms step_avg:40.14ms
step:330/2330 train_time:13252ms step_avg:40.16ms
step:331/2330 train_time:13288ms step_avg:40.14ms
step:332/2330 train_time:13332ms step_avg:40.16ms
step:333/2330 train_time:13367ms step_avg:40.14ms
step:334/2330 train_time:13412ms step_avg:40.15ms
step:335/2330 train_time:13447ms step_avg:40.14ms
step:336/2330 train_time:13492ms step_avg:40.15ms
step:337/2330 train_time:13527ms step_avg:40.14ms
step:338/2330 train_time:13571ms step_avg:40.15ms
step:339/2330 train_time:13606ms step_avg:40.14ms
step:340/2330 train_time:13651ms step_avg:40.15ms
step:341/2330 train_time:13686ms step_avg:40.14ms
step:342/2330 train_time:13730ms step_avg:40.15ms
step:343/2330 train_time:13765ms step_avg:40.13ms
step:344/2330 train_time:13810ms step_avg:40.14ms
step:345/2330 train_time:13845ms step_avg:40.13ms
step:346/2330 train_time:13889ms step_avg:40.14ms
step:347/2330 train_time:13925ms step_avg:40.13ms
step:348/2330 train_time:13969ms step_avg:40.14ms
step:349/2330 train_time:14005ms step_avg:40.13ms
step:350/2330 train_time:14049ms step_avg:40.14ms
step:351/2330 train_time:14085ms step_avg:40.13ms
step:352/2330 train_time:14130ms step_avg:40.14ms
step:353/2330 train_time:14165ms step_avg:40.13ms
step:354/2330 train_time:14210ms step_avg:40.14ms
step:355/2330 train_time:14246ms step_avg:40.13ms
step:356/2330 train_time:14290ms step_avg:40.14ms
step:357/2330 train_time:14326ms step_avg:40.13ms
step:358/2330 train_time:14371ms step_avg:40.14ms
step:359/2330 train_time:14407ms step_avg:40.13ms
step:360/2330 train_time:14451ms step_avg:40.14ms
step:361/2330 train_time:14487ms step_avg:40.13ms
step:362/2330 train_time:14531ms step_avg:40.14ms
step:363/2330 train_time:14566ms step_avg:40.13ms
step:364/2330 train_time:14611ms step_avg:40.14ms
step:365/2330 train_time:14646ms step_avg:40.13ms
step:366/2330 train_time:14690ms step_avg:40.14ms
step:367/2330 train_time:14726ms step_avg:40.13ms
step:368/2330 train_time:14770ms step_avg:40.14ms
step:369/2330 train_time:14805ms step_avg:40.12ms
step:370/2330 train_time:14850ms step_avg:40.14ms
step:371/2330 train_time:14886ms step_avg:40.12ms
step:372/2330 train_time:14930ms step_avg:40.14ms
step:373/2330 train_time:14965ms step_avg:40.12ms
step:374/2330 train_time:15009ms step_avg:40.13ms
step:375/2330 train_time:15045ms step_avg:40.12ms
step:376/2330 train_time:15089ms step_avg:40.13ms
step:377/2330 train_time:15125ms step_avg:40.12ms
step:378/2330 train_time:15170ms step_avg:40.13ms
step:379/2330 train_time:15205ms step_avg:40.12ms
step:380/2330 train_time:15250ms step_avg:40.13ms
step:381/2330 train_time:15286ms step_avg:40.12ms
step:382/2330 train_time:15330ms step_avg:40.13ms
step:383/2330 train_time:15366ms step_avg:40.12ms
step:384/2330 train_time:15411ms step_avg:40.13ms
step:385/2330 train_time:15446ms step_avg:40.12ms
step:386/2330 train_time:15491ms step_avg:40.13ms
step:387/2330 train_time:15527ms step_avg:40.12ms
step:388/2330 train_time:15571ms step_avg:40.13ms
step:389/2330 train_time:15606ms step_avg:40.12ms
step:390/2330 train_time:15651ms step_avg:40.13ms
step:391/2330 train_time:15686ms step_avg:40.12ms
step:392/2330 train_time:15730ms step_avg:40.13ms
step:393/2330 train_time:15765ms step_avg:40.12ms
step:394/2330 train_time:15810ms step_avg:40.13ms
step:395/2330 train_time:15845ms step_avg:40.11ms
step:396/2330 train_time:15890ms step_avg:40.13ms
step:397/2330 train_time:15925ms step_avg:40.11ms
step:398/2330 train_time:15969ms step_avg:40.12ms
step:399/2330 train_time:16005ms step_avg:40.11ms
step:400/2330 train_time:16049ms step_avg:40.12ms
step:401/2330 train_time:16084ms step_avg:40.11ms
step:402/2330 train_time:16129ms step_avg:40.12ms
step:403/2330 train_time:16164ms step_avg:40.11ms
step:404/2330 train_time:16209ms step_avg:40.12ms
step:405/2330 train_time:16245ms step_avg:40.11ms
step:406/2330 train_time:16290ms step_avg:40.12ms
step:407/2330 train_time:16326ms step_avg:40.11ms
step:408/2330 train_time:16370ms step_avg:40.12ms
step:409/2330 train_time:16405ms step_avg:40.11ms
step:410/2330 train_time:16450ms step_avg:40.12ms
step:411/2330 train_time:16485ms step_avg:40.11ms
step:412/2330 train_time:16530ms step_avg:40.12ms
step:413/2330 train_time:16566ms step_avg:40.11ms
step:414/2330 train_time:16610ms step_avg:40.12ms
step:415/2330 train_time:16645ms step_avg:40.11ms
step:416/2330 train_time:16690ms step_avg:40.12ms
step:417/2330 train_time:16725ms step_avg:40.11ms
step:418/2330 train_time:16770ms step_avg:40.12ms
step:419/2330 train_time:16806ms step_avg:40.11ms
step:420/2330 train_time:16850ms step_avg:40.12ms
step:421/2330 train_time:16885ms step_avg:40.11ms
step:422/2330 train_time:16930ms step_avg:40.12ms
step:423/2330 train_time:16965ms step_avg:40.11ms
step:424/2330 train_time:17010ms step_avg:40.12ms
step:425/2330 train_time:17045ms step_avg:40.11ms
step:426/2330 train_time:17089ms step_avg:40.12ms
step:427/2330 train_time:17125ms step_avg:40.11ms
step:428/2330 train_time:17170ms step_avg:40.12ms
step:429/2330 train_time:17206ms step_avg:40.11ms
step:430/2330 train_time:17250ms step_avg:40.12ms
step:431/2330 train_time:17286ms step_avg:40.11ms
step:432/2330 train_time:17330ms step_avg:40.12ms
step:433/2330 train_time:17365ms step_avg:40.10ms
step:434/2330 train_time:17410ms step_avg:40.12ms
step:435/2330 train_time:17445ms step_avg:40.10ms
step:436/2330 train_time:17489ms step_avg:40.11ms
step:437/2330 train_time:17525ms step_avg:40.10ms
step:438/2330 train_time:17570ms step_avg:40.11ms
step:439/2330 train_time:17605ms step_avg:40.10ms
step:440/2330 train_time:17650ms step_avg:40.11ms
step:441/2330 train_time:17685ms step_avg:40.10ms
step:442/2330 train_time:17730ms step_avg:40.11ms
step:443/2330 train_time:17765ms step_avg:40.10ms
step:444/2330 train_time:17810ms step_avg:40.11ms
step:445/2330 train_time:17845ms step_avg:40.10ms
step:446/2330 train_time:17889ms step_avg:40.11ms
step:447/2330 train_time:17925ms step_avg:40.10ms
step:448/2330 train_time:17969ms step_avg:40.11ms
step:449/2330 train_time:18005ms step_avg:40.10ms
step:450/2330 train_time:18049ms step_avg:40.11ms
step:451/2330 train_time:18084ms step_avg:40.10ms
step:452/2330 train_time:18129ms step_avg:40.11ms
step:453/2330 train_time:18165ms step_avg:40.10ms
step:454/2330 train_time:18210ms step_avg:40.11ms
step:455/2330 train_time:18246ms step_avg:40.10ms
step:456/2330 train_time:18290ms step_avg:40.11ms
step:457/2330 train_time:18325ms step_avg:40.10ms
step:458/2330 train_time:18370ms step_avg:40.11ms
step:459/2330 train_time:18405ms step_avg:40.10ms
step:460/2330 train_time:18450ms step_avg:40.11ms
step:461/2330 train_time:18485ms step_avg:40.10ms
step:462/2330 train_time:18530ms step_avg:40.11ms
step:463/2330 train_time:18565ms step_avg:40.10ms
step:464/2330 train_time:18610ms step_avg:40.11ms
step:465/2330 train_time:18645ms step_avg:40.10ms
step:466/2330 train_time:18690ms step_avg:40.11ms
step:467/2330 train_time:18725ms step_avg:40.10ms
step:468/2330 train_time:18770ms step_avg:40.11ms
step:469/2330 train_time:18805ms step_avg:40.10ms
step:470/2330 train_time:18850ms step_avg:40.11ms
step:471/2330 train_time:18885ms step_avg:40.10ms
step:472/2330 train_time:18930ms step_avg:40.11ms
step:473/2330 train_time:18965ms step_avg:40.10ms
step:474/2330 train_time:19009ms step_avg:40.10ms
step:475/2330 train_time:19044ms step_avg:40.09ms
step:476/2330 train_time:19089ms step_avg:40.10ms
step:477/2330 train_time:19125ms step_avg:40.09ms
step:478/2330 train_time:19170ms step_avg:40.10ms
step:479/2330 train_time:19205ms step_avg:40.09ms
step:480/2330 train_time:19250ms step_avg:40.10ms
step:481/2330 train_time:19286ms step_avg:40.09ms
step:482/2330 train_time:19330ms step_avg:40.10ms
step:483/2330 train_time:19366ms step_avg:40.09ms
step:484/2330 train_time:19410ms step_avg:40.10ms
step:485/2330 train_time:19445ms step_avg:40.09ms
step:486/2330 train_time:19490ms step_avg:40.10ms
step:487/2330 train_time:19525ms step_avg:40.09ms
step:488/2330 train_time:19569ms step_avg:40.10ms
step:489/2330 train_time:19605ms step_avg:40.09ms
step:490/2330 train_time:19650ms step_avg:40.10ms
step:491/2330 train_time:19685ms step_avg:40.09ms
step:492/2330 train_time:19729ms step_avg:40.10ms
step:493/2330 train_time:19765ms step_avg:40.09ms
step:494/2330 train_time:19809ms step_avg:40.10ms
step:495/2330 train_time:19844ms step_avg:40.09ms
step:496/2330 train_time:19889ms step_avg:40.10ms
step:497/2330 train_time:19924ms step_avg:40.09ms
step:498/2330 train_time:19969ms step_avg:40.10ms
step:499/2330 train_time:20005ms step_avg:40.09ms
step:500/2330 train_time:20049ms step_avg:40.10ms
step:500/2330 val_loss:5.2804 train_time:20138ms step_avg:40.28ms
step:501/2330 train_time:20151ms step_avg:40.22ms
step:502/2330 train_time:20163ms step_avg:40.17ms
step:503/2330 train_time:20174ms step_avg:40.11ms
step:504/2330 train_time:20211ms step_avg:40.10ms
step:505/2330 train_time:20244ms step_avg:40.09ms
step:506/2330 train_time:20288ms step_avg:40.09ms
step:507/2330 train_time:20322ms step_avg:40.08ms
step:508/2330 train_time:20366ms step_avg:40.09ms
step:509/2330 train_time:20400ms step_avg:40.08ms
step:510/2330 train_time:20446ms step_avg:40.09ms
step:511/2330 train_time:20485ms step_avg:40.09ms
step:512/2330 train_time:20533ms step_avg:40.10ms
step:513/2330 train_time:20572ms step_avg:40.10ms
step:514/2330 train_time:20617ms step_avg:40.11ms
step:515/2330 train_time:20653ms step_avg:40.10ms
step:516/2330 train_time:20697ms step_avg:40.11ms
step:517/2330 train_time:20732ms step_avg:40.10ms
step:518/2330 train_time:20776ms step_avg:40.11ms
step:519/2330 train_time:20810ms step_avg:40.10ms
step:520/2330 train_time:20854ms step_avg:40.10ms
step:521/2330 train_time:20889ms step_avg:40.09ms
step:522/2330 train_time:20933ms step_avg:40.10ms
step:523/2330 train_time:20968ms step_avg:40.09ms
step:524/2330 train_time:21013ms step_avg:40.10ms
step:525/2330 train_time:21049ms step_avg:40.09ms
step:526/2330 train_time:21095ms step_avg:40.10ms
step:527/2330 train_time:21130ms step_avg:40.09ms
step:528/2330 train_time:21174ms step_avg:40.10ms
step:529/2330 train_time:21210ms step_avg:40.09ms
step:530/2330 train_time:21255ms step_avg:40.10ms
step:531/2330 train_time:21290ms step_avg:40.09ms
step:532/2330 train_time:21334ms step_avg:40.10ms
step:533/2330 train_time:21369ms step_avg:40.09ms
step:534/2330 train_time:21415ms step_avg:40.10ms
step:535/2330 train_time:21450ms step_avg:40.09ms
step:536/2330 train_time:21496ms step_avg:40.10ms
step:537/2330 train_time:21532ms step_avg:40.10ms
step:538/2330 train_time:21576ms step_avg:40.10ms
step:539/2330 train_time:21613ms step_avg:40.10ms
step:540/2330 train_time:21658ms step_avg:40.11ms
step:541/2330 train_time:21693ms step_avg:40.10ms
step:542/2330 train_time:21737ms step_avg:40.11ms
step:543/2330 train_time:21772ms step_avg:40.10ms
step:544/2330 train_time:21816ms step_avg:40.10ms
step:545/2330 train_time:21851ms step_avg:40.09ms
step:546/2330 train_time:21895ms step_avg:40.10ms
step:547/2330 train_time:21930ms step_avg:40.09ms
step:548/2330 train_time:21975ms step_avg:40.10ms
step:549/2330 train_time:22010ms step_avg:40.09ms
step:550/2330 train_time:22055ms step_avg:40.10ms
step:551/2330 train_time:22091ms step_avg:40.09ms
step:552/2330 train_time:22135ms step_avg:40.10ms
step:553/2330 train_time:22170ms step_avg:40.09ms
step:554/2330 train_time:22215ms step_avg:40.10ms
step:555/2330 train_time:22251ms step_avg:40.09ms
step:556/2330 train_time:22295ms step_avg:40.10ms
step:557/2330 train_time:22332ms step_avg:40.09ms
step:558/2330 train_time:22376ms step_avg:40.10ms
step:559/2330 train_time:22411ms step_avg:40.09ms
step:560/2330 train_time:22456ms step_avg:40.10ms
step:561/2330 train_time:22492ms step_avg:40.09ms
step:562/2330 train_time:22537ms step_avg:40.10ms
step:563/2330 train_time:22573ms step_avg:40.09ms
step:564/2330 train_time:22617ms step_avg:40.10ms
step:565/2330 train_time:22653ms step_avg:40.09ms
step:566/2330 train_time:22697ms step_avg:40.10ms
step:567/2330 train_time:22732ms step_avg:40.09ms
step:568/2330 train_time:22777ms step_avg:40.10ms
step:569/2330 train_time:22812ms step_avg:40.09ms
step:570/2330 train_time:22856ms step_avg:40.10ms
step:571/2330 train_time:22891ms step_avg:40.09ms
step:572/2330 train_time:22936ms step_avg:40.10ms
step:573/2330 train_time:22971ms step_avg:40.09ms
step:574/2330 train_time:23017ms step_avg:40.10ms
step:575/2330 train_time:23052ms step_avg:40.09ms
step:576/2330 train_time:23097ms step_avg:40.10ms
step:577/2330 train_time:23132ms step_avg:40.09ms
step:578/2330 train_time:23177ms step_avg:40.10ms
step:579/2330 train_time:23212ms step_avg:40.09ms
step:580/2330 train_time:23258ms step_avg:40.10ms
step:581/2330 train_time:23293ms step_avg:40.09ms
step:582/2330 train_time:23338ms step_avg:40.10ms
step:583/2330 train_time:23374ms step_avg:40.09ms
step:584/2330 train_time:23419ms step_avg:40.10ms
step:585/2330 train_time:23454ms step_avg:40.09ms
step:586/2330 train_time:23499ms step_avg:40.10ms
step:587/2330 train_time:23534ms step_avg:40.09ms
step:588/2330 train_time:23579ms step_avg:40.10ms
step:589/2330 train_time:23614ms step_avg:40.09ms
step:590/2330 train_time:23659ms step_avg:40.10ms
step:591/2330 train_time:23694ms step_avg:40.09ms
step:592/2330 train_time:23739ms step_avg:40.10ms
step:593/2330 train_time:23774ms step_avg:40.09ms
step:594/2330 train_time:23818ms step_avg:40.10ms
step:595/2330 train_time:23854ms step_avg:40.09ms
step:596/2330 train_time:23898ms step_avg:40.10ms
step:597/2330 train_time:23933ms step_avg:40.09ms
step:598/2330 train_time:23978ms step_avg:40.10ms
step:599/2330 train_time:24013ms step_avg:40.09ms
step:600/2330 train_time:24058ms step_avg:40.10ms
step:601/2330 train_time:24093ms step_avg:40.09ms
step:602/2330 train_time:24138ms step_avg:40.10ms
step:603/2330 train_time:24174ms step_avg:40.09ms
step:604/2330 train_time:24219ms step_avg:40.10ms
step:605/2330 train_time:24255ms step_avg:40.09ms
step:606/2330 train_time:24300ms step_avg:40.10ms
step:607/2330 train_time:24335ms step_avg:40.09ms
step:608/2330 train_time:24379ms step_avg:40.10ms
step:609/2330 train_time:24415ms step_avg:40.09ms
step:610/2330 train_time:24460ms step_avg:40.10ms
step:611/2330 train_time:24495ms step_avg:40.09ms
step:612/2330 train_time:24540ms step_avg:40.10ms
step:613/2330 train_time:24574ms step_avg:40.09ms
step:614/2330 train_time:24619ms step_avg:40.10ms
step:615/2330 train_time:24654ms step_avg:40.09ms
step:616/2330 train_time:24699ms step_avg:40.10ms
step:617/2330 train_time:24734ms step_avg:40.09ms
step:618/2330 train_time:24778ms step_avg:40.09ms
step:619/2330 train_time:24814ms step_avg:40.09ms
step:620/2330 train_time:24859ms step_avg:40.10ms
step:621/2330 train_time:24894ms step_avg:40.09ms
step:622/2330 train_time:24939ms step_avg:40.10ms
step:623/2330 train_time:24975ms step_avg:40.09ms
step:624/2330 train_time:25020ms step_avg:40.10ms
step:625/2330 train_time:25055ms step_avg:40.09ms
step:626/2330 train_time:25100ms step_avg:40.10ms
step:627/2330 train_time:25135ms step_avg:40.09ms
step:628/2330 train_time:25180ms step_avg:40.10ms
step:629/2330 train_time:25215ms step_avg:40.09ms
step:630/2330 train_time:25260ms step_avg:40.10ms
step:631/2330 train_time:25296ms step_avg:40.09ms
step:632/2330 train_time:25341ms step_avg:40.10ms
step:633/2330 train_time:25376ms step_avg:40.09ms
step:634/2330 train_time:25421ms step_avg:40.10ms
step:635/2330 train_time:25456ms step_avg:40.09ms
step:636/2330 train_time:25501ms step_avg:40.10ms
step:637/2330 train_time:25535ms step_avg:40.09ms
step:638/2330 train_time:25579ms step_avg:40.09ms
step:639/2330 train_time:25614ms step_avg:40.08ms
step:640/2330 train_time:25659ms step_avg:40.09ms
step:641/2330 train_time:25694ms step_avg:40.08ms
step:642/2330 train_time:25739ms step_avg:40.09ms
step:643/2330 train_time:25774ms step_avg:40.08ms
step:644/2330 train_time:25818ms step_avg:40.09ms
step:645/2330 train_time:25854ms step_avg:40.08ms
step:646/2330 train_time:25899ms step_avg:40.09ms
step:647/2330 train_time:25934ms step_avg:40.08ms
step:648/2330 train_time:25978ms step_avg:40.09ms
step:649/2330 train_time:26014ms step_avg:40.08ms
step:650/2330 train_time:26059ms step_avg:40.09ms
step:651/2330 train_time:26095ms step_avg:40.08ms
step:652/2330 train_time:26140ms step_avg:40.09ms
step:653/2330 train_time:26175ms step_avg:40.08ms
step:654/2330 train_time:26220ms step_avg:40.09ms
step:655/2330 train_time:26255ms step_avg:40.08ms
step:656/2330 train_time:26300ms step_avg:40.09ms
step:657/2330 train_time:26335ms step_avg:40.08ms
step:658/2330 train_time:26379ms step_avg:40.09ms
step:659/2330 train_time:26414ms step_avg:40.08ms
step:660/2330 train_time:26459ms step_avg:40.09ms
step:661/2330 train_time:26494ms step_avg:40.08ms
step:662/2330 train_time:26539ms step_avg:40.09ms
step:663/2330 train_time:26573ms step_avg:40.08ms
step:664/2330 train_time:26619ms step_avg:40.09ms
step:665/2330 train_time:26654ms step_avg:40.08ms
step:666/2330 train_time:26699ms step_avg:40.09ms
step:667/2330 train_time:26734ms step_avg:40.08ms
step:668/2330 train_time:26778ms step_avg:40.09ms
step:669/2330 train_time:26814ms step_avg:40.08ms
step:670/2330 train_time:26859ms step_avg:40.09ms
step:671/2330 train_time:26894ms step_avg:40.08ms
step:672/2330 train_time:26939ms step_avg:40.09ms
step:673/2330 train_time:26975ms step_avg:40.08ms
step:674/2330 train_time:27019ms step_avg:40.09ms
step:675/2330 train_time:27056ms step_avg:40.08ms
step:676/2330 train_time:27100ms step_avg:40.09ms
step:677/2330 train_time:27136ms step_avg:40.08ms
step:678/2330 train_time:27180ms step_avg:40.09ms
step:679/2330 train_time:27215ms step_avg:40.08ms
step:680/2330 train_time:27260ms step_avg:40.09ms
step:681/2330 train_time:27295ms step_avg:40.08ms
step:682/2330 train_time:27340ms step_avg:40.09ms
step:683/2330 train_time:27375ms step_avg:40.08ms
step:684/2330 train_time:27420ms step_avg:40.09ms
step:685/2330 train_time:27454ms step_avg:40.08ms
step:686/2330 train_time:27500ms step_avg:40.09ms
step:687/2330 train_time:27534ms step_avg:40.08ms
step:688/2330 train_time:27579ms step_avg:40.09ms
step:689/2330 train_time:27614ms step_avg:40.08ms
step:690/2330 train_time:27658ms step_avg:40.08ms
step:691/2330 train_time:27693ms step_avg:40.08ms
step:692/2330 train_time:27738ms step_avg:40.08ms
step:693/2330 train_time:27773ms step_avg:40.08ms
step:694/2330 train_time:27818ms step_avg:40.08ms
step:695/2330 train_time:27854ms step_avg:40.08ms
step:696/2330 train_time:27898ms step_avg:40.08ms
step:697/2330 train_time:27934ms step_avg:40.08ms
step:698/2330 train_time:27978ms step_avg:40.08ms
step:699/2330 train_time:28013ms step_avg:40.08ms
step:700/2330 train_time:28058ms step_avg:40.08ms
step:701/2330 train_time:28094ms step_avg:40.08ms
step:702/2330 train_time:28140ms step_avg:40.08ms
step:703/2330 train_time:28175ms step_avg:40.08ms
step:704/2330 train_time:28220ms step_avg:40.09ms
step:705/2330 train_time:28255ms step_avg:40.08ms
step:706/2330 train_time:28299ms step_avg:40.08ms
step:707/2330 train_time:28335ms step_avg:40.08ms
step:708/2330 train_time:28380ms step_avg:40.08ms
step:709/2330 train_time:28415ms step_avg:40.08ms
step:710/2330 train_time:28459ms step_avg:40.08ms
step:711/2330 train_time:28495ms step_avg:40.08ms
step:712/2330 train_time:28540ms step_avg:40.08ms
step:713/2330 train_time:28575ms step_avg:40.08ms
step:714/2330 train_time:28620ms step_avg:40.08ms
step:715/2330 train_time:28655ms step_avg:40.08ms
step:716/2330 train_time:28700ms step_avg:40.08ms
step:717/2330 train_time:28734ms step_avg:40.08ms
step:718/2330 train_time:28779ms step_avg:40.08ms
step:719/2330 train_time:28814ms step_avg:40.08ms
step:720/2330 train_time:28859ms step_avg:40.08ms
step:721/2330 train_time:28894ms step_avg:40.07ms
step:722/2330 train_time:28938ms step_avg:40.08ms
step:723/2330 train_time:28974ms step_avg:40.07ms
step:724/2330 train_time:29019ms step_avg:40.08ms
step:725/2330 train_time:29054ms step_avg:40.08ms
step:726/2330 train_time:29100ms step_avg:40.08ms
step:727/2330 train_time:29135ms step_avg:40.08ms
step:728/2330 train_time:29180ms step_avg:40.08ms
step:729/2330 train_time:29215ms step_avg:40.08ms
step:730/2330 train_time:29260ms step_avg:40.08ms
step:731/2330 train_time:29295ms step_avg:40.08ms
step:732/2330 train_time:29340ms step_avg:40.08ms
step:733/2330 train_time:29375ms step_avg:40.07ms
step:734/2330 train_time:29420ms step_avg:40.08ms
step:735/2330 train_time:29455ms step_avg:40.08ms
step:736/2330 train_time:29500ms step_avg:40.08ms
step:737/2330 train_time:29535ms step_avg:40.08ms
step:738/2330 train_time:29579ms step_avg:40.08ms
step:739/2330 train_time:29614ms step_avg:40.07ms
step:740/2330 train_time:29659ms step_avg:40.08ms
step:741/2330 train_time:29694ms step_avg:40.07ms
step:742/2330 train_time:29739ms step_avg:40.08ms
step:743/2330 train_time:29774ms step_avg:40.07ms
step:744/2330 train_time:29818ms step_avg:40.08ms
step:745/2330 train_time:29854ms step_avg:40.07ms
step:746/2330 train_time:29899ms step_avg:40.08ms
step:747/2330 train_time:29934ms step_avg:40.07ms
step:748/2330 train_time:29979ms step_avg:40.08ms
step:749/2330 train_time:30015ms step_avg:40.07ms
step:750/2330 train_time:30060ms step_avg:40.08ms
step:750/2330 val_loss:5.2206 train_time:30147ms step_avg:40.20ms
step:751/2330 train_time:30161ms step_avg:40.16ms
step:752/2330 train_time:30174ms step_avg:40.12ms
step:753/2330 train_time:30184ms step_avg:40.08ms
step:754/2330 train_time:30221ms step_avg:40.08ms
step:755/2330 train_time:30254ms step_avg:40.07ms
step:756/2330 train_time:30298ms step_avg:40.08ms
step:757/2330 train_time:30333ms step_avg:40.07ms
step:758/2330 train_time:30376ms step_avg:40.07ms
step:759/2330 train_time:30411ms step_avg:40.07ms
step:760/2330 train_time:30456ms step_avg:40.07ms
step:761/2330 train_time:30497ms step_avg:40.07ms
step:762/2330 train_time:30544ms step_avg:40.08ms
step:763/2330 train_time:30580ms step_avg:40.08ms
step:764/2330 train_time:30625ms step_avg:40.08ms
step:765/2330 train_time:30662ms step_avg:40.08ms
step:766/2330 train_time:30705ms step_avg:40.09ms
step:767/2330 train_time:30740ms step_avg:40.08ms
step:768/2330 train_time:30784ms step_avg:40.08ms
step:769/2330 train_time:30819ms step_avg:40.08ms
step:770/2330 train_time:30864ms step_avg:40.08ms
step:771/2330 train_time:30899ms step_avg:40.08ms
step:772/2330 train_time:30943ms step_avg:40.08ms
step:773/2330 train_time:30978ms step_avg:40.07ms
step:774/2330 train_time:31022ms step_avg:40.08ms
step:775/2330 train_time:31058ms step_avg:40.07ms
step:776/2330 train_time:31102ms step_avg:40.08ms
step:777/2330 train_time:31139ms step_avg:40.08ms
step:778/2330 train_time:31183ms step_avg:40.08ms
step:779/2330 train_time:31218ms step_avg:40.07ms
step:780/2330 train_time:31262ms step_avg:40.08ms
step:781/2330 train_time:31298ms step_avg:40.07ms
step:782/2330 train_time:31342ms step_avg:40.08ms
step:783/2330 train_time:31378ms step_avg:40.07ms
step:784/2330 train_time:31423ms step_avg:40.08ms
step:785/2330 train_time:31459ms step_avg:40.07ms
step:786/2330 train_time:31504ms step_avg:40.08ms
step:787/2330 train_time:31540ms step_avg:40.08ms
step:788/2330 train_time:31586ms step_avg:40.08ms
step:789/2330 train_time:31621ms step_avg:40.08ms
step:790/2330 train_time:31666ms step_avg:40.08ms
step:791/2330 train_time:31701ms step_avg:40.08ms
step:792/2330 train_time:31744ms step_avg:40.08ms
step:793/2330 train_time:31779ms step_avg:40.07ms
step:794/2330 train_time:31823ms step_avg:40.08ms
step:795/2330 train_time:31858ms step_avg:40.07ms
step:796/2330 train_time:31903ms step_avg:40.08ms
step:797/2330 train_time:31939ms step_avg:40.07ms
step:798/2330 train_time:31983ms step_avg:40.08ms
step:799/2330 train_time:32018ms step_avg:40.07ms
step:800/2330 train_time:32063ms step_avg:40.08ms
step:801/2330 train_time:32098ms step_avg:40.07ms
step:802/2330 train_time:32143ms step_avg:40.08ms
step:803/2330 train_time:32179ms step_avg:40.07ms
step:804/2330 train_time:32224ms step_avg:40.08ms
step:805/2330 train_time:32259ms step_avg:40.07ms
step:806/2330 train_time:32303ms step_avg:40.08ms
step:807/2330 train_time:32339ms step_avg:40.07ms
step:808/2330 train_time:32384ms step_avg:40.08ms
step:809/2330 train_time:32419ms step_avg:40.07ms
step:810/2330 train_time:32464ms step_avg:40.08ms
step:811/2330 train_time:32500ms step_avg:40.07ms
step:812/2330 train_time:32545ms step_avg:40.08ms
step:813/2330 train_time:32581ms step_avg:40.08ms
step:814/2330 train_time:32626ms step_avg:40.08ms
step:815/2330 train_time:32661ms step_avg:40.08ms
step:816/2330 train_time:32706ms step_avg:40.08ms
step:817/2330 train_time:32741ms step_avg:40.07ms
step:818/2330 train_time:32784ms step_avg:40.08ms
step:819/2330 train_time:32820ms step_avg:40.07ms
step:820/2330 train_time:32864ms step_avg:40.08ms
step:821/2330 train_time:32899ms step_avg:40.07ms
step:822/2330 train_time:32943ms step_avg:40.08ms
step:823/2330 train_time:32978ms step_avg:40.07ms
step:824/2330 train_time:33023ms step_avg:40.08ms
step:825/2330 train_time:33058ms step_avg:40.07ms
step:826/2330 train_time:33102ms step_avg:40.08ms
step:827/2330 train_time:33138ms step_avg:40.07ms
step:828/2330 train_time:33183ms step_avg:40.08ms
step:829/2330 train_time:33218ms step_avg:40.07ms
step:830/2330 train_time:33263ms step_avg:40.08ms
step:831/2330 train_time:33299ms step_avg:40.07ms
step:832/2330 train_time:33344ms step_avg:40.08ms
step:833/2330 train_time:33380ms step_avg:40.07ms
step:834/2330 train_time:33424ms step_avg:40.08ms
step:835/2330 train_time:33460ms step_avg:40.07ms
step:836/2330 train_time:33505ms step_avg:40.08ms
step:837/2330 train_time:33540ms step_avg:40.07ms
step:838/2330 train_time:33585ms step_avg:40.08ms
step:839/2330 train_time:33621ms step_avg:40.07ms
step:840/2330 train_time:33665ms step_avg:40.08ms
step:841/2330 train_time:33701ms step_avg:40.07ms
step:842/2330 train_time:33745ms step_avg:40.08ms
step:843/2330 train_time:33779ms step_avg:40.07ms
step:844/2330 train_time:33824ms step_avg:40.08ms
step:845/2330 train_time:33859ms step_avg:40.07ms
step:846/2330 train_time:33903ms step_avg:40.07ms
step:847/2330 train_time:33938ms step_avg:40.07ms
step:848/2330 train_time:33983ms step_avg:40.07ms
step:849/2330 train_time:34018ms step_avg:40.07ms
step:850/2330 train_time:34063ms step_avg:40.07ms
step:851/2330 train_time:34098ms step_avg:40.07ms
step:852/2330 train_time:34143ms step_avg:40.07ms
step:853/2330 train_time:34178ms step_avg:40.07ms
step:854/2330 train_time:34223ms step_avg:40.07ms
step:855/2330 train_time:34259ms step_avg:40.07ms
step:856/2330 train_time:34303ms step_avg:40.07ms
step:857/2330 train_time:34338ms step_avg:40.07ms
step:858/2330 train_time:34384ms step_avg:40.07ms
step:859/2330 train_time:34419ms step_avg:40.07ms
step:860/2330 train_time:34464ms step_avg:40.07ms
step:861/2330 train_time:34499ms step_avg:40.07ms
step:862/2330 train_time:34544ms step_avg:40.07ms
step:863/2330 train_time:34580ms step_avg:40.07ms
step:864/2330 train_time:34624ms step_avg:40.07ms
step:865/2330 train_time:34660ms step_avg:40.07ms
step:866/2330 train_time:34704ms step_avg:40.07ms
step:867/2330 train_time:34740ms step_avg:40.07ms
step:868/2330 train_time:34784ms step_avg:40.07ms
step:869/2330 train_time:34819ms step_avg:40.07ms
step:870/2330 train_time:34863ms step_avg:40.07ms
step:871/2330 train_time:34899ms step_avg:40.07ms
step:872/2330 train_time:34944ms step_avg:40.07ms
step:873/2330 train_time:34979ms step_avg:40.07ms
step:874/2330 train_time:35023ms step_avg:40.07ms
step:875/2330 train_time:35058ms step_avg:40.07ms
step:876/2330 train_time:35104ms step_avg:40.07ms
step:877/2330 train_time:35139ms step_avg:40.07ms
step:878/2330 train_time:35184ms step_avg:40.07ms
step:879/2330 train_time:35219ms step_avg:40.07ms
step:880/2330 train_time:35264ms step_avg:40.07ms
step:881/2330 train_time:35300ms step_avg:40.07ms
step:882/2330 train_time:35344ms step_avg:40.07ms
step:883/2330 train_time:35379ms step_avg:40.07ms
step:884/2330 train_time:35424ms step_avg:40.07ms
step:885/2330 train_time:35460ms step_avg:40.07ms
step:886/2330 train_time:35504ms step_avg:40.07ms
step:887/2330 train_time:35540ms step_avg:40.07ms
step:888/2330 train_time:35585ms step_avg:40.07ms
step:889/2330 train_time:35620ms step_avg:40.07ms
step:890/2330 train_time:35664ms step_avg:40.07ms
step:891/2330 train_time:35700ms step_avg:40.07ms
step:892/2330 train_time:35744ms step_avg:40.07ms
step:893/2330 train_time:35779ms step_avg:40.07ms
step:894/2330 train_time:35824ms step_avg:40.07ms
step:895/2330 train_time:35859ms step_avg:40.07ms
step:896/2330 train_time:35904ms step_avg:40.07ms
step:897/2330 train_time:35939ms step_avg:40.07ms
step:898/2330 train_time:35984ms step_avg:40.07ms
step:899/2330 train_time:36019ms step_avg:40.07ms
step:900/2330 train_time:36064ms step_avg:40.07ms
step:901/2330 train_time:36099ms step_avg:40.07ms
step:902/2330 train_time:36144ms step_avg:40.07ms
step:903/2330 train_time:36179ms step_avg:40.07ms
step:904/2330 train_time:36224ms step_avg:40.07ms
step:905/2330 train_time:36260ms step_avg:40.07ms
step:906/2330 train_time:36304ms step_avg:40.07ms
step:907/2330 train_time:36339ms step_avg:40.07ms
step:908/2330 train_time:36384ms step_avg:40.07ms
step:909/2330 train_time:36420ms step_avg:40.07ms
step:910/2330 train_time:36464ms step_avg:40.07ms
step:911/2330 train_time:36500ms step_avg:40.07ms
step:912/2330 train_time:36544ms step_avg:40.07ms
step:913/2330 train_time:36580ms step_avg:40.07ms
step:914/2330 train_time:36624ms step_avg:40.07ms
step:915/2330 train_time:36659ms step_avg:40.06ms
step:916/2330 train_time:36704ms step_avg:40.07ms
step:917/2330 train_time:36739ms step_avg:40.06ms
step:918/2330 train_time:36784ms step_avg:40.07ms
step:919/2330 train_time:36819ms step_avg:40.06ms
step:920/2330 train_time:36863ms step_avg:40.07ms
step:921/2330 train_time:36899ms step_avg:40.06ms
step:922/2330 train_time:36943ms step_avg:40.07ms
step:923/2330 train_time:36979ms step_avg:40.06ms
step:924/2330 train_time:37024ms step_avg:40.07ms
step:925/2330 train_time:37060ms step_avg:40.06ms
step:926/2330 train_time:37104ms step_avg:40.07ms
step:927/2330 train_time:37140ms step_avg:40.06ms
step:928/2330 train_time:37184ms step_avg:40.07ms
step:929/2330 train_time:37220ms step_avg:40.06ms
step:930/2330 train_time:37265ms step_avg:40.07ms
step:931/2330 train_time:37300ms step_avg:40.06ms
step:932/2330 train_time:37344ms step_avg:40.07ms
step:933/2330 train_time:37380ms step_avg:40.06ms
step:934/2330 train_time:37424ms step_avg:40.07ms
step:935/2330 train_time:37460ms step_avg:40.06ms
step:936/2330 train_time:37505ms step_avg:40.07ms
step:937/2330 train_time:37541ms step_avg:40.06ms
step:938/2330 train_time:37585ms step_avg:40.07ms
step:939/2330 train_time:37620ms step_avg:40.06ms
step:940/2330 train_time:37664ms step_avg:40.07ms
step:941/2330 train_time:37700ms step_avg:40.06ms
step:942/2330 train_time:37744ms step_avg:40.07ms
step:943/2330 train_time:37780ms step_avg:40.06ms
step:944/2330 train_time:37824ms step_avg:40.07ms
step:945/2330 train_time:37860ms step_avg:40.06ms
step:946/2330 train_time:37904ms step_avg:40.07ms
step:947/2330 train_time:37939ms step_avg:40.06ms
step:948/2330 train_time:37984ms step_avg:40.07ms
step:949/2330 train_time:38019ms step_avg:40.06ms
step:950/2330 train_time:38064ms step_avg:40.07ms
step:951/2330 train_time:38099ms step_avg:40.06ms
step:952/2330 train_time:38144ms step_avg:40.07ms
step:953/2330 train_time:38179ms step_avg:40.06ms
step:954/2330 train_time:38224ms step_avg:40.07ms
step:955/2330 train_time:38259ms step_avg:40.06ms
step:956/2330 train_time:38304ms step_avg:40.07ms
step:957/2330 train_time:38339ms step_avg:40.06ms
step:958/2330 train_time:38384ms step_avg:40.07ms
step:959/2330 train_time:38420ms step_avg:40.06ms
step:960/2330 train_time:38464ms step_avg:40.07ms
step:961/2330 train_time:38499ms step_avg:40.06ms
step:962/2330 train_time:38544ms step_avg:40.07ms
step:963/2330 train_time:38580ms step_avg:40.06ms
step:964/2330 train_time:38625ms step_avg:40.07ms
step:965/2330 train_time:38660ms step_avg:40.06ms
step:966/2330 train_time:38704ms step_avg:40.07ms
step:967/2330 train_time:38740ms step_avg:40.06ms
step:968/2330 train_time:38784ms step_avg:40.07ms
step:969/2330 train_time:38819ms step_avg:40.06ms
step:970/2330 train_time:38864ms step_avg:40.07ms
step:971/2330 train_time:38899ms step_avg:40.06ms
step:972/2330 train_time:38944ms step_avg:40.07ms
step:973/2330 train_time:38980ms step_avg:40.06ms
step:974/2330 train_time:39024ms step_avg:40.07ms
step:975/2330 train_time:39060ms step_avg:40.06ms
step:976/2330 train_time:39105ms step_avg:40.07ms
step:977/2330 train_time:39140ms step_avg:40.06ms
step:978/2330 train_time:39184ms step_avg:40.07ms
step:979/2330 train_time:39220ms step_avg:40.06ms
step:980/2330 train_time:39264ms step_avg:40.07ms
step:981/2330 train_time:39300ms step_avg:40.06ms
step:982/2330 train_time:39344ms step_avg:40.07ms
step:983/2330 train_time:39379ms step_avg:40.06ms
step:984/2330 train_time:39424ms step_avg:40.06ms
step:985/2330 train_time:39459ms step_avg:40.06ms
step:986/2330 train_time:39504ms step_avg:40.06ms
step:987/2330 train_time:39540ms step_avg:40.06ms
step:988/2330 train_time:39584ms step_avg:40.07ms
step:989/2330 train_time:39620ms step_avg:40.06ms
step:990/2330 train_time:39664ms step_avg:40.07ms
step:991/2330 train_time:39700ms step_avg:40.06ms
step:992/2330 train_time:39745ms step_avg:40.07ms
step:993/2330 train_time:39780ms step_avg:40.06ms
step:994/2330 train_time:39825ms step_avg:40.07ms
step:995/2330 train_time:39860ms step_avg:40.06ms
step:996/2330 train_time:39904ms step_avg:40.06ms
step:997/2330 train_time:39939ms step_avg:40.06ms
step:998/2330 train_time:39984ms step_avg:40.06ms
step:999/2330 train_time:40020ms step_avg:40.06ms
step:1000/2330 train_time:40064ms step_avg:40.06ms
step:1000/2330 val_loss:5.1865 train_time:40153ms step_avg:40.15ms
step:1001/2330 train_time:40166ms step_avg:40.13ms
step:1002/2330 train_time:40179ms step_avg:40.10ms
step:1003/2330 train_time:40189ms step_avg:40.07ms
step:1004/2330 train_time:40226ms step_avg:40.07ms
step:1005/2330 train_time:40261ms step_avg:40.06ms
step:1006/2330 train_time:40304ms step_avg:40.06ms
step:1007/2330 train_time:40338ms step_avg:40.06ms
step:1008/2330 train_time:40382ms step_avg:40.06ms
step:1009/2330 train_time:40416ms step_avg:40.06ms
step:1010/2330 train_time:40461ms step_avg:40.06ms
step:1011/2330 train_time:40500ms step_avg:40.06ms
step:1012/2330 train_time:40547ms step_avg:40.07ms
step:1013/2330 train_time:40583ms step_avg:40.06ms
step:1014/2330 train_time:40628ms step_avg:40.07ms
step:1015/2330 train_time:40665ms step_avg:40.06ms
step:1016/2330 train_time:40709ms step_avg:40.07ms
step:1017/2330 train_time:40744ms step_avg:40.06ms
step:1018/2330 train_time:40788ms step_avg:40.07ms
step:1019/2330 train_time:40823ms step_avg:40.06ms
step:1020/2330 train_time:40867ms step_avg:40.07ms
step:1021/2330 train_time:40902ms step_avg:40.06ms
step:1022/2330 train_time:40946ms step_avg:40.06ms
step:1023/2330 train_time:40981ms step_avg:40.06ms
step:1024/2330 train_time:41025ms step_avg:40.06ms
step:1025/2330 train_time:41060ms step_avg:40.06ms
step:1026/2330 train_time:41104ms step_avg:40.06ms
step:1027/2330 train_time:41139ms step_avg:40.06ms
step:1028/2330 train_time:41183ms step_avg:40.06ms
step:1029/2330 train_time:41218ms step_avg:40.06ms
step:1030/2330 train_time:41262ms step_avg:40.06ms
step:1031/2330 train_time:41296ms step_avg:40.05ms
step:1032/2330 train_time:41340ms step_avg:40.06ms
step:1033/2330 train_time:41375ms step_avg:40.05ms
step:1034/2330 train_time:41420ms step_avg:40.06ms
step:1035/2330 train_time:41457ms step_avg:40.06ms
step:1036/2330 train_time:41503ms step_avg:40.06ms
step:1037/2330 train_time:41538ms step_avg:40.06ms
step:1038/2330 train_time:41584ms step_avg:40.06ms
step:1039/2330 train_time:41620ms step_avg:40.06ms
step:1040/2330 train_time:41665ms step_avg:40.06ms
step:1041/2330 train_time:41700ms step_avg:40.06ms
step:1042/2330 train_time:41745ms step_avg:40.06ms
step:1043/2330 train_time:41779ms step_avg:40.06ms
step:1044/2330 train_time:41823ms step_avg:40.06ms
step:1045/2330 train_time:41858ms step_avg:40.06ms
step:1046/2330 train_time:41903ms step_avg:40.06ms
step:1047/2330 train_time:41938ms step_avg:40.06ms
step:1048/2330 train_time:41981ms step_avg:40.06ms
step:1049/2330 train_time:42016ms step_avg:40.05ms
step:1050/2330 train_time:42061ms step_avg:40.06ms
step:1051/2330 train_time:42095ms step_avg:40.05ms
step:1052/2330 train_time:42140ms step_avg:40.06ms
step:1053/2330 train_time:42174ms step_avg:40.05ms
step:1054/2330 train_time:42219ms step_avg:40.06ms
step:1055/2330 train_time:42254ms step_avg:40.05ms
step:1056/2330 train_time:42297ms step_avg:40.05ms
step:1057/2330 train_time:42332ms step_avg:40.05ms
step:1058/2330 train_time:42376ms step_avg:40.05ms
step:1059/2330 train_time:42412ms step_avg:40.05ms
step:1060/2330 train_time:42457ms step_avg:40.05ms
step:1061/2330 train_time:42493ms step_avg:40.05ms
step:1062/2330 train_time:42537ms step_avg:40.05ms
step:1063/2330 train_time:42573ms step_avg:40.05ms
step:1064/2330 train_time:42619ms step_avg:40.06ms
step:1065/2330 train_time:42655ms step_avg:40.05ms
step:1066/2330 train_time:42700ms step_avg:40.06ms
step:1067/2330 train_time:42735ms step_avg:40.05ms
step:1068/2330 train_time:42779ms step_avg:40.06ms
step:1069/2330 train_time:42815ms step_avg:40.05ms
step:1070/2330 train_time:42859ms step_avg:40.05ms
step:1071/2330 train_time:42894ms step_avg:40.05ms
step:1072/2330 train_time:42938ms step_avg:40.05ms
step:1073/2330 train_time:42973ms step_avg:40.05ms
step:1074/2330 train_time:43018ms step_avg:40.05ms
step:1075/2330 train_time:43053ms step_avg:40.05ms
step:1076/2330 train_time:43096ms step_avg:40.05ms
step:1077/2330 train_time:43131ms step_avg:40.05ms
step:1078/2330 train_time:43175ms step_avg:40.05ms
step:1079/2330 train_time:43209ms step_avg:40.05ms
step:1080/2330 train_time:43253ms step_avg:40.05ms
step:1081/2330 train_time:43288ms step_avg:40.04ms
step:1082/2330 train_time:43331ms step_avg:40.05ms
step:1083/2330 train_time:43367ms step_avg:40.04ms
step:1084/2330 train_time:43411ms step_avg:40.05ms
step:1085/2330 train_time:43447ms step_avg:40.04ms
step:1086/2330 train_time:43492ms step_avg:40.05ms
step:1087/2330 train_time:43527ms step_avg:40.04ms
step:1088/2330 train_time:43572ms step_avg:40.05ms
step:1089/2330 train_time:43608ms step_avg:40.04ms
step:1090/2330 train_time:43653ms step_avg:40.05ms
step:1091/2330 train_time:43689ms step_avg:40.05ms
step:1092/2330 train_time:43734ms step_avg:40.05ms
step:1093/2330 train_time:43769ms step_avg:40.05ms
step:1094/2330 train_time:43813ms step_avg:40.05ms
step:1095/2330 train_time:43848ms step_avg:40.04ms
step:1096/2330 train_time:43893ms step_avg:40.05ms
step:1097/2330 train_time:43928ms step_avg:40.04ms
step:1098/2330 train_time:43972ms step_avg:40.05ms
step:1099/2330 train_time:44007ms step_avg:40.04ms
step:1100/2330 train_time:44051ms step_avg:40.05ms
step:1101/2330 train_time:44087ms step_avg:40.04ms
step:1102/2330 train_time:44131ms step_avg:40.05ms
step:1103/2330 train_time:44166ms step_avg:40.04ms
step:1104/2330 train_time:44210ms step_avg:40.05ms
step:1105/2330 train_time:44245ms step_avg:40.04ms
step:1106/2330 train_time:44289ms step_avg:40.04ms
step:1107/2330 train_time:44325ms step_avg:40.04ms
step:1108/2330 train_time:44369ms step_avg:40.04ms
step:1109/2330 train_time:44404ms step_avg:40.04ms
step:1110/2330 train_time:44448ms step_avg:40.04ms
step:1111/2330 train_time:44484ms step_avg:40.04ms
step:1112/2330 train_time:44528ms step_avg:40.04ms
step:1113/2330 train_time:44564ms step_avg:40.04ms
step:1114/2330 train_time:44609ms step_avg:40.04ms
step:1115/2330 train_time:44644ms step_avg:40.04ms
step:1116/2330 train_time:44688ms step_avg:40.04ms
step:1117/2330 train_time:44723ms step_avg:40.04ms
step:1118/2330 train_time:44768ms step_avg:40.04ms
step:1119/2330 train_time:44804ms step_avg:40.04ms
step:1120/2330 train_time:44848ms step_avg:40.04ms
step:1121/2330 train_time:44884ms step_avg:40.04ms
step:1122/2330 train_time:44928ms step_avg:40.04ms
step:1123/2330 train_time:44963ms step_avg:40.04ms
step:1124/2330 train_time:45008ms step_avg:40.04ms
step:1125/2330 train_time:45043ms step_avg:40.04ms
step:1126/2330 train_time:45087ms step_avg:40.04ms
step:1127/2330 train_time:45122ms step_avg:40.04ms
step:1128/2330 train_time:45166ms step_avg:40.04ms
step:1129/2330 train_time:45201ms step_avg:40.04ms
step:1130/2330 train_time:45245ms step_avg:40.04ms
step:1131/2330 train_time:45280ms step_avg:40.04ms
step:1132/2330 train_time:45325ms step_avg:40.04ms
step:1133/2330 train_time:45361ms step_avg:40.04ms
step:1134/2330 train_time:45405ms step_avg:40.04ms
step:1135/2330 train_time:45440ms step_avg:40.04ms
step:1136/2330 train_time:45484ms step_avg:40.04ms
step:1137/2330 train_time:45519ms step_avg:40.03ms
step:1138/2330 train_time:45563ms step_avg:40.04ms
step:1139/2330 train_time:45599ms step_avg:40.03ms
step:1140/2330 train_time:45643ms step_avg:40.04ms
step:1141/2330 train_time:45678ms step_avg:40.03ms
step:1142/2330 train_time:45724ms step_avg:40.04ms
step:1143/2330 train_time:45759ms step_avg:40.03ms
step:1144/2330 train_time:45803ms step_avg:40.04ms
step:1145/2330 train_time:45839ms step_avg:40.03ms
step:1146/2330 train_time:45883ms step_avg:40.04ms
step:1147/2330 train_time:45918ms step_avg:40.03ms
step:1148/2330 train_time:45962ms step_avg:40.04ms
step:1149/2330 train_time:45997ms step_avg:40.03ms
step:1150/2330 train_time:46042ms step_avg:40.04ms
step:1151/2330 train_time:46076ms step_avg:40.03ms
step:1152/2330 train_time:46121ms step_avg:40.04ms
step:1153/2330 train_time:46156ms step_avg:40.03ms
step:1154/2330 train_time:46200ms step_avg:40.03ms
step:1155/2330 train_time:46234ms step_avg:40.03ms
step:1156/2330 train_time:46279ms step_avg:40.03ms
step:1157/2330 train_time:46315ms step_avg:40.03ms
step:1158/2330 train_time:46360ms step_avg:40.03ms
step:1159/2330 train_time:46396ms step_avg:40.03ms
step:1160/2330 train_time:46441ms step_avg:40.04ms
step:1161/2330 train_time:46476ms step_avg:40.03ms
step:1162/2330 train_time:46521ms step_avg:40.04ms
step:1163/2330 train_time:46556ms step_avg:40.03ms
step:1164/2330 train_time:46601ms step_avg:40.04ms
step:1165/2330 train_time:46637ms step_avg:40.03ms
step:1166/2330 train_time:46681ms step_avg:40.04ms
step:1167/2330 train_time:46716ms step_avg:40.03ms
step:1168/2330 train_time:46761ms step_avg:40.03ms
step:1169/2330 train_time:46797ms step_avg:40.03ms
step:1170/2330 train_time:46841ms step_avg:40.04ms
step:1171/2330 train_time:46876ms step_avg:40.03ms
step:1172/2330 train_time:46920ms step_avg:40.03ms
step:1173/2330 train_time:46955ms step_avg:40.03ms
step:1174/2330 train_time:46999ms step_avg:40.03ms
step:1175/2330 train_time:47034ms step_avg:40.03ms
step:1176/2330 train_time:47078ms step_avg:40.03ms
step:1177/2330 train_time:47112ms step_avg:40.03ms
step:1178/2330 train_time:47157ms step_avg:40.03ms
step:1179/2330 train_time:47191ms step_avg:40.03ms
step:1180/2330 train_time:47237ms step_avg:40.03ms
step:1181/2330 train_time:47272ms step_avg:40.03ms
step:1182/2330 train_time:47316ms step_avg:40.03ms
step:1183/2330 train_time:47350ms step_avg:40.03ms
step:1184/2330 train_time:47396ms step_avg:40.03ms
step:1185/2330 train_time:47431ms step_avg:40.03ms
step:1186/2330 train_time:47475ms step_avg:40.03ms
step:1187/2330 train_time:47510ms step_avg:40.03ms
step:1188/2330 train_time:47555ms step_avg:40.03ms
step:1189/2330 train_time:47590ms step_avg:40.03ms
step:1190/2330 train_time:47634ms step_avg:40.03ms
step:1191/2330 train_time:47669ms step_avg:40.02ms
step:1192/2330 train_time:47714ms step_avg:40.03ms
step:1193/2330 train_time:47750ms step_avg:40.02ms
step:1194/2330 train_time:47795ms step_avg:40.03ms
step:1195/2330 train_time:47829ms step_avg:40.02ms
step:1196/2330 train_time:47873ms step_avg:40.03ms
step:1197/2330 train_time:47908ms step_avg:40.02ms
step:1198/2330 train_time:47952ms step_avg:40.03ms
step:1199/2330 train_time:47987ms step_avg:40.02ms
step:1200/2330 train_time:48032ms step_avg:40.03ms
step:1201/2330 train_time:48067ms step_avg:40.02ms
step:1202/2330 train_time:48111ms step_avg:40.03ms
step:1203/2330 train_time:48146ms step_avg:40.02ms
step:1204/2330 train_time:48191ms step_avg:40.03ms
step:1205/2330 train_time:48226ms step_avg:40.02ms
step:1206/2330 train_time:48271ms step_avg:40.03ms
step:1207/2330 train_time:48306ms step_avg:40.02ms
step:1208/2330 train_time:48351ms step_avg:40.03ms
step:1209/2330 train_time:48386ms step_avg:40.02ms
step:1210/2330 train_time:48431ms step_avg:40.03ms
step:1211/2330 train_time:48466ms step_avg:40.02ms
step:1212/2330 train_time:48511ms step_avg:40.03ms
step:1213/2330 train_time:48546ms step_avg:40.02ms
step:1214/2330 train_time:48591ms step_avg:40.03ms
step:1215/2330 train_time:48626ms step_avg:40.02ms
step:1216/2330 train_time:48671ms step_avg:40.03ms
step:1217/2330 train_time:48706ms step_avg:40.02ms
step:1218/2330 train_time:48751ms step_avg:40.03ms
step:1219/2330 train_time:48787ms step_avg:40.02ms
step:1220/2330 train_time:48832ms step_avg:40.03ms
step:1221/2330 train_time:48867ms step_avg:40.02ms
step:1222/2330 train_time:48911ms step_avg:40.03ms
step:1223/2330 train_time:48947ms step_avg:40.02ms
step:1224/2330 train_time:48991ms step_avg:40.03ms
step:1225/2330 train_time:49027ms step_avg:40.02ms
step:1226/2330 train_time:49071ms step_avg:40.03ms
step:1227/2330 train_time:49106ms step_avg:40.02ms
step:1228/2330 train_time:49151ms step_avg:40.03ms
step:1229/2330 train_time:49186ms step_avg:40.02ms
step:1230/2330 train_time:49230ms step_avg:40.02ms
step:1231/2330 train_time:49266ms step_avg:40.02ms
step:1232/2330 train_time:49310ms step_avg:40.02ms
step:1233/2330 train_time:49346ms step_avg:40.02ms
step:1234/2330 train_time:49390ms step_avg:40.02ms
step:1235/2330 train_time:49426ms step_avg:40.02ms
step:1236/2330 train_time:49470ms step_avg:40.02ms
step:1237/2330 train_time:49505ms step_avg:40.02ms
step:1238/2330 train_time:49550ms step_avg:40.02ms
step:1239/2330 train_time:49586ms step_avg:40.02ms
step:1240/2330 train_time:49631ms step_avg:40.02ms
step:1241/2330 train_time:49666ms step_avg:40.02ms
step:1242/2330 train_time:49710ms step_avg:40.02ms
step:1243/2330 train_time:49747ms step_avg:40.02ms
step:1244/2330 train_time:49791ms step_avg:40.03ms
step:1245/2330 train_time:49827ms step_avg:40.02ms
step:1246/2330 train_time:49871ms step_avg:40.02ms
step:1247/2330 train_time:49906ms step_avg:40.02ms
step:1248/2330 train_time:49951ms step_avg:40.02ms
step:1249/2330 train_time:49986ms step_avg:40.02ms
step:1250/2330 train_time:50031ms step_avg:40.02ms
step:1250/2330 val_loss:5.1617 train_time:50118ms step_avg:40.09ms
step:1251/2330 train_time:50131ms step_avg:40.07ms
step:1252/2330 train_time:50144ms step_avg:40.05ms
step:1253/2330 train_time:50155ms step_avg:40.03ms
step:1254/2330 train_time:50191ms step_avg:40.02ms
step:1255/2330 train_time:50225ms step_avg:40.02ms
step:1256/2330 train_time:50269ms step_avg:40.02ms
step:1257/2330 train_time:50303ms step_avg:40.02ms
step:1258/2330 train_time:50347ms step_avg:40.02ms
step:1259/2330 train_time:50382ms step_avg:40.02ms
step:1260/2330 train_time:50428ms step_avg:40.02ms
step:1261/2330 train_time:50467ms step_avg:40.02ms
step:1262/2330 train_time:50513ms step_avg:40.03ms
step:1263/2330 train_time:50549ms step_avg:40.02ms
step:1264/2330 train_time:50593ms step_avg:40.03ms
step:1265/2330 train_time:50629ms step_avg:40.02ms
step:1266/2330 train_time:50673ms step_avg:40.03ms
step:1267/2330 train_time:50709ms step_avg:40.02ms
step:1268/2330 train_time:50980ms step_avg:40.21ms
step:1269/2330 train_time:50993ms step_avg:40.18ms
step:1270/2330 train_time:51005ms step_avg:40.16ms
step:1271/2330 train_time:51033ms step_avg:40.15ms
step:1272/2330 train_time:51076ms step_avg:40.15ms
step:1273/2330 train_time:51110ms step_avg:40.15ms
step:1274/2330 train_time:51154ms step_avg:40.15ms
step:1275/2330 train_time:51188ms step_avg:40.15ms
step:1276/2330 train_time:51232ms step_avg:40.15ms
step:1277/2330 train_time:51266ms step_avg:40.15ms
step:1278/2330 train_time:51310ms step_avg:40.15ms
step:1279/2330 train_time:51345ms step_avg:40.14ms
step:1280/2330 train_time:51388ms step_avg:40.15ms
step:1281/2330 train_time:51423ms step_avg:40.14ms
step:1282/2330 train_time:51467ms step_avg:40.15ms
step:1283/2330 train_time:51502ms step_avg:40.14ms
step:1284/2330 train_time:51545ms step_avg:40.14ms
step:1285/2330 train_time:51579ms step_avg:40.14ms
step:1286/2330 train_time:51623ms step_avg:40.14ms
step:1287/2330 train_time:51657ms step_avg:40.14ms
step:1288/2330 train_time:51701ms step_avg:40.14ms
step:1289/2330 train_time:51735ms step_avg:40.14ms
step:1290/2330 train_time:51778ms step_avg:40.14ms
step:1291/2330 train_time:51812ms step_avg:40.13ms
step:1292/2330 train_time:51857ms step_avg:40.14ms
step:1293/2330 train_time:51894ms step_avg:40.13ms
step:1294/2330 train_time:51942ms step_avg:40.14ms
step:1295/2330 train_time:51981ms step_avg:40.14ms
step:1296/2330 train_time:52028ms step_avg:40.15ms
step:1297/2330 train_time:52065ms step_avg:40.14ms
step:1298/2330 train_time:52111ms step_avg:40.15ms
step:1299/2330 train_time:52145ms step_avg:40.14ms
step:1300/2330 train_time:52189ms step_avg:40.15ms
step:1301/2330 train_time:52224ms step_avg:40.14ms
step:1302/2330 train_time:52268ms step_avg:40.14ms
step:1303/2330 train_time:52303ms step_avg:40.14ms
step:1304/2330 train_time:52346ms step_avg:40.14ms
step:1305/2330 train_time:52381ms step_avg:40.14ms
step:1306/2330 train_time:52425ms step_avg:40.14ms
step:1307/2330 train_time:52459ms step_avg:40.14ms
step:1308/2330 train_time:52503ms step_avg:40.14ms
step:1309/2330 train_time:52538ms step_avg:40.14ms
step:1310/2330 train_time:52581ms step_avg:40.14ms
step:1311/2330 train_time:52616ms step_avg:40.13ms
step:1312/2330 train_time:52660ms step_avg:40.14ms
step:1313/2330 train_time:52695ms step_avg:40.13ms
step:1314/2330 train_time:52738ms step_avg:40.14ms
step:1315/2330 train_time:52772ms step_avg:40.13ms
step:1316/2330 train_time:52816ms step_avg:40.13ms
step:1317/2330 train_time:52851ms step_avg:40.13ms
step:1318/2330 train_time:52897ms step_avg:40.13ms
step:1319/2330 train_time:52933ms step_avg:40.13ms
step:1320/2330 train_time:52980ms step_avg:40.14ms
step:1321/2330 train_time:53018ms step_avg:40.13ms
step:1322/2330 train_time:53063ms step_avg:40.14ms
step:1323/2330 train_time:53099ms step_avg:40.14ms
step:1324/2330 train_time:53144ms step_avg:40.14ms
step:1325/2330 train_time:53179ms step_avg:40.14ms
step:1326/2330 train_time:53224ms step_avg:40.14ms
step:1327/2330 train_time:53259ms step_avg:40.14ms
step:1328/2330 train_time:53303ms step_avg:40.14ms
step:1329/2330 train_time:53337ms step_avg:40.13ms
step:1330/2330 train_time:53381ms step_avg:40.14ms
step:1331/2330 train_time:53417ms step_avg:40.13ms
step:1332/2330 train_time:53460ms step_avg:40.14ms
step:1333/2330 train_time:53495ms step_avg:40.13ms
step:1334/2330 train_time:53538ms step_avg:40.13ms
step:1335/2330 train_time:53573ms step_avg:40.13ms
step:1336/2330 train_time:53616ms step_avg:40.13ms
step:1337/2330 train_time:53651ms step_avg:40.13ms
step:1338/2330 train_time:53695ms step_avg:40.13ms
step:1339/2330 train_time:53730ms step_avg:40.13ms
step:1340/2330 train_time:53773ms step_avg:40.13ms
step:1341/2330 train_time:53809ms step_avg:40.13ms
step:1342/2330 train_time:53854ms step_avg:40.13ms
step:1343/2330 train_time:53890ms step_avg:40.13ms
step:1344/2330 train_time:53935ms step_avg:40.13ms
step:1345/2330 train_time:53971ms step_avg:40.13ms
step:1346/2330 train_time:54017ms step_avg:40.13ms
step:1347/2330 train_time:54053ms step_avg:40.13ms
step:1348/2330 train_time:54098ms step_avg:40.13ms
step:1349/2330 train_time:54134ms step_avg:40.13ms
step:1350/2330 train_time:54179ms step_avg:40.13ms
step:1351/2330 train_time:54215ms step_avg:40.13ms
step:1352/2330 train_time:54260ms step_avg:40.13ms
step:1353/2330 train_time:54294ms step_avg:40.13ms
step:1354/2330 train_time:54339ms step_avg:40.13ms
step:1355/2330 train_time:54373ms step_avg:40.13ms
step:1356/2330 train_time:54418ms step_avg:40.13ms
step:1357/2330 train_time:54453ms step_avg:40.13ms
step:1358/2330 train_time:54497ms step_avg:40.13ms
step:1359/2330 train_time:54531ms step_avg:40.13ms
step:1360/2330 train_time:54575ms step_avg:40.13ms
step:1361/2330 train_time:54610ms step_avg:40.12ms
step:1362/2330 train_time:54654ms step_avg:40.13ms
step:1363/2330 train_time:54688ms step_avg:40.12ms
step:1364/2330 train_time:54733ms step_avg:40.13ms
step:1365/2330 train_time:54767ms step_avg:40.12ms
step:1366/2330 train_time:54812ms step_avg:40.13ms
step:1367/2330 train_time:54847ms step_avg:40.12ms
step:1368/2330 train_time:54893ms step_avg:40.13ms
step:1369/2330 train_time:54928ms step_avg:40.12ms
step:1370/2330 train_time:54972ms step_avg:40.13ms
step:1371/2330 train_time:55008ms step_avg:40.12ms
step:1372/2330 train_time:55053ms step_avg:40.13ms
step:1373/2330 train_time:55089ms step_avg:40.12ms
step:1374/2330 train_time:55134ms step_avg:40.13ms
step:1375/2330 train_time:55169ms step_avg:40.12ms
step:1376/2330 train_time:55214ms step_avg:40.13ms
step:1377/2330 train_time:55250ms step_avg:40.12ms
step:1378/2330 train_time:55295ms step_avg:40.13ms
step:1379/2330 train_time:55330ms step_avg:40.12ms
step:1380/2330 train_time:55374ms step_avg:40.13ms
step:1381/2330 train_time:55409ms step_avg:40.12ms
step:1382/2330 train_time:55453ms step_avg:40.13ms
step:1383/2330 train_time:55488ms step_avg:40.12ms
step:1384/2330 train_time:55532ms step_avg:40.12ms
step:1385/2330 train_time:55567ms step_avg:40.12ms
step:1386/2330 train_time:55612ms step_avg:40.12ms
step:1387/2330 train_time:55646ms step_avg:40.12ms
step:1388/2330 train_time:55690ms step_avg:40.12ms
step:1389/2330 train_time:55725ms step_avg:40.12ms
step:1390/2330 train_time:55770ms step_avg:40.12ms
step:1391/2330 train_time:55805ms step_avg:40.12ms
step:1392/2330 train_time:55849ms step_avg:40.12ms
step:1393/2330 train_time:55884ms step_avg:40.12ms
step:1394/2330 train_time:55930ms step_avg:40.12ms
step:1395/2330 train_time:55965ms step_avg:40.12ms
step:1396/2330 train_time:56010ms step_avg:40.12ms
step:1397/2330 train_time:56045ms step_avg:40.12ms
step:1398/2330 train_time:56090ms step_avg:40.12ms
step:1399/2330 train_time:56126ms step_avg:40.12ms
step:1400/2330 train_time:56170ms step_avg:40.12ms
step:1401/2330 train_time:56205ms step_avg:40.12ms
step:1402/2330 train_time:56250ms step_avg:40.12ms
step:1403/2330 train_time:56286ms step_avg:40.12ms
step:1404/2330 train_time:56330ms step_avg:40.12ms
step:1405/2330 train_time:56365ms step_avg:40.12ms
step:1406/2330 train_time:56409ms step_avg:40.12ms
step:1407/2330 train_time:56444ms step_avg:40.12ms
step:1408/2330 train_time:56488ms step_avg:40.12ms
step:1409/2330 train_time:56522ms step_avg:40.12ms
step:1410/2330 train_time:56566ms step_avg:40.12ms
step:1411/2330 train_time:56601ms step_avg:40.11ms
step:1412/2330 train_time:56646ms step_avg:40.12ms
step:1413/2330 train_time:56681ms step_avg:40.11ms
step:1414/2330 train_time:56724ms step_avg:40.12ms
step:1415/2330 train_time:56760ms step_avg:40.11ms
step:1416/2330 train_time:56804ms step_avg:40.12ms
step:1417/2330 train_time:56839ms step_avg:40.11ms
step:1418/2330 train_time:56883ms step_avg:40.11ms
step:1419/2330 train_time:56919ms step_avg:40.11ms
step:1420/2330 train_time:56964ms step_avg:40.12ms
step:1421/2330 train_time:56999ms step_avg:40.11ms
step:1422/2330 train_time:57044ms step_avg:40.12ms
step:1423/2330 train_time:57081ms step_avg:40.11ms
step:1424/2330 train_time:57125ms step_avg:40.12ms
step:1425/2330 train_time:57161ms step_avg:40.11ms
step:1426/2330 train_time:57206ms step_avg:40.12ms
step:1427/2330 train_time:57241ms step_avg:40.11ms
step:1428/2330 train_time:57285ms step_avg:40.12ms
step:1429/2330 train_time:57320ms step_avg:40.11ms
step:1430/2330 train_time:57365ms step_avg:40.12ms
step:1431/2330 train_time:57400ms step_avg:40.11ms
step:1432/2330 train_time:57444ms step_avg:40.11ms
step:1433/2330 train_time:57479ms step_avg:40.11ms
step:1434/2330 train_time:57522ms step_avg:40.11ms
step:1435/2330 train_time:57558ms step_avg:40.11ms
step:1436/2330 train_time:57602ms step_avg:40.11ms
step:1437/2330 train_time:57636ms step_avg:40.11ms
step:1438/2330 train_time:57680ms step_avg:40.11ms
step:1439/2330 train_time:57715ms step_avg:40.11ms
step:1440/2330 train_time:57759ms step_avg:40.11ms
step:1441/2330 train_time:57793ms step_avg:40.11ms
step:1442/2330 train_time:57838ms step_avg:40.11ms
step:1443/2330 train_time:57872ms step_avg:40.11ms
step:1444/2330 train_time:57917ms step_avg:40.11ms
step:1445/2330 train_time:57953ms step_avg:40.11ms
step:1446/2330 train_time:57997ms step_avg:40.11ms
step:1447/2330 train_time:58032ms step_avg:40.11ms
step:1448/2330 train_time:58077ms step_avg:40.11ms
step:1449/2330 train_time:58113ms step_avg:40.11ms
step:1450/2330 train_time:58158ms step_avg:40.11ms
step:1451/2330 train_time:58193ms step_avg:40.11ms
step:1452/2330 train_time:58237ms step_avg:40.11ms
step:1453/2330 train_time:58273ms step_avg:40.11ms
step:1454/2330 train_time:58318ms step_avg:40.11ms
step:1455/2330 train_time:58353ms step_avg:40.11ms
step:1456/2330 train_time:58397ms step_avg:40.11ms
step:1457/2330 train_time:58433ms step_avg:40.10ms
step:1458/2330 train_time:58477ms step_avg:40.11ms
step:1459/2330 train_time:58512ms step_avg:40.10ms
step:1460/2330 train_time:58556ms step_avg:40.11ms
step:1461/2330 train_time:58592ms step_avg:40.10ms
step:1462/2330 train_time:58636ms step_avg:40.11ms
step:1463/2330 train_time:58671ms step_avg:40.10ms
step:1464/2330 train_time:58716ms step_avg:40.11ms
step:1465/2330 train_time:58751ms step_avg:40.10ms
step:1466/2330 train_time:58795ms step_avg:40.11ms
step:1467/2330 train_time:58831ms step_avg:40.10ms
step:1468/2330 train_time:58876ms step_avg:40.11ms
step:1469/2330 train_time:58911ms step_avg:40.10ms
step:1470/2330 train_time:58955ms step_avg:40.11ms
step:1471/2330 train_time:58991ms step_avg:40.10ms
step:1472/2330 train_time:59036ms step_avg:40.11ms
step:1473/2330 train_time:59071ms step_avg:40.10ms
step:1474/2330 train_time:59116ms step_avg:40.11ms
step:1475/2330 train_time:59152ms step_avg:40.10ms
step:1476/2330 train_time:59197ms step_avg:40.11ms
step:1477/2330 train_time:59233ms step_avg:40.10ms
step:1478/2330 train_time:59277ms step_avg:40.11ms
step:1479/2330 train_time:59312ms step_avg:40.10ms
step:1480/2330 train_time:59357ms step_avg:40.11ms
step:1481/2330 train_time:59392ms step_avg:40.10ms
step:1482/2330 train_time:59436ms step_avg:40.11ms
step:1483/2330 train_time:59471ms step_avg:40.10ms
step:1484/2330 train_time:59516ms step_avg:40.10ms
step:1485/2330 train_time:59551ms step_avg:40.10ms
step:1486/2330 train_time:59596ms step_avg:40.10ms
step:1487/2330 train_time:59632ms step_avg:40.10ms
step:1488/2330 train_time:59676ms step_avg:40.10ms
step:1489/2330 train_time:59712ms step_avg:40.10ms
step:1490/2330 train_time:59756ms step_avg:40.10ms
step:1491/2330 train_time:59792ms step_avg:40.10ms
step:1492/2330 train_time:59836ms step_avg:40.10ms
step:1493/2330 train_time:59872ms step_avg:40.10ms
step:1494/2330 train_time:59916ms step_avg:40.10ms
step:1495/2330 train_time:59952ms step_avg:40.10ms
step:1496/2330 train_time:59996ms step_avg:40.10ms
step:1497/2330 train_time:60032ms step_avg:40.10ms
step:1498/2330 train_time:60077ms step_avg:40.10ms
step:1499/2330 train_time:60113ms step_avg:40.10ms
step:1500/2330 train_time:60157ms step_avg:40.10ms
step:1500/2330 val_loss:5.1435 train_time:60246ms step_avg:40.16ms
step:1501/2330 train_time:60260ms step_avg:40.15ms
step:1502/2330 train_time:60272ms step_avg:40.13ms
step:1503/2330 train_time:60283ms step_avg:40.11ms
step:1504/2330 train_time:60318ms step_avg:40.11ms
step:1505/2330 train_time:60352ms step_avg:40.10ms
step:1506/2330 train_time:60396ms step_avg:40.10ms
step:1507/2330 train_time:60430ms step_avg:40.10ms
step:1508/2330 train_time:60474ms step_avg:40.10ms
step:1509/2330 train_time:60509ms step_avg:40.10ms
step:1510/2330 train_time:60556ms step_avg:40.10ms
step:1511/2330 train_time:60595ms step_avg:40.10ms
step:1512/2330 train_time:60642ms step_avg:40.11ms
step:1513/2330 train_time:60678ms step_avg:40.10ms
step:1514/2330 train_time:60723ms step_avg:40.11ms
step:1515/2330 train_time:60759ms step_avg:40.10ms
step:1516/2330 train_time:60803ms step_avg:40.11ms
step:1517/2330 train_time:60838ms step_avg:40.10ms
step:1518/2330 train_time:60882ms step_avg:40.11ms
step:1519/2330 train_time:60917ms step_avg:40.10ms
step:1520/2330 train_time:61159ms step_avg:40.24ms
step:1521/2330 train_time:61192ms step_avg:40.23ms
step:1522/2330 train_time:61236ms step_avg:40.23ms
step:1523/2330 train_time:61270ms step_avg:40.23ms
step:1524/2330 train_time:61313ms step_avg:40.23ms
step:1525/2330 train_time:61348ms step_avg:40.23ms
step:1526/2330 train_time:61483ms step_avg:40.29ms
step:1527/2330 train_time:61516ms step_avg:40.29ms
step:1528/2330 train_time:61560ms step_avg:40.29ms
step:1529/2330 train_time:61595ms step_avg:40.28ms
step:1530/2330 train_time:61639ms step_avg:40.29ms
step:1531/2330 train_time:61672ms step_avg:40.28ms
step:1532/2330 train_time:61715ms step_avg:40.28ms
step:1533/2330 train_time:61750ms step_avg:40.28ms
step:1534/2330 train_time:61793ms step_avg:40.28ms
step:1535/2330 train_time:61828ms step_avg:40.28ms
step:1536/2330 train_time:61872ms step_avg:40.28ms
step:1537/2330 train_time:61906ms step_avg:40.28ms
step:1538/2330 train_time:61950ms step_avg:40.28ms
step:1539/2330 train_time:61984ms step_avg:40.28ms
step:1540/2330 train_time:62027ms step_avg:40.28ms
step:1541/2330 train_time:62062ms step_avg:40.27ms
step:1542/2330 train_time:62105ms step_avg:40.28ms
step:1543/2330 train_time:62140ms step_avg:40.27ms
step:1544/2330 train_time:62183ms step_avg:40.27ms
step:1545/2330 train_time:62218ms step_avg:40.27ms
step:1546/2330 train_time:62262ms step_avg:40.27ms
step:1547/2330 train_time:62300ms step_avg:40.27ms
step:1548/2330 train_time:62351ms step_avg:40.28ms
step:1549/2330 train_time:62392ms step_avg:40.28ms
step:1550/2330 train_time:62438ms step_avg:40.28ms
step:1551/2330 train_time:62475ms step_avg:40.28ms
step:1552/2330 train_time:62520ms step_avg:40.28ms
step:1553/2330 train_time:62554ms step_avg:40.28ms
step:1554/2330 train_time:62599ms step_avg:40.28ms
step:1555/2330 train_time:62633ms step_avg:40.28ms
step:1556/2330 train_time:62677ms step_avg:40.28ms
step:1557/2330 train_time:62712ms step_avg:40.28ms
step:1558/2330 train_time:62756ms step_avg:40.28ms
step:1559/2330 train_time:62791ms step_avg:40.28ms
step:1560/2330 train_time:62834ms step_avg:40.28ms
step:1561/2330 train_time:62868ms step_avg:40.27ms
step:1562/2330 train_time:62913ms step_avg:40.28ms
step:1563/2330 train_time:62947ms step_avg:40.27ms
step:1564/2330 train_time:62990ms step_avg:40.28ms
step:1565/2330 train_time:63025ms step_avg:40.27ms
step:1566/2330 train_time:63069ms step_avg:40.27ms
step:1567/2330 train_time:63104ms step_avg:40.27ms
step:1568/2330 train_time:63148ms step_avg:40.27ms
step:1569/2330 train_time:63182ms step_avg:40.27ms
step:1570/2330 train_time:63228ms step_avg:40.27ms
step:1571/2330 train_time:63264ms step_avg:40.27ms
step:1572/2330 train_time:63310ms step_avg:40.27ms
step:1573/2330 train_time:63347ms step_avg:40.27ms
step:1574/2330 train_time:63392ms step_avg:40.27ms
step:1575/2330 train_time:63428ms step_avg:40.27ms
step:1576/2330 train_time:63475ms step_avg:40.28ms
step:1577/2330 train_time:63511ms step_avg:40.27ms
step:1578/2330 train_time:63555ms step_avg:40.28ms
step:1579/2330 train_time:63590ms step_avg:40.27ms
step:1580/2330 train_time:63635ms step_avg:40.28ms
step:1581/2330 train_time:63670ms step_avg:40.27ms
step:1582/2330 train_time:63714ms step_avg:40.27ms
step:1583/2330 train_time:63749ms step_avg:40.27ms
step:1584/2330 train_time:63793ms step_avg:40.27ms
step:1585/2330 train_time:63827ms step_avg:40.27ms
step:1586/2330 train_time:63871ms step_avg:40.27ms
step:1587/2330 train_time:63905ms step_avg:40.27ms
step:1588/2330 train_time:63950ms step_avg:40.27ms
step:1589/2330 train_time:63985ms step_avg:40.27ms
step:1590/2330 train_time:64029ms step_avg:40.27ms
step:1591/2330 train_time:64064ms step_avg:40.27ms
step:1592/2330 train_time:64109ms step_avg:40.27ms
step:1593/2330 train_time:64143ms step_avg:40.27ms
step:1594/2330 train_time:64188ms step_avg:40.27ms
step:1595/2330 train_time:64222ms step_avg:40.26ms
step:1596/2330 train_time:64267ms step_avg:40.27ms
step:1597/2330 train_time:64302ms step_avg:40.26ms
step:1598/2330 train_time:64348ms step_avg:40.27ms
step:1599/2330 train_time:64383ms step_avg:40.26ms
step:1600/2330 train_time:64430ms step_avg:40.27ms
step:1601/2330 train_time:64466ms step_avg:40.27ms
step:1602/2330 train_time:64511ms step_avg:40.27ms
step:1603/2330 train_time:64546ms step_avg:40.27ms
step:1604/2330 train_time:64591ms step_avg:40.27ms
step:1605/2330 train_time:64626ms step_avg:40.27ms
step:1606/2330 train_time:64671ms step_avg:40.27ms
step:1607/2330 train_time:64705ms step_avg:40.26ms
step:1608/2330 train_time:64750ms step_avg:40.27ms
step:1609/2330 train_time:64784ms step_avg:40.26ms
step:1610/2330 train_time:64828ms step_avg:40.27ms
step:1611/2330 train_time:64864ms step_avg:40.26ms
step:1612/2330 train_time:64908ms step_avg:40.27ms
step:1613/2330 train_time:64942ms step_avg:40.26ms
step:1614/2330 train_time:64985ms step_avg:40.26ms
step:1615/2330 train_time:65019ms step_avg:40.26ms
step:1616/2330 train_time:65063ms step_avg:40.26ms
step:1617/2330 train_time:65098ms step_avg:40.26ms
step:1618/2330 train_time:65143ms step_avg:40.26ms
step:1619/2330 train_time:65178ms step_avg:40.26ms
step:1620/2330 train_time:65222ms step_avg:40.26ms
step:1621/2330 train_time:65257ms step_avg:40.26ms
step:1622/2330 train_time:65302ms step_avg:40.26ms
step:1623/2330 train_time:65338ms step_avg:40.26ms
step:1624/2330 train_time:65384ms step_avg:40.26ms
step:1625/2330 train_time:65419ms step_avg:40.26ms
step:1626/2330 train_time:65465ms step_avg:40.26ms
step:1627/2330 train_time:65501ms step_avg:40.26ms
step:1628/2330 train_time:65546ms step_avg:40.26ms
step:1629/2330 train_time:65582ms step_avg:40.26ms
step:1630/2330 train_time:65627ms step_avg:40.26ms
step:1631/2330 train_time:65663ms step_avg:40.26ms
step:1632/2330 train_time:65707ms step_avg:40.26ms
step:1633/2330 train_time:65742ms step_avg:40.26ms
step:1634/2330 train_time:65786ms step_avg:40.26ms
step:1635/2330 train_time:65820ms step_avg:40.26ms
step:1636/2330 train_time:65864ms step_avg:40.26ms
step:1637/2330 train_time:65900ms step_avg:40.26ms
step:1638/2330 train_time:65944ms step_avg:40.26ms
step:1639/2330 train_time:65978ms step_avg:40.26ms
step:1640/2330 train_time:66023ms step_avg:40.26ms
step:1641/2330 train_time:66058ms step_avg:40.25ms
step:1642/2330 train_time:66102ms step_avg:40.26ms
step:1643/2330 train_time:66137ms step_avg:40.25ms
step:1644/2330 train_time:66182ms step_avg:40.26ms
step:1645/2330 train_time:66217ms step_avg:40.25ms
step:1646/2330 train_time:66261ms step_avg:40.26ms
step:1647/2330 train_time:66297ms step_avg:40.25ms
step:1648/2330 train_time:66341ms step_avg:40.26ms
step:1649/2330 train_time:66377ms step_avg:40.25ms
step:1650/2330 train_time:66422ms step_avg:40.26ms
step:1651/2330 train_time:66458ms step_avg:40.25ms
step:1652/2330 train_time:66502ms step_avg:40.26ms
step:1653/2330 train_time:66538ms step_avg:40.25ms
step:1654/2330 train_time:66583ms step_avg:40.26ms
step:1655/2330 train_time:66618ms step_avg:40.25ms
step:1656/2330 train_time:66663ms step_avg:40.26ms
step:1657/2330 train_time:66699ms step_avg:40.25ms
step:1658/2330 train_time:66743ms step_avg:40.26ms
step:1659/2330 train_time:66779ms step_avg:40.25ms
step:1660/2330 train_time:66823ms step_avg:40.25ms
step:1661/2330 train_time:66858ms step_avg:40.25ms
step:1662/2330 train_time:66902ms step_avg:40.25ms
step:1663/2330 train_time:66937ms step_avg:40.25ms
step:1664/2330 train_time:66981ms step_avg:40.25ms
step:1665/2330 train_time:67016ms step_avg:40.25ms
step:1666/2330 train_time:67060ms step_avg:40.25ms
step:1667/2330 train_time:67095ms step_avg:40.25ms
step:1668/2330 train_time:67140ms step_avg:40.25ms
step:1669/2330 train_time:67175ms step_avg:40.25ms
step:1670/2330 train_time:67219ms step_avg:40.25ms
step:1671/2330 train_time:67254ms step_avg:40.25ms
step:1672/2330 train_time:67299ms step_avg:40.25ms
step:1673/2330 train_time:67335ms step_avg:40.25ms
step:1674/2330 train_time:67379ms step_avg:40.25ms
step:1675/2330 train_time:67415ms step_avg:40.25ms
step:1676/2330 train_time:67460ms step_avg:40.25ms
step:1677/2330 train_time:67496ms step_avg:40.25ms
step:1678/2330 train_time:67541ms step_avg:40.25ms
step:1679/2330 train_time:67577ms step_avg:40.25ms
step:1680/2330 train_time:67622ms step_avg:40.25ms
step:1681/2330 train_time:67657ms step_avg:40.25ms
step:1682/2330 train_time:67702ms step_avg:40.25ms
step:1683/2330 train_time:67737ms step_avg:40.25ms
step:1684/2330 train_time:67782ms step_avg:40.25ms
step:1685/2330 train_time:67817ms step_avg:40.25ms
step:1686/2330 train_time:67861ms step_avg:40.25ms
step:1687/2330 train_time:67896ms step_avg:40.25ms
step:1688/2330 train_time:67941ms step_avg:40.25ms
step:1689/2330 train_time:67975ms step_avg:40.25ms
step:1690/2330 train_time:68020ms step_avg:40.25ms
step:1691/2330 train_time:68055ms step_avg:40.25ms
step:1692/2330 train_time:68099ms step_avg:40.25ms
step:1693/2330 train_time:68135ms step_avg:40.24ms
step:1694/2330 train_time:68179ms step_avg:40.25ms
step:1695/2330 train_time:68214ms step_avg:40.24ms
step:1696/2330 train_time:68258ms step_avg:40.25ms
step:1697/2330 train_time:68293ms step_avg:40.24ms
step:1698/2330 train_time:68338ms step_avg:40.25ms
step:1699/2330 train_time:68373ms step_avg:40.24ms
step:1700/2330 train_time:68419ms step_avg:40.25ms
step:1701/2330 train_time:68453ms step_avg:40.24ms
step:1702/2330 train_time:68498ms step_avg:40.25ms
step:1703/2330 train_time:68534ms step_avg:40.24ms
step:1704/2330 train_time:68579ms step_avg:40.25ms
step:1705/2330 train_time:68615ms step_avg:40.24ms
step:1706/2330 train_time:68660ms step_avg:40.25ms
step:1707/2330 train_time:68695ms step_avg:40.24ms
step:1708/2330 train_time:68740ms step_avg:40.25ms
step:1709/2330 train_time:68775ms step_avg:40.24ms
step:1710/2330 train_time:68819ms step_avg:40.24ms
step:1711/2330 train_time:68853ms step_avg:40.24ms
step:1712/2330 train_time:68897ms step_avg:40.24ms
step:1713/2330 train_time:68932ms step_avg:40.24ms
step:1714/2330 train_time:68976ms step_avg:40.24ms
step:1715/2330 train_time:69011ms step_avg:40.24ms
step:1716/2330 train_time:69054ms step_avg:40.24ms
step:1717/2330 train_time:69090ms step_avg:40.24ms
step:1718/2330 train_time:69134ms step_avg:40.24ms
step:1719/2330 train_time:69169ms step_avg:40.24ms
step:1720/2330 train_time:69214ms step_avg:40.24ms
step:1721/2330 train_time:69249ms step_avg:40.24ms
step:1722/2330 train_time:69294ms step_avg:40.24ms
step:1723/2330 train_time:69329ms step_avg:40.24ms
step:1724/2330 train_time:69373ms step_avg:40.24ms
step:1725/2330 train_time:69407ms step_avg:40.24ms
step:1726/2330 train_time:69452ms step_avg:40.24ms
step:1727/2330 train_time:69487ms step_avg:40.24ms
step:1728/2330 train_time:69532ms step_avg:40.24ms
step:1729/2330 train_time:69567ms step_avg:40.24ms
step:1730/2330 train_time:69612ms step_avg:40.24ms
step:1731/2330 train_time:69647ms step_avg:40.24ms
step:1732/2330 train_time:69691ms step_avg:40.24ms
step:1733/2330 train_time:69726ms step_avg:40.23ms
step:1734/2330 train_time:69771ms step_avg:40.24ms
step:1735/2330 train_time:69806ms step_avg:40.23ms
step:1736/2330 train_time:69851ms step_avg:40.24ms
step:1737/2330 train_time:69886ms step_avg:40.23ms
step:1738/2330 train_time:69930ms step_avg:40.24ms
step:1739/2330 train_time:69965ms step_avg:40.23ms
step:1740/2330 train_time:70009ms step_avg:40.23ms
step:1741/2330 train_time:70044ms step_avg:40.23ms
step:1742/2330 train_time:70088ms step_avg:40.23ms
step:1743/2330 train_time:70124ms step_avg:40.23ms
step:1744/2330 train_time:70168ms step_avg:40.23ms
step:1745/2330 train_time:70203ms step_avg:40.23ms
step:1746/2330 train_time:70248ms step_avg:40.23ms
step:1747/2330 train_time:70282ms step_avg:40.23ms
step:1748/2330 train_time:70326ms step_avg:40.23ms
step:1749/2330 train_time:70361ms step_avg:40.23ms
step:1750/2330 train_time:70405ms step_avg:40.23ms
step:1750/2330 val_loss:5.1272 train_time:70494ms step_avg:40.28ms
step:1751/2330 train_time:70508ms step_avg:40.27ms
step:1752/2330 train_time:70521ms step_avg:40.25ms
step:1753/2330 train_time:70531ms step_avg:40.23ms
step:1754/2330 train_time:70566ms step_avg:40.23ms
step:1755/2330 train_time:70600ms step_avg:40.23ms
step:1756/2330 train_time:70643ms step_avg:40.23ms
step:1757/2330 train_time:70677ms step_avg:40.23ms
step:1758/2330 train_time:70721ms step_avg:40.23ms
step:1759/2330 train_time:70755ms step_avg:40.22ms
step:1760/2330 train_time:70799ms step_avg:40.23ms
step:1761/2330 train_time:70834ms step_avg:40.22ms
step:1762/2330 train_time:70880ms step_avg:40.23ms
step:1763/2330 train_time:70914ms step_avg:40.22ms
step:1764/2330 train_time:70958ms step_avg:40.23ms
step:1765/2330 train_time:70993ms step_avg:40.22ms
step:1766/2330 train_time:71036ms step_avg:40.22ms
step:1767/2330 train_time:71071ms step_avg:40.22ms
step:1768/2330 train_time:71115ms step_avg:40.22ms
step:1769/2330 train_time:71149ms step_avg:40.22ms
step:1770/2330 train_time:71193ms step_avg:40.22ms
step:1771/2330 train_time:71227ms step_avg:40.22ms
step:1772/2330 train_time:71271ms step_avg:40.22ms
step:1773/2330 train_time:71306ms step_avg:40.22ms
step:1774/2330 train_time:71349ms step_avg:40.22ms
step:1775/2330 train_time:71386ms step_avg:40.22ms
step:1776/2330 train_time:71435ms step_avg:40.22ms
step:1777/2330 train_time:71472ms step_avg:40.22ms
step:1778/2330 train_time:71519ms step_avg:40.22ms
step:1779/2330 train_time:71555ms step_avg:40.22ms
step:1780/2330 train_time:71600ms step_avg:40.22ms
step:1781/2330 train_time:71635ms step_avg:40.22ms
step:1782/2330 train_time:71679ms step_avg:40.22ms
step:1783/2330 train_time:71713ms step_avg:40.22ms
step:1784/2330 train_time:71757ms step_avg:40.22ms
step:1785/2330 train_time:71793ms step_avg:40.22ms
step:1786/2330 train_time:71837ms step_avg:40.22ms
step:1787/2330 train_time:71871ms step_avg:40.22ms
step:1788/2330 train_time:71917ms step_avg:40.22ms
step:1789/2330 train_time:71951ms step_avg:40.22ms
step:1790/2330 train_time:71995ms step_avg:40.22ms
step:1791/2330 train_time:72029ms step_avg:40.22ms
step:1792/2330 train_time:72073ms step_avg:40.22ms
step:1793/2330 train_time:72108ms step_avg:40.22ms
step:1794/2330 train_time:72152ms step_avg:40.22ms
step:1795/2330 train_time:72186ms step_avg:40.21ms
step:1796/2330 train_time:72230ms step_avg:40.22ms
step:1797/2330 train_time:72264ms step_avg:40.21ms
step:1798/2330 train_time:72308ms step_avg:40.22ms
step:1799/2330 train_time:72344ms step_avg:40.21ms
step:1800/2330 train_time:72389ms step_avg:40.22ms
step:1801/2330 train_time:72425ms step_avg:40.21ms
step:1802/2330 train_time:72472ms step_avg:40.22ms
step:1803/2330 train_time:72507ms step_avg:40.21ms
step:1804/2330 train_time:72553ms step_avg:40.22ms
step:1805/2330 train_time:72588ms step_avg:40.22ms
step:1806/2330 train_time:72634ms step_avg:40.22ms
step:1807/2330 train_time:72668ms step_avg:40.21ms
step:1808/2330 train_time:72713ms step_avg:40.22ms
step:1809/2330 train_time:72748ms step_avg:40.21ms
step:1810/2330 train_time:72792ms step_avg:40.22ms
step:1811/2330 train_time:72827ms step_avg:40.21ms
step:1812/2330 train_time:72872ms step_avg:40.22ms
step:1813/2330 train_time:72908ms step_avg:40.21ms
step:1814/2330 train_time:72952ms step_avg:40.22ms
step:1815/2330 train_time:72986ms step_avg:40.21ms
step:1816/2330 train_time:73031ms step_avg:40.22ms
step:1817/2330 train_time:73065ms step_avg:40.21ms
step:1818/2330 train_time:73109ms step_avg:40.21ms
step:1819/2330 train_time:73144ms step_avg:40.21ms
step:1820/2330 train_time:73188ms step_avg:40.21ms
step:1821/2330 train_time:73223ms step_avg:40.21ms
step:1822/2330 train_time:73267ms step_avg:40.21ms
step:1823/2330 train_time:73302ms step_avg:40.21ms
step:1824/2330 train_time:73347ms step_avg:40.21ms
step:1825/2330 train_time:73382ms step_avg:40.21ms
step:1826/2330 train_time:73427ms step_avg:40.21ms
step:1827/2330 train_time:73463ms step_avg:40.21ms
step:1828/2330 train_time:73507ms step_avg:40.21ms
step:1829/2330 train_time:73544ms step_avg:40.21ms
step:1830/2330 train_time:73590ms step_avg:40.21ms
step:1831/2330 train_time:73626ms step_avg:40.21ms
step:1832/2330 train_time:73670ms step_avg:40.21ms
step:1833/2330 train_time:73705ms step_avg:40.21ms
step:1834/2330 train_time:73751ms step_avg:40.21ms
step:1835/2330 train_time:73786ms step_avg:40.21ms
step:1836/2330 train_time:73831ms step_avg:40.21ms
step:1837/2330 train_time:73866ms step_avg:40.21ms
step:1838/2330 train_time:73910ms step_avg:40.21ms
step:1839/2330 train_time:73945ms step_avg:40.21ms
step:1840/2330 train_time:73989ms step_avg:40.21ms
step:1841/2330 train_time:74023ms step_avg:40.21ms
step:1842/2330 train_time:74068ms step_avg:40.21ms
step:1843/2330 train_time:74102ms step_avg:40.21ms
step:1844/2330 train_time:74146ms step_avg:40.21ms
step:1845/2330 train_time:74181ms step_avg:40.21ms
step:1846/2330 train_time:74226ms step_avg:40.21ms
step:1847/2330 train_time:74260ms step_avg:40.21ms
step:1848/2330 train_time:74304ms step_avg:40.21ms
step:1849/2330 train_time:74339ms step_avg:40.20ms
step:1850/2330 train_time:74383ms step_avg:40.21ms
step:1851/2330 train_time:74419ms step_avg:40.20ms
step:1852/2330 train_time:74464ms step_avg:40.21ms
step:1853/2330 train_time:74499ms step_avg:40.20ms
step:1854/2330 train_time:74544ms step_avg:40.21ms
step:1855/2330 train_time:74580ms step_avg:40.20ms
step:1856/2330 train_time:74625ms step_avg:40.21ms
step:1857/2330 train_time:74662ms step_avg:40.21ms
step:1858/2330 train_time:74707ms step_avg:40.21ms
step:1859/2330 train_time:74742ms step_avg:40.21ms
step:1860/2330 train_time:74787ms step_avg:40.21ms
step:1861/2330 train_time:74822ms step_avg:40.21ms
step:1862/2330 train_time:74867ms step_avg:40.21ms
step:1863/2330 train_time:74901ms step_avg:40.20ms
step:1864/2330 train_time:74945ms step_avg:40.21ms
step:1865/2330 train_time:74979ms step_avg:40.20ms
step:1866/2330 train_time:75023ms step_avg:40.21ms
step:1867/2330 train_time:75058ms step_avg:40.20ms
step:1868/2330 train_time:75102ms step_avg:40.20ms
step:1869/2330 train_time:75137ms step_avg:40.20ms
step:1870/2330 train_time:75182ms step_avg:40.20ms
step:1871/2330 train_time:75216ms step_avg:40.20ms
step:1872/2330 train_time:75260ms step_avg:40.20ms
step:1873/2330 train_time:75295ms step_avg:40.20ms
step:1874/2330 train_time:75340ms step_avg:40.20ms
step:1875/2330 train_time:75376ms step_avg:40.20ms
step:1876/2330 train_time:75421ms step_avg:40.20ms
step:1877/2330 train_time:75456ms step_avg:40.20ms
step:1878/2330 train_time:75502ms step_avg:40.20ms
step:1879/2330 train_time:75538ms step_avg:40.20ms
step:1880/2330 train_time:75583ms step_avg:40.20ms
step:1881/2330 train_time:75619ms step_avg:40.20ms
step:1882/2330 train_time:75664ms step_avg:40.20ms
step:1883/2330 train_time:75699ms step_avg:40.20ms
step:1884/2330 train_time:75744ms step_avg:40.20ms
step:1885/2330 train_time:75779ms step_avg:40.20ms
step:1886/2330 train_time:75824ms step_avg:40.20ms
step:1887/2330 train_time:75859ms step_avg:40.20ms
step:1888/2330 train_time:75904ms step_avg:40.20ms
step:1889/2330 train_time:75938ms step_avg:40.20ms
step:1890/2330 train_time:75982ms step_avg:40.20ms
step:1891/2330 train_time:76017ms step_avg:40.20ms
step:1892/2330 train_time:76062ms step_avg:40.20ms
step:1893/2330 train_time:76097ms step_avg:40.20ms
step:1894/2330 train_time:76141ms step_avg:40.20ms
step:1895/2330 train_time:76176ms step_avg:40.20ms
step:1896/2330 train_time:76220ms step_avg:40.20ms
step:1897/2330 train_time:76254ms step_avg:40.20ms
step:1898/2330 train_time:76299ms step_avg:40.20ms
step:1899/2330 train_time:76334ms step_avg:40.20ms
step:1900/2330 train_time:76379ms step_avg:40.20ms
step:1901/2330 train_time:76414ms step_avg:40.20ms
step:1902/2330 train_time:76458ms step_avg:40.20ms
step:1903/2330 train_time:76494ms step_avg:40.20ms
step:1904/2330 train_time:76538ms step_avg:40.20ms
step:1905/2330 train_time:76574ms step_avg:40.20ms
step:1906/2330 train_time:76618ms step_avg:40.20ms
step:1907/2330 train_time:76654ms step_avg:40.20ms
step:1908/2330 train_time:76699ms step_avg:40.20ms
step:1909/2330 train_time:76734ms step_avg:40.20ms
step:1910/2330 train_time:76779ms step_avg:40.20ms
step:1911/2330 train_time:76814ms step_avg:40.20ms
step:1912/2330 train_time:76859ms step_avg:40.20ms
step:1913/2330 train_time:76894ms step_avg:40.20ms
step:1914/2330 train_time:76939ms step_avg:40.20ms
step:1915/2330 train_time:76974ms step_avg:40.20ms
step:1916/2330 train_time:77018ms step_avg:40.20ms
step:1917/2330 train_time:77053ms step_avg:40.19ms
step:1918/2330 train_time:77097ms step_avg:40.20ms
step:1919/2330 train_time:77132ms step_avg:40.19ms
step:1920/2330 train_time:77176ms step_avg:40.20ms
step:1921/2330 train_time:77211ms step_avg:40.19ms
step:1922/2330 train_time:77255ms step_avg:40.20ms
step:1923/2330 train_time:77290ms step_avg:40.19ms
step:1924/2330 train_time:77334ms step_avg:40.19ms
step:1925/2330 train_time:77370ms step_avg:40.19ms
step:1926/2330 train_time:77415ms step_avg:40.19ms
step:1927/2330 train_time:77450ms step_avg:40.19ms
step:1928/2330 train_time:77494ms step_avg:40.19ms
step:1929/2330 train_time:77529ms step_avg:40.19ms
step:1930/2330 train_time:77574ms step_avg:40.19ms
step:1931/2330 train_time:77610ms step_avg:40.19ms
step:1932/2330 train_time:77654ms step_avg:40.19ms
step:1933/2330 train_time:77689ms step_avg:40.19ms
step:1934/2330 train_time:77734ms step_avg:40.19ms
step:1935/2330 train_time:77769ms step_avg:40.19ms
step:1936/2330 train_time:77813ms step_avg:40.19ms
step:1937/2330 train_time:77848ms step_avg:40.19ms
step:1938/2330 train_time:77892ms step_avg:40.19ms
step:1939/2330 train_time:77927ms step_avg:40.19ms
step:1940/2330 train_time:77972ms step_avg:40.19ms
step:1941/2330 train_time:78008ms step_avg:40.19ms
step:1942/2330 train_time:78052ms step_avg:40.19ms
step:1943/2330 train_time:78088ms step_avg:40.19ms
step:1944/2330 train_time:78132ms step_avg:40.19ms
step:1945/2330 train_time:78168ms step_avg:40.19ms
step:1946/2330 train_time:78212ms step_avg:40.19ms
step:1947/2330 train_time:78248ms step_avg:40.19ms
step:1948/2330 train_time:78292ms step_avg:40.19ms
step:1949/2330 train_time:78327ms step_avg:40.19ms
step:1950/2330 train_time:78373ms step_avg:40.19ms
step:1951/2330 train_time:78408ms step_avg:40.19ms
step:1952/2330 train_time:78453ms step_avg:40.19ms
step:1953/2330 train_time:78488ms step_avg:40.19ms
step:1954/2330 train_time:78533ms step_avg:40.19ms
step:1955/2330 train_time:78568ms step_avg:40.19ms
step:1956/2330 train_time:78613ms step_avg:40.19ms
step:1957/2330 train_time:78648ms step_avg:40.19ms
step:1958/2330 train_time:78692ms step_avg:40.19ms
step:1959/2330 train_time:78728ms step_avg:40.19ms
step:1960/2330 train_time:78773ms step_avg:40.19ms
step:1961/2330 train_time:78808ms step_avg:40.19ms
step:1962/2330 train_time:78853ms step_avg:40.19ms
step:1963/2330 train_time:78888ms step_avg:40.19ms
step:1964/2330 train_time:78932ms step_avg:40.19ms
step:1965/2330 train_time:78967ms step_avg:40.19ms
step:1966/2330 train_time:79012ms step_avg:40.19ms
step:1967/2330 train_time:79047ms step_avg:40.19ms
step:1968/2330 train_time:79092ms step_avg:40.19ms
step:1969/2330 train_time:79127ms step_avg:40.19ms
step:1970/2330 train_time:79172ms step_avg:40.19ms
step:1971/2330 train_time:79207ms step_avg:40.19ms
step:1972/2330 train_time:79251ms step_avg:40.19ms
step:1973/2330 train_time:79286ms step_avg:40.19ms
step:1974/2330 train_time:79330ms step_avg:40.19ms
step:1975/2330 train_time:79365ms step_avg:40.18ms
step:1976/2330 train_time:79409ms step_avg:40.19ms
step:1977/2330 train_time:79444ms step_avg:40.18ms
step:1978/2330 train_time:79489ms step_avg:40.19ms
step:1979/2330 train_time:79525ms step_avg:40.18ms
step:1980/2330 train_time:79569ms step_avg:40.19ms
step:1981/2330 train_time:79604ms step_avg:40.18ms
step:1982/2330 train_time:79649ms step_avg:40.19ms
step:1983/2330 train_time:79683ms step_avg:40.18ms
step:1984/2330 train_time:79728ms step_avg:40.19ms
step:1985/2330 train_time:79763ms step_avg:40.18ms
step:1986/2330 train_time:79809ms step_avg:40.19ms
step:1987/2330 train_time:79843ms step_avg:40.18ms
step:1988/2330 train_time:79887ms step_avg:40.18ms
step:1989/2330 train_time:79922ms step_avg:40.18ms
step:1990/2330 train_time:79967ms step_avg:40.18ms
step:1991/2330 train_time:80002ms step_avg:40.18ms
step:1992/2330 train_time:80046ms step_avg:40.18ms
step:1993/2330 train_time:80081ms step_avg:40.18ms
step:1994/2330 train_time:80125ms step_avg:40.18ms
step:1995/2330 train_time:80160ms step_avg:40.18ms
step:1996/2330 train_time:80203ms step_avg:40.18ms
step:1997/2330 train_time:80238ms step_avg:40.18ms
step:1998/2330 train_time:80283ms step_avg:40.18ms
step:1999/2330 train_time:80318ms step_avg:40.18ms
step:2000/2330 train_time:80362ms step_avg:40.18ms
step:2000/2330 val_loss:5.1148 train_time:80450ms step_avg:40.23ms
step:2001/2330 train_time:80463ms step_avg:40.21ms
step:2002/2330 train_time:80476ms step_avg:40.20ms
step:2003/2330 train_time:80486ms step_avg:40.18ms
step:2004/2330 train_time:80522ms step_avg:40.18ms
step:2005/2330 train_time:80557ms step_avg:40.18ms
step:2006/2330 train_time:80600ms step_avg:40.18ms
step:2007/2330 train_time:80635ms step_avg:40.18ms
step:2008/2330 train_time:80679ms step_avg:40.18ms
step:2009/2330 train_time:80714ms step_avg:40.18ms
step:2010/2330 train_time:80759ms step_avg:40.18ms
step:2011/2330 train_time:80796ms step_avg:40.18ms
step:2012/2330 train_time:80843ms step_avg:40.18ms
step:2013/2330 train_time:80879ms step_avg:40.18ms
step:2014/2330 train_time:80925ms step_avg:40.18ms
step:2015/2330 train_time:80960ms step_avg:40.18ms
step:2016/2330 train_time:81004ms step_avg:40.18ms
step:2017/2330 train_time:81039ms step_avg:40.18ms
step:2018/2330 train_time:81082ms step_avg:40.18ms
step:2019/2330 train_time:81117ms step_avg:40.18ms
step:2020/2330 train_time:81162ms step_avg:40.18ms
step:2021/2330 train_time:81196ms step_avg:40.18ms
step:2022/2330 train_time:81240ms step_avg:40.18ms
step:2023/2330 train_time:81276ms step_avg:40.18ms
step:2024/2330 train_time:81320ms step_avg:40.18ms
step:2025/2330 train_time:81355ms step_avg:40.18ms
step:2026/2330 train_time:81401ms step_avg:40.18ms
step:2027/2330 train_time:81437ms step_avg:40.18ms
step:2028/2330 train_time:81482ms step_avg:40.18ms
step:2029/2330 train_time:81517ms step_avg:40.18ms
step:2030/2330 train_time:81561ms step_avg:40.18ms
step:2031/2330 train_time:81595ms step_avg:40.17ms
step:2032/2330 train_time:81639ms step_avg:40.18ms
step:2033/2330 train_time:81674ms step_avg:40.17ms
step:2034/2330 train_time:81720ms step_avg:40.18ms
step:2035/2330 train_time:81756ms step_avg:40.17ms
step:2036/2330 train_time:81801ms step_avg:40.18ms
step:2037/2330 train_time:81838ms step_avg:40.18ms
step:2038/2330 train_time:81883ms step_avg:40.18ms
step:2039/2330 train_time:81919ms step_avg:40.18ms
step:2040/2330 train_time:81963ms step_avg:40.18ms
step:2041/2330 train_time:81998ms step_avg:40.18ms
step:2042/2330 train_time:82042ms step_avg:40.18ms
step:2043/2330 train_time:82077ms step_avg:40.17ms
step:2044/2330 train_time:82121ms step_avg:40.18ms
step:2045/2330 train_time:82156ms step_avg:40.17ms
step:2046/2330 train_time:82199ms step_avg:40.18ms
step:2047/2330 train_time:82234ms step_avg:40.17ms
step:2048/2330 train_time:82278ms step_avg:40.17ms
step:2049/2330 train_time:82313ms step_avg:40.17ms
step:2050/2330 train_time:82357ms step_avg:40.17ms
step:2051/2330 train_time:82392ms step_avg:40.17ms
step:2052/2330 train_time:82437ms step_avg:40.17ms
step:2053/2330 train_time:82473ms step_avg:40.17ms
step:2054/2330 train_time:82516ms step_avg:40.17ms
step:2055/2330 train_time:82551ms step_avg:40.17ms
step:2056/2330 train_time:82596ms step_avg:40.17ms
step:2057/2330 train_time:82631ms step_avg:40.17ms
step:2058/2330 train_time:82675ms step_avg:40.17ms
step:2059/2330 train_time:82711ms step_avg:40.17ms
step:2060/2330 train_time:82755ms step_avg:40.17ms
step:2061/2330 train_time:82790ms step_avg:40.17ms
step:2062/2330 train_time:82836ms step_avg:40.17ms
step:2063/2330 train_time:82872ms step_avg:40.17ms
step:2064/2330 train_time:82917ms step_avg:40.17ms
step:2065/2330 train_time:82952ms step_avg:40.17ms
step:2066/2330 train_time:82996ms step_avg:40.17ms
step:2067/2330 train_time:83030ms step_avg:40.17ms
step:2068/2330 train_time:83075ms step_avg:40.17ms
step:2069/2330 train_time:83109ms step_avg:40.17ms
step:2070/2330 train_time:83154ms step_avg:40.17ms
step:2071/2330 train_time:83189ms step_avg:40.17ms
step:2072/2330 train_time:83233ms step_avg:40.17ms
step:2073/2330 train_time:83268ms step_avg:40.17ms
step:2074/2330 train_time:83312ms step_avg:40.17ms
step:2075/2330 train_time:83347ms step_avg:40.17ms
step:2076/2330 train_time:83392ms step_avg:40.17ms
step:2077/2330 train_time:83426ms step_avg:40.17ms
step:2078/2330 train_time:83471ms step_avg:40.17ms
step:2079/2330 train_time:83506ms step_avg:40.17ms
step:2080/2330 train_time:83550ms step_avg:40.17ms
step:2081/2330 train_time:83585ms step_avg:40.17ms
step:2082/2330 train_time:83631ms step_avg:40.17ms
step:2083/2330 train_time:83665ms step_avg:40.17ms
step:2084/2330 train_time:83709ms step_avg:40.17ms
step:2085/2330 train_time:83745ms step_avg:40.17ms
step:2086/2330 train_time:83790ms step_avg:40.17ms
step:2087/2330 train_time:83825ms step_avg:40.17ms
step:2088/2330 train_time:83869ms step_avg:40.17ms
step:2089/2330 train_time:83905ms step_avg:40.17ms
step:2090/2330 train_time:83949ms step_avg:40.17ms
step:2091/2330 train_time:83983ms step_avg:40.16ms
step:2092/2330 train_time:84028ms step_avg:40.17ms
step:2093/2330 train_time:84062ms step_avg:40.16ms
step:2094/2330 train_time:84105ms step_avg:40.16ms
step:2095/2330 train_time:84141ms step_avg:40.16ms
step:2096/2330 train_time:84184ms step_avg:40.16ms
step:2097/2330 train_time:84220ms step_avg:40.16ms
step:2098/2330 train_time:84264ms step_avg:40.16ms
step:2099/2330 train_time:84300ms step_avg:40.16ms
step:2100/2330 train_time:84344ms step_avg:40.16ms
step:2101/2330 train_time:84380ms step_avg:40.16ms
step:2102/2330 train_time:84424ms step_avg:40.16ms
step:2103/2330 train_time:84459ms step_avg:40.16ms
step:2104/2330 train_time:84503ms step_avg:40.16ms
step:2105/2330 train_time:84539ms step_avg:40.16ms
step:2106/2330 train_time:84583ms step_avg:40.16ms
step:2107/2330 train_time:84619ms step_avg:40.16ms
step:2108/2330 train_time:84664ms step_avg:40.16ms
step:2109/2330 train_time:84700ms step_avg:40.16ms
step:2110/2330 train_time:84744ms step_avg:40.16ms
step:2111/2330 train_time:84780ms step_avg:40.16ms
step:2112/2330 train_time:84824ms step_avg:40.16ms
step:2113/2330 train_time:84859ms step_avg:40.16ms
step:2114/2330 train_time:84904ms step_avg:40.16ms
step:2115/2330 train_time:84939ms step_avg:40.16ms
step:2116/2330 train_time:84984ms step_avg:40.16ms
step:2117/2330 train_time:85020ms step_avg:40.16ms
step:2118/2330 train_time:85064ms step_avg:40.16ms
step:2119/2330 train_time:85099ms step_avg:40.16ms
step:2120/2330 train_time:85143ms step_avg:40.16ms
step:2121/2330 train_time:85179ms step_avg:40.16ms
step:2122/2330 train_time:85223ms step_avg:40.16ms
step:2123/2330 train_time:85258ms step_avg:40.16ms
step:2124/2330 train_time:85303ms step_avg:40.16ms
step:2125/2330 train_time:85338ms step_avg:40.16ms
step:2126/2330 train_time:85382ms step_avg:40.16ms
step:2127/2330 train_time:85416ms step_avg:40.16ms
step:2128/2330 train_time:85461ms step_avg:40.16ms
step:2129/2330 train_time:85496ms step_avg:40.16ms
step:2130/2330 train_time:85541ms step_avg:40.16ms
step:2131/2330 train_time:85576ms step_avg:40.16ms
step:2132/2330 train_time:85620ms step_avg:40.16ms
step:2133/2330 train_time:85656ms step_avg:40.16ms
step:2134/2330 train_time:85701ms step_avg:40.16ms
step:2135/2330 train_time:85736ms step_avg:40.16ms
step:2136/2330 train_time:85781ms step_avg:40.16ms
step:2137/2330 train_time:85816ms step_avg:40.16ms
step:2138/2330 train_time:85861ms step_avg:40.16ms
step:2139/2330 train_time:85896ms step_avg:40.16ms
step:2140/2330 train_time:85941ms step_avg:40.16ms
step:2141/2330 train_time:85975ms step_avg:40.16ms
step:2142/2330 train_time:86019ms step_avg:40.16ms
step:2143/2330 train_time:86055ms step_avg:40.16ms
step:2144/2330 train_time:86100ms step_avg:40.16ms
step:2145/2330 train_time:86134ms step_avg:40.16ms
step:2146/2330 train_time:86179ms step_avg:40.16ms
step:2147/2330 train_time:86214ms step_avg:40.16ms
step:2148/2330 train_time:86257ms step_avg:40.16ms
step:2149/2330 train_time:86292ms step_avg:40.15ms
step:2150/2330 train_time:86336ms step_avg:40.16ms
step:2151/2330 train_time:86371ms step_avg:40.15ms
step:2152/2330 train_time:86416ms step_avg:40.16ms
step:2153/2330 train_time:86450ms step_avg:40.15ms
step:2154/2330 train_time:86495ms step_avg:40.16ms
step:2155/2330 train_time:86531ms step_avg:40.15ms
step:2156/2330 train_time:86576ms step_avg:40.16ms
step:2157/2330 train_time:86610ms step_avg:40.15ms
step:2158/2330 train_time:86655ms step_avg:40.16ms
step:2159/2330 train_time:86691ms step_avg:40.15ms
step:2160/2330 train_time:86735ms step_avg:40.16ms
step:2161/2330 train_time:86770ms step_avg:40.15ms
step:2162/2330 train_time:86815ms step_avg:40.15ms
step:2163/2330 train_time:86850ms step_avg:40.15ms
step:2164/2330 train_time:86895ms step_avg:40.15ms
step:2165/2330 train_time:86930ms step_avg:40.15ms
step:2166/2330 train_time:86975ms step_avg:40.15ms
step:2167/2330 train_time:87011ms step_avg:40.15ms
step:2168/2330 train_time:87055ms step_avg:40.15ms
step:2169/2330 train_time:87091ms step_avg:40.15ms
step:2170/2330 train_time:87136ms step_avg:40.15ms
step:2171/2330 train_time:87170ms step_avg:40.15ms
step:2172/2330 train_time:87215ms step_avg:40.15ms
step:2173/2330 train_time:87249ms step_avg:40.15ms
step:2174/2330 train_time:87294ms step_avg:40.15ms
step:2175/2330 train_time:87329ms step_avg:40.15ms
step:2176/2330 train_time:87373ms step_avg:40.15ms
step:2177/2330 train_time:87407ms step_avg:40.15ms
step:2178/2330 train_time:87452ms step_avg:40.15ms
step:2179/2330 train_time:87487ms step_avg:40.15ms
step:2180/2330 train_time:87531ms step_avg:40.15ms
step:2181/2330 train_time:87566ms step_avg:40.15ms
step:2182/2330 train_time:87610ms step_avg:40.15ms
step:2183/2330 train_time:87645ms step_avg:40.15ms
step:2184/2330 train_time:87690ms step_avg:40.15ms
step:2185/2330 train_time:87726ms step_avg:40.15ms
step:2186/2330 train_time:87770ms step_avg:40.15ms
step:2187/2330 train_time:87805ms step_avg:40.15ms
step:2188/2330 train_time:87849ms step_avg:40.15ms
step:2189/2330 train_time:87884ms step_avg:40.15ms
step:2190/2330 train_time:87930ms step_avg:40.15ms
step:2191/2330 train_time:87964ms step_avg:40.15ms
step:2192/2330 train_time:88009ms step_avg:40.15ms
step:2193/2330 train_time:88045ms step_avg:40.15ms
step:2194/2330 train_time:88089ms step_avg:40.15ms
step:2195/2330 train_time:88124ms step_avg:40.15ms
step:2196/2330 train_time:88168ms step_avg:40.15ms
step:2197/2330 train_time:88202ms step_avg:40.15ms
step:2198/2330 train_time:88246ms step_avg:40.15ms
step:2199/2330 train_time:88281ms step_avg:40.15ms
step:2200/2330 train_time:88325ms step_avg:40.15ms
step:2201/2330 train_time:88360ms step_avg:40.15ms
step:2202/2330 train_time:88404ms step_avg:40.15ms
step:2203/2330 train_time:88439ms step_avg:40.14ms
step:2204/2330 train_time:88483ms step_avg:40.15ms
step:2205/2330 train_time:88519ms step_avg:40.14ms
step:2206/2330 train_time:88563ms step_avg:40.15ms
step:2207/2330 train_time:88599ms step_avg:40.14ms
step:2208/2330 train_time:88643ms step_avg:40.15ms
step:2209/2330 train_time:88678ms step_avg:40.14ms
step:2210/2330 train_time:88722ms step_avg:40.15ms
step:2211/2330 train_time:88758ms step_avg:40.14ms
step:2212/2330 train_time:88802ms step_avg:40.15ms
step:2213/2330 train_time:88838ms step_avg:40.14ms
step:2214/2330 train_time:88882ms step_avg:40.15ms
step:2215/2330 train_time:88917ms step_avg:40.14ms
step:2216/2330 train_time:88962ms step_avg:40.15ms
step:2217/2330 train_time:88999ms step_avg:40.14ms
step:2218/2330 train_time:89043ms step_avg:40.15ms
step:2219/2330 train_time:89078ms step_avg:40.14ms
step:2220/2330 train_time:89122ms step_avg:40.15ms
step:2221/2330 train_time:89157ms step_avg:40.14ms
step:2222/2330 train_time:89202ms step_avg:40.14ms
step:2223/2330 train_time:89237ms step_avg:40.14ms
step:2224/2330 train_time:89281ms step_avg:40.14ms
step:2225/2330 train_time:89317ms step_avg:40.14ms
step:2226/2330 train_time:89361ms step_avg:40.14ms
step:2227/2330 train_time:89396ms step_avg:40.14ms
step:2228/2330 train_time:89441ms step_avg:40.14ms
step:2229/2330 train_time:89476ms step_avg:40.14ms
step:2230/2330 train_time:89520ms step_avg:40.14ms
step:2231/2330 train_time:89556ms step_avg:40.14ms
step:2232/2330 train_time:89600ms step_avg:40.14ms
step:2233/2330 train_time:89635ms step_avg:40.14ms
step:2234/2330 train_time:89679ms step_avg:40.14ms
step:2235/2330 train_time:89715ms step_avg:40.14ms
step:2236/2330 train_time:89759ms step_avg:40.14ms
step:2237/2330 train_time:89794ms step_avg:40.14ms
step:2238/2330 train_time:89839ms step_avg:40.14ms
step:2239/2330 train_time:89875ms step_avg:40.14ms
step:2240/2330 train_time:89920ms step_avg:40.14ms
step:2241/2330 train_time:89955ms step_avg:40.14ms
step:2242/2330 train_time:90000ms step_avg:40.14ms
step:2243/2330 train_time:90035ms step_avg:40.14ms
step:2244/2330 train_time:90079ms step_avg:40.14ms
step:2245/2330 train_time:90115ms step_avg:40.14ms
step:2246/2330 train_time:90159ms step_avg:40.14ms
step:2247/2330 train_time:90194ms step_avg:40.14ms
step:2248/2330 train_time:90238ms step_avg:40.14ms
step:2249/2330 train_time:90273ms step_avg:40.14ms
step:2250/2330 train_time:90317ms step_avg:40.14ms
step:2250/2330 val_loss:5.1055 train_time:90403ms step_avg:40.18ms
step:2251/2330 train_time:90417ms step_avg:40.17ms
step:2252/2330 train_time:90429ms step_avg:40.16ms
step:2253/2330 train_time:90440ms step_avg:40.14ms
step:2254/2330 train_time:90477ms step_avg:40.14ms
step:2255/2330 train_time:90511ms step_avg:40.14ms
step:2256/2330 train_time:90555ms step_avg:40.14ms
step:2257/2330 train_time:90589ms step_avg:40.14ms
step:2258/2330 train_time:90633ms step_avg:40.14ms
step:2259/2330 train_time:90667ms step_avg:40.14ms
step:2260/2330 train_time:90714ms step_avg:40.14ms
step:2261/2330 train_time:90753ms step_avg:40.14ms
step:2262/2330 train_time:90801ms step_avg:40.14ms
step:2263/2330 train_time:90839ms step_avg:40.14ms
step:2264/2330 train_time:90884ms step_avg:40.14ms
step:2265/2330 train_time:90919ms step_avg:40.14ms
step:2266/2330 train_time:90963ms step_avg:40.14ms
step:2267/2330 train_time:90998ms step_avg:40.14ms
step:2268/2330 train_time:91041ms step_avg:40.14ms
step:2269/2330 train_time:91076ms step_avg:40.14ms
step:2270/2330 train_time:91120ms step_avg:40.14ms
step:2271/2330 train_time:91155ms step_avg:40.14ms
step:2272/2330 train_time:91199ms step_avg:40.14ms
step:2273/2330 train_time:91234ms step_avg:40.14ms
step:2274/2330 train_time:91277ms step_avg:40.14ms
step:2275/2330 train_time:91312ms step_avg:40.14ms
step:2276/2330 train_time:91357ms step_avg:40.14ms
step:2277/2330 train_time:91392ms step_avg:40.14ms
step:2278/2330 train_time:91436ms step_avg:40.14ms
step:2279/2330 train_time:91471ms step_avg:40.14ms
step:2280/2330 train_time:91514ms step_avg:40.14ms
step:2281/2330 train_time:91549ms step_avg:40.14ms
step:2282/2330 train_time:91593ms step_avg:40.14ms
step:2283/2330 train_time:91629ms step_avg:40.14ms
step:2284/2330 train_time:91674ms step_avg:40.14ms
step:2285/2330 train_time:91711ms step_avg:40.14ms
step:2286/2330 train_time:91757ms step_avg:40.14ms
step:2287/2330 train_time:91793ms step_avg:40.14ms
step:2288/2330 train_time:91839ms step_avg:40.14ms
step:2289/2330 train_time:91875ms step_avg:40.14ms
step:2290/2330 train_time:91919ms step_avg:40.14ms
step:2291/2330 train_time:91954ms step_avg:40.14ms
step:2292/2330 train_time:91999ms step_avg:40.14ms
step:2293/2330 train_time:92033ms step_avg:40.14ms
step:2294/2330 train_time:92077ms step_avg:40.14ms
step:2295/2330 train_time:92112ms step_avg:40.14ms
step:2296/2330 train_time:92156ms step_avg:40.14ms
step:2297/2330 train_time:92191ms step_avg:40.14ms
step:2298/2330 train_time:92236ms step_avg:40.14ms
step:2299/2330 train_time:92270ms step_avg:40.13ms
step:2300/2330 train_time:92314ms step_avg:40.14ms
step:2301/2330 train_time:92349ms step_avg:40.13ms
step:2302/2330 train_time:92394ms step_avg:40.14ms
step:2303/2330 train_time:92428ms step_avg:40.13ms
step:2304/2330 train_time:92472ms step_avg:40.14ms
step:2305/2330 train_time:92506ms step_avg:40.13ms
step:2306/2330 train_time:92550ms step_avg:40.13ms
step:2307/2330 train_time:92585ms step_avg:40.13ms
step:2308/2330 train_time:92630ms step_avg:40.13ms
step:2309/2330 train_time:92664ms step_avg:40.13ms
step:2310/2330 train_time:92710ms step_avg:40.13ms
step:2311/2330 train_time:92746ms step_avg:40.13ms
step:2312/2330 train_time:92792ms step_avg:40.14ms
step:2313/2330 train_time:92828ms step_avg:40.13ms
step:2314/2330 train_time:92874ms step_avg:40.14ms
step:2315/2330 train_time:92908ms step_avg:40.13ms
step:2316/2330 train_time:92953ms step_avg:40.14ms
step:2317/2330 train_time:92988ms step_avg:40.13ms
step:2318/2330 train_time:93033ms step_avg:40.14ms
step:2319/2330 train_time:93068ms step_avg:40.13ms
step:2320/2330 train_time:93112ms step_avg:40.13ms
step:2321/2330 train_time:93147ms step_avg:40.13ms
step:2322/2330 train_time:93192ms step_avg:40.13ms
step:2323/2330 train_time:93226ms step_avg:40.13ms
step:2324/2330 train_time:93270ms step_avg:40.13ms
step:2325/2330 train_time:93304ms step_avg:40.13ms
step:2326/2330 train_time:93349ms step_avg:40.13ms
step:2327/2330 train_time:93383ms step_avg:40.13ms
step:2328/2330 train_time:93427ms step_avg:40.13ms
step:2329/2330 train_time:93461ms step_avg:40.13ms
step:2330/2330 train_time:93506ms step_avg:40.13ms
step:2330/2330 val_loss:5.1021 train_time:93594ms step_avg:40.17ms
peak memory allocated: 29226 MiB reserved: 38888 MiB
