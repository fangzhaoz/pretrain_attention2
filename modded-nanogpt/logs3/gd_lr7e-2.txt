import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def divide_by_norm(G: torch.Tensor):
    X = G.bfloat16()
    norms = X.flatten(1).norm(dim=1, keepdim=True).clamp_min(1e-6) 
    X = X / norms.view(-1, 1, 1)
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = divide_by_norm(batched)
                # v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = divide_by_norm(batched_update_grads)
                # v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # self.attn = None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = "gd_lr7e-2"
    os.makedirs("logs3", exist_ok=True)
    logfile = f"logs3/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
# embed_params = [p for n, p in model.named_parameters() if ("embed" in n) and ("value_embeds" not in n)]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=7e-2, momentum=0.95, weight_decay=0.0, custom_sizing=False)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    return 1.0

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum_min + 0.5 * (momentum_max - momentum_min)

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs3/{run_id}", exist_ok=True)
            torch.save(log, f"logs3/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251028+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Oct 29 07:53:22 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   33C    P0             112W / 700W |   5775MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   31C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   28C    P0             112W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   31C    P0             110W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             109W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             116W / 700W |   1465MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:81ms step_avg:81.19ms
step:2/2330 train_time:197ms step_avg:98.64ms
step:3/2330 train_time:218ms step_avg:72.64ms
step:4/2330 train_time:247ms step_avg:61.85ms
step:5/2330 train_time:304ms step_avg:60.83ms
step:6/2330 train_time:365ms step_avg:60.80ms
step:7/2330 train_time:422ms step_avg:60.33ms
step:8/2330 train_time:484ms step_avg:60.47ms
step:9/2330 train_time:542ms step_avg:60.22ms
step:10/2330 train_time:603ms step_avg:60.33ms
step:11/2330 train_time:661ms step_avg:60.05ms
step:12/2330 train_time:722ms step_avg:60.16ms
step:13/2330 train_time:780ms step_avg:59.99ms
step:14/2330 train_time:841ms step_avg:60.10ms
step:15/2330 train_time:899ms step_avg:59.96ms
step:16/2330 train_time:961ms step_avg:60.08ms
step:17/2330 train_time:1019ms step_avg:59.94ms
step:18/2330 train_time:1083ms step_avg:60.17ms
step:19/2330 train_time:1147ms step_avg:60.36ms
step:20/2330 train_time:1209ms step_avg:60.47ms
step:21/2330 train_time:1269ms step_avg:60.43ms
step:22/2330 train_time:1331ms step_avg:60.51ms
step:23/2330 train_time:1390ms step_avg:60.42ms
step:24/2330 train_time:1452ms step_avg:60.51ms
step:25/2330 train_time:1510ms step_avg:60.41ms
step:26/2330 train_time:1573ms step_avg:60.50ms
step:27/2330 train_time:1631ms step_avg:60.41ms
step:28/2330 train_time:1693ms step_avg:60.45ms
step:29/2330 train_time:1752ms step_avg:60.40ms
step:30/2330 train_time:1813ms step_avg:60.45ms
step:31/2330 train_time:1872ms step_avg:60.38ms
step:32/2330 train_time:1934ms step_avg:60.44ms
step:33/2330 train_time:1992ms step_avg:60.37ms
step:34/2330 train_time:2055ms step_avg:60.44ms
step:35/2330 train_time:2113ms step_avg:60.38ms
step:36/2330 train_time:2176ms step_avg:60.44ms
step:37/2330 train_time:2235ms step_avg:60.41ms
step:38/2330 train_time:2298ms step_avg:60.48ms
step:39/2330 train_time:2358ms step_avg:60.46ms
step:40/2330 train_time:2421ms step_avg:60.52ms
step:41/2330 train_time:2480ms step_avg:60.48ms
step:42/2330 train_time:2543ms step_avg:60.54ms
step:43/2330 train_time:2601ms step_avg:60.49ms
step:44/2330 train_time:2662ms step_avg:60.51ms
step:45/2330 train_time:2721ms step_avg:60.48ms
step:46/2330 train_time:2783ms step_avg:60.50ms
step:47/2330 train_time:2842ms step_avg:60.46ms
step:48/2330 train_time:2904ms step_avg:60.49ms
step:49/2330 train_time:2962ms step_avg:60.45ms
step:50/2330 train_time:3024ms step_avg:60.48ms
step:51/2330 train_time:3082ms step_avg:60.43ms
step:52/2330 train_time:3144ms step_avg:60.47ms
step:53/2330 train_time:3203ms step_avg:60.44ms
step:54/2330 train_time:3265ms step_avg:60.47ms
step:55/2330 train_time:3323ms step_avg:60.42ms
step:56/2330 train_time:3385ms step_avg:60.45ms
step:57/2330 train_time:3444ms step_avg:60.42ms
step:58/2330 train_time:3506ms step_avg:60.45ms
step:59/2330 train_time:3564ms step_avg:60.41ms
step:60/2330 train_time:3627ms step_avg:60.45ms
step:61/2330 train_time:3685ms step_avg:60.41ms
step:62/2330 train_time:3747ms step_avg:60.44ms
step:63/2330 train_time:3806ms step_avg:60.41ms
step:64/2330 train_time:3868ms step_avg:60.44ms
step:65/2330 train_time:3926ms step_avg:60.40ms
step:66/2330 train_time:3988ms step_avg:60.42ms
step:67/2330 train_time:4047ms step_avg:60.41ms
step:68/2330 train_time:4110ms step_avg:60.44ms
step:69/2330 train_time:4169ms step_avg:60.42ms
step:70/2330 train_time:4230ms step_avg:60.43ms
step:71/2330 train_time:4289ms step_avg:60.40ms
step:72/2330 train_time:4351ms step_avg:60.43ms
step:73/2330 train_time:4409ms step_avg:60.39ms
step:74/2330 train_time:4471ms step_avg:60.43ms
step:75/2330 train_time:4529ms step_avg:60.39ms
step:76/2330 train_time:4592ms step_avg:60.42ms
step:77/2330 train_time:4652ms step_avg:60.41ms
step:78/2330 train_time:4714ms step_avg:60.43ms
step:79/2330 train_time:4772ms step_avg:60.41ms
step:80/2330 train_time:4835ms step_avg:60.44ms
step:81/2330 train_time:4893ms step_avg:60.41ms
step:82/2330 train_time:4956ms step_avg:60.44ms
step:83/2330 train_time:5015ms step_avg:60.42ms
step:84/2330 train_time:5076ms step_avg:60.43ms
step:85/2330 train_time:5135ms step_avg:60.41ms
step:86/2330 train_time:5196ms step_avg:60.42ms
step:87/2330 train_time:5256ms step_avg:60.41ms
step:88/2330 train_time:5318ms step_avg:60.43ms
step:89/2330 train_time:5378ms step_avg:60.43ms
step:90/2330 train_time:5440ms step_avg:60.45ms
step:91/2330 train_time:5499ms step_avg:60.43ms
step:92/2330 train_time:5562ms step_avg:60.46ms
step:93/2330 train_time:5621ms step_avg:60.44ms
step:94/2330 train_time:5683ms step_avg:60.45ms
step:95/2330 train_time:5741ms step_avg:60.43ms
step:96/2330 train_time:5804ms step_avg:60.46ms
step:97/2330 train_time:5863ms step_avg:60.44ms
step:98/2330 train_time:5925ms step_avg:60.46ms
step:99/2330 train_time:5983ms step_avg:60.44ms
step:100/2330 train_time:6045ms step_avg:60.45ms
step:101/2330 train_time:6104ms step_avg:60.43ms
step:102/2330 train_time:6166ms step_avg:60.45ms
step:103/2330 train_time:6224ms step_avg:60.43ms
step:104/2330 train_time:6287ms step_avg:60.45ms
step:105/2330 train_time:6346ms step_avg:60.43ms
step:106/2330 train_time:6408ms step_avg:60.46ms
step:107/2330 train_time:6467ms step_avg:60.44ms
step:108/2330 train_time:6530ms step_avg:60.46ms
step:109/2330 train_time:6589ms step_avg:60.45ms
step:110/2330 train_time:6651ms step_avg:60.47ms
step:111/2330 train_time:6710ms step_avg:60.45ms
step:112/2330 train_time:6772ms step_avg:60.47ms
step:113/2330 train_time:6831ms step_avg:60.45ms
step:114/2330 train_time:6893ms step_avg:60.47ms
step:115/2330 train_time:6952ms step_avg:60.45ms
step:116/2330 train_time:7013ms step_avg:60.46ms
step:117/2330 train_time:7072ms step_avg:60.44ms
step:118/2330 train_time:7134ms step_avg:60.46ms
step:119/2330 train_time:7193ms step_avg:60.44ms
step:120/2330 train_time:7255ms step_avg:60.46ms
step:121/2330 train_time:7314ms step_avg:60.44ms
step:122/2330 train_time:7376ms step_avg:60.46ms
step:123/2330 train_time:7436ms step_avg:60.45ms
step:124/2330 train_time:7499ms step_avg:60.47ms
step:125/2330 train_time:7558ms step_avg:60.47ms
step:126/2330 train_time:7621ms step_avg:60.48ms
step:127/2330 train_time:7680ms step_avg:60.48ms
step:128/2330 train_time:7744ms step_avg:60.50ms
step:129/2330 train_time:7802ms step_avg:60.48ms
step:130/2330 train_time:7864ms step_avg:60.50ms
step:131/2330 train_time:7923ms step_avg:60.48ms
step:132/2330 train_time:7984ms step_avg:60.49ms
step:133/2330 train_time:8043ms step_avg:60.47ms
step:134/2330 train_time:8105ms step_avg:60.48ms
step:135/2330 train_time:8163ms step_avg:60.47ms
step:136/2330 train_time:8225ms step_avg:60.47ms
step:137/2330 train_time:8283ms step_avg:60.46ms
step:138/2330 train_time:8346ms step_avg:60.48ms
step:139/2330 train_time:8404ms step_avg:60.46ms
step:140/2330 train_time:8466ms step_avg:60.47ms
step:141/2330 train_time:8524ms step_avg:60.46ms
step:142/2330 train_time:8586ms step_avg:60.47ms
step:143/2330 train_time:8645ms step_avg:60.46ms
step:144/2330 train_time:8708ms step_avg:60.47ms
step:145/2330 train_time:8766ms step_avg:60.46ms
step:146/2330 train_time:8828ms step_avg:60.47ms
step:147/2330 train_time:8887ms step_avg:60.45ms
step:148/2330 train_time:8950ms step_avg:60.47ms
step:149/2330 train_time:9007ms step_avg:60.45ms
step:150/2330 train_time:9069ms step_avg:60.46ms
step:151/2330 train_time:9128ms step_avg:60.45ms
step:152/2330 train_time:9190ms step_avg:60.46ms
step:153/2330 train_time:9248ms step_avg:60.45ms
step:154/2330 train_time:9311ms step_avg:60.46ms
step:155/2330 train_time:9369ms step_avg:60.45ms
step:156/2330 train_time:9433ms step_avg:60.47ms
step:157/2330 train_time:9491ms step_avg:60.45ms
step:158/2330 train_time:9554ms step_avg:60.47ms
step:159/2330 train_time:9612ms step_avg:60.45ms
step:160/2330 train_time:9675ms step_avg:60.47ms
step:161/2330 train_time:9733ms step_avg:60.46ms
step:162/2330 train_time:9796ms step_avg:60.47ms
step:163/2330 train_time:9855ms step_avg:60.46ms
step:164/2330 train_time:9917ms step_avg:60.47ms
step:165/2330 train_time:9976ms step_avg:60.46ms
step:166/2330 train_time:10039ms step_avg:60.47ms
step:167/2330 train_time:10098ms step_avg:60.47ms
step:168/2330 train_time:10160ms step_avg:60.48ms
step:169/2330 train_time:10219ms step_avg:60.47ms
step:170/2330 train_time:10281ms step_avg:60.48ms
step:171/2330 train_time:10341ms step_avg:60.47ms
step:172/2330 train_time:10403ms step_avg:60.48ms
step:173/2330 train_time:10462ms step_avg:60.47ms
step:174/2330 train_time:10524ms step_avg:60.48ms
step:175/2330 train_time:10582ms step_avg:60.47ms
step:176/2330 train_time:10646ms step_avg:60.49ms
step:177/2330 train_time:10704ms step_avg:60.48ms
step:178/2330 train_time:10766ms step_avg:60.48ms
step:179/2330 train_time:10823ms step_avg:60.46ms
step:180/2330 train_time:10885ms step_avg:60.47ms
step:181/2330 train_time:10944ms step_avg:60.46ms
step:182/2330 train_time:11006ms step_avg:60.47ms
step:183/2330 train_time:11065ms step_avg:60.46ms
step:184/2330 train_time:11127ms step_avg:60.47ms
step:185/2330 train_time:11185ms step_avg:60.46ms
step:186/2330 train_time:11247ms step_avg:60.47ms
step:187/2330 train_time:11306ms step_avg:60.46ms
step:188/2330 train_time:11368ms step_avg:60.47ms
step:189/2330 train_time:11426ms step_avg:60.45ms
step:190/2330 train_time:11489ms step_avg:60.47ms
step:191/2330 train_time:11548ms step_avg:60.46ms
step:192/2330 train_time:11611ms step_avg:60.47ms
step:193/2330 train_time:11669ms step_avg:60.46ms
step:194/2330 train_time:11731ms step_avg:60.47ms
step:195/2330 train_time:11789ms step_avg:60.46ms
step:196/2330 train_time:11852ms step_avg:60.47ms
step:197/2330 train_time:11910ms step_avg:60.46ms
step:198/2330 train_time:11972ms step_avg:60.46ms
step:199/2330 train_time:12030ms step_avg:60.45ms
step:200/2330 train_time:12093ms step_avg:60.46ms
step:201/2330 train_time:12151ms step_avg:60.45ms
step:202/2330 train_time:12213ms step_avg:60.46ms
step:203/2330 train_time:12272ms step_avg:60.45ms
step:204/2330 train_time:12335ms step_avg:60.46ms
step:205/2330 train_time:12393ms step_avg:60.46ms
step:206/2330 train_time:12456ms step_avg:60.47ms
step:207/2330 train_time:12515ms step_avg:60.46ms
step:208/2330 train_time:12578ms step_avg:60.47ms
step:209/2330 train_time:12637ms step_avg:60.46ms
step:210/2330 train_time:12699ms step_avg:60.47ms
step:211/2330 train_time:12758ms step_avg:60.47ms
step:212/2330 train_time:12821ms step_avg:60.47ms
step:213/2330 train_time:12879ms step_avg:60.47ms
step:214/2330 train_time:12942ms step_avg:60.48ms
step:215/2330 train_time:13001ms step_avg:60.47ms
step:216/2330 train_time:13063ms step_avg:60.48ms
step:217/2330 train_time:13122ms step_avg:60.47ms
step:218/2330 train_time:13184ms step_avg:60.48ms
step:219/2330 train_time:13243ms step_avg:60.47ms
step:220/2330 train_time:13304ms step_avg:60.47ms
step:221/2330 train_time:13363ms step_avg:60.47ms
step:222/2330 train_time:13425ms step_avg:60.47ms
step:223/2330 train_time:13483ms step_avg:60.46ms
step:224/2330 train_time:13546ms step_avg:60.47ms
step:225/2330 train_time:13605ms step_avg:60.47ms
step:226/2330 train_time:13667ms step_avg:60.47ms
step:227/2330 train_time:13725ms step_avg:60.46ms
step:228/2330 train_time:13787ms step_avg:60.47ms
step:229/2330 train_time:13847ms step_avg:60.47ms
step:230/2330 train_time:13908ms step_avg:60.47ms
step:231/2330 train_time:13968ms step_avg:60.47ms
step:232/2330 train_time:14030ms step_avg:60.47ms
step:233/2330 train_time:14088ms step_avg:60.46ms
step:234/2330 train_time:14151ms step_avg:60.47ms
step:235/2330 train_time:14209ms step_avg:60.46ms
step:236/2330 train_time:14272ms step_avg:60.47ms
step:237/2330 train_time:14330ms step_avg:60.47ms
step:238/2330 train_time:14392ms step_avg:60.47ms
step:239/2330 train_time:14451ms step_avg:60.46ms
step:240/2330 train_time:14513ms step_avg:60.47ms
step:241/2330 train_time:14572ms step_avg:60.46ms
step:242/2330 train_time:14634ms step_avg:60.47ms
step:243/2330 train_time:14693ms step_avg:60.46ms
step:244/2330 train_time:14756ms step_avg:60.48ms
step:245/2330 train_time:14815ms step_avg:60.47ms
step:246/2330 train_time:14878ms step_avg:60.48ms
step:247/2330 train_time:14937ms step_avg:60.47ms
step:248/2330 train_time:15000ms step_avg:60.48ms
step:249/2330 train_time:15060ms step_avg:60.48ms
step:250/2330 train_time:15122ms step_avg:60.49ms
step:250/2330 val_loss:4.7900 train_time:15192ms step_avg:60.77ms
step:251/2330 train_time:15213ms step_avg:60.61ms
step:252/2330 train_time:15244ms step_avg:60.49ms
step:253/2330 train_time:15308ms step_avg:60.51ms
step:254/2330 train_time:15373ms step_avg:60.52ms
step:255/2330 train_time:15435ms step_avg:60.53ms
step:256/2330 train_time:15499ms step_avg:60.54ms
step:257/2330 train_time:15558ms step_avg:60.54ms
step:258/2330 train_time:15621ms step_avg:60.55ms
step:259/2330 train_time:15680ms step_avg:60.54ms
step:260/2330 train_time:15740ms step_avg:60.54ms
step:261/2330 train_time:15799ms step_avg:60.53ms
step:262/2330 train_time:15860ms step_avg:60.54ms
step:263/2330 train_time:15918ms step_avg:60.52ms
step:264/2330 train_time:15979ms step_avg:60.53ms
step:265/2330 train_time:16037ms step_avg:60.52ms
step:266/2330 train_time:16099ms step_avg:60.52ms
step:267/2330 train_time:16158ms step_avg:60.52ms
step:268/2330 train_time:16221ms step_avg:60.53ms
step:269/2330 train_time:16282ms step_avg:60.53ms
step:270/2330 train_time:16346ms step_avg:60.54ms
step:271/2330 train_time:16407ms step_avg:60.54ms
step:272/2330 train_time:16468ms step_avg:60.54ms
step:273/2330 train_time:16526ms step_avg:60.54ms
step:274/2330 train_time:16588ms step_avg:60.54ms
step:275/2330 train_time:16647ms step_avg:60.53ms
step:276/2330 train_time:16709ms step_avg:60.54ms
step:277/2330 train_time:16767ms step_avg:60.53ms
step:278/2330 train_time:16829ms step_avg:60.54ms
step:279/2330 train_time:16888ms step_avg:60.53ms
step:280/2330 train_time:16950ms step_avg:60.54ms
step:281/2330 train_time:17010ms step_avg:60.53ms
step:282/2330 train_time:17071ms step_avg:60.54ms
step:283/2330 train_time:17130ms step_avg:60.53ms
step:284/2330 train_time:17193ms step_avg:60.54ms
step:285/2330 train_time:17252ms step_avg:60.53ms
step:286/2330 train_time:17314ms step_avg:60.54ms
step:287/2330 train_time:17372ms step_avg:60.53ms
step:288/2330 train_time:17434ms step_avg:60.54ms
step:289/2330 train_time:17493ms step_avg:60.53ms
step:290/2330 train_time:17555ms step_avg:60.54ms
step:291/2330 train_time:17614ms step_avg:60.53ms
step:292/2330 train_time:17677ms step_avg:60.54ms
step:293/2330 train_time:17736ms step_avg:60.53ms
step:294/2330 train_time:17798ms step_avg:60.54ms
step:295/2330 train_time:17857ms step_avg:60.53ms
step:296/2330 train_time:17919ms step_avg:60.54ms
step:297/2330 train_time:17979ms step_avg:60.54ms
step:298/2330 train_time:18042ms step_avg:60.54ms
step:299/2330 train_time:18102ms step_avg:60.54ms
step:300/2330 train_time:18164ms step_avg:60.55ms
step:301/2330 train_time:18222ms step_avg:60.54ms
step:302/2330 train_time:18284ms step_avg:60.54ms
step:303/2330 train_time:18344ms step_avg:60.54ms
step:304/2330 train_time:18406ms step_avg:60.55ms
step:305/2330 train_time:18465ms step_avg:60.54ms
step:306/2330 train_time:18527ms step_avg:60.54ms
step:307/2330 train_time:18585ms step_avg:60.54ms
step:308/2330 train_time:18647ms step_avg:60.54ms
step:309/2330 train_time:18706ms step_avg:60.54ms
step:310/2330 train_time:18768ms step_avg:60.54ms
step:311/2330 train_time:18827ms step_avg:60.54ms
step:312/2330 train_time:18889ms step_avg:60.54ms
step:313/2330 train_time:18948ms step_avg:60.54ms
step:314/2330 train_time:19011ms step_avg:60.54ms
step:315/2330 train_time:19069ms step_avg:60.54ms
step:316/2330 train_time:19132ms step_avg:60.54ms
step:317/2330 train_time:19190ms step_avg:60.54ms
step:318/2330 train_time:19253ms step_avg:60.54ms
step:319/2330 train_time:19311ms step_avg:60.54ms
step:320/2330 train_time:19373ms step_avg:60.54ms
step:321/2330 train_time:19431ms step_avg:60.53ms
step:322/2330 train_time:19493ms step_avg:60.54ms
step:323/2330 train_time:19552ms step_avg:60.53ms
step:324/2330 train_time:19615ms step_avg:60.54ms
step:325/2330 train_time:19672ms step_avg:60.53ms
step:326/2330 train_time:19735ms step_avg:60.54ms
step:327/2330 train_time:19793ms step_avg:60.53ms
step:328/2330 train_time:19856ms step_avg:60.54ms
step:329/2330 train_time:19915ms step_avg:60.53ms
step:330/2330 train_time:19978ms step_avg:60.54ms
step:331/2330 train_time:20038ms step_avg:60.54ms
step:332/2330 train_time:20100ms step_avg:60.54ms
step:333/2330 train_time:20159ms step_avg:60.54ms
step:334/2330 train_time:20221ms step_avg:60.54ms
step:335/2330 train_time:20281ms step_avg:60.54ms
step:336/2330 train_time:20343ms step_avg:60.54ms
step:337/2330 train_time:20402ms step_avg:60.54ms
step:338/2330 train_time:20464ms step_avg:60.55ms
step:339/2330 train_time:20523ms step_avg:60.54ms
step:340/2330 train_time:20586ms step_avg:60.55ms
step:341/2330 train_time:20644ms step_avg:60.54ms
step:342/2330 train_time:20706ms step_avg:60.54ms
step:343/2330 train_time:20765ms step_avg:60.54ms
step:344/2330 train_time:20827ms step_avg:60.54ms
step:345/2330 train_time:20886ms step_avg:60.54ms
step:346/2330 train_time:20948ms step_avg:60.54ms
step:347/2330 train_time:21007ms step_avg:60.54ms
step:348/2330 train_time:21070ms step_avg:60.54ms
step:349/2330 train_time:21129ms step_avg:60.54ms
step:350/2330 train_time:21191ms step_avg:60.55ms
step:351/2330 train_time:21250ms step_avg:60.54ms
step:352/2330 train_time:21313ms step_avg:60.55ms
step:353/2330 train_time:21372ms step_avg:60.54ms
step:354/2330 train_time:21434ms step_avg:60.55ms
step:355/2330 train_time:21492ms step_avg:60.54ms
step:356/2330 train_time:21555ms step_avg:60.55ms
step:357/2330 train_time:21613ms step_avg:60.54ms
step:358/2330 train_time:21675ms step_avg:60.54ms
step:359/2330 train_time:21734ms step_avg:60.54ms
step:360/2330 train_time:21796ms step_avg:60.54ms
step:361/2330 train_time:21855ms step_avg:60.54ms
step:362/2330 train_time:21918ms step_avg:60.55ms
step:363/2330 train_time:21977ms step_avg:60.54ms
step:364/2330 train_time:22040ms step_avg:60.55ms
step:365/2330 train_time:22100ms step_avg:60.55ms
step:366/2330 train_time:22163ms step_avg:60.55ms
step:367/2330 train_time:22222ms step_avg:60.55ms
step:368/2330 train_time:22285ms step_avg:60.56ms
step:369/2330 train_time:22345ms step_avg:60.56ms
step:370/2330 train_time:22407ms step_avg:60.56ms
step:371/2330 train_time:22465ms step_avg:60.55ms
step:372/2330 train_time:22527ms step_avg:60.56ms
step:373/2330 train_time:22585ms step_avg:60.55ms
step:374/2330 train_time:22647ms step_avg:60.55ms
step:375/2330 train_time:22705ms step_avg:60.55ms
step:376/2330 train_time:22767ms step_avg:60.55ms
step:377/2330 train_time:22825ms step_avg:60.54ms
step:378/2330 train_time:22888ms step_avg:60.55ms
step:379/2330 train_time:22947ms step_avg:60.54ms
step:380/2330 train_time:23009ms step_avg:60.55ms
step:381/2330 train_time:23068ms step_avg:60.55ms
step:382/2330 train_time:23131ms step_avg:60.55ms
step:383/2330 train_time:23190ms step_avg:60.55ms
step:384/2330 train_time:23254ms step_avg:60.56ms
step:385/2330 train_time:23313ms step_avg:60.55ms
step:386/2330 train_time:23374ms step_avg:60.56ms
step:387/2330 train_time:23433ms step_avg:60.55ms
step:388/2330 train_time:23495ms step_avg:60.56ms
step:389/2330 train_time:23553ms step_avg:60.55ms
step:390/2330 train_time:23616ms step_avg:60.55ms
step:391/2330 train_time:23674ms step_avg:60.55ms
step:392/2330 train_time:23736ms step_avg:60.55ms
step:393/2330 train_time:23795ms step_avg:60.55ms
step:394/2330 train_time:23857ms step_avg:60.55ms
step:395/2330 train_time:23916ms step_avg:60.55ms
step:396/2330 train_time:23979ms step_avg:60.55ms
step:397/2330 train_time:24038ms step_avg:60.55ms
step:398/2330 train_time:24101ms step_avg:60.56ms
step:399/2330 train_time:24162ms step_avg:60.56ms
step:400/2330 train_time:24224ms step_avg:60.56ms
step:401/2330 train_time:24284ms step_avg:60.56ms
step:402/2330 train_time:24346ms step_avg:60.56ms
step:403/2330 train_time:24404ms step_avg:60.56ms
step:404/2330 train_time:24467ms step_avg:60.56ms
step:405/2330 train_time:24525ms step_avg:60.56ms
step:406/2330 train_time:24587ms step_avg:60.56ms
step:407/2330 train_time:24646ms step_avg:60.55ms
step:408/2330 train_time:24707ms step_avg:60.56ms
step:409/2330 train_time:24765ms step_avg:60.55ms
step:410/2330 train_time:24827ms step_avg:60.55ms
step:411/2330 train_time:24886ms step_avg:60.55ms
step:412/2330 train_time:24947ms step_avg:60.55ms
step:413/2330 train_time:25007ms step_avg:60.55ms
step:414/2330 train_time:25070ms step_avg:60.55ms
step:415/2330 train_time:25129ms step_avg:60.55ms
step:416/2330 train_time:25192ms step_avg:60.56ms
step:417/2330 train_time:25251ms step_avg:60.55ms
step:418/2330 train_time:25314ms step_avg:60.56ms
step:419/2330 train_time:25372ms step_avg:60.55ms
step:420/2330 train_time:25434ms step_avg:60.56ms
step:421/2330 train_time:25493ms step_avg:60.55ms
step:422/2330 train_time:25555ms step_avg:60.56ms
step:423/2330 train_time:25613ms step_avg:60.55ms
step:424/2330 train_time:25676ms step_avg:60.56ms
step:425/2330 train_time:25734ms step_avg:60.55ms
step:426/2330 train_time:25797ms step_avg:60.56ms
step:427/2330 train_time:25855ms step_avg:60.55ms
step:428/2330 train_time:25918ms step_avg:60.55ms
step:429/2330 train_time:25976ms step_avg:60.55ms
step:430/2330 train_time:26040ms step_avg:60.56ms
step:431/2330 train_time:26101ms step_avg:60.56ms
step:432/2330 train_time:26163ms step_avg:60.56ms
step:433/2330 train_time:26223ms step_avg:60.56ms
step:434/2330 train_time:26285ms step_avg:60.56ms
step:435/2330 train_time:26344ms step_avg:60.56ms
step:436/2330 train_time:26406ms step_avg:60.57ms
step:437/2330 train_time:26464ms step_avg:60.56ms
step:438/2330 train_time:26526ms step_avg:60.56ms
step:439/2330 train_time:26585ms step_avg:60.56ms
step:440/2330 train_time:26646ms step_avg:60.56ms
step:441/2330 train_time:26705ms step_avg:60.56ms
step:442/2330 train_time:26768ms step_avg:60.56ms
step:443/2330 train_time:26826ms step_avg:60.55ms
step:444/2330 train_time:26889ms step_avg:60.56ms
step:445/2330 train_time:26947ms step_avg:60.56ms
step:446/2330 train_time:27010ms step_avg:60.56ms
step:447/2330 train_time:27069ms step_avg:60.56ms
step:448/2330 train_time:27132ms step_avg:60.56ms
step:449/2330 train_time:27192ms step_avg:60.56ms
step:450/2330 train_time:27255ms step_avg:60.57ms
step:451/2330 train_time:27314ms step_avg:60.56ms
step:452/2330 train_time:27376ms step_avg:60.57ms
step:453/2330 train_time:27435ms step_avg:60.56ms
step:454/2330 train_time:27497ms step_avg:60.57ms
step:455/2330 train_time:27557ms step_avg:60.56ms
step:456/2330 train_time:27619ms step_avg:60.57ms
step:457/2330 train_time:27677ms step_avg:60.56ms
step:458/2330 train_time:27740ms step_avg:60.57ms
step:459/2330 train_time:27799ms step_avg:60.57ms
step:460/2330 train_time:27863ms step_avg:60.57ms
step:461/2330 train_time:27922ms step_avg:60.57ms
step:462/2330 train_time:27984ms step_avg:60.57ms
step:463/2330 train_time:28043ms step_avg:60.57ms
step:464/2330 train_time:28106ms step_avg:60.57ms
step:465/2330 train_time:28165ms step_avg:60.57ms
step:466/2330 train_time:28226ms step_avg:60.57ms
step:467/2330 train_time:28285ms step_avg:60.57ms
step:468/2330 train_time:28346ms step_avg:60.57ms
step:469/2330 train_time:28405ms step_avg:60.57ms
step:470/2330 train_time:28468ms step_avg:60.57ms
step:471/2330 train_time:28526ms step_avg:60.56ms
step:472/2330 train_time:28588ms step_avg:60.57ms
step:473/2330 train_time:28647ms step_avg:60.56ms
step:474/2330 train_time:28710ms step_avg:60.57ms
step:475/2330 train_time:28769ms step_avg:60.57ms
step:476/2330 train_time:28831ms step_avg:60.57ms
step:477/2330 train_time:28889ms step_avg:60.56ms
step:478/2330 train_time:28952ms step_avg:60.57ms
step:479/2330 train_time:29010ms step_avg:60.56ms
step:480/2330 train_time:29072ms step_avg:60.57ms
step:481/2330 train_time:29131ms step_avg:60.56ms
step:482/2330 train_time:29193ms step_avg:60.57ms
step:483/2330 train_time:29252ms step_avg:60.56ms
step:484/2330 train_time:29316ms step_avg:60.57ms
step:485/2330 train_time:29374ms step_avg:60.56ms
step:486/2330 train_time:29436ms step_avg:60.57ms
step:487/2330 train_time:29495ms step_avg:60.56ms
step:488/2330 train_time:29558ms step_avg:60.57ms
step:489/2330 train_time:29617ms step_avg:60.57ms
step:490/2330 train_time:29679ms step_avg:60.57ms
step:491/2330 train_time:29739ms step_avg:60.57ms
step:492/2330 train_time:29801ms step_avg:60.57ms
step:493/2330 train_time:29861ms step_avg:60.57ms
step:494/2330 train_time:29924ms step_avg:60.57ms
step:495/2330 train_time:29983ms step_avg:60.57ms
step:496/2330 train_time:30045ms step_avg:60.57ms
step:497/2330 train_time:30104ms step_avg:60.57ms
step:498/2330 train_time:30166ms step_avg:60.57ms
step:499/2330 train_time:30225ms step_avg:60.57ms
step:500/2330 train_time:30287ms step_avg:60.57ms
step:500/2330 val_loss:4.2319 train_time:30356ms step_avg:60.71ms
step:501/2330 train_time:30377ms step_avg:60.63ms
step:502/2330 train_time:30408ms step_avg:60.57ms
step:503/2330 train_time:30468ms step_avg:60.57ms
step:504/2330 train_time:30534ms step_avg:60.58ms
step:505/2330 train_time:30595ms step_avg:60.58ms
step:506/2330 train_time:30658ms step_avg:60.59ms
step:507/2330 train_time:30716ms step_avg:60.58ms
step:508/2330 train_time:30777ms step_avg:60.59ms
step:509/2330 train_time:30835ms step_avg:60.58ms
step:510/2330 train_time:30897ms step_avg:60.58ms
step:511/2330 train_time:30956ms step_avg:60.58ms
step:512/2330 train_time:31017ms step_avg:60.58ms
step:513/2330 train_time:31075ms step_avg:60.58ms
step:514/2330 train_time:31136ms step_avg:60.58ms
step:515/2330 train_time:31195ms step_avg:60.57ms
step:516/2330 train_time:31257ms step_avg:60.58ms
step:517/2330 train_time:31315ms step_avg:60.57ms
step:518/2330 train_time:31379ms step_avg:60.58ms
step:519/2330 train_time:31439ms step_avg:60.58ms
step:520/2330 train_time:31503ms step_avg:60.58ms
step:521/2330 train_time:31564ms step_avg:60.58ms
step:522/2330 train_time:31627ms step_avg:60.59ms
step:523/2330 train_time:31686ms step_avg:60.58ms
step:524/2330 train_time:31749ms step_avg:60.59ms
step:525/2330 train_time:31806ms step_avg:60.58ms
step:526/2330 train_time:31869ms step_avg:60.59ms
step:527/2330 train_time:31927ms step_avg:60.58ms
step:528/2330 train_time:31988ms step_avg:60.58ms
step:529/2330 train_time:32047ms step_avg:60.58ms
step:530/2330 train_time:32109ms step_avg:60.58ms
step:531/2330 train_time:32168ms step_avg:60.58ms
step:532/2330 train_time:32229ms step_avg:60.58ms
step:533/2330 train_time:32288ms step_avg:60.58ms
step:534/2330 train_time:32350ms step_avg:60.58ms
step:535/2330 train_time:32408ms step_avg:60.58ms
step:536/2330 train_time:32472ms step_avg:60.58ms
step:537/2330 train_time:32531ms step_avg:60.58ms
step:538/2330 train_time:32595ms step_avg:60.59ms
step:539/2330 train_time:32655ms step_avg:60.58ms
step:540/2330 train_time:32718ms step_avg:60.59ms
step:541/2330 train_time:32777ms step_avg:60.59ms
step:542/2330 train_time:32838ms step_avg:60.59ms
step:543/2330 train_time:32896ms step_avg:60.58ms
step:544/2330 train_time:32959ms step_avg:60.59ms
step:545/2330 train_time:33017ms step_avg:60.58ms
step:546/2330 train_time:33080ms step_avg:60.59ms
step:547/2330 train_time:33138ms step_avg:60.58ms
step:548/2330 train_time:33200ms step_avg:60.58ms
step:549/2330 train_time:33259ms step_avg:60.58ms
step:550/2330 train_time:33321ms step_avg:60.58ms
step:551/2330 train_time:33381ms step_avg:60.58ms
step:552/2330 train_time:33444ms step_avg:60.59ms
step:553/2330 train_time:33503ms step_avg:60.58ms
step:554/2330 train_time:33567ms step_avg:60.59ms
step:555/2330 train_time:33626ms step_avg:60.59ms
step:556/2330 train_time:33688ms step_avg:60.59ms
step:557/2330 train_time:33748ms step_avg:60.59ms
step:558/2330 train_time:33809ms step_avg:60.59ms
step:559/2330 train_time:33867ms step_avg:60.59ms
step:560/2330 train_time:33929ms step_avg:60.59ms
step:561/2330 train_time:33988ms step_avg:60.58ms
step:562/2330 train_time:34050ms step_avg:60.59ms
step:563/2330 train_time:34109ms step_avg:60.58ms
step:564/2330 train_time:34171ms step_avg:60.59ms
step:565/2330 train_time:34230ms step_avg:60.58ms
step:566/2330 train_time:34292ms step_avg:60.59ms
step:567/2330 train_time:34351ms step_avg:60.58ms
step:568/2330 train_time:34413ms step_avg:60.59ms
step:569/2330 train_time:34473ms step_avg:60.58ms
step:570/2330 train_time:34536ms step_avg:60.59ms
step:571/2330 train_time:34594ms step_avg:60.59ms
step:572/2330 train_time:34658ms step_avg:60.59ms
step:573/2330 train_time:34716ms step_avg:60.59ms
step:574/2330 train_time:34779ms step_avg:60.59ms
step:575/2330 train_time:34837ms step_avg:60.59ms
step:576/2330 train_time:34900ms step_avg:60.59ms
step:577/2330 train_time:34958ms step_avg:60.59ms
step:578/2330 train_time:35020ms step_avg:60.59ms
step:579/2330 train_time:35079ms step_avg:60.59ms
step:580/2330 train_time:35142ms step_avg:60.59ms
step:581/2330 train_time:35201ms step_avg:60.59ms
step:582/2330 train_time:35264ms step_avg:60.59ms
step:583/2330 train_time:35323ms step_avg:60.59ms
step:584/2330 train_time:35386ms step_avg:60.59ms
step:585/2330 train_time:35446ms step_avg:60.59ms
step:586/2330 train_time:35508ms step_avg:60.59ms
step:587/2330 train_time:35566ms step_avg:60.59ms
step:588/2330 train_time:35628ms step_avg:60.59ms
step:589/2330 train_time:35687ms step_avg:60.59ms
step:590/2330 train_time:35749ms step_avg:60.59ms
step:591/2330 train_time:35808ms step_avg:60.59ms
step:592/2330 train_time:35870ms step_avg:60.59ms
step:593/2330 train_time:35928ms step_avg:60.59ms
step:594/2330 train_time:35991ms step_avg:60.59ms
step:595/2330 train_time:36049ms step_avg:60.59ms
step:596/2330 train_time:36111ms step_avg:60.59ms
step:597/2330 train_time:36170ms step_avg:60.59ms
step:598/2330 train_time:36233ms step_avg:60.59ms
step:599/2330 train_time:36291ms step_avg:60.59ms
step:600/2330 train_time:36354ms step_avg:60.59ms
step:601/2330 train_time:36413ms step_avg:60.59ms
step:602/2330 train_time:36476ms step_avg:60.59ms
step:603/2330 train_time:36535ms step_avg:60.59ms
step:604/2330 train_time:36597ms step_avg:60.59ms
step:605/2330 train_time:36656ms step_avg:60.59ms
step:606/2330 train_time:36717ms step_avg:60.59ms
step:607/2330 train_time:36776ms step_avg:60.59ms
step:608/2330 train_time:36839ms step_avg:60.59ms
step:609/2330 train_time:36898ms step_avg:60.59ms
step:610/2330 train_time:36961ms step_avg:60.59ms
step:611/2330 train_time:37020ms step_avg:60.59ms
step:612/2330 train_time:37083ms step_avg:60.59ms
step:613/2330 train_time:37142ms step_avg:60.59ms
step:614/2330 train_time:37204ms step_avg:60.59ms
step:615/2330 train_time:37264ms step_avg:60.59ms
step:616/2330 train_time:37326ms step_avg:60.59ms
step:617/2330 train_time:37385ms step_avg:60.59ms
step:618/2330 train_time:37447ms step_avg:60.59ms
step:619/2330 train_time:37506ms step_avg:60.59ms
step:620/2330 train_time:37569ms step_avg:60.60ms
step:621/2330 train_time:37627ms step_avg:60.59ms
step:622/2330 train_time:37689ms step_avg:60.59ms
step:623/2330 train_time:37748ms step_avg:60.59ms
step:624/2330 train_time:37809ms step_avg:60.59ms
step:625/2330 train_time:37868ms step_avg:60.59ms
step:626/2330 train_time:37931ms step_avg:60.59ms
step:627/2330 train_time:37990ms step_avg:60.59ms
step:628/2330 train_time:38052ms step_avg:60.59ms
step:629/2330 train_time:38111ms step_avg:60.59ms
step:630/2330 train_time:38174ms step_avg:60.59ms
step:631/2330 train_time:38233ms step_avg:60.59ms
step:632/2330 train_time:38296ms step_avg:60.59ms
step:633/2330 train_time:38354ms step_avg:60.59ms
step:634/2330 train_time:38416ms step_avg:60.59ms
step:635/2330 train_time:38476ms step_avg:60.59ms
step:636/2330 train_time:38538ms step_avg:60.59ms
step:637/2330 train_time:38596ms step_avg:60.59ms
step:638/2330 train_time:38658ms step_avg:60.59ms
step:639/2330 train_time:38717ms step_avg:60.59ms
step:640/2330 train_time:38779ms step_avg:60.59ms
step:641/2330 train_time:38838ms step_avg:60.59ms
step:642/2330 train_time:38900ms step_avg:60.59ms
step:643/2330 train_time:38959ms step_avg:60.59ms
step:644/2330 train_time:39022ms step_avg:60.59ms
step:645/2330 train_time:39082ms step_avg:60.59ms
step:646/2330 train_time:39145ms step_avg:60.60ms
step:647/2330 train_time:39205ms step_avg:60.59ms
step:648/2330 train_time:39266ms step_avg:60.60ms
step:649/2330 train_time:39325ms step_avg:60.59ms
step:650/2330 train_time:39388ms step_avg:60.60ms
step:651/2330 train_time:39447ms step_avg:60.59ms
step:652/2330 train_time:39509ms step_avg:60.60ms
step:653/2330 train_time:39567ms step_avg:60.59ms
step:654/2330 train_time:39629ms step_avg:60.59ms
step:655/2330 train_time:39687ms step_avg:60.59ms
step:656/2330 train_time:39749ms step_avg:60.59ms
step:657/2330 train_time:39808ms step_avg:60.59ms
step:658/2330 train_time:39871ms step_avg:60.59ms
step:659/2330 train_time:39929ms step_avg:60.59ms
step:660/2330 train_time:39992ms step_avg:60.59ms
step:661/2330 train_time:40051ms step_avg:60.59ms
step:662/2330 train_time:40114ms step_avg:60.60ms
step:663/2330 train_time:40173ms step_avg:60.59ms
step:664/2330 train_time:40235ms step_avg:60.59ms
step:665/2330 train_time:40294ms step_avg:60.59ms
step:666/2330 train_time:40358ms step_avg:60.60ms
step:667/2330 train_time:40416ms step_avg:60.59ms
step:668/2330 train_time:40478ms step_avg:60.60ms
step:669/2330 train_time:40536ms step_avg:60.59ms
step:670/2330 train_time:40599ms step_avg:60.60ms
step:671/2330 train_time:40657ms step_avg:60.59ms
step:672/2330 train_time:40720ms step_avg:60.59ms
step:673/2330 train_time:40778ms step_avg:60.59ms
step:674/2330 train_time:40841ms step_avg:60.60ms
step:675/2330 train_time:40901ms step_avg:60.59ms
step:676/2330 train_time:40963ms step_avg:60.60ms
step:677/2330 train_time:41022ms step_avg:60.59ms
step:678/2330 train_time:41085ms step_avg:60.60ms
step:679/2330 train_time:41144ms step_avg:60.60ms
step:680/2330 train_time:41207ms step_avg:60.60ms
step:681/2330 train_time:41266ms step_avg:60.60ms
step:682/2330 train_time:41327ms step_avg:60.60ms
step:683/2330 train_time:41386ms step_avg:60.59ms
step:684/2330 train_time:41448ms step_avg:60.60ms
step:685/2330 train_time:41507ms step_avg:60.59ms
step:686/2330 train_time:41568ms step_avg:60.60ms
step:687/2330 train_time:41627ms step_avg:60.59ms
step:688/2330 train_time:41688ms step_avg:60.59ms
step:689/2330 train_time:41747ms step_avg:60.59ms
step:690/2330 train_time:41810ms step_avg:60.59ms
step:691/2330 train_time:41868ms step_avg:60.59ms
step:692/2330 train_time:41931ms step_avg:60.59ms
step:693/2330 train_time:41990ms step_avg:60.59ms
step:694/2330 train_time:42052ms step_avg:60.59ms
step:695/2330 train_time:42111ms step_avg:60.59ms
step:696/2330 train_time:42174ms step_avg:60.59ms
step:697/2330 train_time:42232ms step_avg:60.59ms
step:698/2330 train_time:42294ms step_avg:60.59ms
step:699/2330 train_time:42354ms step_avg:60.59ms
step:700/2330 train_time:42417ms step_avg:60.60ms
step:701/2330 train_time:42476ms step_avg:60.59ms
step:702/2330 train_time:42538ms step_avg:60.60ms
step:703/2330 train_time:42596ms step_avg:60.59ms
step:704/2330 train_time:42659ms step_avg:60.59ms
step:705/2330 train_time:42717ms step_avg:60.59ms
step:706/2330 train_time:42780ms step_avg:60.59ms
step:707/2330 train_time:42839ms step_avg:60.59ms
step:708/2330 train_time:42901ms step_avg:60.60ms
step:709/2330 train_time:42961ms step_avg:60.59ms
step:710/2330 train_time:43024ms step_avg:60.60ms
step:711/2330 train_time:43083ms step_avg:60.60ms
step:712/2330 train_time:43147ms step_avg:60.60ms
step:713/2330 train_time:43206ms step_avg:60.60ms
step:714/2330 train_time:43269ms step_avg:60.60ms
step:715/2330 train_time:43328ms step_avg:60.60ms
step:716/2330 train_time:43390ms step_avg:60.60ms
step:717/2330 train_time:43449ms step_avg:60.60ms
step:718/2330 train_time:43511ms step_avg:60.60ms
step:719/2330 train_time:43570ms step_avg:60.60ms
step:720/2330 train_time:43631ms step_avg:60.60ms
step:721/2330 train_time:43690ms step_avg:60.60ms
step:722/2330 train_time:43753ms step_avg:60.60ms
step:723/2330 train_time:43812ms step_avg:60.60ms
step:724/2330 train_time:43874ms step_avg:60.60ms
step:725/2330 train_time:43933ms step_avg:60.60ms
step:726/2330 train_time:43995ms step_avg:60.60ms
step:727/2330 train_time:44054ms step_avg:60.60ms
step:728/2330 train_time:44117ms step_avg:60.60ms
step:729/2330 train_time:44176ms step_avg:60.60ms
step:730/2330 train_time:44239ms step_avg:60.60ms
step:731/2330 train_time:44297ms step_avg:60.60ms
step:732/2330 train_time:44360ms step_avg:60.60ms
step:733/2330 train_time:44419ms step_avg:60.60ms
step:734/2330 train_time:44481ms step_avg:60.60ms
step:735/2330 train_time:44541ms step_avg:60.60ms
step:736/2330 train_time:44603ms step_avg:60.60ms
step:737/2330 train_time:44662ms step_avg:60.60ms
step:738/2330 train_time:44725ms step_avg:60.60ms
step:739/2330 train_time:44784ms step_avg:60.60ms
step:740/2330 train_time:44847ms step_avg:60.60ms
step:741/2330 train_time:44907ms step_avg:60.60ms
step:742/2330 train_time:44969ms step_avg:60.60ms
step:743/2330 train_time:45028ms step_avg:60.60ms
step:744/2330 train_time:45089ms step_avg:60.60ms
step:745/2330 train_time:45149ms step_avg:60.60ms
step:746/2330 train_time:45210ms step_avg:60.60ms
step:747/2330 train_time:45268ms step_avg:60.60ms
step:748/2330 train_time:45331ms step_avg:60.60ms
step:749/2330 train_time:45389ms step_avg:60.60ms
step:750/2330 train_time:45454ms step_avg:60.61ms
step:750/2330 val_loss:4.0189 train_time:45525ms step_avg:60.70ms
step:751/2330 train_time:45547ms step_avg:60.65ms
step:752/2330 train_time:45578ms step_avg:60.61ms
step:753/2330 train_time:45638ms step_avg:60.61ms
step:754/2330 train_time:45705ms step_avg:60.62ms
step:755/2330 train_time:45764ms step_avg:60.61ms
step:756/2330 train_time:45826ms step_avg:60.62ms
step:757/2330 train_time:45884ms step_avg:60.61ms
step:758/2330 train_time:45946ms step_avg:60.61ms
step:759/2330 train_time:46004ms step_avg:60.61ms
step:760/2330 train_time:46066ms step_avg:60.61ms
step:761/2330 train_time:46124ms step_avg:60.61ms
step:762/2330 train_time:46186ms step_avg:60.61ms
step:763/2330 train_time:46243ms step_avg:60.61ms
step:764/2330 train_time:46306ms step_avg:60.61ms
step:765/2330 train_time:46365ms step_avg:60.61ms
step:766/2330 train_time:46426ms step_avg:60.61ms
step:767/2330 train_time:46486ms step_avg:60.61ms
step:768/2330 train_time:46551ms step_avg:60.61ms
step:769/2330 train_time:46612ms step_avg:60.61ms
step:770/2330 train_time:46678ms step_avg:60.62ms
step:771/2330 train_time:46738ms step_avg:60.62ms
step:772/2330 train_time:46801ms step_avg:60.62ms
step:773/2330 train_time:46860ms step_avg:60.62ms
step:774/2330 train_time:46924ms step_avg:60.63ms
step:775/2330 train_time:46983ms step_avg:60.62ms
step:776/2330 train_time:47045ms step_avg:60.63ms
step:777/2330 train_time:47105ms step_avg:60.62ms
step:778/2330 train_time:47167ms step_avg:60.63ms
step:779/2330 train_time:47225ms step_avg:60.62ms
step:780/2330 train_time:47287ms step_avg:60.62ms
step:781/2330 train_time:47346ms step_avg:60.62ms
step:782/2330 train_time:47409ms step_avg:60.62ms
step:783/2330 train_time:47468ms step_avg:60.62ms
step:784/2330 train_time:47530ms step_avg:60.63ms
step:785/2330 train_time:47592ms step_avg:60.63ms
step:786/2330 train_time:47656ms step_avg:60.63ms
step:787/2330 train_time:47716ms step_avg:60.63ms
step:788/2330 train_time:47779ms step_avg:60.63ms
step:789/2330 train_time:47839ms step_avg:60.63ms
step:790/2330 train_time:47902ms step_avg:60.64ms
step:791/2330 train_time:47961ms step_avg:60.63ms
step:792/2330 train_time:48024ms step_avg:60.64ms
step:793/2330 train_time:48082ms step_avg:60.63ms
step:794/2330 train_time:48145ms step_avg:60.64ms
step:795/2330 train_time:48204ms step_avg:60.63ms
step:796/2330 train_time:48267ms step_avg:60.64ms
step:797/2330 train_time:48325ms step_avg:60.63ms
step:798/2330 train_time:48388ms step_avg:60.64ms
step:799/2330 train_time:48447ms step_avg:60.63ms
step:800/2330 train_time:48510ms step_avg:60.64ms
step:801/2330 train_time:48570ms step_avg:60.64ms
step:802/2330 train_time:48634ms step_avg:60.64ms
step:803/2330 train_time:48695ms step_avg:60.64ms
step:804/2330 train_time:48758ms step_avg:60.64ms
step:805/2330 train_time:48818ms step_avg:60.64ms
step:806/2330 train_time:48880ms step_avg:60.65ms
step:807/2330 train_time:48940ms step_avg:60.64ms
step:808/2330 train_time:49002ms step_avg:60.65ms
step:809/2330 train_time:49062ms step_avg:60.64ms
step:810/2330 train_time:49125ms step_avg:60.65ms
step:811/2330 train_time:49184ms step_avg:60.65ms
step:812/2330 train_time:49247ms step_avg:60.65ms
step:813/2330 train_time:49306ms step_avg:60.65ms
step:814/2330 train_time:49368ms step_avg:60.65ms
step:815/2330 train_time:49428ms step_avg:60.65ms
step:816/2330 train_time:49490ms step_avg:60.65ms
step:817/2330 train_time:49550ms step_avg:60.65ms
step:818/2330 train_time:49613ms step_avg:60.65ms
step:819/2330 train_time:49673ms step_avg:60.65ms
step:820/2330 train_time:49738ms step_avg:60.66ms
step:821/2330 train_time:49797ms step_avg:60.65ms
step:822/2330 train_time:49860ms step_avg:60.66ms
step:823/2330 train_time:49919ms step_avg:60.66ms
step:824/2330 train_time:49982ms step_avg:60.66ms
step:825/2330 train_time:50041ms step_avg:60.66ms
step:826/2330 train_time:50105ms step_avg:60.66ms
step:827/2330 train_time:50163ms step_avg:60.66ms
step:828/2330 train_time:50226ms step_avg:60.66ms
step:829/2330 train_time:50284ms step_avg:60.66ms
step:830/2330 train_time:50348ms step_avg:60.66ms
step:831/2330 train_time:50408ms step_avg:60.66ms
step:832/2330 train_time:50469ms step_avg:60.66ms
step:833/2330 train_time:50529ms step_avg:60.66ms
step:834/2330 train_time:50592ms step_avg:60.66ms
step:835/2330 train_time:50652ms step_avg:60.66ms
step:836/2330 train_time:50716ms step_avg:60.67ms
step:837/2330 train_time:50776ms step_avg:60.66ms
step:838/2330 train_time:50839ms step_avg:60.67ms
step:839/2330 train_time:50899ms step_avg:60.67ms
step:840/2330 train_time:50962ms step_avg:60.67ms
step:841/2330 train_time:51021ms step_avg:60.67ms
step:842/2330 train_time:51083ms step_avg:60.67ms
step:843/2330 train_time:51143ms step_avg:60.67ms
step:844/2330 train_time:51206ms step_avg:60.67ms
step:845/2330 train_time:51264ms step_avg:60.67ms
step:846/2330 train_time:51328ms step_avg:60.67ms
step:847/2330 train_time:51386ms step_avg:60.67ms
step:848/2330 train_time:51448ms step_avg:60.67ms
step:849/2330 train_time:51508ms step_avg:60.67ms
step:850/2330 train_time:51570ms step_avg:60.67ms
step:851/2330 train_time:51629ms step_avg:60.67ms
step:852/2330 train_time:51694ms step_avg:60.67ms
step:853/2330 train_time:51754ms step_avg:60.67ms
step:854/2330 train_time:51817ms step_avg:60.68ms
step:855/2330 train_time:51877ms step_avg:60.67ms
step:856/2330 train_time:51940ms step_avg:60.68ms
step:857/2330 train_time:51999ms step_avg:60.68ms
step:858/2330 train_time:52061ms step_avg:60.68ms
step:859/2330 train_time:52121ms step_avg:60.68ms
step:860/2330 train_time:52184ms step_avg:60.68ms
step:861/2330 train_time:52242ms step_avg:60.68ms
step:862/2330 train_time:52305ms step_avg:60.68ms
step:863/2330 train_time:52365ms step_avg:60.68ms
step:864/2330 train_time:52428ms step_avg:60.68ms
step:865/2330 train_time:52487ms step_avg:60.68ms
step:866/2330 train_time:52550ms step_avg:60.68ms
step:867/2330 train_time:52610ms step_avg:60.68ms
step:868/2330 train_time:52673ms step_avg:60.68ms
step:869/2330 train_time:52732ms step_avg:60.68ms
step:870/2330 train_time:52796ms step_avg:60.68ms
step:871/2330 train_time:52856ms step_avg:60.68ms
step:872/2330 train_time:52919ms step_avg:60.69ms
step:873/2330 train_time:52978ms step_avg:60.69ms
step:874/2330 train_time:53040ms step_avg:60.69ms
step:875/2330 train_time:53100ms step_avg:60.69ms
step:876/2330 train_time:53162ms step_avg:60.69ms
step:877/2330 train_time:53221ms step_avg:60.69ms
step:878/2330 train_time:53285ms step_avg:60.69ms
step:879/2330 train_time:53344ms step_avg:60.69ms
step:880/2330 train_time:53409ms step_avg:60.69ms
step:881/2330 train_time:53468ms step_avg:60.69ms
step:882/2330 train_time:53531ms step_avg:60.69ms
step:883/2330 train_time:53590ms step_avg:60.69ms
step:884/2330 train_time:53653ms step_avg:60.69ms
step:885/2330 train_time:53713ms step_avg:60.69ms
step:886/2330 train_time:53776ms step_avg:60.70ms
step:887/2330 train_time:53836ms step_avg:60.69ms
step:888/2330 train_time:53899ms step_avg:60.70ms
step:889/2330 train_time:53958ms step_avg:60.70ms
step:890/2330 train_time:54021ms step_avg:60.70ms
step:891/2330 train_time:54081ms step_avg:60.70ms
step:892/2330 train_time:54143ms step_avg:60.70ms
step:893/2330 train_time:54203ms step_avg:60.70ms
step:894/2330 train_time:54265ms step_avg:60.70ms
step:895/2330 train_time:54324ms step_avg:60.70ms
step:896/2330 train_time:54387ms step_avg:60.70ms
step:897/2330 train_time:54445ms step_avg:60.70ms
step:898/2330 train_time:54509ms step_avg:60.70ms
step:899/2330 train_time:54568ms step_avg:60.70ms
step:900/2330 train_time:54631ms step_avg:60.70ms
step:901/2330 train_time:54691ms step_avg:60.70ms
step:902/2330 train_time:54754ms step_avg:60.70ms
step:903/2330 train_time:54814ms step_avg:60.70ms
step:904/2330 train_time:54877ms step_avg:60.70ms
step:905/2330 train_time:54936ms step_avg:60.70ms
step:906/2330 train_time:55000ms step_avg:60.71ms
step:907/2330 train_time:55058ms step_avg:60.70ms
step:908/2330 train_time:55121ms step_avg:60.71ms
step:909/2330 train_time:55179ms step_avg:60.70ms
step:910/2330 train_time:55242ms step_avg:60.71ms
step:911/2330 train_time:55302ms step_avg:60.71ms
step:912/2330 train_time:55365ms step_avg:60.71ms
step:913/2330 train_time:55425ms step_avg:60.71ms
step:914/2330 train_time:55488ms step_avg:60.71ms
step:915/2330 train_time:55547ms step_avg:60.71ms
step:916/2330 train_time:55610ms step_avg:60.71ms
step:917/2330 train_time:55669ms step_avg:60.71ms
step:918/2330 train_time:55732ms step_avg:60.71ms
step:919/2330 train_time:55792ms step_avg:60.71ms
step:920/2330 train_time:55856ms step_avg:60.71ms
step:921/2330 train_time:55916ms step_avg:60.71ms
step:922/2330 train_time:55979ms step_avg:60.71ms
step:923/2330 train_time:56038ms step_avg:60.71ms
step:924/2330 train_time:56101ms step_avg:60.72ms
step:925/2330 train_time:56161ms step_avg:60.71ms
step:926/2330 train_time:56223ms step_avg:60.72ms
step:927/2330 train_time:56281ms step_avg:60.71ms
step:928/2330 train_time:56344ms step_avg:60.72ms
step:929/2330 train_time:56405ms step_avg:60.72ms
step:930/2330 train_time:56467ms step_avg:60.72ms
step:931/2330 train_time:56525ms step_avg:60.71ms
step:932/2330 train_time:56588ms step_avg:60.72ms
step:933/2330 train_time:56648ms step_avg:60.72ms
step:934/2330 train_time:56711ms step_avg:60.72ms
step:935/2330 train_time:56770ms step_avg:60.72ms
step:936/2330 train_time:56833ms step_avg:60.72ms
step:937/2330 train_time:56893ms step_avg:60.72ms
step:938/2330 train_time:56957ms step_avg:60.72ms
step:939/2330 train_time:57016ms step_avg:60.72ms
step:940/2330 train_time:57079ms step_avg:60.72ms
step:941/2330 train_time:57138ms step_avg:60.72ms
step:942/2330 train_time:57202ms step_avg:60.72ms
step:943/2330 train_time:57261ms step_avg:60.72ms
step:944/2330 train_time:57325ms step_avg:60.73ms
step:945/2330 train_time:57383ms step_avg:60.72ms
step:946/2330 train_time:57447ms step_avg:60.73ms
step:947/2330 train_time:57506ms step_avg:60.72ms
step:948/2330 train_time:57568ms step_avg:60.73ms
step:949/2330 train_time:57627ms step_avg:60.72ms
step:950/2330 train_time:57690ms step_avg:60.73ms
step:951/2330 train_time:57749ms step_avg:60.72ms
step:952/2330 train_time:57812ms step_avg:60.73ms
step:953/2330 train_time:57871ms step_avg:60.73ms
step:954/2330 train_time:57935ms step_avg:60.73ms
step:955/2330 train_time:57995ms step_avg:60.73ms
step:956/2330 train_time:58059ms step_avg:60.73ms
step:957/2330 train_time:58117ms step_avg:60.73ms
step:958/2330 train_time:58180ms step_avg:60.73ms
step:959/2330 train_time:58240ms step_avg:60.73ms
step:960/2330 train_time:58303ms step_avg:60.73ms
step:961/2330 train_time:58362ms step_avg:60.73ms
step:962/2330 train_time:58425ms step_avg:60.73ms
step:963/2330 train_time:58484ms step_avg:60.73ms
step:964/2330 train_time:58546ms step_avg:60.73ms
step:965/2330 train_time:58606ms step_avg:60.73ms
step:966/2330 train_time:58669ms step_avg:60.73ms
step:967/2330 train_time:58728ms step_avg:60.73ms
step:968/2330 train_time:58791ms step_avg:60.73ms
step:969/2330 train_time:58851ms step_avg:60.73ms
step:970/2330 train_time:58914ms step_avg:60.74ms
step:971/2330 train_time:58974ms step_avg:60.74ms
step:972/2330 train_time:59038ms step_avg:60.74ms
step:973/2330 train_time:59098ms step_avg:60.74ms
step:974/2330 train_time:59161ms step_avg:60.74ms
step:975/2330 train_time:59220ms step_avg:60.74ms
step:976/2330 train_time:59282ms step_avg:60.74ms
step:977/2330 train_time:59342ms step_avg:60.74ms
step:978/2330 train_time:59405ms step_avg:60.74ms
step:979/2330 train_time:59463ms step_avg:60.74ms
step:980/2330 train_time:59527ms step_avg:60.74ms
step:981/2330 train_time:59586ms step_avg:60.74ms
step:982/2330 train_time:59650ms step_avg:60.74ms
step:983/2330 train_time:59710ms step_avg:60.74ms
step:984/2330 train_time:59772ms step_avg:60.74ms
step:985/2330 train_time:59832ms step_avg:60.74ms
step:986/2330 train_time:59895ms step_avg:60.75ms
step:987/2330 train_time:59955ms step_avg:60.74ms
step:988/2330 train_time:60017ms step_avg:60.75ms
step:989/2330 train_time:60077ms step_avg:60.75ms
step:990/2330 train_time:60140ms step_avg:60.75ms
step:991/2330 train_time:60200ms step_avg:60.75ms
step:992/2330 train_time:60262ms step_avg:60.75ms
step:993/2330 train_time:60322ms step_avg:60.75ms
step:994/2330 train_time:60384ms step_avg:60.75ms
step:995/2330 train_time:60444ms step_avg:60.75ms
step:996/2330 train_time:60508ms step_avg:60.75ms
step:997/2330 train_time:60567ms step_avg:60.75ms
step:998/2330 train_time:60629ms step_avg:60.75ms
step:999/2330 train_time:60689ms step_avg:60.75ms
step:1000/2330 train_time:60751ms step_avg:60.75ms
step:1000/2330 val_loss:3.8621 train_time:60823ms step_avg:60.82ms
step:1001/2330 train_time:60844ms step_avg:60.78ms
step:1002/2330 train_time:60875ms step_avg:60.75ms
step:1003/2330 train_time:60934ms step_avg:60.75ms
step:1004/2330 train_time:61004ms step_avg:60.76ms
step:1005/2330 train_time:61068ms step_avg:60.76ms
step:1006/2330 train_time:61131ms step_avg:60.77ms
step:1007/2330 train_time:61190ms step_avg:60.76ms
step:1008/2330 train_time:61252ms step_avg:60.77ms
step:1009/2330 train_time:61311ms step_avg:60.76ms
step:1010/2330 train_time:61373ms step_avg:60.77ms
step:1011/2330 train_time:61432ms step_avg:60.76ms
step:1012/2330 train_time:61494ms step_avg:60.76ms
step:1013/2330 train_time:61553ms step_avg:60.76ms
step:1014/2330 train_time:61614ms step_avg:60.76ms
step:1015/2330 train_time:61673ms step_avg:60.76ms
step:1016/2330 train_time:61735ms step_avg:60.76ms
step:1017/2330 train_time:61798ms step_avg:60.76ms
step:1018/2330 train_time:61862ms step_avg:60.77ms
step:1019/2330 train_time:61923ms step_avg:60.77ms
step:1020/2330 train_time:61987ms step_avg:60.77ms
step:1021/2330 train_time:62048ms step_avg:60.77ms
step:1022/2330 train_time:62112ms step_avg:60.78ms
step:1023/2330 train_time:62171ms step_avg:60.77ms
step:1024/2330 train_time:62234ms step_avg:60.78ms
step:1025/2330 train_time:62293ms step_avg:60.77ms
step:1026/2330 train_time:62355ms step_avg:60.78ms
step:1027/2330 train_time:62414ms step_avg:60.77ms
step:1028/2330 train_time:62476ms step_avg:60.77ms
step:1029/2330 train_time:62534ms step_avg:60.77ms
step:1030/2330 train_time:62597ms step_avg:60.77ms
step:1031/2330 train_time:62656ms step_avg:60.77ms
step:1032/2330 train_time:62719ms step_avg:60.77ms
step:1033/2330 train_time:62778ms step_avg:60.77ms
step:1034/2330 train_time:62841ms step_avg:60.77ms
step:1035/2330 train_time:62902ms step_avg:60.77ms
step:1036/2330 train_time:62965ms step_avg:60.78ms
step:1037/2330 train_time:63025ms step_avg:60.78ms
step:1038/2330 train_time:63090ms step_avg:60.78ms
step:1039/2330 train_time:63150ms step_avg:60.78ms
step:1040/2330 train_time:63213ms step_avg:60.78ms
step:1041/2330 train_time:63272ms step_avg:60.78ms
step:1042/2330 train_time:63334ms step_avg:60.78ms
step:1043/2330 train_time:63394ms step_avg:60.78ms
step:1044/2330 train_time:63456ms step_avg:60.78ms
step:1045/2330 train_time:63515ms step_avg:60.78ms
step:1046/2330 train_time:63577ms step_avg:60.78ms
step:1047/2330 train_time:63637ms step_avg:60.78ms
step:1048/2330 train_time:63699ms step_avg:60.78ms
step:1049/2330 train_time:63759ms step_avg:60.78ms
step:1050/2330 train_time:63823ms step_avg:60.78ms
step:1051/2330 train_time:63883ms step_avg:60.78ms
step:1052/2330 train_time:63947ms step_avg:60.79ms
step:1053/2330 train_time:64006ms step_avg:60.78ms
step:1054/2330 train_time:64069ms step_avg:60.79ms
step:1055/2330 train_time:64128ms step_avg:60.79ms
step:1056/2330 train_time:64193ms step_avg:60.79ms
step:1057/2330 train_time:64251ms step_avg:60.79ms
step:1058/2330 train_time:64314ms step_avg:60.79ms
step:1059/2330 train_time:64372ms step_avg:60.79ms
step:1060/2330 train_time:64435ms step_avg:60.79ms
step:1061/2330 train_time:64494ms step_avg:60.79ms
step:1062/2330 train_time:64557ms step_avg:60.79ms
step:1063/2330 train_time:64616ms step_avg:60.79ms
step:1064/2330 train_time:64679ms step_avg:60.79ms
step:1065/2330 train_time:64739ms step_avg:60.79ms
step:1066/2330 train_time:64803ms step_avg:60.79ms
step:1067/2330 train_time:64863ms step_avg:60.79ms
step:1068/2330 train_time:64926ms step_avg:60.79ms
step:1069/2330 train_time:64986ms step_avg:60.79ms
step:1070/2330 train_time:65050ms step_avg:60.79ms
step:1071/2330 train_time:65109ms step_avg:60.79ms
step:1072/2330 train_time:65172ms step_avg:60.79ms
step:1073/2330 train_time:65232ms step_avg:60.79ms
step:1074/2330 train_time:65295ms step_avg:60.80ms
step:1075/2330 train_time:65354ms step_avg:60.79ms
step:1076/2330 train_time:65417ms step_avg:60.80ms
step:1077/2330 train_time:65476ms step_avg:60.79ms
step:1078/2330 train_time:65539ms step_avg:60.80ms
step:1079/2330 train_time:65598ms step_avg:60.80ms
step:1080/2330 train_time:65660ms step_avg:60.80ms
step:1081/2330 train_time:65719ms step_avg:60.79ms
step:1082/2330 train_time:65783ms step_avg:60.80ms
step:1083/2330 train_time:65842ms step_avg:60.80ms
step:1084/2330 train_time:65905ms step_avg:60.80ms
step:1085/2330 train_time:65964ms step_avg:60.80ms
step:1086/2330 train_time:66028ms step_avg:60.80ms
step:1087/2330 train_time:66088ms step_avg:60.80ms
step:1088/2330 train_time:66152ms step_avg:60.80ms
step:1089/2330 train_time:66211ms step_avg:60.80ms
step:1090/2330 train_time:66274ms step_avg:60.80ms
step:1091/2330 train_time:66333ms step_avg:60.80ms
step:1092/2330 train_time:66397ms step_avg:60.80ms
step:1093/2330 train_time:66456ms step_avg:60.80ms
step:1094/2330 train_time:66519ms step_avg:60.80ms
step:1095/2330 train_time:66577ms step_avg:60.80ms
step:1096/2330 train_time:66640ms step_avg:60.80ms
step:1097/2330 train_time:66700ms step_avg:60.80ms
step:1098/2330 train_time:66763ms step_avg:60.80ms
step:1099/2330 train_time:66823ms step_avg:60.80ms
step:1100/2330 train_time:66886ms step_avg:60.81ms
step:1101/2330 train_time:66945ms step_avg:60.80ms
step:1102/2330 train_time:67009ms step_avg:60.81ms
step:1103/2330 train_time:67068ms step_avg:60.81ms
step:1104/2330 train_time:67132ms step_avg:60.81ms
step:1105/2330 train_time:67192ms step_avg:60.81ms
step:1106/2330 train_time:67254ms step_avg:60.81ms
step:1107/2330 train_time:67314ms step_avg:60.81ms
step:1108/2330 train_time:67377ms step_avg:60.81ms
step:1109/2330 train_time:67437ms step_avg:60.81ms
step:1110/2330 train_time:67500ms step_avg:60.81ms
step:1111/2330 train_time:67558ms step_avg:60.81ms
step:1112/2330 train_time:67622ms step_avg:60.81ms
step:1113/2330 train_time:67681ms step_avg:60.81ms
step:1114/2330 train_time:67744ms step_avg:60.81ms
step:1115/2330 train_time:67804ms step_avg:60.81ms
step:1116/2330 train_time:67867ms step_avg:60.81ms
step:1117/2330 train_time:67926ms step_avg:60.81ms
step:1118/2330 train_time:67990ms step_avg:60.81ms
step:1119/2330 train_time:68049ms step_avg:60.81ms
step:1120/2330 train_time:68112ms step_avg:60.81ms
step:1121/2330 train_time:68172ms step_avg:60.81ms
step:1122/2330 train_time:68234ms step_avg:60.81ms
step:1123/2330 train_time:68294ms step_avg:60.81ms
step:1124/2330 train_time:68357ms step_avg:60.82ms
step:1125/2330 train_time:68416ms step_avg:60.81ms
step:1126/2330 train_time:68479ms step_avg:60.82ms
step:1127/2330 train_time:68538ms step_avg:60.81ms
step:1128/2330 train_time:68602ms step_avg:60.82ms
step:1129/2330 train_time:68661ms step_avg:60.82ms
step:1130/2330 train_time:68723ms step_avg:60.82ms
step:1131/2330 train_time:68782ms step_avg:60.82ms
step:1132/2330 train_time:68846ms step_avg:60.82ms
step:1133/2330 train_time:68906ms step_avg:60.82ms
step:1134/2330 train_time:68969ms step_avg:60.82ms
step:1135/2330 train_time:69028ms step_avg:60.82ms
step:1136/2330 train_time:69092ms step_avg:60.82ms
step:1137/2330 train_time:69152ms step_avg:60.82ms
step:1138/2330 train_time:69215ms step_avg:60.82ms
step:1139/2330 train_time:69274ms step_avg:60.82ms
step:1140/2330 train_time:69337ms step_avg:60.82ms
step:1141/2330 train_time:69397ms step_avg:60.82ms
step:1142/2330 train_time:69459ms step_avg:60.82ms
step:1143/2330 train_time:69518ms step_avg:60.82ms
step:1144/2330 train_time:69582ms step_avg:60.82ms
step:1145/2330 train_time:69640ms step_avg:60.82ms
step:1146/2330 train_time:69703ms step_avg:60.82ms
step:1147/2330 train_time:69763ms step_avg:60.82ms
step:1148/2330 train_time:69826ms step_avg:60.82ms
step:1149/2330 train_time:69886ms step_avg:60.82ms
step:1150/2330 train_time:69949ms step_avg:60.83ms
step:1151/2330 train_time:70009ms step_avg:60.82ms
step:1152/2330 train_time:70071ms step_avg:60.83ms
step:1153/2330 train_time:70132ms step_avg:60.83ms
step:1154/2330 train_time:70195ms step_avg:60.83ms
step:1155/2330 train_time:70254ms step_avg:60.83ms
step:1156/2330 train_time:70316ms step_avg:60.83ms
step:1157/2330 train_time:70375ms step_avg:60.83ms
step:1158/2330 train_time:70439ms step_avg:60.83ms
step:1159/2330 train_time:70498ms step_avg:60.83ms
step:1160/2330 train_time:70560ms step_avg:60.83ms
step:1161/2330 train_time:70619ms step_avg:60.83ms
step:1162/2330 train_time:70683ms step_avg:60.83ms
step:1163/2330 train_time:70742ms step_avg:60.83ms
step:1164/2330 train_time:70805ms step_avg:60.83ms
step:1165/2330 train_time:70865ms step_avg:60.83ms
step:1166/2330 train_time:70928ms step_avg:60.83ms
step:1167/2330 train_time:70989ms step_avg:60.83ms
step:1168/2330 train_time:71052ms step_avg:60.83ms
step:1169/2330 train_time:71111ms step_avg:60.83ms
step:1170/2330 train_time:71175ms step_avg:60.83ms
step:1171/2330 train_time:71233ms step_avg:60.83ms
step:1172/2330 train_time:71296ms step_avg:60.83ms
step:1173/2330 train_time:71356ms step_avg:60.83ms
step:1174/2330 train_time:71419ms step_avg:60.83ms
step:1175/2330 train_time:71478ms step_avg:60.83ms
step:1176/2330 train_time:71541ms step_avg:60.83ms
step:1177/2330 train_time:71601ms step_avg:60.83ms
step:1178/2330 train_time:71663ms step_avg:60.83ms
step:1179/2330 train_time:71722ms step_avg:60.83ms
step:1180/2330 train_time:71785ms step_avg:60.83ms
step:1181/2330 train_time:71845ms step_avg:60.83ms
step:1182/2330 train_time:71908ms step_avg:60.84ms
step:1183/2330 train_time:71969ms step_avg:60.84ms
step:1184/2330 train_time:72031ms step_avg:60.84ms
step:1185/2330 train_time:72091ms step_avg:60.84ms
step:1186/2330 train_time:72154ms step_avg:60.84ms
step:1187/2330 train_time:72214ms step_avg:60.84ms
step:1188/2330 train_time:72277ms step_avg:60.84ms
step:1189/2330 train_time:72337ms step_avg:60.84ms
step:1190/2330 train_time:72400ms step_avg:60.84ms
step:1191/2330 train_time:72459ms step_avg:60.84ms
step:1192/2330 train_time:72523ms step_avg:60.84ms
step:1193/2330 train_time:72582ms step_avg:60.84ms
step:1194/2330 train_time:72645ms step_avg:60.84ms
step:1195/2330 train_time:72705ms step_avg:60.84ms
step:1196/2330 train_time:72768ms step_avg:60.84ms
step:1197/2330 train_time:72827ms step_avg:60.84ms
step:1198/2330 train_time:72891ms step_avg:60.84ms
step:1199/2330 train_time:72950ms step_avg:60.84ms
step:1200/2330 train_time:73013ms step_avg:60.84ms
step:1201/2330 train_time:73071ms step_avg:60.84ms
step:1202/2330 train_time:73134ms step_avg:60.84ms
step:1203/2330 train_time:73194ms step_avg:60.84ms
step:1204/2330 train_time:73257ms step_avg:60.84ms
step:1205/2330 train_time:73315ms step_avg:60.84ms
step:1206/2330 train_time:73378ms step_avg:60.84ms
step:1207/2330 train_time:73437ms step_avg:60.84ms
step:1208/2330 train_time:73500ms step_avg:60.84ms
step:1209/2330 train_time:73558ms step_avg:60.84ms
step:1210/2330 train_time:73622ms step_avg:60.84ms
step:1211/2330 train_time:73682ms step_avg:60.84ms
step:1212/2330 train_time:73745ms step_avg:60.85ms
step:1213/2330 train_time:73805ms step_avg:60.84ms
step:1214/2330 train_time:73868ms step_avg:60.85ms
step:1215/2330 train_time:73929ms step_avg:60.85ms
step:1216/2330 train_time:73992ms step_avg:60.85ms
step:1217/2330 train_time:74052ms step_avg:60.85ms
step:1218/2330 train_time:74114ms step_avg:60.85ms
step:1219/2330 train_time:74174ms step_avg:60.85ms
step:1220/2330 train_time:74237ms step_avg:60.85ms
step:1221/2330 train_time:74296ms step_avg:60.85ms
step:1222/2330 train_time:74359ms step_avg:60.85ms
step:1223/2330 train_time:74418ms step_avg:60.85ms
step:1224/2330 train_time:74481ms step_avg:60.85ms
step:1225/2330 train_time:74540ms step_avg:60.85ms
step:1226/2330 train_time:74603ms step_avg:60.85ms
step:1227/2330 train_time:74663ms step_avg:60.85ms
step:1228/2330 train_time:74725ms step_avg:60.85ms
step:1229/2330 train_time:74785ms step_avg:60.85ms
step:1230/2330 train_time:74848ms step_avg:60.85ms
step:1231/2330 train_time:74908ms step_avg:60.85ms
step:1232/2330 train_time:74972ms step_avg:60.85ms
step:1233/2330 train_time:75032ms step_avg:60.85ms
step:1234/2330 train_time:75094ms step_avg:60.85ms
step:1235/2330 train_time:75153ms step_avg:60.85ms
step:1236/2330 train_time:75216ms step_avg:60.85ms
step:1237/2330 train_time:75275ms step_avg:60.85ms
step:1238/2330 train_time:75339ms step_avg:60.86ms
step:1239/2330 train_time:75398ms step_avg:60.85ms
step:1240/2330 train_time:75460ms step_avg:60.86ms
step:1241/2330 train_time:75520ms step_avg:60.85ms
step:1242/2330 train_time:75582ms step_avg:60.86ms
step:1243/2330 train_time:75642ms step_avg:60.85ms
step:1244/2330 train_time:75705ms step_avg:60.86ms
step:1245/2330 train_time:75765ms step_avg:60.86ms
step:1246/2330 train_time:75828ms step_avg:60.86ms
step:1247/2330 train_time:75889ms step_avg:60.86ms
step:1248/2330 train_time:75951ms step_avg:60.86ms
step:1249/2330 train_time:76010ms step_avg:60.86ms
step:1250/2330 train_time:76074ms step_avg:60.86ms
step:1250/2330 val_loss:3.8022 train_time:76146ms step_avg:60.92ms
step:1251/2330 train_time:76167ms step_avg:60.89ms
step:1252/2330 train_time:76199ms step_avg:60.86ms
step:1253/2330 train_time:76260ms step_avg:60.86ms
step:1254/2330 train_time:76330ms step_avg:60.87ms
step:1255/2330 train_time:76392ms step_avg:60.87ms
step:1256/2330 train_time:76455ms step_avg:60.87ms
step:1257/2330 train_time:76514ms step_avg:60.87ms
step:1258/2330 train_time:76576ms step_avg:60.87ms
step:1259/2330 train_time:76635ms step_avg:60.87ms
step:1260/2330 train_time:76697ms step_avg:60.87ms
step:1261/2330 train_time:76755ms step_avg:60.87ms
step:1262/2330 train_time:76818ms step_avg:60.87ms
step:1263/2330 train_time:76877ms step_avg:60.87ms
step:1264/2330 train_time:76939ms step_avg:60.87ms
step:1265/2330 train_time:76998ms step_avg:60.87ms
step:1266/2330 train_time:77060ms step_avg:60.87ms
step:1267/2330 train_time:77121ms step_avg:60.87ms
step:1268/2330 train_time:77185ms step_avg:60.87ms
step:1269/2330 train_time:77247ms step_avg:60.87ms
step:1270/2330 train_time:77312ms step_avg:60.88ms
step:1271/2330 train_time:77372ms step_avg:60.88ms
step:1272/2330 train_time:77435ms step_avg:60.88ms
step:1273/2330 train_time:77495ms step_avg:60.88ms
step:1274/2330 train_time:77558ms step_avg:60.88ms
step:1275/2330 train_time:77617ms step_avg:60.88ms
step:1276/2330 train_time:77680ms step_avg:60.88ms
step:1277/2330 train_time:77739ms step_avg:60.88ms
step:1278/2330 train_time:77801ms step_avg:60.88ms
step:1279/2330 train_time:77860ms step_avg:60.88ms
step:1280/2330 train_time:77924ms step_avg:60.88ms
step:1281/2330 train_time:77983ms step_avg:60.88ms
step:1282/2330 train_time:78046ms step_avg:60.88ms
step:1283/2330 train_time:78106ms step_avg:60.88ms
step:1284/2330 train_time:78169ms step_avg:60.88ms
step:1285/2330 train_time:78230ms step_avg:60.88ms
step:1286/2330 train_time:78293ms step_avg:60.88ms
step:1287/2330 train_time:78353ms step_avg:60.88ms
step:1288/2330 train_time:78416ms step_avg:60.88ms
step:1289/2330 train_time:78475ms step_avg:60.88ms
step:1290/2330 train_time:78538ms step_avg:60.88ms
step:1291/2330 train_time:78598ms step_avg:60.88ms
step:1292/2330 train_time:78661ms step_avg:60.88ms
step:1293/2330 train_time:78720ms step_avg:60.88ms
step:1294/2330 train_time:78782ms step_avg:60.88ms
step:1295/2330 train_time:78841ms step_avg:60.88ms
step:1296/2330 train_time:78904ms step_avg:60.88ms
step:1297/2330 train_time:78964ms step_avg:60.88ms
step:1298/2330 train_time:79027ms step_avg:60.88ms
step:1299/2330 train_time:79086ms step_avg:60.88ms
step:1300/2330 train_time:79150ms step_avg:60.88ms
step:1301/2330 train_time:79209ms step_avg:60.88ms
step:1302/2330 train_time:79274ms step_avg:60.89ms
step:1303/2330 train_time:79333ms step_avg:60.89ms
step:1304/2330 train_time:79397ms step_avg:60.89ms
step:1305/2330 train_time:79456ms step_avg:60.89ms
step:1306/2330 train_time:79520ms step_avg:60.89ms
step:1307/2330 train_time:79578ms step_avg:60.89ms
step:1308/2330 train_time:79642ms step_avg:60.89ms
step:1309/2330 train_time:79700ms step_avg:60.89ms
step:1310/2330 train_time:79763ms step_avg:60.89ms
step:1311/2330 train_time:79822ms step_avg:60.89ms
step:1312/2330 train_time:79885ms step_avg:60.89ms
step:1313/2330 train_time:79944ms step_avg:60.89ms
step:1314/2330 train_time:80008ms step_avg:60.89ms
step:1315/2330 train_time:80068ms step_avg:60.89ms
step:1316/2330 train_time:80131ms step_avg:60.89ms
step:1317/2330 train_time:80190ms step_avg:60.89ms
step:1318/2330 train_time:80254ms step_avg:60.89ms
step:1319/2330 train_time:80314ms step_avg:60.89ms
step:1320/2330 train_time:80377ms step_avg:60.89ms
step:1321/2330 train_time:80437ms step_avg:60.89ms
step:1322/2330 train_time:80500ms step_avg:60.89ms
step:1323/2330 train_time:80559ms step_avg:60.89ms
step:1324/2330 train_time:80623ms step_avg:60.89ms
step:1325/2330 train_time:80682ms step_avg:60.89ms
step:1326/2330 train_time:80745ms step_avg:60.89ms
step:1327/2330 train_time:80804ms step_avg:60.89ms
step:1328/2330 train_time:80867ms step_avg:60.89ms
step:1329/2330 train_time:80927ms step_avg:60.89ms
step:1330/2330 train_time:80990ms step_avg:60.89ms
step:1331/2330 train_time:81050ms step_avg:60.89ms
step:1332/2330 train_time:81113ms step_avg:60.90ms
step:1333/2330 train_time:81173ms step_avg:60.89ms
step:1334/2330 train_time:81236ms step_avg:60.90ms
step:1335/2330 train_time:81295ms step_avg:60.89ms
step:1336/2330 train_time:81359ms step_avg:60.90ms
step:1337/2330 train_time:81419ms step_avg:60.90ms
step:1338/2330 train_time:81482ms step_avg:60.90ms
step:1339/2330 train_time:81541ms step_avg:60.90ms
step:1340/2330 train_time:81604ms step_avg:60.90ms
step:1341/2330 train_time:81664ms step_avg:60.90ms
step:1342/2330 train_time:81727ms step_avg:60.90ms
step:1343/2330 train_time:81786ms step_avg:60.90ms
step:1344/2330 train_time:81849ms step_avg:60.90ms
step:1345/2330 train_time:81908ms step_avg:60.90ms
step:1346/2330 train_time:81972ms step_avg:60.90ms
step:1347/2330 train_time:82031ms step_avg:60.90ms
step:1348/2330 train_time:82094ms step_avg:60.90ms
step:1349/2330 train_time:82154ms step_avg:60.90ms
step:1350/2330 train_time:82216ms step_avg:60.90ms
step:1351/2330 train_time:82275ms step_avg:60.90ms
step:1352/2330 train_time:82338ms step_avg:60.90ms
step:1353/2330 train_time:82398ms step_avg:60.90ms
step:1354/2330 train_time:82461ms step_avg:60.90ms
step:1355/2330 train_time:82520ms step_avg:60.90ms
step:1356/2330 train_time:82583ms step_avg:60.90ms
step:1357/2330 train_time:82643ms step_avg:60.90ms
step:1358/2330 train_time:82706ms step_avg:60.90ms
step:1359/2330 train_time:82766ms step_avg:60.90ms
step:1360/2330 train_time:82829ms step_avg:60.90ms
step:1361/2330 train_time:82889ms step_avg:60.90ms
step:1362/2330 train_time:82952ms step_avg:60.90ms
step:1363/2330 train_time:83012ms step_avg:60.90ms
step:1364/2330 train_time:83074ms step_avg:60.90ms
step:1365/2330 train_time:83133ms step_avg:60.90ms
step:1366/2330 train_time:83197ms step_avg:60.91ms
step:1367/2330 train_time:83256ms step_avg:60.90ms
step:1368/2330 train_time:83320ms step_avg:60.91ms
step:1369/2330 train_time:83379ms step_avg:60.91ms
step:1370/2330 train_time:83442ms step_avg:60.91ms
step:1371/2330 train_time:83501ms step_avg:60.91ms
step:1372/2330 train_time:83564ms step_avg:60.91ms
step:1373/2330 train_time:83624ms step_avg:60.91ms
step:1374/2330 train_time:83686ms step_avg:60.91ms
step:1375/2330 train_time:83746ms step_avg:60.91ms
step:1376/2330 train_time:83809ms step_avg:60.91ms
step:1377/2330 train_time:83869ms step_avg:60.91ms
step:1378/2330 train_time:83932ms step_avg:60.91ms
step:1379/2330 train_time:83992ms step_avg:60.91ms
step:1380/2330 train_time:84054ms step_avg:60.91ms
step:1381/2330 train_time:84113ms step_avg:60.91ms
step:1382/2330 train_time:84177ms step_avg:60.91ms
step:1383/2330 train_time:84237ms step_avg:60.91ms
step:1384/2330 train_time:84299ms step_avg:60.91ms
step:1385/2330 train_time:84358ms step_avg:60.91ms
step:1386/2330 train_time:84421ms step_avg:60.91ms
step:1387/2330 train_time:84480ms step_avg:60.91ms
step:1388/2330 train_time:84543ms step_avg:60.91ms
step:1389/2330 train_time:84603ms step_avg:60.91ms
step:1390/2330 train_time:84666ms step_avg:60.91ms
step:1391/2330 train_time:84726ms step_avg:60.91ms
step:1392/2330 train_time:84790ms step_avg:60.91ms
step:1393/2330 train_time:84849ms step_avg:60.91ms
step:1394/2330 train_time:84913ms step_avg:60.91ms
step:1395/2330 train_time:84973ms step_avg:60.91ms
step:1396/2330 train_time:85035ms step_avg:60.91ms
step:1397/2330 train_time:85094ms step_avg:60.91ms
step:1398/2330 train_time:85158ms step_avg:60.91ms
step:1399/2330 train_time:85218ms step_avg:60.91ms
step:1400/2330 train_time:85280ms step_avg:60.91ms
step:1401/2330 train_time:85339ms step_avg:60.91ms
step:1402/2330 train_time:85403ms step_avg:60.91ms
step:1403/2330 train_time:85462ms step_avg:60.91ms
step:1404/2330 train_time:85526ms step_avg:60.92ms
step:1405/2330 train_time:85585ms step_avg:60.91ms
step:1406/2330 train_time:85647ms step_avg:60.92ms
step:1407/2330 train_time:85708ms step_avg:60.92ms
step:1408/2330 train_time:85771ms step_avg:60.92ms
step:1409/2330 train_time:85831ms step_avg:60.92ms
step:1410/2330 train_time:85895ms step_avg:60.92ms
step:1411/2330 train_time:85954ms step_avg:60.92ms
step:1412/2330 train_time:86018ms step_avg:60.92ms
step:1413/2330 train_time:86077ms step_avg:60.92ms
step:1414/2330 train_time:86140ms step_avg:60.92ms
step:1415/2330 train_time:86199ms step_avg:60.92ms
step:1416/2330 train_time:86263ms step_avg:60.92ms
step:1417/2330 train_time:86322ms step_avg:60.92ms
step:1418/2330 train_time:86385ms step_avg:60.92ms
step:1419/2330 train_time:86445ms step_avg:60.92ms
step:1420/2330 train_time:86508ms step_avg:60.92ms
step:1421/2330 train_time:86567ms step_avg:60.92ms
step:1422/2330 train_time:86631ms step_avg:60.92ms
step:1423/2330 train_time:86690ms step_avg:60.92ms
step:1424/2330 train_time:86753ms step_avg:60.92ms
step:1425/2330 train_time:86812ms step_avg:60.92ms
step:1426/2330 train_time:86875ms step_avg:60.92ms
step:1427/2330 train_time:86934ms step_avg:60.92ms
step:1428/2330 train_time:86998ms step_avg:60.92ms
step:1429/2330 train_time:87057ms step_avg:60.92ms
step:1430/2330 train_time:87121ms step_avg:60.92ms
step:1431/2330 train_time:87179ms step_avg:60.92ms
step:1432/2330 train_time:87242ms step_avg:60.92ms
step:1433/2330 train_time:87302ms step_avg:60.92ms
step:1434/2330 train_time:87364ms step_avg:60.92ms
step:1435/2330 train_time:87425ms step_avg:60.92ms
step:1436/2330 train_time:87488ms step_avg:60.92ms
step:1437/2330 train_time:87547ms step_avg:60.92ms
step:1438/2330 train_time:87610ms step_avg:60.93ms
step:1439/2330 train_time:87670ms step_avg:60.92ms
step:1440/2330 train_time:87733ms step_avg:60.93ms
step:1441/2330 train_time:87792ms step_avg:60.92ms
step:1442/2330 train_time:87855ms step_avg:60.93ms
step:1443/2330 train_time:87915ms step_avg:60.92ms
step:1444/2330 train_time:87977ms step_avg:60.93ms
step:1445/2330 train_time:88037ms step_avg:60.93ms
step:1446/2330 train_time:88099ms step_avg:60.93ms
step:1447/2330 train_time:88158ms step_avg:60.92ms
step:1448/2330 train_time:88221ms step_avg:60.93ms
step:1449/2330 train_time:88281ms step_avg:60.93ms
step:1450/2330 train_time:88344ms step_avg:60.93ms
step:1451/2330 train_time:88404ms step_avg:60.93ms
step:1452/2330 train_time:88467ms step_avg:60.93ms
step:1453/2330 train_time:88527ms step_avg:60.93ms
step:1454/2330 train_time:88590ms step_avg:60.93ms
step:1455/2330 train_time:88650ms step_avg:60.93ms
step:1456/2330 train_time:88714ms step_avg:60.93ms
step:1457/2330 train_time:88773ms step_avg:60.93ms
step:1458/2330 train_time:88837ms step_avg:60.93ms
step:1459/2330 train_time:88895ms step_avg:60.93ms
step:1460/2330 train_time:88958ms step_avg:60.93ms
step:1461/2330 train_time:89018ms step_avg:60.93ms
step:1462/2330 train_time:89082ms step_avg:60.93ms
step:1463/2330 train_time:89140ms step_avg:60.93ms
step:1464/2330 train_time:89203ms step_avg:60.93ms
step:1465/2330 train_time:89262ms step_avg:60.93ms
step:1466/2330 train_time:89326ms step_avg:60.93ms
step:1467/2330 train_time:89385ms step_avg:60.93ms
step:1468/2330 train_time:89449ms step_avg:60.93ms
step:1469/2330 train_time:89508ms step_avg:60.93ms
step:1470/2330 train_time:89571ms step_avg:60.93ms
step:1471/2330 train_time:89630ms step_avg:60.93ms
step:1472/2330 train_time:89693ms step_avg:60.93ms
step:1473/2330 train_time:89753ms step_avg:60.93ms
step:1474/2330 train_time:89815ms step_avg:60.93ms
step:1475/2330 train_time:89875ms step_avg:60.93ms
step:1476/2330 train_time:89938ms step_avg:60.93ms
step:1477/2330 train_time:89997ms step_avg:60.93ms
step:1478/2330 train_time:90060ms step_avg:60.93ms
step:1479/2330 train_time:90119ms step_avg:60.93ms
step:1480/2330 train_time:90182ms step_avg:60.93ms
step:1481/2330 train_time:90241ms step_avg:60.93ms
step:1482/2330 train_time:90305ms step_avg:60.93ms
step:1483/2330 train_time:90363ms step_avg:60.93ms
step:1484/2330 train_time:90426ms step_avg:60.93ms
step:1485/2330 train_time:90487ms step_avg:60.93ms
step:1486/2330 train_time:90549ms step_avg:60.94ms
step:1487/2330 train_time:90609ms step_avg:60.93ms
step:1488/2330 train_time:90672ms step_avg:60.94ms
step:1489/2330 train_time:90732ms step_avg:60.93ms
step:1490/2330 train_time:90795ms step_avg:60.94ms
step:1491/2330 train_time:90854ms step_avg:60.94ms
step:1492/2330 train_time:90917ms step_avg:60.94ms
step:1493/2330 train_time:90976ms step_avg:60.94ms
step:1494/2330 train_time:91040ms step_avg:60.94ms
step:1495/2330 train_time:91099ms step_avg:60.94ms
step:1496/2330 train_time:91163ms step_avg:60.94ms
step:1497/2330 train_time:91222ms step_avg:60.94ms
step:1498/2330 train_time:91285ms step_avg:60.94ms
step:1499/2330 train_time:91345ms step_avg:60.94ms
step:1500/2330 train_time:91408ms step_avg:60.94ms
step:1500/2330 val_loss:3.7063 train_time:91480ms step_avg:60.99ms
step:1501/2330 train_time:91504ms step_avg:60.96ms
step:1502/2330 train_time:91533ms step_avg:60.94ms
step:1503/2330 train_time:91596ms step_avg:60.94ms
step:1504/2330 train_time:91663ms step_avg:60.95ms
step:1505/2330 train_time:91724ms step_avg:60.95ms
step:1506/2330 train_time:91788ms step_avg:60.95ms
step:1507/2330 train_time:91847ms step_avg:60.95ms
step:1508/2330 train_time:91909ms step_avg:60.95ms
step:1509/2330 train_time:91967ms step_avg:60.95ms
step:1510/2330 train_time:92030ms step_avg:60.95ms
step:1511/2330 train_time:92088ms step_avg:60.95ms
step:1512/2330 train_time:92152ms step_avg:60.95ms
step:1513/2330 train_time:92210ms step_avg:60.94ms
step:1514/2330 train_time:92272ms step_avg:60.95ms
step:1515/2330 train_time:92331ms step_avg:60.94ms
step:1516/2330 train_time:92394ms step_avg:60.95ms
step:1517/2330 train_time:92454ms step_avg:60.95ms
step:1518/2330 train_time:92519ms step_avg:60.95ms
step:1519/2330 train_time:92580ms step_avg:60.95ms
step:1520/2330 train_time:92645ms step_avg:60.95ms
step:1521/2330 train_time:92704ms step_avg:60.95ms
step:1522/2330 train_time:92768ms step_avg:60.95ms
step:1523/2330 train_time:92827ms step_avg:60.95ms
step:1524/2330 train_time:92891ms step_avg:60.95ms
step:1525/2330 train_time:92950ms step_avg:60.95ms
step:1526/2330 train_time:93013ms step_avg:60.95ms
step:1527/2330 train_time:93072ms step_avg:60.95ms
step:1528/2330 train_time:93135ms step_avg:60.95ms
step:1529/2330 train_time:93195ms step_avg:60.95ms
step:1530/2330 train_time:93257ms step_avg:60.95ms
step:1531/2330 train_time:93316ms step_avg:60.95ms
step:1532/2330 train_time:93380ms step_avg:60.95ms
step:1533/2330 train_time:93440ms step_avg:60.95ms
step:1534/2330 train_time:93504ms step_avg:60.95ms
step:1535/2330 train_time:93563ms step_avg:60.95ms
step:1536/2330 train_time:93628ms step_avg:60.96ms
step:1537/2330 train_time:93688ms step_avg:60.96ms
step:1538/2330 train_time:93753ms step_avg:60.96ms
step:1539/2330 train_time:93813ms step_avg:60.96ms
step:1540/2330 train_time:93876ms step_avg:60.96ms
step:1541/2330 train_time:93937ms step_avg:60.96ms
step:1542/2330 train_time:94001ms step_avg:60.96ms
step:1543/2330 train_time:94061ms step_avg:60.96ms
step:1544/2330 train_time:94123ms step_avg:60.96ms
step:1545/2330 train_time:94182ms step_avg:60.96ms
step:1546/2330 train_time:94246ms step_avg:60.96ms
step:1547/2330 train_time:94305ms step_avg:60.96ms
step:1548/2330 train_time:94369ms step_avg:60.96ms
step:1549/2330 train_time:94429ms step_avg:60.96ms
step:1550/2330 train_time:94493ms step_avg:60.96ms
step:1551/2330 train_time:94554ms step_avg:60.96ms
step:1552/2330 train_time:94619ms step_avg:60.97ms
step:1553/2330 train_time:94679ms step_avg:60.97ms
step:1554/2330 train_time:94744ms step_avg:60.97ms
step:1555/2330 train_time:94803ms step_avg:60.97ms
step:1556/2330 train_time:94867ms step_avg:60.97ms
step:1557/2330 train_time:94926ms step_avg:60.97ms
step:1558/2330 train_time:94990ms step_avg:60.97ms
step:1559/2330 train_time:95050ms step_avg:60.97ms
step:1560/2330 train_time:95113ms step_avg:60.97ms
step:1561/2330 train_time:95173ms step_avg:60.97ms
step:1562/2330 train_time:95237ms step_avg:60.97ms
step:1563/2330 train_time:95297ms step_avg:60.97ms
step:1564/2330 train_time:95360ms step_avg:60.97ms
step:1565/2330 train_time:95421ms step_avg:60.97ms
step:1566/2330 train_time:95483ms step_avg:60.97ms
step:1567/2330 train_time:95543ms step_avg:60.97ms
step:1568/2330 train_time:95607ms step_avg:60.97ms
step:1569/2330 train_time:95667ms step_avg:60.97ms
step:1570/2330 train_time:95731ms step_avg:60.98ms
step:1571/2330 train_time:95791ms step_avg:60.97ms
step:1572/2330 train_time:95855ms step_avg:60.98ms
step:1573/2330 train_time:95916ms step_avg:60.98ms
step:1574/2330 train_time:95980ms step_avg:60.98ms
step:1575/2330 train_time:96040ms step_avg:60.98ms
step:1576/2330 train_time:96104ms step_avg:60.98ms
step:1577/2330 train_time:96163ms step_avg:60.98ms
step:1578/2330 train_time:96227ms step_avg:60.98ms
step:1579/2330 train_time:96286ms step_avg:60.98ms
step:1580/2330 train_time:96352ms step_avg:60.98ms
step:1581/2330 train_time:96411ms step_avg:60.98ms
step:1582/2330 train_time:96474ms step_avg:60.98ms
step:1583/2330 train_time:96536ms step_avg:60.98ms
step:1584/2330 train_time:96601ms step_avg:60.99ms
step:1585/2330 train_time:96660ms step_avg:60.98ms
step:1586/2330 train_time:96723ms step_avg:60.99ms
step:1587/2330 train_time:96783ms step_avg:60.98ms
step:1588/2330 train_time:96847ms step_avg:60.99ms
step:1589/2330 train_time:96907ms step_avg:60.99ms
step:1590/2330 train_time:96972ms step_avg:60.99ms
step:1591/2330 train_time:97032ms step_avg:60.99ms
step:1592/2330 train_time:97095ms step_avg:60.99ms
step:1593/2330 train_time:97156ms step_avg:60.99ms
step:1594/2330 train_time:97221ms step_avg:60.99ms
step:1595/2330 train_time:97280ms step_avg:60.99ms
step:1596/2330 train_time:97344ms step_avg:60.99ms
step:1597/2330 train_time:97402ms step_avg:60.99ms
step:1598/2330 train_time:97465ms step_avg:60.99ms
step:1599/2330 train_time:97525ms step_avg:60.99ms
step:1600/2330 train_time:97590ms step_avg:60.99ms
step:1601/2330 train_time:97650ms step_avg:60.99ms
step:1602/2330 train_time:97713ms step_avg:60.99ms
step:1603/2330 train_time:97774ms step_avg:60.99ms
step:1604/2330 train_time:97839ms step_avg:61.00ms
step:1605/2330 train_time:97899ms step_avg:61.00ms
step:1606/2330 train_time:97962ms step_avg:61.00ms
step:1607/2330 train_time:98021ms step_avg:61.00ms
step:1608/2330 train_time:98084ms step_avg:61.00ms
step:1609/2330 train_time:98145ms step_avg:61.00ms
step:1610/2330 train_time:98208ms step_avg:61.00ms
step:1611/2330 train_time:98267ms step_avg:61.00ms
step:1612/2330 train_time:98331ms step_avg:61.00ms
step:1613/2330 train_time:98390ms step_avg:61.00ms
step:1614/2330 train_time:98454ms step_avg:61.00ms
step:1615/2330 train_time:98515ms step_avg:61.00ms
step:1616/2330 train_time:98578ms step_avg:61.00ms
step:1617/2330 train_time:98638ms step_avg:61.00ms
step:1618/2330 train_time:98702ms step_avg:61.00ms
step:1619/2330 train_time:98761ms step_avg:61.00ms
step:1620/2330 train_time:98825ms step_avg:61.00ms
step:1621/2330 train_time:98885ms step_avg:61.00ms
step:1622/2330 train_time:98950ms step_avg:61.01ms
step:1623/2330 train_time:99009ms step_avg:61.00ms
step:1624/2330 train_time:99073ms step_avg:61.01ms
step:1625/2330 train_time:99133ms step_avg:61.01ms
step:1626/2330 train_time:99198ms step_avg:61.01ms
step:1627/2330 train_time:99258ms step_avg:61.01ms
step:1628/2330 train_time:99322ms step_avg:61.01ms
step:1629/2330 train_time:99382ms step_avg:61.01ms
step:1630/2330 train_time:99445ms step_avg:61.01ms
step:1631/2330 train_time:99505ms step_avg:61.01ms
step:1632/2330 train_time:99569ms step_avg:61.01ms
step:1633/2330 train_time:99628ms step_avg:61.01ms
step:1634/2330 train_time:99691ms step_avg:61.01ms
step:1635/2330 train_time:99751ms step_avg:61.01ms
step:1636/2330 train_time:99815ms step_avg:61.01ms
step:1637/2330 train_time:99876ms step_avg:61.01ms
step:1638/2330 train_time:99940ms step_avg:61.01ms
step:1639/2330 train_time:100000ms step_avg:61.01ms
step:1640/2330 train_time:100063ms step_avg:61.01ms
step:1641/2330 train_time:100122ms step_avg:61.01ms
step:1642/2330 train_time:100186ms step_avg:61.01ms
step:1643/2330 train_time:100246ms step_avg:61.01ms
step:1644/2330 train_time:100310ms step_avg:61.02ms
step:1645/2330 train_time:100369ms step_avg:61.01ms
step:1646/2330 train_time:100433ms step_avg:61.02ms
step:1647/2330 train_time:100493ms step_avg:61.02ms
step:1648/2330 train_time:100557ms step_avg:61.02ms
step:1649/2330 train_time:100618ms step_avg:61.02ms
step:1650/2330 train_time:100681ms step_avg:61.02ms
step:1651/2330 train_time:100740ms step_avg:61.02ms
step:1652/2330 train_time:100803ms step_avg:61.02ms
step:1653/2330 train_time:100863ms step_avg:61.02ms
step:1654/2330 train_time:100927ms step_avg:61.02ms
step:1655/2330 train_time:100987ms step_avg:61.02ms
step:1656/2330 train_time:101051ms step_avg:61.02ms
step:1657/2330 train_time:101110ms step_avg:61.02ms
step:1658/2330 train_time:101174ms step_avg:61.02ms
step:1659/2330 train_time:101235ms step_avg:61.02ms
step:1660/2330 train_time:101298ms step_avg:61.02ms
step:1661/2330 train_time:101358ms step_avg:61.02ms
step:1662/2330 train_time:101422ms step_avg:61.02ms
step:1663/2330 train_time:101482ms step_avg:61.02ms
step:1664/2330 train_time:101546ms step_avg:61.02ms
step:1665/2330 train_time:101605ms step_avg:61.02ms
step:1666/2330 train_time:101669ms step_avg:61.03ms
step:1667/2330 train_time:101728ms step_avg:61.02ms
step:1668/2330 train_time:101793ms step_avg:61.03ms
step:1669/2330 train_time:101852ms step_avg:61.03ms
step:1670/2330 train_time:101916ms step_avg:61.03ms
step:1671/2330 train_time:101977ms step_avg:61.03ms
step:1672/2330 train_time:102041ms step_avg:61.03ms
step:1673/2330 train_time:102100ms step_avg:61.03ms
step:1674/2330 train_time:102163ms step_avg:61.03ms
step:1675/2330 train_time:102223ms step_avg:61.03ms
step:1676/2330 train_time:102287ms step_avg:61.03ms
step:1677/2330 train_time:102347ms step_avg:61.03ms
step:1678/2330 train_time:102410ms step_avg:61.03ms
step:1679/2330 train_time:102469ms step_avg:61.03ms
step:1680/2330 train_time:102534ms step_avg:61.03ms
step:1681/2330 train_time:102596ms step_avg:61.03ms
step:1682/2330 train_time:102659ms step_avg:61.03ms
step:1683/2330 train_time:102719ms step_avg:61.03ms
step:1684/2330 train_time:102782ms step_avg:61.03ms
step:1685/2330 train_time:102842ms step_avg:61.03ms
step:1686/2330 train_time:102906ms step_avg:61.04ms
step:1687/2330 train_time:102966ms step_avg:61.04ms
step:1688/2330 train_time:103030ms step_avg:61.04ms
step:1689/2330 train_time:103090ms step_avg:61.04ms
step:1690/2330 train_time:103154ms step_avg:61.04ms
step:1691/2330 train_time:103215ms step_avg:61.04ms
step:1692/2330 train_time:103280ms step_avg:61.04ms
step:1693/2330 train_time:103340ms step_avg:61.04ms
step:1694/2330 train_time:103403ms step_avg:61.04ms
step:1695/2330 train_time:103462ms step_avg:61.04ms
step:1696/2330 train_time:103525ms step_avg:61.04ms
step:1697/2330 train_time:103585ms step_avg:61.04ms
step:1698/2330 train_time:103649ms step_avg:61.04ms
step:1699/2330 train_time:103708ms step_avg:61.04ms
step:1700/2330 train_time:103771ms step_avg:61.04ms
step:1701/2330 train_time:103830ms step_avg:61.04ms
step:1702/2330 train_time:103894ms step_avg:61.04ms
step:1703/2330 train_time:103955ms step_avg:61.04ms
step:1704/2330 train_time:104018ms step_avg:61.04ms
step:1705/2330 train_time:104078ms step_avg:61.04ms
step:1706/2330 train_time:104141ms step_avg:61.04ms
step:1707/2330 train_time:104202ms step_avg:61.04ms
step:1708/2330 train_time:104266ms step_avg:61.05ms
step:1709/2330 train_time:104325ms step_avg:61.04ms
step:1710/2330 train_time:104389ms step_avg:61.05ms
step:1711/2330 train_time:104448ms step_avg:61.05ms
step:1712/2330 train_time:104512ms step_avg:61.05ms
step:1713/2330 train_time:104572ms step_avg:61.05ms
step:1714/2330 train_time:104636ms step_avg:61.05ms
step:1715/2330 train_time:104697ms step_avg:61.05ms
step:1716/2330 train_time:104761ms step_avg:61.05ms
step:1717/2330 train_time:104820ms step_avg:61.05ms
step:1718/2330 train_time:104883ms step_avg:61.05ms
step:1719/2330 train_time:104943ms step_avg:61.05ms
step:1720/2330 train_time:105007ms step_avg:61.05ms
step:1721/2330 train_time:105065ms step_avg:61.05ms
step:1722/2330 train_time:105129ms step_avg:61.05ms
step:1723/2330 train_time:105188ms step_avg:61.05ms
step:1724/2330 train_time:105253ms step_avg:61.05ms
step:1725/2330 train_time:105313ms step_avg:61.05ms
step:1726/2330 train_time:105376ms step_avg:61.05ms
step:1727/2330 train_time:105437ms step_avg:61.05ms
step:1728/2330 train_time:105501ms step_avg:61.05ms
step:1729/2330 train_time:105561ms step_avg:61.05ms
step:1730/2330 train_time:105623ms step_avg:61.05ms
step:1731/2330 train_time:105683ms step_avg:61.05ms
step:1732/2330 train_time:105747ms step_avg:61.05ms
step:1733/2330 train_time:105806ms step_avg:61.05ms
step:1734/2330 train_time:105870ms step_avg:61.06ms
step:1735/2330 train_time:105929ms step_avg:61.05ms
step:1736/2330 train_time:105993ms step_avg:61.06ms
step:1737/2330 train_time:106054ms step_avg:61.06ms
step:1738/2330 train_time:106117ms step_avg:61.06ms
step:1739/2330 train_time:106178ms step_avg:61.06ms
step:1740/2330 train_time:106242ms step_avg:61.06ms
step:1741/2330 train_time:106302ms step_avg:61.06ms
step:1742/2330 train_time:106365ms step_avg:61.06ms
step:1743/2330 train_time:106425ms step_avg:61.06ms
step:1744/2330 train_time:106488ms step_avg:61.06ms
step:1745/2330 train_time:106548ms step_avg:61.06ms
step:1746/2330 train_time:106611ms step_avg:61.06ms
step:1747/2330 train_time:106671ms step_avg:61.06ms
step:1748/2330 train_time:106735ms step_avg:61.06ms
step:1749/2330 train_time:106797ms step_avg:61.06ms
step:1750/2330 train_time:106860ms step_avg:61.06ms
step:1750/2330 val_loss:3.6490 train_time:106932ms step_avg:61.10ms
step:1751/2330 train_time:106954ms step_avg:61.08ms
step:1752/2330 train_time:106984ms step_avg:61.06ms
step:1753/2330 train_time:107045ms step_avg:61.06ms
step:1754/2330 train_time:107112ms step_avg:61.07ms
step:1755/2330 train_time:107175ms step_avg:61.07ms
step:1756/2330 train_time:107239ms step_avg:61.07ms
step:1757/2330 train_time:107299ms step_avg:61.07ms
step:1758/2330 train_time:107362ms step_avg:61.07ms
step:1759/2330 train_time:107421ms step_avg:61.07ms
step:1760/2330 train_time:107483ms step_avg:61.07ms
step:1761/2330 train_time:107542ms step_avg:61.07ms
step:1762/2330 train_time:107604ms step_avg:61.07ms
step:1763/2330 train_time:107663ms step_avg:61.07ms
step:1764/2330 train_time:107726ms step_avg:61.07ms
step:1765/2330 train_time:107785ms step_avg:61.07ms
step:1766/2330 train_time:107849ms step_avg:61.07ms
step:1767/2330 train_time:107910ms step_avg:61.07ms
step:1768/2330 train_time:107974ms step_avg:61.07ms
step:1769/2330 train_time:108034ms step_avg:61.07ms
step:1770/2330 train_time:108099ms step_avg:61.07ms
step:1771/2330 train_time:108160ms step_avg:61.07ms
step:1772/2330 train_time:108223ms step_avg:61.07ms
step:1773/2330 train_time:108284ms step_avg:61.07ms
step:1774/2330 train_time:108346ms step_avg:61.07ms
step:1775/2330 train_time:108406ms step_avg:61.07ms
step:1776/2330 train_time:108468ms step_avg:61.07ms
step:1777/2330 train_time:108527ms step_avg:61.07ms
step:1778/2330 train_time:108590ms step_avg:61.07ms
step:1779/2330 train_time:108649ms step_avg:61.07ms
step:1780/2330 train_time:108712ms step_avg:61.07ms
step:1781/2330 train_time:108771ms step_avg:61.07ms
step:1782/2330 train_time:108834ms step_avg:61.07ms
step:1783/2330 train_time:108894ms step_avg:61.07ms
step:1784/2330 train_time:108958ms step_avg:61.08ms
step:1785/2330 train_time:109018ms step_avg:61.07ms
step:1786/2330 train_time:109083ms step_avg:61.08ms
step:1787/2330 train_time:109144ms step_avg:61.08ms
step:1788/2330 train_time:109208ms step_avg:61.08ms
step:1789/2330 train_time:109268ms step_avg:61.08ms
step:1790/2330 train_time:109331ms step_avg:61.08ms
step:1791/2330 train_time:109391ms step_avg:61.08ms
step:1792/2330 train_time:109454ms step_avg:61.08ms
step:1793/2330 train_time:109513ms step_avg:61.08ms
step:1794/2330 train_time:109576ms step_avg:61.08ms
step:1795/2330 train_time:109636ms step_avg:61.08ms
step:1796/2330 train_time:109699ms step_avg:61.08ms
step:1797/2330 train_time:109758ms step_avg:61.08ms
step:1798/2330 train_time:109821ms step_avg:61.08ms
step:1799/2330 train_time:109880ms step_avg:61.08ms
step:1800/2330 train_time:109944ms step_avg:61.08ms
step:1801/2330 train_time:110006ms step_avg:61.08ms
step:1802/2330 train_time:110070ms step_avg:61.08ms
step:1803/2330 train_time:110130ms step_avg:61.08ms
step:1804/2330 train_time:110193ms step_avg:61.08ms
step:1805/2330 train_time:110253ms step_avg:61.08ms
step:1806/2330 train_time:110317ms step_avg:61.08ms
step:1807/2330 train_time:110377ms step_avg:61.08ms
step:1808/2330 train_time:110440ms step_avg:61.08ms
step:1809/2330 train_time:110499ms step_avg:61.08ms
step:1810/2330 train_time:110562ms step_avg:61.08ms
step:1811/2330 train_time:110621ms step_avg:61.08ms
step:1812/2330 train_time:110684ms step_avg:61.08ms
step:1813/2330 train_time:110744ms step_avg:61.08ms
step:1814/2330 train_time:110807ms step_avg:61.08ms
step:1815/2330 train_time:110868ms step_avg:61.08ms
step:1816/2330 train_time:110931ms step_avg:61.09ms
step:1817/2330 train_time:110991ms step_avg:61.08ms
step:1818/2330 train_time:111054ms step_avg:61.09ms
step:1819/2330 train_time:111114ms step_avg:61.09ms
step:1820/2330 train_time:111177ms step_avg:61.09ms
step:1821/2330 train_time:111237ms step_avg:61.09ms
step:1822/2330 train_time:111300ms step_avg:61.09ms
step:1823/2330 train_time:111360ms step_avg:61.09ms
step:1824/2330 train_time:111423ms step_avg:61.09ms
step:1825/2330 train_time:111483ms step_avg:61.09ms
step:1826/2330 train_time:111545ms step_avg:61.09ms
step:1827/2330 train_time:111606ms step_avg:61.09ms
step:1828/2330 train_time:111668ms step_avg:61.09ms
step:1829/2330 train_time:111727ms step_avg:61.09ms
step:1830/2330 train_time:111791ms step_avg:61.09ms
step:1831/2330 train_time:111851ms step_avg:61.09ms
step:1832/2330 train_time:111914ms step_avg:61.09ms
step:1833/2330 train_time:111973ms step_avg:61.09ms
step:1834/2330 train_time:112037ms step_avg:61.09ms
step:1835/2330 train_time:112097ms step_avg:61.09ms
step:1836/2330 train_time:112161ms step_avg:61.09ms
step:1837/2330 train_time:112220ms step_avg:61.09ms
step:1838/2330 train_time:112284ms step_avg:61.09ms
step:1839/2330 train_time:112343ms step_avg:61.09ms
step:1840/2330 train_time:112407ms step_avg:61.09ms
step:1841/2330 train_time:112467ms step_avg:61.09ms
step:1842/2330 train_time:112531ms step_avg:61.09ms
step:1843/2330 train_time:112591ms step_avg:61.09ms
step:1844/2330 train_time:112653ms step_avg:61.09ms
step:1845/2330 train_time:112713ms step_avg:61.09ms
step:1846/2330 train_time:112776ms step_avg:61.09ms
step:1847/2330 train_time:112835ms step_avg:61.09ms
step:1848/2330 train_time:112898ms step_avg:61.09ms
step:1849/2330 train_time:112958ms step_avg:61.09ms
step:1850/2330 train_time:113022ms step_avg:61.09ms
step:1851/2330 train_time:113081ms step_avg:61.09ms
step:1852/2330 train_time:113145ms step_avg:61.09ms
step:1853/2330 train_time:113206ms step_avg:61.09ms
step:1854/2330 train_time:113269ms step_avg:61.09ms
step:1855/2330 train_time:113329ms step_avg:61.09ms
step:1856/2330 train_time:113393ms step_avg:61.10ms
step:1857/2330 train_time:113453ms step_avg:61.09ms
step:1858/2330 train_time:113515ms step_avg:61.10ms
step:1859/2330 train_time:113577ms step_avg:61.10ms
step:1860/2330 train_time:113641ms step_avg:61.10ms
step:1861/2330 train_time:113700ms step_avg:61.10ms
step:1862/2330 train_time:113763ms step_avg:61.10ms
step:1863/2330 train_time:113821ms step_avg:61.10ms
step:1864/2330 train_time:113885ms step_avg:61.10ms
step:1865/2330 train_time:113944ms step_avg:61.10ms
step:1866/2330 train_time:114009ms step_avg:61.10ms
step:1867/2330 train_time:114069ms step_avg:61.10ms
step:1868/2330 train_time:114132ms step_avg:61.10ms
step:1869/2330 train_time:114191ms step_avg:61.10ms
step:1870/2330 train_time:114254ms step_avg:61.10ms
step:1871/2330 train_time:114313ms step_avg:61.10ms
step:1872/2330 train_time:114377ms step_avg:61.10ms
step:1873/2330 train_time:114437ms step_avg:61.10ms
step:1874/2330 train_time:114500ms step_avg:61.10ms
step:1875/2330 train_time:114559ms step_avg:61.10ms
step:1876/2330 train_time:114623ms step_avg:61.10ms
step:1877/2330 train_time:114682ms step_avg:61.10ms
step:1878/2330 train_time:114745ms step_avg:61.10ms
step:1879/2330 train_time:114805ms step_avg:61.10ms
step:1880/2330 train_time:114868ms step_avg:61.10ms
step:1881/2330 train_time:114928ms step_avg:61.10ms
step:1882/2330 train_time:114991ms step_avg:61.10ms
step:1883/2330 train_time:115051ms step_avg:61.10ms
step:1884/2330 train_time:115114ms step_avg:61.10ms
step:1885/2330 train_time:115174ms step_avg:61.10ms
step:1886/2330 train_time:115236ms step_avg:61.10ms
step:1887/2330 train_time:115296ms step_avg:61.10ms
step:1888/2330 train_time:115358ms step_avg:61.10ms
step:1889/2330 train_time:115418ms step_avg:61.10ms
step:1890/2330 train_time:115482ms step_avg:61.10ms
step:1891/2330 train_time:115541ms step_avg:61.10ms
step:1892/2330 train_time:115604ms step_avg:61.10ms
step:1893/2330 train_time:115663ms step_avg:61.10ms
step:1894/2330 train_time:115727ms step_avg:61.10ms
step:1895/2330 train_time:115786ms step_avg:61.10ms
step:1896/2330 train_time:115849ms step_avg:61.10ms
step:1897/2330 train_time:115909ms step_avg:61.10ms
step:1898/2330 train_time:115971ms step_avg:61.10ms
step:1899/2330 train_time:116031ms step_avg:61.10ms
step:1900/2330 train_time:116094ms step_avg:61.10ms
step:1901/2330 train_time:116154ms step_avg:61.10ms
step:1902/2330 train_time:116217ms step_avg:61.10ms
step:1903/2330 train_time:116276ms step_avg:61.10ms
step:1904/2330 train_time:116339ms step_avg:61.10ms
step:1905/2330 train_time:116399ms step_avg:61.10ms
step:1906/2330 train_time:116463ms step_avg:61.10ms
step:1907/2330 train_time:116522ms step_avg:61.10ms
step:1908/2330 train_time:116586ms step_avg:61.10ms
step:1909/2330 train_time:116645ms step_avg:61.10ms
step:1910/2330 train_time:116709ms step_avg:61.10ms
step:1911/2330 train_time:116768ms step_avg:61.10ms
step:1912/2330 train_time:116831ms step_avg:61.10ms
step:1913/2330 train_time:116891ms step_avg:61.10ms
step:1914/2330 train_time:116954ms step_avg:61.10ms
step:1915/2330 train_time:117014ms step_avg:61.10ms
step:1916/2330 train_time:117078ms step_avg:61.11ms
step:1917/2330 train_time:117137ms step_avg:61.10ms
step:1918/2330 train_time:117200ms step_avg:61.11ms
step:1919/2330 train_time:117259ms step_avg:61.10ms
step:1920/2330 train_time:117322ms step_avg:61.11ms
step:1921/2330 train_time:117382ms step_avg:61.10ms
step:1922/2330 train_time:117445ms step_avg:61.11ms
step:1923/2330 train_time:117506ms step_avg:61.11ms
step:1924/2330 train_time:117569ms step_avg:61.11ms
step:1925/2330 train_time:117629ms step_avg:61.11ms
step:1926/2330 train_time:117692ms step_avg:61.11ms
step:1927/2330 train_time:117752ms step_avg:61.11ms
step:1928/2330 train_time:117815ms step_avg:61.11ms
step:1929/2330 train_time:117876ms step_avg:61.11ms
step:1930/2330 train_time:117939ms step_avg:61.11ms
step:1931/2330 train_time:117999ms step_avg:61.11ms
step:1932/2330 train_time:118062ms step_avg:61.11ms
step:1933/2330 train_time:118123ms step_avg:61.11ms
step:1934/2330 train_time:118186ms step_avg:61.11ms
step:1935/2330 train_time:118246ms step_avg:61.11ms
step:1936/2330 train_time:118311ms step_avg:61.11ms
step:1937/2330 train_time:118371ms step_avg:61.11ms
step:1938/2330 train_time:118433ms step_avg:61.11ms
step:1939/2330 train_time:118492ms step_avg:61.11ms
step:1940/2330 train_time:118556ms step_avg:61.11ms
step:1941/2330 train_time:118616ms step_avg:61.11ms
step:1942/2330 train_time:118679ms step_avg:61.11ms
step:1943/2330 train_time:118739ms step_avg:61.11ms
step:1944/2330 train_time:118802ms step_avg:61.11ms
step:1945/2330 train_time:118862ms step_avg:61.11ms
step:1946/2330 train_time:118926ms step_avg:61.11ms
step:1947/2330 train_time:118986ms step_avg:61.11ms
step:1948/2330 train_time:119050ms step_avg:61.11ms
step:1949/2330 train_time:119109ms step_avg:61.11ms
step:1950/2330 train_time:119173ms step_avg:61.11ms
step:1951/2330 train_time:119233ms step_avg:61.11ms
step:1952/2330 train_time:119296ms step_avg:61.11ms
step:1953/2330 train_time:119356ms step_avg:61.11ms
step:1954/2330 train_time:119419ms step_avg:61.12ms
step:1955/2330 train_time:119479ms step_avg:61.11ms
step:1956/2330 train_time:119542ms step_avg:61.12ms
step:1957/2330 train_time:119601ms step_avg:61.11ms
step:1958/2330 train_time:119665ms step_avg:61.12ms
step:1959/2330 train_time:119725ms step_avg:61.12ms
step:1960/2330 train_time:119788ms step_avg:61.12ms
step:1961/2330 train_time:119847ms step_avg:61.12ms
step:1962/2330 train_time:119911ms step_avg:61.12ms
step:1963/2330 train_time:119971ms step_avg:61.12ms
step:1964/2330 train_time:120033ms step_avg:61.12ms
step:1965/2330 train_time:120093ms step_avg:61.12ms
step:1966/2330 train_time:120157ms step_avg:61.12ms
step:1967/2330 train_time:120217ms step_avg:61.12ms
step:1968/2330 train_time:120281ms step_avg:61.12ms
step:1969/2330 train_time:120340ms step_avg:61.12ms
step:1970/2330 train_time:120404ms step_avg:61.12ms
step:1971/2330 train_time:120464ms step_avg:61.12ms
step:1972/2330 train_time:120527ms step_avg:61.12ms
step:1973/2330 train_time:120587ms step_avg:61.12ms
step:1974/2330 train_time:120650ms step_avg:61.12ms
step:1975/2330 train_time:120710ms step_avg:61.12ms
step:1976/2330 train_time:120773ms step_avg:61.12ms
step:1977/2330 train_time:120832ms step_avg:61.12ms
step:1978/2330 train_time:120895ms step_avg:61.12ms
step:1979/2330 train_time:120954ms step_avg:61.12ms
step:1980/2330 train_time:121017ms step_avg:61.12ms
step:1981/2330 train_time:121077ms step_avg:61.12ms
step:1982/2330 train_time:121140ms step_avg:61.12ms
step:1983/2330 train_time:121199ms step_avg:61.12ms
step:1984/2330 train_time:121262ms step_avg:61.12ms
step:1985/2330 train_time:121322ms step_avg:61.12ms
step:1986/2330 train_time:121385ms step_avg:61.12ms
step:1987/2330 train_time:121445ms step_avg:61.12ms
step:1988/2330 train_time:121509ms step_avg:61.12ms
step:1989/2330 train_time:121568ms step_avg:61.12ms
step:1990/2330 train_time:121632ms step_avg:61.12ms
step:1991/2330 train_time:121691ms step_avg:61.12ms
step:1992/2330 train_time:121754ms step_avg:61.12ms
step:1993/2330 train_time:121814ms step_avg:61.12ms
step:1994/2330 train_time:121876ms step_avg:61.12ms
step:1995/2330 train_time:121935ms step_avg:61.12ms
step:1996/2330 train_time:121999ms step_avg:61.12ms
step:1997/2330 train_time:122058ms step_avg:61.12ms
step:1998/2330 train_time:122121ms step_avg:61.12ms
step:1999/2330 train_time:122181ms step_avg:61.12ms
step:2000/2330 train_time:122244ms step_avg:61.12ms
step:2000/2330 val_loss:3.6069 train_time:122317ms step_avg:61.16ms
step:2001/2330 train_time:122338ms step_avg:61.14ms
step:2002/2330 train_time:122371ms step_avg:61.12ms
step:2003/2330 train_time:122433ms step_avg:61.12ms
step:2004/2330 train_time:122501ms step_avg:61.13ms
step:2005/2330 train_time:122561ms step_avg:61.13ms
step:2006/2330 train_time:122623ms step_avg:61.13ms
step:2007/2330 train_time:122682ms step_avg:61.13ms
step:2008/2330 train_time:122745ms step_avg:61.13ms
step:2009/2330 train_time:122804ms step_avg:61.13ms
step:2010/2330 train_time:122867ms step_avg:61.13ms
step:2011/2330 train_time:122926ms step_avg:61.13ms
step:2012/2330 train_time:122988ms step_avg:61.13ms
step:2013/2330 train_time:123046ms step_avg:61.13ms
step:2014/2330 train_time:123109ms step_avg:61.13ms
step:2015/2330 train_time:123168ms step_avg:61.13ms
step:2016/2330 train_time:123231ms step_avg:61.13ms
step:2017/2330 train_time:123293ms step_avg:61.13ms
step:2018/2330 train_time:123358ms step_avg:61.13ms
step:2019/2330 train_time:123419ms step_avg:61.13ms
step:2020/2330 train_time:123484ms step_avg:61.13ms
step:2021/2330 train_time:123544ms step_avg:61.13ms
step:2022/2330 train_time:123608ms step_avg:61.13ms
step:2023/2330 train_time:123667ms step_avg:61.13ms
step:2024/2330 train_time:123731ms step_avg:61.13ms
step:2025/2330 train_time:123791ms step_avg:61.13ms
step:2026/2330 train_time:123855ms step_avg:61.13ms
step:2027/2330 train_time:123915ms step_avg:61.13ms
step:2028/2330 train_time:123977ms step_avg:61.13ms
step:2029/2330 train_time:124036ms step_avg:61.13ms
step:2030/2330 train_time:124099ms step_avg:61.13ms
step:2031/2330 train_time:124157ms step_avg:61.13ms
step:2032/2330 train_time:124220ms step_avg:61.13ms
step:2033/2330 train_time:124279ms step_avg:61.13ms
step:2034/2330 train_time:124343ms step_avg:61.13ms
step:2035/2330 train_time:124403ms step_avg:61.13ms
step:2036/2330 train_time:124467ms step_avg:61.13ms
step:2037/2330 train_time:124528ms step_avg:61.13ms
step:2038/2330 train_time:124592ms step_avg:61.13ms
step:2039/2330 train_time:124653ms step_avg:61.13ms
step:2040/2330 train_time:124717ms step_avg:61.14ms
step:2041/2330 train_time:124776ms step_avg:61.13ms
step:2042/2330 train_time:124840ms step_avg:61.14ms
step:2043/2330 train_time:124899ms step_avg:61.14ms
step:2044/2330 train_time:124962ms step_avg:61.14ms
step:2045/2330 train_time:125021ms step_avg:61.14ms
step:2046/2330 train_time:125085ms step_avg:61.14ms
step:2047/2330 train_time:125144ms step_avg:61.14ms
step:2048/2330 train_time:125207ms step_avg:61.14ms
step:2049/2330 train_time:125266ms step_avg:61.14ms
step:2050/2330 train_time:125330ms step_avg:61.14ms
step:2051/2330 train_time:125390ms step_avg:61.14ms
step:2052/2330 train_time:125454ms step_avg:61.14ms
step:2053/2330 train_time:125514ms step_avg:61.14ms
step:2054/2330 train_time:125578ms step_avg:61.14ms
step:2055/2330 train_time:125638ms step_avg:61.14ms
step:2056/2330 train_time:125702ms step_avg:61.14ms
step:2057/2330 train_time:125760ms step_avg:61.14ms
step:2058/2330 train_time:125823ms step_avg:61.14ms
step:2059/2330 train_time:125882ms step_avg:61.14ms
step:2060/2330 train_time:125946ms step_avg:61.14ms
step:2061/2330 train_time:126005ms step_avg:61.14ms
step:2062/2330 train_time:126068ms step_avg:61.14ms
step:2063/2330 train_time:126128ms step_avg:61.14ms
step:2064/2330 train_time:126191ms step_avg:61.14ms
step:2065/2330 train_time:126251ms step_avg:61.14ms
step:2066/2330 train_time:126314ms step_avg:61.14ms
step:2067/2330 train_time:126374ms step_avg:61.14ms
step:2068/2330 train_time:126438ms step_avg:61.14ms
step:2069/2330 train_time:126498ms step_avg:61.14ms
step:2070/2330 train_time:126560ms step_avg:61.14ms
step:2071/2330 train_time:126621ms step_avg:61.14ms
step:2072/2330 train_time:126684ms step_avg:61.14ms
step:2073/2330 train_time:126744ms step_avg:61.14ms
step:2074/2330 train_time:126807ms step_avg:61.14ms
step:2075/2330 train_time:126867ms step_avg:61.14ms
step:2076/2330 train_time:126930ms step_avg:61.14ms
step:2077/2330 train_time:126989ms step_avg:61.14ms
step:2078/2330 train_time:127052ms step_avg:61.14ms
step:2079/2330 train_time:127111ms step_avg:61.14ms
step:2080/2330 train_time:127175ms step_avg:61.14ms
step:2081/2330 train_time:127235ms step_avg:61.14ms
step:2082/2330 train_time:127299ms step_avg:61.14ms
step:2083/2330 train_time:127358ms step_avg:61.14ms
step:2084/2330 train_time:127421ms step_avg:61.14ms
step:2085/2330 train_time:127480ms step_avg:61.14ms
step:2086/2330 train_time:127544ms step_avg:61.14ms
step:2087/2330 train_time:127604ms step_avg:61.14ms
step:2088/2330 train_time:127667ms step_avg:61.14ms
step:2089/2330 train_time:127727ms step_avg:61.14ms
step:2090/2330 train_time:127791ms step_avg:61.14ms
step:2091/2330 train_time:127851ms step_avg:61.14ms
step:2092/2330 train_time:127915ms step_avg:61.14ms
step:2093/2330 train_time:127974ms step_avg:61.14ms
step:2094/2330 train_time:128037ms step_avg:61.14ms
step:2095/2330 train_time:128097ms step_avg:61.14ms
step:2096/2330 train_time:128159ms step_avg:61.14ms
step:2097/2330 train_time:128219ms step_avg:61.14ms
step:2098/2330 train_time:128282ms step_avg:61.15ms
step:2099/2330 train_time:128342ms step_avg:61.14ms
step:2100/2330 train_time:128406ms step_avg:61.15ms
step:2101/2330 train_time:128465ms step_avg:61.14ms
step:2102/2330 train_time:128528ms step_avg:61.15ms
step:2103/2330 train_time:128588ms step_avg:61.15ms
step:2104/2330 train_time:128652ms step_avg:61.15ms
step:2105/2330 train_time:128712ms step_avg:61.15ms
step:2106/2330 train_time:128776ms step_avg:61.15ms
step:2107/2330 train_time:128834ms step_avg:61.15ms
step:2108/2330 train_time:128898ms step_avg:61.15ms
step:2109/2330 train_time:128958ms step_avg:61.15ms
step:2110/2330 train_time:129021ms step_avg:61.15ms
step:2111/2330 train_time:129080ms step_avg:61.15ms
step:2112/2330 train_time:129143ms step_avg:61.15ms
step:2113/2330 train_time:129203ms step_avg:61.15ms
step:2114/2330 train_time:129266ms step_avg:61.15ms
step:2115/2330 train_time:129326ms step_avg:61.15ms
step:2116/2330 train_time:129389ms step_avg:61.15ms
step:2117/2330 train_time:129449ms step_avg:61.15ms
step:2118/2330 train_time:129512ms step_avg:61.15ms
step:2119/2330 train_time:129572ms step_avg:61.15ms
step:2120/2330 train_time:129636ms step_avg:61.15ms
step:2121/2330 train_time:129696ms step_avg:61.15ms
step:2122/2330 train_time:129760ms step_avg:61.15ms
step:2123/2330 train_time:129819ms step_avg:61.15ms
step:2124/2330 train_time:129883ms step_avg:61.15ms
step:2125/2330 train_time:129943ms step_avg:61.15ms
step:2126/2330 train_time:130006ms step_avg:61.15ms
step:2127/2330 train_time:130065ms step_avg:61.15ms
step:2128/2330 train_time:130129ms step_avg:61.15ms
step:2129/2330 train_time:130189ms step_avg:61.15ms
step:2130/2330 train_time:130253ms step_avg:61.15ms
step:2131/2330 train_time:130313ms step_avg:61.15ms
step:2132/2330 train_time:130377ms step_avg:61.15ms
step:2133/2330 train_time:130436ms step_avg:61.15ms
step:2134/2330 train_time:130499ms step_avg:61.15ms
step:2135/2330 train_time:130557ms step_avg:61.15ms
step:2136/2330 train_time:130621ms step_avg:61.15ms
step:2137/2330 train_time:130680ms step_avg:61.15ms
step:2138/2330 train_time:130744ms step_avg:61.15ms
step:2139/2330 train_time:130804ms step_avg:61.15ms
step:2140/2330 train_time:130867ms step_avg:61.15ms
step:2141/2330 train_time:130926ms step_avg:61.15ms
step:2142/2330 train_time:130989ms step_avg:61.15ms
step:2143/2330 train_time:131050ms step_avg:61.15ms
step:2144/2330 train_time:131113ms step_avg:61.15ms
step:2145/2330 train_time:131173ms step_avg:61.15ms
step:2146/2330 train_time:131236ms step_avg:61.15ms
step:2147/2330 train_time:131296ms step_avg:61.15ms
step:2148/2330 train_time:131358ms step_avg:61.15ms
step:2149/2330 train_time:131418ms step_avg:61.15ms
step:2150/2330 train_time:131480ms step_avg:61.15ms
step:2151/2330 train_time:131540ms step_avg:61.15ms
step:2152/2330 train_time:131603ms step_avg:61.15ms
step:2153/2330 train_time:131663ms step_avg:61.15ms
step:2154/2330 train_time:131725ms step_avg:61.15ms
step:2155/2330 train_time:131785ms step_avg:61.15ms
step:2156/2330 train_time:131849ms step_avg:61.15ms
step:2157/2330 train_time:131908ms step_avg:61.15ms
step:2158/2330 train_time:131971ms step_avg:61.15ms
step:2159/2330 train_time:132033ms step_avg:61.15ms
step:2160/2330 train_time:132096ms step_avg:61.16ms
step:2161/2330 train_time:132157ms step_avg:61.16ms
step:2162/2330 train_time:132219ms step_avg:61.16ms
step:2163/2330 train_time:132278ms step_avg:61.16ms
step:2164/2330 train_time:132341ms step_avg:61.16ms
step:2165/2330 train_time:132402ms step_avg:61.16ms
step:2166/2330 train_time:132465ms step_avg:61.16ms
step:2167/2330 train_time:132524ms step_avg:61.16ms
step:2168/2330 train_time:132587ms step_avg:61.16ms
step:2169/2330 train_time:132647ms step_avg:61.16ms
step:2170/2330 train_time:132710ms step_avg:61.16ms
step:2171/2330 train_time:132771ms step_avg:61.16ms
step:2172/2330 train_time:132834ms step_avg:61.16ms
step:2173/2330 train_time:132894ms step_avg:61.16ms
step:2174/2330 train_time:132957ms step_avg:61.16ms
step:2175/2330 train_time:133017ms step_avg:61.16ms
step:2176/2330 train_time:133080ms step_avg:61.16ms
step:2177/2330 train_time:133140ms step_avg:61.16ms
step:2178/2330 train_time:133203ms step_avg:61.16ms
step:2179/2330 train_time:133262ms step_avg:61.16ms
step:2180/2330 train_time:133326ms step_avg:61.16ms
step:2181/2330 train_time:133385ms step_avg:61.16ms
step:2182/2330 train_time:133449ms step_avg:61.16ms
step:2183/2330 train_time:133510ms step_avg:61.16ms
step:2184/2330 train_time:133573ms step_avg:61.16ms
step:2185/2330 train_time:133633ms step_avg:61.16ms
step:2186/2330 train_time:133697ms step_avg:61.16ms
step:2187/2330 train_time:133757ms step_avg:61.16ms
step:2188/2330 train_time:133820ms step_avg:61.16ms
step:2189/2330 train_time:133879ms step_avg:61.16ms
step:2190/2330 train_time:133941ms step_avg:61.16ms
step:2191/2330 train_time:134002ms step_avg:61.16ms
step:2192/2330 train_time:134064ms step_avg:61.16ms
step:2193/2330 train_time:134124ms step_avg:61.16ms
step:2194/2330 train_time:134187ms step_avg:61.16ms
step:2195/2330 train_time:134246ms step_avg:61.16ms
step:2196/2330 train_time:134309ms step_avg:61.16ms
step:2197/2330 train_time:134368ms step_avg:61.16ms
step:2198/2330 train_time:134432ms step_avg:61.16ms
step:2199/2330 train_time:134493ms step_avg:61.16ms
step:2200/2330 train_time:134556ms step_avg:61.16ms
step:2201/2330 train_time:134615ms step_avg:61.16ms
step:2202/2330 train_time:134678ms step_avg:61.16ms
step:2203/2330 train_time:134738ms step_avg:61.16ms
step:2204/2330 train_time:134801ms step_avg:61.16ms
step:2205/2330 train_time:134860ms step_avg:61.16ms
step:2206/2330 train_time:134924ms step_avg:61.16ms
step:2207/2330 train_time:134983ms step_avg:61.16ms
step:2208/2330 train_time:135047ms step_avg:61.16ms
step:2209/2330 train_time:135107ms step_avg:61.16ms
step:2210/2330 train_time:135170ms step_avg:61.16ms
step:2211/2330 train_time:135230ms step_avg:61.16ms
step:2212/2330 train_time:135293ms step_avg:61.16ms
step:2213/2330 train_time:135352ms step_avg:61.16ms
step:2214/2330 train_time:135416ms step_avg:61.16ms
step:2215/2330 train_time:135476ms step_avg:61.16ms
step:2216/2330 train_time:135539ms step_avg:61.16ms
step:2217/2330 train_time:135599ms step_avg:61.16ms
step:2218/2330 train_time:135662ms step_avg:61.16ms
step:2219/2330 train_time:135722ms step_avg:61.16ms
step:2220/2330 train_time:135785ms step_avg:61.16ms
step:2221/2330 train_time:135845ms step_avg:61.16ms
step:2222/2330 train_time:135909ms step_avg:61.17ms
step:2223/2330 train_time:135968ms step_avg:61.16ms
step:2224/2330 train_time:136032ms step_avg:61.17ms
step:2225/2330 train_time:136093ms step_avg:61.17ms
step:2226/2330 train_time:136157ms step_avg:61.17ms
step:2227/2330 train_time:136216ms step_avg:61.17ms
step:2228/2330 train_time:136280ms step_avg:61.17ms
step:2229/2330 train_time:136339ms step_avg:61.17ms
step:2230/2330 train_time:136402ms step_avg:61.17ms
step:2231/2330 train_time:136461ms step_avg:61.17ms
step:2232/2330 train_time:136525ms step_avg:61.17ms
step:2233/2330 train_time:136585ms step_avg:61.17ms
step:2234/2330 train_time:136648ms step_avg:61.17ms
step:2235/2330 train_time:136709ms step_avg:61.17ms
step:2236/2330 train_time:136773ms step_avg:61.17ms
step:2237/2330 train_time:136833ms step_avg:61.17ms
step:2238/2330 train_time:136897ms step_avg:61.17ms
step:2239/2330 train_time:136957ms step_avg:61.17ms
step:2240/2330 train_time:137020ms step_avg:61.17ms
step:2241/2330 train_time:137079ms step_avg:61.17ms
step:2242/2330 train_time:137142ms step_avg:61.17ms
step:2243/2330 train_time:137202ms step_avg:61.17ms
step:2244/2330 train_time:137265ms step_avg:61.17ms
step:2245/2330 train_time:137325ms step_avg:61.17ms
step:2246/2330 train_time:137387ms step_avg:61.17ms
step:2247/2330 train_time:137447ms step_avg:61.17ms
step:2248/2330 train_time:137511ms step_avg:61.17ms
step:2249/2330 train_time:137570ms step_avg:61.17ms
step:2250/2330 train_time:137633ms step_avg:61.17ms
step:2250/2330 val_loss:3.5738 train_time:137706ms step_avg:61.20ms
step:2251/2330 train_time:137728ms step_avg:61.19ms
step:2252/2330 train_time:137762ms step_avg:61.17ms
step:2253/2330 train_time:137825ms step_avg:61.17ms
step:2254/2330 train_time:137890ms step_avg:61.18ms
step:2255/2330 train_time:137950ms step_avg:61.18ms
step:2256/2330 train_time:138013ms step_avg:61.18ms
step:2257/2330 train_time:138072ms step_avg:61.17ms
step:2258/2330 train_time:138135ms step_avg:61.18ms
step:2259/2330 train_time:138195ms step_avg:61.18ms
step:2260/2330 train_time:138258ms step_avg:61.18ms
step:2261/2330 train_time:138317ms step_avg:61.18ms
step:2262/2330 train_time:138379ms step_avg:61.18ms
step:2263/2330 train_time:138439ms step_avg:61.17ms
step:2264/2330 train_time:138501ms step_avg:61.18ms
step:2265/2330 train_time:138561ms step_avg:61.17ms
step:2266/2330 train_time:138623ms step_avg:61.18ms
step:2267/2330 train_time:138685ms step_avg:61.18ms
step:2268/2330 train_time:138750ms step_avg:61.18ms
step:2269/2330 train_time:138811ms step_avg:61.18ms
step:2270/2330 train_time:138876ms step_avg:61.18ms
step:2271/2330 train_time:138936ms step_avg:61.18ms
step:2272/2330 train_time:139000ms step_avg:61.18ms
step:2273/2330 train_time:139059ms step_avg:61.18ms
step:2274/2330 train_time:139123ms step_avg:61.18ms
step:2275/2330 train_time:139183ms step_avg:61.18ms
step:2276/2330 train_time:139246ms step_avg:61.18ms
step:2277/2330 train_time:139306ms step_avg:61.18ms
step:2278/2330 train_time:139368ms step_avg:61.18ms
step:2279/2330 train_time:139427ms step_avg:61.18ms
step:2280/2330 train_time:139489ms step_avg:61.18ms
step:2281/2330 train_time:139549ms step_avg:61.18ms
step:2282/2330 train_time:139611ms step_avg:61.18ms
step:2283/2330 train_time:139672ms step_avg:61.18ms
step:2284/2330 train_time:139736ms step_avg:61.18ms
step:2285/2330 train_time:139796ms step_avg:61.18ms
step:2286/2330 train_time:139860ms step_avg:61.18ms
step:2287/2330 train_time:139920ms step_avg:61.18ms
step:2288/2330 train_time:139983ms step_avg:61.18ms
step:2289/2330 train_time:140044ms step_avg:61.18ms
step:2290/2330 train_time:140108ms step_avg:61.18ms
step:2291/2330 train_time:140168ms step_avg:61.18ms
step:2292/2330 train_time:140231ms step_avg:61.18ms
step:2293/2330 train_time:140290ms step_avg:61.18ms
step:2294/2330 train_time:140353ms step_avg:61.18ms
step:2295/2330 train_time:140413ms step_avg:61.18ms
step:2296/2330 train_time:140476ms step_avg:61.18ms
step:2297/2330 train_time:140535ms step_avg:61.18ms
step:2298/2330 train_time:140599ms step_avg:61.18ms
step:2299/2330 train_time:140659ms step_avg:61.18ms
step:2300/2330 train_time:140722ms step_avg:61.18ms
step:2301/2330 train_time:140782ms step_avg:61.18ms
step:2302/2330 train_time:140846ms step_avg:61.18ms
step:2303/2330 train_time:140906ms step_avg:61.18ms
step:2304/2330 train_time:140969ms step_avg:61.18ms
step:2305/2330 train_time:141029ms step_avg:61.18ms
step:2306/2330 train_time:141093ms step_avg:61.19ms
step:2307/2330 train_time:141152ms step_avg:61.18ms
step:2308/2330 train_time:141216ms step_avg:61.19ms
step:2309/2330 train_time:141275ms step_avg:61.18ms
step:2310/2330 train_time:141339ms step_avg:61.19ms
step:2311/2330 train_time:141398ms step_avg:61.18ms
step:2312/2330 train_time:141462ms step_avg:61.19ms
step:2313/2330 train_time:141522ms step_avg:61.19ms
step:2314/2330 train_time:141585ms step_avg:61.19ms
step:2315/2330 train_time:141645ms step_avg:61.19ms
step:2316/2330 train_time:141708ms step_avg:61.19ms
step:2317/2330 train_time:141769ms step_avg:61.19ms
step:2318/2330 train_time:141832ms step_avg:61.19ms
step:2319/2330 train_time:141891ms step_avg:61.19ms
step:2320/2330 train_time:141955ms step_avg:61.19ms
step:2321/2330 train_time:142015ms step_avg:61.19ms
step:2322/2330 train_time:142079ms step_avg:61.19ms
step:2323/2330 train_time:142139ms step_avg:61.19ms
step:2324/2330 train_time:142202ms step_avg:61.19ms
step:2325/2330 train_time:142263ms step_avg:61.19ms
step:2326/2330 train_time:142327ms step_avg:61.19ms
step:2327/2330 train_time:142386ms step_avg:61.19ms
step:2328/2330 train_time:142449ms step_avg:61.19ms
step:2329/2330 train_time:142508ms step_avg:61.19ms
step:2330/2330 train_time:142570ms step_avg:61.19ms
step:2330/2330 val_loss:3.5589 train_time:142643ms step_avg:61.22ms
peak memory allocated: 29998 MiB reserved: 44076 MiB
