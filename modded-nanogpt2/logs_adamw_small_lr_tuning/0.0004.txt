import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--adamw_lr', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_lr_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_lr_tuning/{args2.adamw_lr}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=float(args2.adamw_lr),  betas=(0.85, 0.85), eps=1e-10, weight_decay=0.0, fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_lr_tuning/{args2.adamw_lr}", exist_ok=True)
            torch.save(log, f"logs_adamw_small_lr_tuning/{args2.adamw_lr}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 01:12:15 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   35C    P0             118W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   35C    P0             117W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             118W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   28C    P0             115W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   34C    P0             119W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             114W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:83ms step_avg:82.71ms
step:2/2330 train_time:178ms step_avg:88.76ms
step:3/2330 train_time:196ms step_avg:65.42ms
step:4/2330 train_time:217ms step_avg:54.18ms
step:5/2330 train_time:269ms step_avg:53.83ms
step:6/2330 train_time:327ms step_avg:54.49ms
step:7/2330 train_time:382ms step_avg:54.58ms
step:8/2330 train_time:441ms step_avg:55.07ms
step:9/2330 train_time:496ms step_avg:55.13ms
step:10/2330 train_time:555ms step_avg:55.48ms
step:11/2330 train_time:611ms step_avg:55.51ms
step:12/2330 train_time:669ms step_avg:55.73ms
step:13/2330 train_time:725ms step_avg:55.75ms
step:14/2330 train_time:783ms step_avg:55.94ms
step:15/2330 train_time:839ms step_avg:55.93ms
step:16/2330 train_time:897ms step_avg:56.07ms
step:17/2330 train_time:953ms step_avg:56.06ms
step:18/2330 train_time:1012ms step_avg:56.23ms
step:19/2330 train_time:1070ms step_avg:56.33ms
step:20/2330 train_time:1133ms step_avg:56.63ms
step:21/2330 train_time:1190ms step_avg:56.66ms
step:22/2330 train_time:1250ms step_avg:56.82ms
step:23/2330 train_time:1307ms step_avg:56.83ms
step:24/2330 train_time:1366ms step_avg:56.92ms
step:25/2330 train_time:1422ms step_avg:56.88ms
step:26/2330 train_time:1481ms step_avg:56.97ms
step:27/2330 train_time:1537ms step_avg:56.91ms
step:28/2330 train_time:1595ms step_avg:56.97ms
step:29/2330 train_time:1651ms step_avg:56.92ms
step:30/2330 train_time:1709ms step_avg:56.97ms
step:31/2330 train_time:1765ms step_avg:56.94ms
step:32/2330 train_time:1823ms step_avg:56.97ms
step:33/2330 train_time:1879ms step_avg:56.94ms
step:34/2330 train_time:1937ms step_avg:56.98ms
step:35/2330 train_time:1994ms step_avg:56.98ms
step:36/2330 train_time:2054ms step_avg:57.05ms
step:37/2330 train_time:2111ms step_avg:57.05ms
step:38/2330 train_time:2170ms step_avg:57.12ms
step:39/2330 train_time:2228ms step_avg:57.14ms
step:40/2330 train_time:2287ms step_avg:57.18ms
step:41/2330 train_time:2344ms step_avg:57.16ms
step:42/2330 train_time:2403ms step_avg:57.21ms
step:43/2330 train_time:2459ms step_avg:57.18ms
step:44/2330 train_time:2517ms step_avg:57.19ms
step:45/2330 train_time:2572ms step_avg:57.16ms
step:46/2330 train_time:2631ms step_avg:57.21ms
step:47/2330 train_time:2687ms step_avg:57.17ms
step:48/2330 train_time:2746ms step_avg:57.21ms
step:49/2330 train_time:2802ms step_avg:57.19ms
step:50/2330 train_time:2860ms step_avg:57.21ms
step:51/2330 train_time:2916ms step_avg:57.18ms
step:52/2330 train_time:2974ms step_avg:57.20ms
step:53/2330 train_time:3031ms step_avg:57.19ms
step:54/2330 train_time:3091ms step_avg:57.24ms
step:55/2330 train_time:3148ms step_avg:57.24ms
step:56/2330 train_time:3207ms step_avg:57.27ms
step:57/2330 train_time:3264ms step_avg:57.27ms
step:58/2330 train_time:3323ms step_avg:57.29ms
step:59/2330 train_time:3380ms step_avg:57.28ms
step:60/2330 train_time:3439ms step_avg:57.31ms
step:61/2330 train_time:3495ms step_avg:57.29ms
step:62/2330 train_time:3553ms step_avg:57.31ms
step:63/2330 train_time:3609ms step_avg:57.28ms
step:64/2330 train_time:3668ms step_avg:57.31ms
step:65/2330 train_time:3724ms step_avg:57.29ms
step:66/2330 train_time:3782ms step_avg:57.30ms
step:67/2330 train_time:3838ms step_avg:57.28ms
step:68/2330 train_time:3896ms step_avg:57.30ms
step:69/2330 train_time:3953ms step_avg:57.29ms
step:70/2330 train_time:4011ms step_avg:57.30ms
step:71/2330 train_time:4067ms step_avg:57.29ms
step:72/2330 train_time:4127ms step_avg:57.32ms
step:73/2330 train_time:4184ms step_avg:57.31ms
step:74/2330 train_time:4242ms step_avg:57.33ms
step:75/2330 train_time:4299ms step_avg:57.32ms
step:76/2330 train_time:4358ms step_avg:57.34ms
step:77/2330 train_time:4415ms step_avg:57.34ms
step:78/2330 train_time:4473ms step_avg:57.35ms
step:79/2330 train_time:4529ms step_avg:57.33ms
step:80/2330 train_time:4589ms step_avg:57.36ms
step:81/2330 train_time:4645ms step_avg:57.35ms
step:82/2330 train_time:4704ms step_avg:57.36ms
step:83/2330 train_time:4760ms step_avg:57.35ms
step:84/2330 train_time:4818ms step_avg:57.36ms
step:85/2330 train_time:4874ms step_avg:57.34ms
step:86/2330 train_time:4933ms step_avg:57.36ms
step:87/2330 train_time:4989ms step_avg:57.34ms
step:88/2330 train_time:5048ms step_avg:57.37ms
step:89/2330 train_time:5104ms step_avg:57.35ms
step:90/2330 train_time:5163ms step_avg:57.37ms
step:91/2330 train_time:5220ms step_avg:57.36ms
step:92/2330 train_time:5279ms step_avg:57.38ms
step:93/2330 train_time:5335ms step_avg:57.37ms
step:94/2330 train_time:5395ms step_avg:57.39ms
step:95/2330 train_time:5451ms step_avg:57.38ms
step:96/2330 train_time:5509ms step_avg:57.39ms
step:97/2330 train_time:5565ms step_avg:57.38ms
step:98/2330 train_time:5625ms step_avg:57.40ms
step:99/2330 train_time:5682ms step_avg:57.39ms
step:100/2330 train_time:5740ms step_avg:57.40ms
step:101/2330 train_time:5796ms step_avg:57.39ms
step:102/2330 train_time:5855ms step_avg:57.41ms
step:103/2330 train_time:5911ms step_avg:57.39ms
step:104/2330 train_time:5972ms step_avg:57.42ms
step:105/2330 train_time:6028ms step_avg:57.41ms
step:106/2330 train_time:6087ms step_avg:57.42ms
step:107/2330 train_time:6143ms step_avg:57.42ms
step:108/2330 train_time:6202ms step_avg:57.43ms
step:109/2330 train_time:6259ms step_avg:57.42ms
step:110/2330 train_time:6318ms step_avg:57.44ms
step:111/2330 train_time:6375ms step_avg:57.43ms
step:112/2330 train_time:6433ms step_avg:57.44ms
step:113/2330 train_time:6489ms step_avg:57.43ms
step:114/2330 train_time:6548ms step_avg:57.44ms
step:115/2330 train_time:6604ms step_avg:57.43ms
step:116/2330 train_time:6664ms step_avg:57.44ms
step:117/2330 train_time:6720ms step_avg:57.43ms
step:118/2330 train_time:6778ms step_avg:57.44ms
step:119/2330 train_time:6834ms step_avg:57.43ms
step:120/2330 train_time:6893ms step_avg:57.44ms
step:121/2330 train_time:6949ms step_avg:57.43ms
step:122/2330 train_time:7009ms step_avg:57.45ms
step:123/2330 train_time:7065ms step_avg:57.44ms
step:124/2330 train_time:7124ms step_avg:57.45ms
step:125/2330 train_time:7181ms step_avg:57.45ms
step:126/2330 train_time:7240ms step_avg:57.46ms
step:127/2330 train_time:7296ms step_avg:57.45ms
step:128/2330 train_time:7355ms step_avg:57.46ms
step:129/2330 train_time:7412ms step_avg:57.46ms
step:130/2330 train_time:7470ms step_avg:57.46ms
step:131/2330 train_time:7526ms step_avg:57.45ms
step:132/2330 train_time:7586ms step_avg:57.47ms
step:133/2330 train_time:7643ms step_avg:57.46ms
step:134/2330 train_time:7701ms step_avg:57.47ms
step:135/2330 train_time:7757ms step_avg:57.46ms
step:136/2330 train_time:7816ms step_avg:57.47ms
step:137/2330 train_time:7872ms step_avg:57.46ms
step:138/2330 train_time:7932ms step_avg:57.48ms
step:139/2330 train_time:7988ms step_avg:57.46ms
step:140/2330 train_time:8046ms step_avg:57.47ms
step:141/2330 train_time:8102ms step_avg:57.46ms
step:142/2330 train_time:8161ms step_avg:57.47ms
step:143/2330 train_time:8217ms step_avg:57.46ms
step:144/2330 train_time:8276ms step_avg:57.47ms
step:145/2330 train_time:8333ms step_avg:57.47ms
step:146/2330 train_time:8392ms step_avg:57.48ms
step:147/2330 train_time:8449ms step_avg:57.47ms
step:148/2330 train_time:8506ms step_avg:57.48ms
step:149/2330 train_time:8563ms step_avg:57.47ms
step:150/2330 train_time:8622ms step_avg:57.48ms
step:151/2330 train_time:8678ms step_avg:57.47ms
step:152/2330 train_time:8737ms step_avg:57.48ms
step:153/2330 train_time:8792ms step_avg:57.47ms
step:154/2330 train_time:8853ms step_avg:57.48ms
step:155/2330 train_time:8909ms step_avg:57.48ms
step:156/2330 train_time:8967ms step_avg:57.48ms
step:157/2330 train_time:9024ms step_avg:57.48ms
step:158/2330 train_time:9083ms step_avg:57.49ms
step:159/2330 train_time:9139ms step_avg:57.48ms
step:160/2330 train_time:9198ms step_avg:57.49ms
step:161/2330 train_time:9254ms step_avg:57.48ms
step:162/2330 train_time:9314ms step_avg:57.50ms
step:163/2330 train_time:9370ms step_avg:57.48ms
step:164/2330 train_time:9430ms step_avg:57.50ms
step:165/2330 train_time:9486ms step_avg:57.49ms
step:166/2330 train_time:9546ms step_avg:57.50ms
step:167/2330 train_time:9602ms step_avg:57.50ms
step:168/2330 train_time:9661ms step_avg:57.50ms
step:169/2330 train_time:9717ms step_avg:57.50ms
step:170/2330 train_time:9776ms step_avg:57.50ms
step:171/2330 train_time:9832ms step_avg:57.50ms
step:172/2330 train_time:9891ms step_avg:57.51ms
step:173/2330 train_time:9947ms step_avg:57.50ms
step:174/2330 train_time:10006ms step_avg:57.51ms
step:175/2330 train_time:10063ms step_avg:57.50ms
step:176/2330 train_time:10122ms step_avg:57.51ms
step:177/2330 train_time:10179ms step_avg:57.51ms
step:178/2330 train_time:10237ms step_avg:57.51ms
step:179/2330 train_time:10293ms step_avg:57.50ms
step:180/2330 train_time:10354ms step_avg:57.52ms
step:181/2330 train_time:10409ms step_avg:57.51ms
step:182/2330 train_time:10470ms step_avg:57.53ms
step:183/2330 train_time:10527ms step_avg:57.52ms
step:184/2330 train_time:10586ms step_avg:57.53ms
step:185/2330 train_time:10643ms step_avg:57.53ms
step:186/2330 train_time:10701ms step_avg:57.53ms
step:187/2330 train_time:10757ms step_avg:57.53ms
step:188/2330 train_time:10817ms step_avg:57.54ms
step:189/2330 train_time:10873ms step_avg:57.53ms
step:190/2330 train_time:10933ms step_avg:57.54ms
step:191/2330 train_time:10989ms step_avg:57.53ms
step:192/2330 train_time:11049ms step_avg:57.54ms
step:193/2330 train_time:11105ms step_avg:57.54ms
step:194/2330 train_time:11164ms step_avg:57.55ms
step:195/2330 train_time:11221ms step_avg:57.54ms
step:196/2330 train_time:11280ms step_avg:57.55ms
step:197/2330 train_time:11336ms step_avg:57.54ms
step:198/2330 train_time:11394ms step_avg:57.55ms
step:199/2330 train_time:11450ms step_avg:57.54ms
step:200/2330 train_time:11511ms step_avg:57.56ms
step:201/2330 train_time:11567ms step_avg:57.55ms
step:202/2330 train_time:11627ms step_avg:57.56ms
step:203/2330 train_time:11684ms step_avg:57.56ms
step:204/2330 train_time:11744ms step_avg:57.57ms
step:205/2330 train_time:11800ms step_avg:57.56ms
step:206/2330 train_time:11858ms step_avg:57.56ms
step:207/2330 train_time:11914ms step_avg:57.55ms
step:208/2330 train_time:11973ms step_avg:57.56ms
step:209/2330 train_time:12028ms step_avg:57.55ms
step:210/2330 train_time:12088ms step_avg:57.56ms
step:211/2330 train_time:12146ms step_avg:57.56ms
step:212/2330 train_time:12205ms step_avg:57.57ms
step:213/2330 train_time:12261ms step_avg:57.57ms
step:214/2330 train_time:12320ms step_avg:57.57ms
step:215/2330 train_time:12376ms step_avg:57.56ms
step:216/2330 train_time:12436ms step_avg:57.57ms
step:217/2330 train_time:12491ms step_avg:57.56ms
step:218/2330 train_time:12551ms step_avg:57.57ms
step:219/2330 train_time:12607ms step_avg:57.56ms
step:220/2330 train_time:12665ms step_avg:57.57ms
step:221/2330 train_time:12722ms step_avg:57.57ms
step:222/2330 train_time:12781ms step_avg:57.57ms
step:223/2330 train_time:12838ms step_avg:57.57ms
step:224/2330 train_time:12896ms step_avg:57.57ms
step:225/2330 train_time:12952ms step_avg:57.57ms
step:226/2330 train_time:13012ms step_avg:57.58ms
step:227/2330 train_time:13068ms step_avg:57.57ms
step:228/2330 train_time:13128ms step_avg:57.58ms
step:229/2330 train_time:13184ms step_avg:57.57ms
step:230/2330 train_time:13243ms step_avg:57.58ms
step:231/2330 train_time:13300ms step_avg:57.58ms
step:232/2330 train_time:13359ms step_avg:57.58ms
step:233/2330 train_time:13415ms step_avg:57.57ms
step:234/2330 train_time:13474ms step_avg:57.58ms
step:235/2330 train_time:13530ms step_avg:57.57ms
step:236/2330 train_time:13589ms step_avg:57.58ms
step:237/2330 train_time:13646ms step_avg:57.58ms
step:238/2330 train_time:13705ms step_avg:57.59ms
step:239/2330 train_time:13762ms step_avg:57.58ms
step:240/2330 train_time:13821ms step_avg:57.59ms
step:241/2330 train_time:13877ms step_avg:57.58ms
step:242/2330 train_time:13936ms step_avg:57.58ms
step:243/2330 train_time:13992ms step_avg:57.58ms
step:244/2330 train_time:14051ms step_avg:57.59ms
step:245/2330 train_time:14108ms step_avg:57.58ms
step:246/2330 train_time:14166ms step_avg:57.58ms
step:247/2330 train_time:14222ms step_avg:57.58ms
step:248/2330 train_time:14282ms step_avg:57.59ms
step:249/2330 train_time:14339ms step_avg:57.58ms
step:250/2330 train_time:14397ms step_avg:57.59ms
step:250/2330 val_loss:4.9471 train_time:14476ms step_avg:57.91ms
step:251/2330 train_time:14495ms step_avg:57.75ms
step:252/2330 train_time:14515ms step_avg:57.60ms
step:253/2330 train_time:14570ms step_avg:57.59ms
step:254/2330 train_time:14632ms step_avg:57.61ms
step:255/2330 train_time:14688ms step_avg:57.60ms
step:256/2330 train_time:14754ms step_avg:57.63ms
step:257/2330 train_time:14810ms step_avg:57.63ms
step:258/2330 train_time:14870ms step_avg:57.64ms
step:259/2330 train_time:14925ms step_avg:57.63ms
step:260/2330 train_time:14984ms step_avg:57.63ms
step:261/2330 train_time:15040ms step_avg:57.62ms
step:262/2330 train_time:15098ms step_avg:57.63ms
step:263/2330 train_time:15154ms step_avg:57.62ms
step:264/2330 train_time:15213ms step_avg:57.62ms
step:265/2330 train_time:15269ms step_avg:57.62ms
step:266/2330 train_time:15327ms step_avg:57.62ms
step:267/2330 train_time:15383ms step_avg:57.61ms
step:268/2330 train_time:15442ms step_avg:57.62ms
step:269/2330 train_time:15499ms step_avg:57.62ms
step:270/2330 train_time:15560ms step_avg:57.63ms
step:271/2330 train_time:15617ms step_avg:57.63ms
step:272/2330 train_time:15677ms step_avg:57.64ms
step:273/2330 train_time:15734ms step_avg:57.63ms
step:274/2330 train_time:15794ms step_avg:57.64ms
step:275/2330 train_time:15850ms step_avg:57.64ms
step:276/2330 train_time:15909ms step_avg:57.64ms
step:277/2330 train_time:15965ms step_avg:57.64ms
step:278/2330 train_time:16024ms step_avg:57.64ms
step:279/2330 train_time:16080ms step_avg:57.64ms
step:280/2330 train_time:16139ms step_avg:57.64ms
step:281/2330 train_time:16194ms step_avg:57.63ms
step:282/2330 train_time:16253ms step_avg:57.64ms
step:283/2330 train_time:16309ms step_avg:57.63ms
step:284/2330 train_time:16368ms step_avg:57.63ms
step:285/2330 train_time:16425ms step_avg:57.63ms
step:286/2330 train_time:16484ms step_avg:57.64ms
step:287/2330 train_time:16540ms step_avg:57.63ms
step:288/2330 train_time:16601ms step_avg:57.64ms
step:289/2330 train_time:16657ms step_avg:57.64ms
step:290/2330 train_time:16719ms step_avg:57.65ms
step:291/2330 train_time:16775ms step_avg:57.65ms
step:292/2330 train_time:16835ms step_avg:57.65ms
step:293/2330 train_time:16891ms step_avg:57.65ms
step:294/2330 train_time:16951ms step_avg:57.66ms
step:295/2330 train_time:17007ms step_avg:57.65ms
step:296/2330 train_time:17067ms step_avg:57.66ms
step:297/2330 train_time:17123ms step_avg:57.65ms
step:298/2330 train_time:17181ms step_avg:57.66ms
step:299/2330 train_time:17236ms step_avg:57.65ms
step:300/2330 train_time:17296ms step_avg:57.65ms
step:301/2330 train_time:17352ms step_avg:57.65ms
step:302/2330 train_time:17411ms step_avg:57.65ms
step:303/2330 train_time:17467ms step_avg:57.65ms
step:304/2330 train_time:17526ms step_avg:57.65ms
step:305/2330 train_time:17582ms step_avg:57.65ms
step:306/2330 train_time:17643ms step_avg:57.66ms
step:307/2330 train_time:17699ms step_avg:57.65ms
step:308/2330 train_time:17760ms step_avg:57.66ms
step:309/2330 train_time:17817ms step_avg:57.66ms
step:310/2330 train_time:17876ms step_avg:57.66ms
step:311/2330 train_time:17932ms step_avg:57.66ms
step:312/2330 train_time:17992ms step_avg:57.67ms
step:313/2330 train_time:18048ms step_avg:57.66ms
step:314/2330 train_time:18107ms step_avg:57.66ms
step:315/2330 train_time:18162ms step_avg:57.66ms
step:316/2330 train_time:18222ms step_avg:57.66ms
step:317/2330 train_time:18278ms step_avg:57.66ms
step:318/2330 train_time:18337ms step_avg:57.66ms
step:319/2330 train_time:18394ms step_avg:57.66ms
step:320/2330 train_time:18453ms step_avg:57.67ms
step:321/2330 train_time:18510ms step_avg:57.66ms
step:322/2330 train_time:18569ms step_avg:57.67ms
step:323/2330 train_time:18625ms step_avg:57.66ms
step:324/2330 train_time:18684ms step_avg:57.67ms
step:325/2330 train_time:18740ms step_avg:57.66ms
step:326/2330 train_time:18801ms step_avg:57.67ms
step:327/2330 train_time:18857ms step_avg:57.67ms
step:328/2330 train_time:18917ms step_avg:57.67ms
step:329/2330 train_time:18974ms step_avg:57.67ms
step:330/2330 train_time:19033ms step_avg:57.67ms
step:331/2330 train_time:19089ms step_avg:57.67ms
step:332/2330 train_time:19147ms step_avg:57.67ms
step:333/2330 train_time:19203ms step_avg:57.67ms
step:334/2330 train_time:19263ms step_avg:57.67ms
step:335/2330 train_time:19319ms step_avg:57.67ms
step:336/2330 train_time:19378ms step_avg:57.67ms
step:337/2330 train_time:19435ms step_avg:57.67ms
step:338/2330 train_time:19494ms step_avg:57.67ms
step:339/2330 train_time:19550ms step_avg:57.67ms
step:340/2330 train_time:19609ms step_avg:57.67ms
step:341/2330 train_time:19665ms step_avg:57.67ms
step:342/2330 train_time:19724ms step_avg:57.67ms
step:343/2330 train_time:19780ms step_avg:57.67ms
step:344/2330 train_time:19841ms step_avg:57.68ms
step:345/2330 train_time:19898ms step_avg:57.67ms
step:346/2330 train_time:19956ms step_avg:57.68ms
step:347/2330 train_time:20014ms step_avg:57.68ms
step:348/2330 train_time:20073ms step_avg:57.68ms
step:349/2330 train_time:20129ms step_avg:57.68ms
step:350/2330 train_time:20187ms step_avg:57.68ms
step:351/2330 train_time:20244ms step_avg:57.67ms
step:352/2330 train_time:20303ms step_avg:57.68ms
step:353/2330 train_time:20359ms step_avg:57.67ms
step:354/2330 train_time:20419ms step_avg:57.68ms
step:355/2330 train_time:20475ms step_avg:57.68ms
step:356/2330 train_time:20535ms step_avg:57.68ms
step:357/2330 train_time:20592ms step_avg:57.68ms
step:358/2330 train_time:20650ms step_avg:57.68ms
step:359/2330 train_time:20707ms step_avg:57.68ms
step:360/2330 train_time:20766ms step_avg:57.68ms
step:361/2330 train_time:20822ms step_avg:57.68ms
step:362/2330 train_time:20882ms step_avg:57.69ms
step:363/2330 train_time:20938ms step_avg:57.68ms
step:364/2330 train_time:20998ms step_avg:57.69ms
step:365/2330 train_time:21054ms step_avg:57.68ms
step:366/2330 train_time:21115ms step_avg:57.69ms
step:367/2330 train_time:21172ms step_avg:57.69ms
step:368/2330 train_time:21230ms step_avg:57.69ms
step:369/2330 train_time:21287ms step_avg:57.69ms
step:370/2330 train_time:21345ms step_avg:57.69ms
step:371/2330 train_time:21401ms step_avg:57.69ms
step:372/2330 train_time:21460ms step_avg:57.69ms
step:373/2330 train_time:21516ms step_avg:57.68ms
step:374/2330 train_time:21576ms step_avg:57.69ms
step:375/2330 train_time:21632ms step_avg:57.69ms
step:376/2330 train_time:21692ms step_avg:57.69ms
step:377/2330 train_time:21748ms step_avg:57.69ms
step:378/2330 train_time:21807ms step_avg:57.69ms
step:379/2330 train_time:21863ms step_avg:57.69ms
step:380/2330 train_time:21923ms step_avg:57.69ms
step:381/2330 train_time:21979ms step_avg:57.69ms
step:382/2330 train_time:22039ms step_avg:57.69ms
step:383/2330 train_time:22096ms step_avg:57.69ms
step:384/2330 train_time:22155ms step_avg:57.70ms
step:385/2330 train_time:22212ms step_avg:57.69ms
step:386/2330 train_time:22272ms step_avg:57.70ms
step:387/2330 train_time:22329ms step_avg:57.70ms
step:388/2330 train_time:22387ms step_avg:57.70ms
step:389/2330 train_time:22443ms step_avg:57.69ms
step:390/2330 train_time:22502ms step_avg:57.70ms
step:391/2330 train_time:22559ms step_avg:57.70ms
step:392/2330 train_time:22619ms step_avg:57.70ms
step:393/2330 train_time:22675ms step_avg:57.70ms
step:394/2330 train_time:22734ms step_avg:57.70ms
step:395/2330 train_time:22792ms step_avg:57.70ms
step:396/2330 train_time:22851ms step_avg:57.70ms
step:397/2330 train_time:22907ms step_avg:57.70ms
step:398/2330 train_time:22966ms step_avg:57.70ms
step:399/2330 train_time:23023ms step_avg:57.70ms
step:400/2330 train_time:23082ms step_avg:57.71ms
step:401/2330 train_time:23138ms step_avg:57.70ms
step:402/2330 train_time:23198ms step_avg:57.71ms
step:403/2330 train_time:23254ms step_avg:57.70ms
step:404/2330 train_time:23315ms step_avg:57.71ms
step:405/2330 train_time:23371ms step_avg:57.71ms
step:406/2330 train_time:23429ms step_avg:57.71ms
step:407/2330 train_time:23486ms step_avg:57.70ms
step:408/2330 train_time:23545ms step_avg:57.71ms
step:409/2330 train_time:23601ms step_avg:57.70ms
step:410/2330 train_time:23660ms step_avg:57.71ms
step:411/2330 train_time:23716ms step_avg:57.70ms
step:412/2330 train_time:23777ms step_avg:57.71ms
step:413/2330 train_time:23834ms step_avg:57.71ms
step:414/2330 train_time:23893ms step_avg:57.71ms
step:415/2330 train_time:23950ms step_avg:57.71ms
step:416/2330 train_time:24008ms step_avg:57.71ms
step:417/2330 train_time:24064ms step_avg:57.71ms
step:418/2330 train_time:24124ms step_avg:57.71ms
step:419/2330 train_time:24180ms step_avg:57.71ms
step:420/2330 train_time:24241ms step_avg:57.72ms
step:421/2330 train_time:24296ms step_avg:57.71ms
step:422/2330 train_time:24356ms step_avg:57.72ms
step:423/2330 train_time:24412ms step_avg:57.71ms
step:424/2330 train_time:24471ms step_avg:57.72ms
step:425/2330 train_time:24528ms step_avg:57.71ms
step:426/2330 train_time:24586ms step_avg:57.71ms
step:427/2330 train_time:24642ms step_avg:57.71ms
step:428/2330 train_time:24702ms step_avg:57.71ms
step:429/2330 train_time:24758ms step_avg:57.71ms
step:430/2330 train_time:24818ms step_avg:57.72ms
step:431/2330 train_time:24875ms step_avg:57.72ms
step:432/2330 train_time:24935ms step_avg:57.72ms
step:433/2330 train_time:24991ms step_avg:57.72ms
step:434/2330 train_time:25050ms step_avg:57.72ms
step:435/2330 train_time:25106ms step_avg:57.71ms
step:436/2330 train_time:25165ms step_avg:57.72ms
step:437/2330 train_time:25222ms step_avg:57.72ms
step:438/2330 train_time:25281ms step_avg:57.72ms
step:439/2330 train_time:25337ms step_avg:57.72ms
step:440/2330 train_time:25396ms step_avg:57.72ms
step:441/2330 train_time:25452ms step_avg:57.71ms
step:442/2330 train_time:25511ms step_avg:57.72ms
step:443/2330 train_time:25567ms step_avg:57.71ms
step:444/2330 train_time:25626ms step_avg:57.72ms
step:445/2330 train_time:25682ms step_avg:57.71ms
step:446/2330 train_time:25741ms step_avg:57.71ms
step:447/2330 train_time:25797ms step_avg:57.71ms
step:448/2330 train_time:25856ms step_avg:57.72ms
step:449/2330 train_time:25913ms step_avg:57.71ms
step:450/2330 train_time:25973ms step_avg:57.72ms
step:451/2330 train_time:26030ms step_avg:57.72ms
step:452/2330 train_time:26088ms step_avg:57.72ms
step:453/2330 train_time:26144ms step_avg:57.71ms
step:454/2330 train_time:26204ms step_avg:57.72ms
step:455/2330 train_time:26260ms step_avg:57.71ms
step:456/2330 train_time:26320ms step_avg:57.72ms
step:457/2330 train_time:26377ms step_avg:57.72ms
step:458/2330 train_time:26436ms step_avg:57.72ms
step:459/2330 train_time:26492ms step_avg:57.72ms
step:460/2330 train_time:26550ms step_avg:57.72ms
step:461/2330 train_time:26607ms step_avg:57.72ms
step:462/2330 train_time:26667ms step_avg:57.72ms
step:463/2330 train_time:26723ms step_avg:57.72ms
step:464/2330 train_time:26782ms step_avg:57.72ms
step:465/2330 train_time:26837ms step_avg:57.72ms
step:466/2330 train_time:26897ms step_avg:57.72ms
step:467/2330 train_time:26954ms step_avg:57.72ms
step:468/2330 train_time:27013ms step_avg:57.72ms
step:469/2330 train_time:27069ms step_avg:57.72ms
step:470/2330 train_time:27128ms step_avg:57.72ms
step:471/2330 train_time:27185ms step_avg:57.72ms
step:472/2330 train_time:27243ms step_avg:57.72ms
step:473/2330 train_time:27299ms step_avg:57.72ms
step:474/2330 train_time:27359ms step_avg:57.72ms
step:475/2330 train_time:27415ms step_avg:57.72ms
step:476/2330 train_time:27476ms step_avg:57.72ms
step:477/2330 train_time:27533ms step_avg:57.72ms
step:478/2330 train_time:27592ms step_avg:57.72ms
step:479/2330 train_time:27648ms step_avg:57.72ms
step:480/2330 train_time:27707ms step_avg:57.72ms
step:481/2330 train_time:27763ms step_avg:57.72ms
step:482/2330 train_time:27823ms step_avg:57.72ms
step:483/2330 train_time:27879ms step_avg:57.72ms
step:484/2330 train_time:27940ms step_avg:57.73ms
step:485/2330 train_time:27996ms step_avg:57.72ms
step:486/2330 train_time:28056ms step_avg:57.73ms
step:487/2330 train_time:28112ms step_avg:57.72ms
step:488/2330 train_time:28171ms step_avg:57.73ms
step:489/2330 train_time:28227ms step_avg:57.72ms
step:490/2330 train_time:28286ms step_avg:57.73ms
step:491/2330 train_time:28342ms step_avg:57.72ms
step:492/2330 train_time:28402ms step_avg:57.73ms
step:493/2330 train_time:28458ms step_avg:57.72ms
step:494/2330 train_time:28520ms step_avg:57.73ms
step:495/2330 train_time:28576ms step_avg:57.73ms
step:496/2330 train_time:28637ms step_avg:57.74ms
step:497/2330 train_time:28694ms step_avg:57.73ms
step:498/2330 train_time:28753ms step_avg:57.74ms
step:499/2330 train_time:28809ms step_avg:57.73ms
step:500/2330 train_time:28868ms step_avg:57.74ms
step:500/2330 val_loss:4.4659 train_time:28947ms step_avg:57.89ms
step:501/2330 train_time:28966ms step_avg:57.82ms
step:502/2330 train_time:28985ms step_avg:57.74ms
step:503/2330 train_time:29043ms step_avg:57.74ms
step:504/2330 train_time:29107ms step_avg:57.75ms
step:505/2330 train_time:29164ms step_avg:57.75ms
step:506/2330 train_time:29227ms step_avg:57.76ms
step:507/2330 train_time:29282ms step_avg:57.75ms
step:508/2330 train_time:29342ms step_avg:57.76ms
step:509/2330 train_time:29398ms step_avg:57.76ms
step:510/2330 train_time:29458ms step_avg:57.76ms
step:511/2330 train_time:29513ms step_avg:57.76ms
step:512/2330 train_time:29572ms step_avg:57.76ms
step:513/2330 train_time:29627ms step_avg:57.75ms
step:514/2330 train_time:29685ms step_avg:57.75ms
step:515/2330 train_time:29741ms step_avg:57.75ms
step:516/2330 train_time:29800ms step_avg:57.75ms
step:517/2330 train_time:29856ms step_avg:57.75ms
step:518/2330 train_time:29915ms step_avg:57.75ms
step:519/2330 train_time:29972ms step_avg:57.75ms
step:520/2330 train_time:30033ms step_avg:57.76ms
step:521/2330 train_time:30091ms step_avg:57.76ms
step:522/2330 train_time:30153ms step_avg:57.76ms
step:523/2330 train_time:30210ms step_avg:57.76ms
step:524/2330 train_time:30270ms step_avg:57.77ms
step:525/2330 train_time:30326ms step_avg:57.76ms
step:526/2330 train_time:30385ms step_avg:57.77ms
step:527/2330 train_time:30440ms step_avg:57.76ms
step:528/2330 train_time:30500ms step_avg:57.76ms
step:529/2330 train_time:30555ms step_avg:57.76ms
step:530/2330 train_time:30615ms step_avg:57.76ms
step:531/2330 train_time:30670ms step_avg:57.76ms
step:532/2330 train_time:30729ms step_avg:57.76ms
step:533/2330 train_time:30784ms step_avg:57.76ms
step:534/2330 train_time:30843ms step_avg:57.76ms
step:535/2330 train_time:30900ms step_avg:57.76ms
step:536/2330 train_time:30959ms step_avg:57.76ms
step:537/2330 train_time:31016ms step_avg:57.76ms
step:538/2330 train_time:31077ms step_avg:57.76ms
step:539/2330 train_time:31134ms step_avg:57.76ms
step:540/2330 train_time:31194ms step_avg:57.77ms
step:541/2330 train_time:31251ms step_avg:57.77ms
step:542/2330 train_time:31311ms step_avg:57.77ms
step:543/2330 train_time:31368ms step_avg:57.77ms
step:544/2330 train_time:31426ms step_avg:57.77ms
step:545/2330 train_time:31482ms step_avg:57.76ms
step:546/2330 train_time:31541ms step_avg:57.77ms
step:547/2330 train_time:31598ms step_avg:57.77ms
step:548/2330 train_time:31657ms step_avg:57.77ms
step:549/2330 train_time:31712ms step_avg:57.76ms
step:550/2330 train_time:31771ms step_avg:57.77ms
step:551/2330 train_time:31827ms step_avg:57.76ms
step:552/2330 train_time:31885ms step_avg:57.76ms
step:553/2330 train_time:31941ms step_avg:57.76ms
step:554/2330 train_time:32002ms step_avg:57.76ms
step:555/2330 train_time:32058ms step_avg:57.76ms
step:556/2330 train_time:32119ms step_avg:57.77ms
step:557/2330 train_time:32175ms step_avg:57.76ms
step:558/2330 train_time:32236ms step_avg:57.77ms
step:559/2330 train_time:32294ms step_avg:57.77ms
step:560/2330 train_time:32353ms step_avg:57.77ms
step:561/2330 train_time:32411ms step_avg:57.77ms
step:562/2330 train_time:32469ms step_avg:57.77ms
step:563/2330 train_time:32526ms step_avg:57.77ms
step:564/2330 train_time:32584ms step_avg:57.77ms
step:565/2330 train_time:32640ms step_avg:57.77ms
step:566/2330 train_time:32701ms step_avg:57.78ms
step:567/2330 train_time:32757ms step_avg:57.77ms
step:568/2330 train_time:32815ms step_avg:57.77ms
step:569/2330 train_time:32872ms step_avg:57.77ms
step:570/2330 train_time:32931ms step_avg:57.77ms
step:571/2330 train_time:32987ms step_avg:57.77ms
step:572/2330 train_time:33047ms step_avg:57.77ms
step:573/2330 train_time:33103ms step_avg:57.77ms
step:574/2330 train_time:33163ms step_avg:57.78ms
step:575/2330 train_time:33219ms step_avg:57.77ms
step:576/2330 train_time:33281ms step_avg:57.78ms
step:577/2330 train_time:33337ms step_avg:57.78ms
step:578/2330 train_time:33397ms step_avg:57.78ms
step:579/2330 train_time:33454ms step_avg:57.78ms
step:580/2330 train_time:33513ms step_avg:57.78ms
step:581/2330 train_time:33570ms step_avg:57.78ms
step:582/2330 train_time:33629ms step_avg:57.78ms
step:583/2330 train_time:33685ms step_avg:57.78ms
step:584/2330 train_time:33743ms step_avg:57.78ms
step:585/2330 train_time:33799ms step_avg:57.78ms
step:586/2330 train_time:33858ms step_avg:57.78ms
step:587/2330 train_time:33914ms step_avg:57.78ms
step:588/2330 train_time:33973ms step_avg:57.78ms
step:589/2330 train_time:34029ms step_avg:57.77ms
step:590/2330 train_time:34089ms step_avg:57.78ms
step:591/2330 train_time:34145ms step_avg:57.78ms
step:592/2330 train_time:34205ms step_avg:57.78ms
step:593/2330 train_time:34261ms step_avg:57.78ms
step:594/2330 train_time:34321ms step_avg:57.78ms
step:595/2330 train_time:34377ms step_avg:57.78ms
step:596/2330 train_time:34439ms step_avg:57.78ms
step:597/2330 train_time:34495ms step_avg:57.78ms
step:598/2330 train_time:34554ms step_avg:57.78ms
step:599/2330 train_time:34610ms step_avg:57.78ms
step:600/2330 train_time:34671ms step_avg:57.78ms
step:601/2330 train_time:34726ms step_avg:57.78ms
step:602/2330 train_time:34785ms step_avg:57.78ms
step:603/2330 train_time:34842ms step_avg:57.78ms
step:604/2330 train_time:34900ms step_avg:57.78ms
step:605/2330 train_time:34956ms step_avg:57.78ms
step:606/2330 train_time:35016ms step_avg:57.78ms
step:607/2330 train_time:35072ms step_avg:57.78ms
step:608/2330 train_time:35131ms step_avg:57.78ms
step:609/2330 train_time:35187ms step_avg:57.78ms
step:610/2330 train_time:35247ms step_avg:57.78ms
step:611/2330 train_time:35303ms step_avg:57.78ms
step:612/2330 train_time:35363ms step_avg:57.78ms
step:613/2330 train_time:35420ms step_avg:57.78ms
step:614/2330 train_time:35479ms step_avg:57.78ms
step:615/2330 train_time:35536ms step_avg:57.78ms
step:616/2330 train_time:35595ms step_avg:57.78ms
step:617/2330 train_time:35653ms step_avg:57.78ms
step:618/2330 train_time:35711ms step_avg:57.79ms
step:619/2330 train_time:35768ms step_avg:57.78ms
step:620/2330 train_time:35826ms step_avg:57.78ms
step:621/2330 train_time:35882ms step_avg:57.78ms
step:622/2330 train_time:35941ms step_avg:57.78ms
step:623/2330 train_time:35998ms step_avg:57.78ms
step:624/2330 train_time:36058ms step_avg:57.78ms
step:625/2330 train_time:36114ms step_avg:57.78ms
step:626/2330 train_time:36174ms step_avg:57.79ms
step:627/2330 train_time:36231ms step_avg:57.78ms
step:628/2330 train_time:36290ms step_avg:57.79ms
step:629/2330 train_time:36346ms step_avg:57.78ms
step:630/2330 train_time:36405ms step_avg:57.79ms
step:631/2330 train_time:36462ms step_avg:57.78ms
step:632/2330 train_time:36520ms step_avg:57.79ms
step:633/2330 train_time:36577ms step_avg:57.78ms
step:634/2330 train_time:36636ms step_avg:57.79ms
step:635/2330 train_time:36692ms step_avg:57.78ms
step:636/2330 train_time:36753ms step_avg:57.79ms
step:637/2330 train_time:36809ms step_avg:57.79ms
step:638/2330 train_time:36869ms step_avg:57.79ms
step:639/2330 train_time:36925ms step_avg:57.79ms
step:640/2330 train_time:36983ms step_avg:57.79ms
step:641/2330 train_time:37039ms step_avg:57.78ms
step:642/2330 train_time:37100ms step_avg:57.79ms
step:643/2330 train_time:37156ms step_avg:57.78ms
step:644/2330 train_time:37216ms step_avg:57.79ms
step:645/2330 train_time:37273ms step_avg:57.79ms
step:646/2330 train_time:37333ms step_avg:57.79ms
step:647/2330 train_time:37389ms step_avg:57.79ms
step:648/2330 train_time:37448ms step_avg:57.79ms
step:649/2330 train_time:37504ms step_avg:57.79ms
step:650/2330 train_time:37563ms step_avg:57.79ms
step:651/2330 train_time:37619ms step_avg:57.79ms
step:652/2330 train_time:37679ms step_avg:57.79ms
step:653/2330 train_time:37735ms step_avg:57.79ms
step:654/2330 train_time:37794ms step_avg:57.79ms
step:655/2330 train_time:37850ms step_avg:57.79ms
step:656/2330 train_time:37910ms step_avg:57.79ms
step:657/2330 train_time:37966ms step_avg:57.79ms
step:658/2330 train_time:38026ms step_avg:57.79ms
step:659/2330 train_time:38081ms step_avg:57.79ms
step:660/2330 train_time:38142ms step_avg:57.79ms
step:661/2330 train_time:38198ms step_avg:57.79ms
step:662/2330 train_time:38258ms step_avg:57.79ms
step:663/2330 train_time:38314ms step_avg:57.79ms
step:664/2330 train_time:38373ms step_avg:57.79ms
step:665/2330 train_time:38430ms step_avg:57.79ms
step:666/2330 train_time:38489ms step_avg:57.79ms
step:667/2330 train_time:38545ms step_avg:57.79ms
step:668/2330 train_time:38604ms step_avg:57.79ms
step:669/2330 train_time:38661ms step_avg:57.79ms
step:670/2330 train_time:38721ms step_avg:57.79ms
step:671/2330 train_time:38777ms step_avg:57.79ms
step:672/2330 train_time:38837ms step_avg:57.79ms
step:673/2330 train_time:38893ms step_avg:57.79ms
step:674/2330 train_time:38954ms step_avg:57.80ms
step:675/2330 train_time:39010ms step_avg:57.79ms
step:676/2330 train_time:39070ms step_avg:57.80ms
step:677/2330 train_time:39126ms step_avg:57.79ms
step:678/2330 train_time:39185ms step_avg:57.79ms
step:679/2330 train_time:39241ms step_avg:57.79ms
step:680/2330 train_time:39300ms step_avg:57.79ms
step:681/2330 train_time:39356ms step_avg:57.79ms
step:682/2330 train_time:39417ms step_avg:57.80ms
step:683/2330 train_time:39474ms step_avg:57.80ms
step:684/2330 train_time:39533ms step_avg:57.80ms
step:685/2330 train_time:39590ms step_avg:57.80ms
step:686/2330 train_time:39649ms step_avg:57.80ms
step:687/2330 train_time:39705ms step_avg:57.80ms
step:688/2330 train_time:39765ms step_avg:57.80ms
step:689/2330 train_time:39821ms step_avg:57.80ms
step:690/2330 train_time:39880ms step_avg:57.80ms
step:691/2330 train_time:39936ms step_avg:57.79ms
step:692/2330 train_time:39996ms step_avg:57.80ms
step:693/2330 train_time:40052ms step_avg:57.80ms
step:694/2330 train_time:40112ms step_avg:57.80ms
step:695/2330 train_time:40168ms step_avg:57.80ms
step:696/2330 train_time:40227ms step_avg:57.80ms
step:697/2330 train_time:40283ms step_avg:57.79ms
step:698/2330 train_time:40343ms step_avg:57.80ms
step:699/2330 train_time:40399ms step_avg:57.80ms
step:700/2330 train_time:40458ms step_avg:57.80ms
step:701/2330 train_time:40515ms step_avg:57.80ms
step:702/2330 train_time:40574ms step_avg:57.80ms
step:703/2330 train_time:40631ms step_avg:57.80ms
step:704/2330 train_time:40690ms step_avg:57.80ms
step:705/2330 train_time:40747ms step_avg:57.80ms
step:706/2330 train_time:40805ms step_avg:57.80ms
step:707/2330 train_time:40861ms step_avg:57.80ms
step:708/2330 train_time:40922ms step_avg:57.80ms
step:709/2330 train_time:40977ms step_avg:57.80ms
step:710/2330 train_time:41038ms step_avg:57.80ms
step:711/2330 train_time:41094ms step_avg:57.80ms
step:712/2330 train_time:41154ms step_avg:57.80ms
step:713/2330 train_time:41211ms step_avg:57.80ms
step:714/2330 train_time:41270ms step_avg:57.80ms
step:715/2330 train_time:41327ms step_avg:57.80ms
step:716/2330 train_time:41385ms step_avg:57.80ms
step:717/2330 train_time:41441ms step_avg:57.80ms
step:718/2330 train_time:41500ms step_avg:57.80ms
step:719/2330 train_time:41556ms step_avg:57.80ms
step:720/2330 train_time:41616ms step_avg:57.80ms
step:721/2330 train_time:41673ms step_avg:57.80ms
step:722/2330 train_time:41733ms step_avg:57.80ms
step:723/2330 train_time:41789ms step_avg:57.80ms
step:724/2330 train_time:41849ms step_avg:57.80ms
step:725/2330 train_time:41905ms step_avg:57.80ms
step:726/2330 train_time:41964ms step_avg:57.80ms
step:727/2330 train_time:42020ms step_avg:57.80ms
step:728/2330 train_time:42080ms step_avg:57.80ms
step:729/2330 train_time:42136ms step_avg:57.80ms
step:730/2330 train_time:42195ms step_avg:57.80ms
step:731/2330 train_time:42251ms step_avg:57.80ms
step:732/2330 train_time:42311ms step_avg:57.80ms
step:733/2330 train_time:42367ms step_avg:57.80ms
step:734/2330 train_time:42426ms step_avg:57.80ms
step:735/2330 train_time:42482ms step_avg:57.80ms
step:736/2330 train_time:42542ms step_avg:57.80ms
step:737/2330 train_time:42598ms step_avg:57.80ms
step:738/2330 train_time:42658ms step_avg:57.80ms
step:739/2330 train_time:42715ms step_avg:57.80ms
step:740/2330 train_time:42774ms step_avg:57.80ms
step:741/2330 train_time:42830ms step_avg:57.80ms
step:742/2330 train_time:42890ms step_avg:57.80ms
step:743/2330 train_time:42946ms step_avg:57.80ms
step:744/2330 train_time:43005ms step_avg:57.80ms
step:745/2330 train_time:43062ms step_avg:57.80ms
step:746/2330 train_time:43121ms step_avg:57.80ms
step:747/2330 train_time:43176ms step_avg:57.80ms
step:748/2330 train_time:43236ms step_avg:57.80ms
step:749/2330 train_time:43293ms step_avg:57.80ms
step:750/2330 train_time:43354ms step_avg:57.81ms
step:750/2330 val_loss:4.2349 train_time:43434ms step_avg:57.91ms
step:751/2330 train_time:43452ms step_avg:57.86ms
step:752/2330 train_time:43472ms step_avg:57.81ms
step:753/2330 train_time:43528ms step_avg:57.81ms
step:754/2330 train_time:43592ms step_avg:57.81ms
step:755/2330 train_time:43649ms step_avg:57.81ms
step:756/2330 train_time:43710ms step_avg:57.82ms
step:757/2330 train_time:43765ms step_avg:57.81ms
step:758/2330 train_time:43826ms step_avg:57.82ms
step:759/2330 train_time:43881ms step_avg:57.81ms
step:760/2330 train_time:43941ms step_avg:57.82ms
step:761/2330 train_time:43996ms step_avg:57.81ms
step:762/2330 train_time:44055ms step_avg:57.81ms
step:763/2330 train_time:44111ms step_avg:57.81ms
step:764/2330 train_time:44169ms step_avg:57.81ms
step:765/2330 train_time:44225ms step_avg:57.81ms
step:766/2330 train_time:44284ms step_avg:57.81ms
step:767/2330 train_time:44341ms step_avg:57.81ms
step:768/2330 train_time:44401ms step_avg:57.81ms
step:769/2330 train_time:44459ms step_avg:57.81ms
step:770/2330 train_time:44520ms step_avg:57.82ms
step:771/2330 train_time:44577ms step_avg:57.82ms
step:772/2330 train_time:44639ms step_avg:57.82ms
step:773/2330 train_time:44696ms step_avg:57.82ms
step:774/2330 train_time:44757ms step_avg:57.83ms
step:775/2330 train_time:44814ms step_avg:57.83ms
step:776/2330 train_time:44874ms step_avg:57.83ms
step:777/2330 train_time:44931ms step_avg:57.83ms
step:778/2330 train_time:44990ms step_avg:57.83ms
step:779/2330 train_time:45047ms step_avg:57.83ms
step:780/2330 train_time:45106ms step_avg:57.83ms
step:781/2330 train_time:45162ms step_avg:57.83ms
step:782/2330 train_time:45222ms step_avg:57.83ms
step:783/2330 train_time:45279ms step_avg:57.83ms
step:784/2330 train_time:45338ms step_avg:57.83ms
step:785/2330 train_time:45395ms step_avg:57.83ms
step:786/2330 train_time:45456ms step_avg:57.83ms
step:787/2330 train_time:45514ms step_avg:57.83ms
step:788/2330 train_time:45574ms step_avg:57.84ms
step:789/2330 train_time:45632ms step_avg:57.84ms
step:790/2330 train_time:45692ms step_avg:57.84ms
step:791/2330 train_time:45751ms step_avg:57.84ms
step:792/2330 train_time:45811ms step_avg:57.84ms
step:793/2330 train_time:45868ms step_avg:57.84ms
step:794/2330 train_time:45927ms step_avg:57.84ms
step:795/2330 train_time:45983ms step_avg:57.84ms
step:796/2330 train_time:46043ms step_avg:57.84ms
step:797/2330 train_time:46099ms step_avg:57.84ms
step:798/2330 train_time:46160ms step_avg:57.84ms
step:799/2330 train_time:46215ms step_avg:57.84ms
step:800/2330 train_time:46277ms step_avg:57.85ms
step:801/2330 train_time:46334ms step_avg:57.84ms
step:802/2330 train_time:46393ms step_avg:57.85ms
step:803/2330 train_time:46450ms step_avg:57.85ms
step:804/2330 train_time:46511ms step_avg:57.85ms
step:805/2330 train_time:46568ms step_avg:57.85ms
step:806/2330 train_time:46628ms step_avg:57.85ms
step:807/2330 train_time:46685ms step_avg:57.85ms
step:808/2330 train_time:46747ms step_avg:57.86ms
step:809/2330 train_time:46804ms step_avg:57.85ms
step:810/2330 train_time:46865ms step_avg:57.86ms
step:811/2330 train_time:46922ms step_avg:57.86ms
step:812/2330 train_time:46982ms step_avg:57.86ms
step:813/2330 train_time:47039ms step_avg:57.86ms
step:814/2330 train_time:47100ms step_avg:57.86ms
step:815/2330 train_time:47156ms step_avg:57.86ms
step:816/2330 train_time:47216ms step_avg:57.86ms
step:817/2330 train_time:47272ms step_avg:57.86ms
step:818/2330 train_time:47332ms step_avg:57.86ms
step:819/2330 train_time:47388ms step_avg:57.86ms
step:820/2330 train_time:47448ms step_avg:57.86ms
step:821/2330 train_time:47505ms step_avg:57.86ms
step:822/2330 train_time:47565ms step_avg:57.86ms
step:823/2330 train_time:47621ms step_avg:57.86ms
step:824/2330 train_time:47684ms step_avg:57.87ms
step:825/2330 train_time:47740ms step_avg:57.87ms
step:826/2330 train_time:47802ms step_avg:57.87ms
step:827/2330 train_time:47859ms step_avg:57.87ms
step:828/2330 train_time:47919ms step_avg:57.87ms
step:829/2330 train_time:47976ms step_avg:57.87ms
step:830/2330 train_time:48036ms step_avg:57.87ms
step:831/2330 train_time:48093ms step_avg:57.87ms
step:832/2330 train_time:48153ms step_avg:57.88ms
step:833/2330 train_time:48209ms step_avg:57.87ms
step:834/2330 train_time:48268ms step_avg:57.88ms
step:835/2330 train_time:48324ms step_avg:57.87ms
step:836/2330 train_time:48387ms step_avg:57.88ms
step:837/2330 train_time:48443ms step_avg:57.88ms
step:838/2330 train_time:48503ms step_avg:57.88ms
step:839/2330 train_time:48560ms step_avg:57.88ms
step:840/2330 train_time:48621ms step_avg:57.88ms
step:841/2330 train_time:48678ms step_avg:57.88ms
step:842/2330 train_time:48738ms step_avg:57.88ms
step:843/2330 train_time:48796ms step_avg:57.88ms
step:844/2330 train_time:48855ms step_avg:57.89ms
step:845/2330 train_time:48913ms step_avg:57.88ms
step:846/2330 train_time:48973ms step_avg:57.89ms
step:847/2330 train_time:49030ms step_avg:57.89ms
step:848/2330 train_time:49090ms step_avg:57.89ms
step:849/2330 train_time:49147ms step_avg:57.89ms
step:850/2330 train_time:49205ms step_avg:57.89ms
step:851/2330 train_time:49262ms step_avg:57.89ms
step:852/2330 train_time:49323ms step_avg:57.89ms
step:853/2330 train_time:49380ms step_avg:57.89ms
step:854/2330 train_time:49440ms step_avg:57.89ms
step:855/2330 train_time:49496ms step_avg:57.89ms
step:856/2330 train_time:49558ms step_avg:57.89ms
step:857/2330 train_time:49615ms step_avg:57.89ms
step:858/2330 train_time:49675ms step_avg:57.90ms
step:859/2330 train_time:49732ms step_avg:57.90ms
step:860/2330 train_time:49792ms step_avg:57.90ms
step:861/2330 train_time:49849ms step_avg:57.90ms
step:862/2330 train_time:49909ms step_avg:57.90ms
step:863/2330 train_time:49965ms step_avg:57.90ms
step:864/2330 train_time:50026ms step_avg:57.90ms
step:865/2330 train_time:50083ms step_avg:57.90ms
step:866/2330 train_time:50142ms step_avg:57.90ms
step:867/2330 train_time:50198ms step_avg:57.90ms
step:868/2330 train_time:50259ms step_avg:57.90ms
step:869/2330 train_time:50316ms step_avg:57.90ms
step:870/2330 train_time:50376ms step_avg:57.90ms
step:871/2330 train_time:50432ms step_avg:57.90ms
step:872/2330 train_time:50492ms step_avg:57.90ms
step:873/2330 train_time:50550ms step_avg:57.90ms
step:874/2330 train_time:50610ms step_avg:57.91ms
step:875/2330 train_time:50667ms step_avg:57.90ms
step:876/2330 train_time:50726ms step_avg:57.91ms
step:877/2330 train_time:50782ms step_avg:57.90ms
step:878/2330 train_time:50844ms step_avg:57.91ms
step:879/2330 train_time:50901ms step_avg:57.91ms
step:880/2330 train_time:50962ms step_avg:57.91ms
step:881/2330 train_time:51018ms step_avg:57.91ms
step:882/2330 train_time:51079ms step_avg:57.91ms
step:883/2330 train_time:51136ms step_avg:57.91ms
step:884/2330 train_time:51196ms step_avg:57.91ms
step:885/2330 train_time:51253ms step_avg:57.91ms
step:886/2330 train_time:51313ms step_avg:57.92ms
step:887/2330 train_time:51371ms step_avg:57.92ms
step:888/2330 train_time:51430ms step_avg:57.92ms
step:889/2330 train_time:51487ms step_avg:57.92ms
step:890/2330 train_time:51546ms step_avg:57.92ms
step:891/2330 train_time:51603ms step_avg:57.92ms
step:892/2330 train_time:51663ms step_avg:57.92ms
step:893/2330 train_time:51720ms step_avg:57.92ms
step:894/2330 train_time:51781ms step_avg:57.92ms
step:895/2330 train_time:51838ms step_avg:57.92ms
step:896/2330 train_time:51899ms step_avg:57.92ms
step:897/2330 train_time:51956ms step_avg:57.92ms
step:898/2330 train_time:52016ms step_avg:57.92ms
step:899/2330 train_time:52073ms step_avg:57.92ms
step:900/2330 train_time:52132ms step_avg:57.92ms
step:901/2330 train_time:52189ms step_avg:57.92ms
step:902/2330 train_time:52248ms step_avg:57.93ms
step:903/2330 train_time:52306ms step_avg:57.92ms
step:904/2330 train_time:52366ms step_avg:57.93ms
step:905/2330 train_time:52422ms step_avg:57.92ms
step:906/2330 train_time:52482ms step_avg:57.93ms
step:907/2330 train_time:52539ms step_avg:57.93ms
step:908/2330 train_time:52600ms step_avg:57.93ms
step:909/2330 train_time:52657ms step_avg:57.93ms
step:910/2330 train_time:52717ms step_avg:57.93ms
step:911/2330 train_time:52774ms step_avg:57.93ms
step:912/2330 train_time:52834ms step_avg:57.93ms
step:913/2330 train_time:52892ms step_avg:57.93ms
step:914/2330 train_time:52951ms step_avg:57.93ms
step:915/2330 train_time:53008ms step_avg:57.93ms
step:916/2330 train_time:53068ms step_avg:57.93ms
step:917/2330 train_time:53124ms step_avg:57.93ms
step:918/2330 train_time:53185ms step_avg:57.94ms
step:919/2330 train_time:53242ms step_avg:57.93ms
step:920/2330 train_time:53303ms step_avg:57.94ms
step:921/2330 train_time:53359ms step_avg:57.94ms
step:922/2330 train_time:53420ms step_avg:57.94ms
step:923/2330 train_time:53477ms step_avg:57.94ms
step:924/2330 train_time:53538ms step_avg:57.94ms
step:925/2330 train_time:53595ms step_avg:57.94ms
step:926/2330 train_time:53655ms step_avg:57.94ms
step:927/2330 train_time:53712ms step_avg:57.94ms
step:928/2330 train_time:53772ms step_avg:57.94ms
step:929/2330 train_time:53829ms step_avg:57.94ms
step:930/2330 train_time:53889ms step_avg:57.95ms
step:931/2330 train_time:53946ms step_avg:57.94ms
step:932/2330 train_time:54006ms step_avg:57.95ms
step:933/2330 train_time:54062ms step_avg:57.94ms
step:934/2330 train_time:54122ms step_avg:57.95ms
step:935/2330 train_time:54179ms step_avg:57.95ms
step:936/2330 train_time:54240ms step_avg:57.95ms
step:937/2330 train_time:54297ms step_avg:57.95ms
step:938/2330 train_time:54357ms step_avg:57.95ms
step:939/2330 train_time:54414ms step_avg:57.95ms
step:940/2330 train_time:54474ms step_avg:57.95ms
step:941/2330 train_time:54531ms step_avg:57.95ms
step:942/2330 train_time:54591ms step_avg:57.95ms
step:943/2330 train_time:54648ms step_avg:57.95ms
step:944/2330 train_time:54708ms step_avg:57.95ms
step:945/2330 train_time:54764ms step_avg:57.95ms
step:946/2330 train_time:54825ms step_avg:57.95ms
step:947/2330 train_time:54882ms step_avg:57.95ms
step:948/2330 train_time:54943ms step_avg:57.96ms
step:949/2330 train_time:55000ms step_avg:57.96ms
step:950/2330 train_time:55061ms step_avg:57.96ms
step:951/2330 train_time:55117ms step_avg:57.96ms
step:952/2330 train_time:55178ms step_avg:57.96ms
step:953/2330 train_time:55235ms step_avg:57.96ms
step:954/2330 train_time:55295ms step_avg:57.96ms
step:955/2330 train_time:55352ms step_avg:57.96ms
step:956/2330 train_time:55412ms step_avg:57.96ms
step:957/2330 train_time:55469ms step_avg:57.96ms
step:958/2330 train_time:55529ms step_avg:57.96ms
step:959/2330 train_time:55586ms step_avg:57.96ms
step:960/2330 train_time:55646ms step_avg:57.96ms
step:961/2330 train_time:55703ms step_avg:57.96ms
step:962/2330 train_time:55763ms step_avg:57.97ms
step:963/2330 train_time:55820ms step_avg:57.96ms
step:964/2330 train_time:55881ms step_avg:57.97ms
step:965/2330 train_time:55938ms step_avg:57.97ms
step:966/2330 train_time:55998ms step_avg:57.97ms
step:967/2330 train_time:56055ms step_avg:57.97ms
step:968/2330 train_time:56115ms step_avg:57.97ms
step:969/2330 train_time:56172ms step_avg:57.97ms
step:970/2330 train_time:56232ms step_avg:57.97ms
step:971/2330 train_time:56289ms step_avg:57.97ms
step:972/2330 train_time:56349ms step_avg:57.97ms
step:973/2330 train_time:56406ms step_avg:57.97ms
step:974/2330 train_time:56465ms step_avg:57.97ms
step:975/2330 train_time:56521ms step_avg:57.97ms
step:976/2330 train_time:56582ms step_avg:57.97ms
step:977/2330 train_time:56639ms step_avg:57.97ms
step:978/2330 train_time:56699ms step_avg:57.97ms
step:979/2330 train_time:56756ms step_avg:57.97ms
step:980/2330 train_time:56816ms step_avg:57.98ms
step:981/2330 train_time:56874ms step_avg:57.98ms
step:982/2330 train_time:56933ms step_avg:57.98ms
step:983/2330 train_time:56991ms step_avg:57.98ms
step:984/2330 train_time:57050ms step_avg:57.98ms
step:985/2330 train_time:57107ms step_avg:57.98ms
step:986/2330 train_time:57167ms step_avg:57.98ms
step:987/2330 train_time:57223ms step_avg:57.98ms
step:988/2330 train_time:57285ms step_avg:57.98ms
step:989/2330 train_time:57342ms step_avg:57.98ms
step:990/2330 train_time:57402ms step_avg:57.98ms
step:991/2330 train_time:57459ms step_avg:57.98ms
step:992/2330 train_time:57519ms step_avg:57.98ms
step:993/2330 train_time:57576ms step_avg:57.98ms
step:994/2330 train_time:57636ms step_avg:57.98ms
step:995/2330 train_time:57694ms step_avg:57.98ms
step:996/2330 train_time:57754ms step_avg:57.99ms
step:997/2330 train_time:57811ms step_avg:57.98ms
step:998/2330 train_time:57871ms step_avg:57.99ms
step:999/2330 train_time:57928ms step_avg:57.99ms
step:1000/2330 train_time:57988ms step_avg:57.99ms
step:1000/2330 val_loss:4.0818 train_time:58068ms step_avg:58.07ms
step:1001/2330 train_time:58088ms step_avg:58.03ms
step:1002/2330 train_time:58109ms step_avg:57.99ms
step:1003/2330 train_time:58162ms step_avg:57.99ms
step:1004/2330 train_time:58229ms step_avg:58.00ms
step:1005/2330 train_time:58284ms step_avg:57.99ms
step:1006/2330 train_time:58350ms step_avg:58.00ms
step:1007/2330 train_time:58407ms step_avg:58.00ms
step:1008/2330 train_time:58466ms step_avg:58.00ms
step:1009/2330 train_time:58523ms step_avg:58.00ms
step:1010/2330 train_time:58582ms step_avg:58.00ms
step:1011/2330 train_time:58639ms step_avg:58.00ms
step:1012/2330 train_time:58698ms step_avg:58.00ms
step:1013/2330 train_time:58754ms step_avg:58.00ms
step:1014/2330 train_time:58814ms step_avg:58.00ms
step:1015/2330 train_time:58870ms step_avg:58.00ms
step:1016/2330 train_time:58929ms step_avg:58.00ms
step:1017/2330 train_time:58986ms step_avg:58.00ms
step:1018/2330 train_time:59048ms step_avg:58.00ms
step:1019/2330 train_time:59106ms step_avg:58.00ms
step:1020/2330 train_time:59167ms step_avg:58.01ms
step:1021/2330 train_time:59225ms step_avg:58.01ms
step:1022/2330 train_time:59286ms step_avg:58.01ms
step:1023/2330 train_time:59343ms step_avg:58.01ms
step:1024/2330 train_time:59404ms step_avg:58.01ms
step:1025/2330 train_time:59460ms step_avg:58.01ms
step:1026/2330 train_time:59521ms step_avg:58.01ms
step:1027/2330 train_time:59577ms step_avg:58.01ms
step:1028/2330 train_time:59637ms step_avg:58.01ms
step:1029/2330 train_time:59693ms step_avg:58.01ms
step:1030/2330 train_time:59753ms step_avg:58.01ms
step:1031/2330 train_time:59809ms step_avg:58.01ms
step:1032/2330 train_time:59868ms step_avg:58.01ms
step:1033/2330 train_time:59925ms step_avg:58.01ms
step:1034/2330 train_time:59985ms step_avg:58.01ms
step:1035/2330 train_time:60042ms step_avg:58.01ms
step:1036/2330 train_time:60102ms step_avg:58.01ms
step:1037/2330 train_time:60159ms step_avg:58.01ms
step:1038/2330 train_time:60221ms step_avg:58.02ms
step:1039/2330 train_time:60279ms step_avg:58.02ms
step:1040/2330 train_time:60341ms step_avg:58.02ms
step:1041/2330 train_time:60398ms step_avg:58.02ms
step:1042/2330 train_time:60458ms step_avg:58.02ms
step:1043/2330 train_time:60515ms step_avg:58.02ms
step:1044/2330 train_time:60575ms step_avg:58.02ms
step:1045/2330 train_time:60632ms step_avg:58.02ms
step:1046/2330 train_time:60691ms step_avg:58.02ms
step:1047/2330 train_time:60748ms step_avg:58.02ms
step:1048/2330 train_time:60807ms step_avg:58.02ms
step:1049/2330 train_time:60864ms step_avg:58.02ms
step:1050/2330 train_time:60924ms step_avg:58.02ms
step:1051/2330 train_time:60981ms step_avg:58.02ms
step:1052/2330 train_time:61041ms step_avg:58.02ms
step:1053/2330 train_time:61098ms step_avg:58.02ms
step:1054/2330 train_time:61159ms step_avg:58.03ms
step:1055/2330 train_time:61217ms step_avg:58.03ms
step:1056/2330 train_time:61278ms step_avg:58.03ms
step:1057/2330 train_time:61335ms step_avg:58.03ms
step:1058/2330 train_time:61396ms step_avg:58.03ms
step:1059/2330 train_time:61453ms step_avg:58.03ms
step:1060/2330 train_time:61512ms step_avg:58.03ms
step:1061/2330 train_time:61569ms step_avg:58.03ms
step:1062/2330 train_time:61629ms step_avg:58.03ms
step:1063/2330 train_time:61685ms step_avg:58.03ms
step:1064/2330 train_time:61745ms step_avg:58.03ms
step:1065/2330 train_time:61801ms step_avg:58.03ms
step:1066/2330 train_time:61861ms step_avg:58.03ms
step:1067/2330 train_time:61918ms step_avg:58.03ms
step:1068/2330 train_time:61979ms step_avg:58.03ms
step:1069/2330 train_time:62036ms step_avg:58.03ms
step:1070/2330 train_time:62096ms step_avg:58.03ms
step:1071/2330 train_time:62153ms step_avg:58.03ms
step:1072/2330 train_time:62214ms step_avg:58.04ms
step:1073/2330 train_time:62271ms step_avg:58.03ms
step:1074/2330 train_time:62333ms step_avg:58.04ms
step:1075/2330 train_time:62391ms step_avg:58.04ms
step:1076/2330 train_time:62451ms step_avg:58.04ms
step:1077/2330 train_time:62509ms step_avg:58.04ms
step:1078/2330 train_time:62568ms step_avg:58.04ms
step:1079/2330 train_time:62625ms step_avg:58.04ms
step:1080/2330 train_time:62683ms step_avg:58.04ms
step:1081/2330 train_time:62740ms step_avg:58.04ms
step:1082/2330 train_time:62800ms step_avg:58.04ms
step:1083/2330 train_time:62857ms step_avg:58.04ms
step:1084/2330 train_time:62917ms step_avg:58.04ms
step:1085/2330 train_time:62975ms step_avg:58.04ms
step:1086/2330 train_time:63035ms step_avg:58.04ms
step:1087/2330 train_time:63092ms step_avg:58.04ms
step:1088/2330 train_time:63153ms step_avg:58.05ms
step:1089/2330 train_time:63211ms step_avg:58.04ms
step:1090/2330 train_time:63272ms step_avg:58.05ms
step:1091/2330 train_time:63329ms step_avg:58.05ms
step:1092/2330 train_time:63389ms step_avg:58.05ms
step:1093/2330 train_time:63447ms step_avg:58.05ms
step:1094/2330 train_time:63506ms step_avg:58.05ms
step:1095/2330 train_time:63563ms step_avg:58.05ms
step:1096/2330 train_time:63624ms step_avg:58.05ms
step:1097/2330 train_time:63681ms step_avg:58.05ms
step:1098/2330 train_time:63741ms step_avg:58.05ms
step:1099/2330 train_time:63798ms step_avg:58.05ms
step:1100/2330 train_time:63859ms step_avg:58.05ms
step:1101/2330 train_time:63915ms step_avg:58.05ms
step:1102/2330 train_time:63976ms step_avg:58.05ms
step:1103/2330 train_time:64032ms step_avg:58.05ms
step:1104/2330 train_time:64093ms step_avg:58.06ms
step:1105/2330 train_time:64150ms step_avg:58.05ms
step:1106/2330 train_time:64211ms step_avg:58.06ms
step:1107/2330 train_time:64269ms step_avg:58.06ms
step:1108/2330 train_time:64329ms step_avg:58.06ms
step:1109/2330 train_time:64387ms step_avg:58.06ms
step:1110/2330 train_time:64447ms step_avg:58.06ms
step:1111/2330 train_time:64506ms step_avg:58.06ms
step:1112/2330 train_time:64565ms step_avg:58.06ms
step:1113/2330 train_time:64623ms step_avg:58.06ms
step:1114/2330 train_time:64682ms step_avg:58.06ms
step:1115/2330 train_time:64740ms step_avg:58.06ms
step:1116/2330 train_time:64798ms step_avg:58.06ms
step:1117/2330 train_time:64855ms step_avg:58.06ms
step:1118/2330 train_time:64915ms step_avg:58.06ms
step:1119/2330 train_time:64972ms step_avg:58.06ms
step:1120/2330 train_time:65032ms step_avg:58.06ms
step:1121/2330 train_time:65090ms step_avg:58.06ms
step:1122/2330 train_time:65150ms step_avg:58.07ms
step:1123/2330 train_time:65206ms step_avg:58.06ms
step:1124/2330 train_time:65267ms step_avg:58.07ms
step:1125/2330 train_time:65324ms step_avg:58.07ms
step:1126/2330 train_time:65384ms step_avg:58.07ms
step:1127/2330 train_time:65441ms step_avg:58.07ms
step:1128/2330 train_time:65501ms step_avg:58.07ms
step:1129/2330 train_time:65558ms step_avg:58.07ms
step:1130/2330 train_time:65618ms step_avg:58.07ms
step:1131/2330 train_time:65675ms step_avg:58.07ms
step:1132/2330 train_time:65735ms step_avg:58.07ms
step:1133/2330 train_time:65792ms step_avg:58.07ms
step:1134/2330 train_time:65852ms step_avg:58.07ms
step:1135/2330 train_time:65909ms step_avg:58.07ms
step:1136/2330 train_time:65969ms step_avg:58.07ms
step:1137/2330 train_time:66026ms step_avg:58.07ms
step:1138/2330 train_time:66086ms step_avg:58.07ms
step:1139/2330 train_time:66143ms step_avg:58.07ms
step:1140/2330 train_time:66204ms step_avg:58.07ms
step:1141/2330 train_time:66260ms step_avg:58.07ms
step:1142/2330 train_time:66321ms step_avg:58.07ms
step:1143/2330 train_time:66379ms step_avg:58.07ms
step:1144/2330 train_time:66440ms step_avg:58.08ms
step:1145/2330 train_time:66496ms step_avg:58.08ms
step:1146/2330 train_time:66556ms step_avg:58.08ms
step:1147/2330 train_time:66614ms step_avg:58.08ms
step:1148/2330 train_time:66674ms step_avg:58.08ms
step:1149/2330 train_time:66731ms step_avg:58.08ms
step:1150/2330 train_time:66792ms step_avg:58.08ms
step:1151/2330 train_time:66849ms step_avg:58.08ms
step:1152/2330 train_time:66910ms step_avg:58.08ms
step:1153/2330 train_time:66966ms step_avg:58.08ms
step:1154/2330 train_time:67027ms step_avg:58.08ms
step:1155/2330 train_time:67084ms step_avg:58.08ms
step:1156/2330 train_time:67144ms step_avg:58.08ms
step:1157/2330 train_time:67201ms step_avg:58.08ms
step:1158/2330 train_time:67261ms step_avg:58.08ms
step:1159/2330 train_time:67318ms step_avg:58.08ms
step:1160/2330 train_time:67378ms step_avg:58.08ms
step:1161/2330 train_time:67435ms step_avg:58.08ms
step:1162/2330 train_time:67497ms step_avg:58.09ms
step:1163/2330 train_time:67553ms step_avg:58.09ms
step:1164/2330 train_time:67614ms step_avg:58.09ms
step:1165/2330 train_time:67671ms step_avg:58.09ms
step:1166/2330 train_time:67732ms step_avg:58.09ms
step:1167/2330 train_time:67789ms step_avg:58.09ms
step:1168/2330 train_time:67849ms step_avg:58.09ms
step:1169/2330 train_time:67907ms step_avg:58.09ms
step:1170/2330 train_time:67966ms step_avg:58.09ms
step:1171/2330 train_time:68023ms step_avg:58.09ms
step:1172/2330 train_time:68083ms step_avg:58.09ms
step:1173/2330 train_time:68140ms step_avg:58.09ms
step:1174/2330 train_time:68200ms step_avg:58.09ms
step:1175/2330 train_time:68257ms step_avg:58.09ms
step:1176/2330 train_time:68317ms step_avg:58.09ms
step:1177/2330 train_time:68374ms step_avg:58.09ms
step:1178/2330 train_time:68435ms step_avg:58.09ms
step:1179/2330 train_time:68492ms step_avg:58.09ms
step:1180/2330 train_time:68552ms step_avg:58.09ms
step:1181/2330 train_time:68610ms step_avg:58.09ms
step:1182/2330 train_time:68669ms step_avg:58.10ms
step:1183/2330 train_time:68726ms step_avg:58.09ms
step:1184/2330 train_time:68786ms step_avg:58.10ms
step:1185/2330 train_time:68843ms step_avg:58.10ms
step:1186/2330 train_time:68903ms step_avg:58.10ms
step:1187/2330 train_time:68960ms step_avg:58.10ms
step:1188/2330 train_time:69020ms step_avg:58.10ms
step:1189/2330 train_time:69077ms step_avg:58.10ms
step:1190/2330 train_time:69138ms step_avg:58.10ms
step:1191/2330 train_time:69194ms step_avg:58.10ms
step:1192/2330 train_time:69254ms step_avg:58.10ms
step:1193/2330 train_time:69311ms step_avg:58.10ms
step:1194/2330 train_time:69371ms step_avg:58.10ms
step:1195/2330 train_time:69428ms step_avg:58.10ms
step:1196/2330 train_time:69488ms step_avg:58.10ms
step:1197/2330 train_time:69544ms step_avg:58.10ms
step:1198/2330 train_time:69604ms step_avg:58.10ms
step:1199/2330 train_time:69660ms step_avg:58.10ms
step:1200/2330 train_time:69722ms step_avg:58.10ms
step:1201/2330 train_time:69779ms step_avg:58.10ms
step:1202/2330 train_time:69841ms step_avg:58.10ms
step:1203/2330 train_time:69897ms step_avg:58.10ms
step:1204/2330 train_time:69958ms step_avg:58.10ms
step:1205/2330 train_time:70015ms step_avg:58.10ms
step:1206/2330 train_time:70075ms step_avg:58.11ms
step:1207/2330 train_time:70132ms step_avg:58.10ms
step:1208/2330 train_time:70193ms step_avg:58.11ms
step:1209/2330 train_time:70250ms step_avg:58.11ms
step:1210/2330 train_time:70310ms step_avg:58.11ms
step:1211/2330 train_time:70367ms step_avg:58.11ms
step:1212/2330 train_time:70427ms step_avg:58.11ms
step:1213/2330 train_time:70483ms step_avg:58.11ms
step:1214/2330 train_time:70544ms step_avg:58.11ms
step:1215/2330 train_time:70601ms step_avg:58.11ms
step:1216/2330 train_time:70660ms step_avg:58.11ms
step:1217/2330 train_time:70717ms step_avg:58.11ms
step:1218/2330 train_time:70778ms step_avg:58.11ms
step:1219/2330 train_time:70835ms step_avg:58.11ms
step:1220/2330 train_time:70895ms step_avg:58.11ms
step:1221/2330 train_time:70952ms step_avg:58.11ms
step:1222/2330 train_time:71013ms step_avg:58.11ms
step:1223/2330 train_time:71070ms step_avg:58.11ms
step:1224/2330 train_time:71131ms step_avg:58.11ms
step:1225/2330 train_time:71188ms step_avg:58.11ms
step:1226/2330 train_time:71247ms step_avg:58.11ms
step:1227/2330 train_time:71305ms step_avg:58.11ms
step:1228/2330 train_time:71364ms step_avg:58.11ms
step:1229/2330 train_time:71421ms step_avg:58.11ms
step:1230/2330 train_time:71482ms step_avg:58.12ms
step:1231/2330 train_time:71539ms step_avg:58.11ms
step:1232/2330 train_time:71599ms step_avg:58.12ms
step:1233/2330 train_time:71655ms step_avg:58.11ms
step:1234/2330 train_time:71717ms step_avg:58.12ms
step:1235/2330 train_time:71774ms step_avg:58.12ms
step:1236/2330 train_time:71835ms step_avg:58.12ms
step:1237/2330 train_time:71892ms step_avg:58.12ms
step:1238/2330 train_time:71953ms step_avg:58.12ms
step:1239/2330 train_time:72011ms step_avg:58.12ms
step:1240/2330 train_time:72071ms step_avg:58.12ms
step:1241/2330 train_time:72128ms step_avg:58.12ms
step:1242/2330 train_time:72187ms step_avg:58.12ms
step:1243/2330 train_time:72245ms step_avg:58.12ms
step:1244/2330 train_time:72304ms step_avg:58.12ms
step:1245/2330 train_time:72361ms step_avg:58.12ms
step:1246/2330 train_time:72421ms step_avg:58.12ms
step:1247/2330 train_time:72479ms step_avg:58.12ms
step:1248/2330 train_time:72539ms step_avg:58.12ms
step:1249/2330 train_time:72595ms step_avg:58.12ms
step:1250/2330 train_time:72656ms step_avg:58.12ms
step:1250/2330 val_loss:4.0045 train_time:72736ms step_avg:58.19ms
step:1251/2330 train_time:72756ms step_avg:58.16ms
step:1252/2330 train_time:72776ms step_avg:58.13ms
step:1253/2330 train_time:72834ms step_avg:58.13ms
step:1254/2330 train_time:72899ms step_avg:58.13ms
step:1255/2330 train_time:72957ms step_avg:58.13ms
step:1256/2330 train_time:73018ms step_avg:58.13ms
step:1257/2330 train_time:73075ms step_avg:58.13ms
step:1258/2330 train_time:73134ms step_avg:58.14ms
step:1259/2330 train_time:73191ms step_avg:58.13ms
step:1260/2330 train_time:73251ms step_avg:58.14ms
step:1261/2330 train_time:73308ms step_avg:58.13ms
step:1262/2330 train_time:73368ms step_avg:58.14ms
step:1263/2330 train_time:73425ms step_avg:58.14ms
step:1264/2330 train_time:73484ms step_avg:58.14ms
step:1265/2330 train_time:73541ms step_avg:58.14ms
step:1266/2330 train_time:73600ms step_avg:58.14ms
step:1267/2330 train_time:73656ms step_avg:58.13ms
step:1268/2330 train_time:73716ms step_avg:58.14ms
step:1269/2330 train_time:73775ms step_avg:58.14ms
step:1270/2330 train_time:73835ms step_avg:58.14ms
step:1271/2330 train_time:73892ms step_avg:58.14ms
step:1272/2330 train_time:73955ms step_avg:58.14ms
step:1273/2330 train_time:74012ms step_avg:58.14ms
step:1274/2330 train_time:74072ms step_avg:58.14ms
step:1275/2330 train_time:74129ms step_avg:58.14ms
step:1276/2330 train_time:74190ms step_avg:58.14ms
step:1277/2330 train_time:74246ms step_avg:58.14ms
step:1278/2330 train_time:74308ms step_avg:58.14ms
step:1279/2330 train_time:74364ms step_avg:58.14ms
step:1280/2330 train_time:74424ms step_avg:58.14ms
step:1281/2330 train_time:74480ms step_avg:58.14ms
step:1282/2330 train_time:74541ms step_avg:58.14ms
step:1283/2330 train_time:74597ms step_avg:58.14ms
step:1284/2330 train_time:74657ms step_avg:58.14ms
step:1285/2330 train_time:74714ms step_avg:58.14ms
step:1286/2330 train_time:74774ms step_avg:58.14ms
step:1287/2330 train_time:74831ms step_avg:58.14ms
step:1288/2330 train_time:74892ms step_avg:58.15ms
step:1289/2330 train_time:74950ms step_avg:58.15ms
step:1290/2330 train_time:75010ms step_avg:58.15ms
step:1291/2330 train_time:75068ms step_avg:58.15ms
step:1292/2330 train_time:75128ms step_avg:58.15ms
step:1293/2330 train_time:75185ms step_avg:58.15ms
step:1294/2330 train_time:75245ms step_avg:58.15ms
step:1295/2330 train_time:75302ms step_avg:58.15ms
step:1296/2330 train_time:75361ms step_avg:58.15ms
step:1297/2330 train_time:75418ms step_avg:58.15ms
step:1298/2330 train_time:75477ms step_avg:58.15ms
step:1299/2330 train_time:75533ms step_avg:58.15ms
step:1300/2330 train_time:75594ms step_avg:58.15ms
step:1301/2330 train_time:75650ms step_avg:58.15ms
step:1302/2330 train_time:75710ms step_avg:58.15ms
step:1303/2330 train_time:75767ms step_avg:58.15ms
step:1304/2330 train_time:75829ms step_avg:58.15ms
step:1305/2330 train_time:75886ms step_avg:58.15ms
step:1306/2330 train_time:75947ms step_avg:58.15ms
step:1307/2330 train_time:76004ms step_avg:58.15ms
step:1308/2330 train_time:76065ms step_avg:58.15ms
step:1309/2330 train_time:76123ms step_avg:58.15ms
step:1310/2330 train_time:76183ms step_avg:58.15ms
step:1311/2330 train_time:76240ms step_avg:58.15ms
step:1312/2330 train_time:76299ms step_avg:58.16ms
step:1313/2330 train_time:76356ms step_avg:58.15ms
step:1314/2330 train_time:76415ms step_avg:58.15ms
step:1315/2330 train_time:76473ms step_avg:58.15ms
step:1316/2330 train_time:76532ms step_avg:58.15ms
step:1317/2330 train_time:76588ms step_avg:58.15ms
step:1318/2330 train_time:76649ms step_avg:58.16ms
step:1319/2330 train_time:76706ms step_avg:58.15ms
step:1320/2330 train_time:76766ms step_avg:58.16ms
step:1321/2330 train_time:76824ms step_avg:58.16ms
step:1322/2330 train_time:76884ms step_avg:58.16ms
step:1323/2330 train_time:76942ms step_avg:58.16ms
step:1324/2330 train_time:77001ms step_avg:58.16ms
step:1325/2330 train_time:77059ms step_avg:58.16ms
step:1326/2330 train_time:77119ms step_avg:58.16ms
step:1327/2330 train_time:77176ms step_avg:58.16ms
step:1328/2330 train_time:77236ms step_avg:58.16ms
step:1329/2330 train_time:77293ms step_avg:58.16ms
step:1330/2330 train_time:77352ms step_avg:58.16ms
step:1331/2330 train_time:77410ms step_avg:58.16ms
step:1332/2330 train_time:77470ms step_avg:58.16ms
step:1333/2330 train_time:77527ms step_avg:58.16ms
step:1334/2330 train_time:77587ms step_avg:58.16ms
step:1335/2330 train_time:77644ms step_avg:58.16ms
step:1336/2330 train_time:77703ms step_avg:58.16ms
step:1337/2330 train_time:77761ms step_avg:58.16ms
step:1338/2330 train_time:77821ms step_avg:58.16ms
step:1339/2330 train_time:77879ms step_avg:58.16ms
step:1340/2330 train_time:77938ms step_avg:58.16ms
step:1341/2330 train_time:77995ms step_avg:58.16ms
step:1342/2330 train_time:78055ms step_avg:58.16ms
step:1343/2330 train_time:78112ms step_avg:58.16ms
step:1344/2330 train_time:78173ms step_avg:58.16ms
step:1345/2330 train_time:78230ms step_avg:58.16ms
step:1346/2330 train_time:78290ms step_avg:58.17ms
step:1347/2330 train_time:78347ms step_avg:58.16ms
step:1348/2330 train_time:78407ms step_avg:58.17ms
step:1349/2330 train_time:78464ms step_avg:58.16ms
step:1350/2330 train_time:78524ms step_avg:58.17ms
step:1351/2330 train_time:78581ms step_avg:58.16ms
step:1352/2330 train_time:78640ms step_avg:58.17ms
step:1353/2330 train_time:78697ms step_avg:58.16ms
step:1354/2330 train_time:78756ms step_avg:58.17ms
step:1355/2330 train_time:78814ms step_avg:58.17ms
step:1356/2330 train_time:78874ms step_avg:58.17ms
step:1357/2330 train_time:78931ms step_avg:58.17ms
step:1358/2330 train_time:78991ms step_avg:58.17ms
step:1359/2330 train_time:79048ms step_avg:58.17ms
step:1360/2330 train_time:79109ms step_avg:58.17ms
step:1361/2330 train_time:79166ms step_avg:58.17ms
step:1362/2330 train_time:79227ms step_avg:58.17ms
step:1363/2330 train_time:79283ms step_avg:58.17ms
step:1364/2330 train_time:79343ms step_avg:58.17ms
step:1365/2330 train_time:79400ms step_avg:58.17ms
step:1366/2330 train_time:79460ms step_avg:58.17ms
step:1367/2330 train_time:79516ms step_avg:58.17ms
step:1368/2330 train_time:79576ms step_avg:58.17ms
step:1369/2330 train_time:79633ms step_avg:58.17ms
step:1370/2330 train_time:79692ms step_avg:58.17ms
step:1371/2330 train_time:79750ms step_avg:58.17ms
step:1372/2330 train_time:79810ms step_avg:58.17ms
step:1373/2330 train_time:79867ms step_avg:58.17ms
step:1374/2330 train_time:79928ms step_avg:58.17ms
step:1375/2330 train_time:79986ms step_avg:58.17ms
step:1376/2330 train_time:80045ms step_avg:58.17ms
step:1377/2330 train_time:80103ms step_avg:58.17ms
step:1378/2330 train_time:80162ms step_avg:58.17ms
step:1379/2330 train_time:80219ms step_avg:58.17ms
step:1380/2330 train_time:80279ms step_avg:58.17ms
step:1381/2330 train_time:80336ms step_avg:58.17ms
step:1382/2330 train_time:80395ms step_avg:58.17ms
step:1383/2330 train_time:80452ms step_avg:58.17ms
step:1384/2330 train_time:80512ms step_avg:58.17ms
step:1385/2330 train_time:80570ms step_avg:58.17ms
step:1386/2330 train_time:80629ms step_avg:58.17ms
step:1387/2330 train_time:80686ms step_avg:58.17ms
step:1388/2330 train_time:80747ms step_avg:58.18ms
step:1389/2330 train_time:80805ms step_avg:58.17ms
step:1390/2330 train_time:80865ms step_avg:58.18ms
step:1391/2330 train_time:80922ms step_avg:58.18ms
step:1392/2330 train_time:80983ms step_avg:58.18ms
step:1393/2330 train_time:81040ms step_avg:58.18ms
step:1394/2330 train_time:81100ms step_avg:58.18ms
step:1395/2330 train_time:81157ms step_avg:58.18ms
step:1396/2330 train_time:81216ms step_avg:58.18ms
step:1397/2330 train_time:81274ms step_avg:58.18ms
step:1398/2330 train_time:81333ms step_avg:58.18ms
step:1399/2330 train_time:81390ms step_avg:58.18ms
step:1400/2330 train_time:81450ms step_avg:58.18ms
step:1401/2330 train_time:81507ms step_avg:58.18ms
step:1402/2330 train_time:81568ms step_avg:58.18ms
step:1403/2330 train_time:81625ms step_avg:58.18ms
step:1404/2330 train_time:81685ms step_avg:58.18ms
step:1405/2330 train_time:81742ms step_avg:58.18ms
step:1406/2330 train_time:81802ms step_avg:58.18ms
step:1407/2330 train_time:81859ms step_avg:58.18ms
step:1408/2330 train_time:81919ms step_avg:58.18ms
step:1409/2330 train_time:81976ms step_avg:58.18ms
step:1410/2330 train_time:82037ms step_avg:58.18ms
step:1411/2330 train_time:82094ms step_avg:58.18ms
step:1412/2330 train_time:82153ms step_avg:58.18ms
step:1413/2330 train_time:82210ms step_avg:58.18ms
step:1414/2330 train_time:82271ms step_avg:58.18ms
step:1415/2330 train_time:82327ms step_avg:58.18ms
step:1416/2330 train_time:82387ms step_avg:58.18ms
step:1417/2330 train_time:82444ms step_avg:58.18ms
step:1418/2330 train_time:82504ms step_avg:58.18ms
step:1419/2330 train_time:82561ms step_avg:58.18ms
step:1420/2330 train_time:82621ms step_avg:58.18ms
step:1421/2330 train_time:82679ms step_avg:58.18ms
step:1422/2330 train_time:82739ms step_avg:58.18ms
step:1423/2330 train_time:82796ms step_avg:58.18ms
step:1424/2330 train_time:82855ms step_avg:58.18ms
step:1425/2330 train_time:82912ms step_avg:58.18ms
step:1426/2330 train_time:82973ms step_avg:58.19ms
step:1427/2330 train_time:83030ms step_avg:58.19ms
step:1428/2330 train_time:83091ms step_avg:58.19ms
step:1429/2330 train_time:83147ms step_avg:58.19ms
step:1430/2330 train_time:83208ms step_avg:58.19ms
step:1431/2330 train_time:83265ms step_avg:58.19ms
step:1432/2330 train_time:83325ms step_avg:58.19ms
step:1433/2330 train_time:83383ms step_avg:58.19ms
step:1434/2330 train_time:83443ms step_avg:58.19ms
step:1435/2330 train_time:83500ms step_avg:58.19ms
step:1436/2330 train_time:83560ms step_avg:58.19ms
step:1437/2330 train_time:83617ms step_avg:58.19ms
step:1438/2330 train_time:83676ms step_avg:58.19ms
step:1439/2330 train_time:83733ms step_avg:58.19ms
step:1440/2330 train_time:83793ms step_avg:58.19ms
step:1441/2330 train_time:83850ms step_avg:58.19ms
step:1442/2330 train_time:83910ms step_avg:58.19ms
step:1443/2330 train_time:83967ms step_avg:58.19ms
step:1444/2330 train_time:84028ms step_avg:58.19ms
step:1445/2330 train_time:84085ms step_avg:58.19ms
step:1446/2330 train_time:84145ms step_avg:58.19ms
step:1447/2330 train_time:84203ms step_avg:58.19ms
step:1448/2330 train_time:84262ms step_avg:58.19ms
step:1449/2330 train_time:84320ms step_avg:58.19ms
step:1450/2330 train_time:84380ms step_avg:58.19ms
step:1451/2330 train_time:84437ms step_avg:58.19ms
step:1452/2330 train_time:84496ms step_avg:58.19ms
step:1453/2330 train_time:84553ms step_avg:58.19ms
step:1454/2330 train_time:84614ms step_avg:58.19ms
step:1455/2330 train_time:84671ms step_avg:58.19ms
step:1456/2330 train_time:84732ms step_avg:58.19ms
step:1457/2330 train_time:84788ms step_avg:58.19ms
step:1458/2330 train_time:84849ms step_avg:58.20ms
step:1459/2330 train_time:84905ms step_avg:58.19ms
step:1460/2330 train_time:84966ms step_avg:58.20ms
step:1461/2330 train_time:85024ms step_avg:58.20ms
step:1462/2330 train_time:85083ms step_avg:58.20ms
step:1463/2330 train_time:85141ms step_avg:58.20ms
step:1464/2330 train_time:85201ms step_avg:58.20ms
step:1465/2330 train_time:85259ms step_avg:58.20ms
step:1466/2330 train_time:85318ms step_avg:58.20ms
step:1467/2330 train_time:85376ms step_avg:58.20ms
step:1468/2330 train_time:85436ms step_avg:58.20ms
step:1469/2330 train_time:85493ms step_avg:58.20ms
step:1470/2330 train_time:85553ms step_avg:58.20ms
step:1471/2330 train_time:85611ms step_avg:58.20ms
step:1472/2330 train_time:85671ms step_avg:58.20ms
step:1473/2330 train_time:85728ms step_avg:58.20ms
step:1474/2330 train_time:85789ms step_avg:58.20ms
step:1475/2330 train_time:85845ms step_avg:58.20ms
step:1476/2330 train_time:85905ms step_avg:58.20ms
step:1477/2330 train_time:85963ms step_avg:58.20ms
step:1478/2330 train_time:86023ms step_avg:58.20ms
step:1479/2330 train_time:86080ms step_avg:58.20ms
step:1480/2330 train_time:86139ms step_avg:58.20ms
step:1481/2330 train_time:86196ms step_avg:58.20ms
step:1482/2330 train_time:86256ms step_avg:58.20ms
step:1483/2330 train_time:86314ms step_avg:58.20ms
step:1484/2330 train_time:86374ms step_avg:58.20ms
step:1485/2330 train_time:86431ms step_avg:58.20ms
step:1486/2330 train_time:86491ms step_avg:58.20ms
step:1487/2330 train_time:86548ms step_avg:58.20ms
step:1488/2330 train_time:86609ms step_avg:58.20ms
step:1489/2330 train_time:86666ms step_avg:58.20ms
step:1490/2330 train_time:86725ms step_avg:58.20ms
step:1491/2330 train_time:86782ms step_avg:58.20ms
step:1492/2330 train_time:86843ms step_avg:58.21ms
step:1493/2330 train_time:86900ms step_avg:58.20ms
step:1494/2330 train_time:86960ms step_avg:58.21ms
step:1495/2330 train_time:87018ms step_avg:58.21ms
step:1496/2330 train_time:87078ms step_avg:58.21ms
step:1497/2330 train_time:87136ms step_avg:58.21ms
step:1498/2330 train_time:87195ms step_avg:58.21ms
step:1499/2330 train_time:87253ms step_avg:58.21ms
step:1500/2330 train_time:87313ms step_avg:58.21ms
step:1500/2330 val_loss:3.9132 train_time:87393ms step_avg:58.26ms
step:1501/2330 train_time:87411ms step_avg:58.24ms
step:1502/2330 train_time:87432ms step_avg:58.21ms
step:1503/2330 train_time:87490ms step_avg:58.21ms
step:1504/2330 train_time:87557ms step_avg:58.22ms
step:1505/2330 train_time:87615ms step_avg:58.22ms
step:1506/2330 train_time:87675ms step_avg:58.22ms
step:1507/2330 train_time:87733ms step_avg:58.22ms
step:1508/2330 train_time:87792ms step_avg:58.22ms
step:1509/2330 train_time:87849ms step_avg:58.22ms
step:1510/2330 train_time:87909ms step_avg:58.22ms
step:1511/2330 train_time:87965ms step_avg:58.22ms
step:1512/2330 train_time:88024ms step_avg:58.22ms
step:1513/2330 train_time:88081ms step_avg:58.22ms
step:1514/2330 train_time:88140ms step_avg:58.22ms
step:1515/2330 train_time:88196ms step_avg:58.22ms
step:1516/2330 train_time:88256ms step_avg:58.22ms
step:1517/2330 train_time:88312ms step_avg:58.22ms
step:1518/2330 train_time:88373ms step_avg:58.22ms
step:1519/2330 train_time:88432ms step_avg:58.22ms
step:1520/2330 train_time:88495ms step_avg:58.22ms
step:1521/2330 train_time:88553ms step_avg:58.22ms
step:1522/2330 train_time:88614ms step_avg:58.22ms
step:1523/2330 train_time:88672ms step_avg:58.22ms
step:1524/2330 train_time:88732ms step_avg:58.22ms
step:1525/2330 train_time:88790ms step_avg:58.22ms
step:1526/2330 train_time:88850ms step_avg:58.22ms
step:1527/2330 train_time:88907ms step_avg:58.22ms
step:1528/2330 train_time:88966ms step_avg:58.22ms
step:1529/2330 train_time:89025ms step_avg:58.22ms
step:1530/2330 train_time:89083ms step_avg:58.22ms
step:1531/2330 train_time:89140ms step_avg:58.22ms
step:1532/2330 train_time:89199ms step_avg:58.22ms
step:1533/2330 train_time:89256ms step_avg:58.22ms
step:1534/2330 train_time:89316ms step_avg:58.22ms
step:1535/2330 train_time:89374ms step_avg:58.22ms
step:1536/2330 train_time:89435ms step_avg:58.23ms
step:1537/2330 train_time:89494ms step_avg:58.23ms
step:1538/2330 train_time:89555ms step_avg:58.23ms
step:1539/2330 train_time:89613ms step_avg:58.23ms
step:1540/2330 train_time:89674ms step_avg:58.23ms
step:1541/2330 train_time:89733ms step_avg:58.23ms
step:1542/2330 train_time:89792ms step_avg:58.23ms
step:1543/2330 train_time:89849ms step_avg:58.23ms
step:1544/2330 train_time:89910ms step_avg:58.23ms
step:1545/2330 train_time:89968ms step_avg:58.23ms
step:1546/2330 train_time:90029ms step_avg:58.23ms
step:1547/2330 train_time:90086ms step_avg:58.23ms
step:1548/2330 train_time:90145ms step_avg:58.23ms
step:1549/2330 train_time:90202ms step_avg:58.23ms
step:1550/2330 train_time:90262ms step_avg:58.23ms
step:1551/2330 train_time:90319ms step_avg:58.23ms
step:1552/2330 train_time:90380ms step_avg:58.23ms
step:1553/2330 train_time:90438ms step_avg:58.23ms
step:1554/2330 train_time:90500ms step_avg:58.24ms
step:1555/2330 train_time:90557ms step_avg:58.24ms
step:1556/2330 train_time:90621ms step_avg:58.24ms
step:1557/2330 train_time:90678ms step_avg:58.24ms
step:1558/2330 train_time:90741ms step_avg:58.24ms
step:1559/2330 train_time:90798ms step_avg:58.24ms
step:1560/2330 train_time:90860ms step_avg:58.24ms
step:1561/2330 train_time:90917ms step_avg:58.24ms
step:1562/2330 train_time:90979ms step_avg:58.25ms
step:1563/2330 train_time:91036ms step_avg:58.24ms
step:1564/2330 train_time:91097ms step_avg:58.25ms
step:1565/2330 train_time:91153ms step_avg:58.25ms
step:1566/2330 train_time:91215ms step_avg:58.25ms
step:1567/2330 train_time:91272ms step_avg:58.25ms
step:1568/2330 train_time:91332ms step_avg:58.25ms
step:1569/2330 train_time:91389ms step_avg:58.25ms
step:1570/2330 train_time:91451ms step_avg:58.25ms
step:1571/2330 train_time:91509ms step_avg:58.25ms
step:1572/2330 train_time:91569ms step_avg:58.25ms
step:1573/2330 train_time:91629ms step_avg:58.25ms
step:1574/2330 train_time:91690ms step_avg:58.25ms
step:1575/2330 train_time:91749ms step_avg:58.25ms
step:1576/2330 train_time:91808ms step_avg:58.25ms
step:1577/2330 train_time:91867ms step_avg:58.25ms
step:1578/2330 train_time:91927ms step_avg:58.26ms
step:1579/2330 train_time:91985ms step_avg:58.26ms
step:1580/2330 train_time:92044ms step_avg:58.26ms
step:1581/2330 train_time:92101ms step_avg:58.26ms
step:1582/2330 train_time:92162ms step_avg:58.26ms
step:1583/2330 train_time:92219ms step_avg:58.26ms
step:1584/2330 train_time:92281ms step_avg:58.26ms
step:1585/2330 train_time:92337ms step_avg:58.26ms
step:1586/2330 train_time:92399ms step_avg:58.26ms
step:1587/2330 train_time:92455ms step_avg:58.26ms
step:1588/2330 train_time:92518ms step_avg:58.26ms
step:1589/2330 train_time:92575ms step_avg:58.26ms
step:1590/2330 train_time:92638ms step_avg:58.26ms
step:1591/2330 train_time:92696ms step_avg:58.26ms
step:1592/2330 train_time:92757ms step_avg:58.26ms
step:1593/2330 train_time:92814ms step_avg:58.26ms
step:1594/2330 train_time:92875ms step_avg:58.27ms
step:1595/2330 train_time:92933ms step_avg:58.27ms
step:1596/2330 train_time:92994ms step_avg:58.27ms
step:1597/2330 train_time:93052ms step_avg:58.27ms
step:1598/2330 train_time:93112ms step_avg:58.27ms
step:1599/2330 train_time:93170ms step_avg:58.27ms
step:1600/2330 train_time:93230ms step_avg:58.27ms
step:1601/2330 train_time:93288ms step_avg:58.27ms
step:1602/2330 train_time:93348ms step_avg:58.27ms
step:1603/2330 train_time:93407ms step_avg:58.27ms
step:1604/2330 train_time:93468ms step_avg:58.27ms
step:1605/2330 train_time:93526ms step_avg:58.27ms
step:1606/2330 train_time:93587ms step_avg:58.27ms
step:1607/2330 train_time:93645ms step_avg:58.27ms
step:1608/2330 train_time:93706ms step_avg:58.27ms
step:1609/2330 train_time:93763ms step_avg:58.27ms
step:1610/2330 train_time:93824ms step_avg:58.28ms
step:1611/2330 train_time:93881ms step_avg:58.28ms
step:1612/2330 train_time:93944ms step_avg:58.28ms
step:1613/2330 train_time:94001ms step_avg:58.28ms
step:1614/2330 train_time:94063ms step_avg:58.28ms
step:1615/2330 train_time:94120ms step_avg:58.28ms
step:1616/2330 train_time:94181ms step_avg:58.28ms
step:1617/2330 train_time:94238ms step_avg:58.28ms
step:1618/2330 train_time:94299ms step_avg:58.28ms
step:1619/2330 train_time:94356ms step_avg:58.28ms
step:1620/2330 train_time:94417ms step_avg:58.28ms
step:1621/2330 train_time:94474ms step_avg:58.28ms
step:1622/2330 train_time:94536ms step_avg:58.28ms
step:1623/2330 train_time:94593ms step_avg:58.28ms
step:1624/2330 train_time:94655ms step_avg:58.29ms
step:1625/2330 train_time:94713ms step_avg:58.29ms
step:1626/2330 train_time:94774ms step_avg:58.29ms
step:1627/2330 train_time:94832ms step_avg:58.29ms
step:1628/2330 train_time:94894ms step_avg:58.29ms
step:1629/2330 train_time:94952ms step_avg:58.29ms
step:1630/2330 train_time:95013ms step_avg:58.29ms
step:1631/2330 train_time:95071ms step_avg:58.29ms
step:1632/2330 train_time:95131ms step_avg:58.29ms
step:1633/2330 train_time:95189ms step_avg:58.29ms
step:1634/2330 train_time:95249ms step_avg:58.29ms
step:1635/2330 train_time:95306ms step_avg:58.29ms
step:1636/2330 train_time:95366ms step_avg:58.29ms
step:1637/2330 train_time:95424ms step_avg:58.29ms
step:1638/2330 train_time:95485ms step_avg:58.29ms
step:1639/2330 train_time:95542ms step_avg:58.29ms
step:1640/2330 train_time:95603ms step_avg:58.29ms
step:1641/2330 train_time:95661ms step_avg:58.29ms
step:1642/2330 train_time:95721ms step_avg:58.30ms
step:1643/2330 train_time:95778ms step_avg:58.29ms
step:1644/2330 train_time:95840ms step_avg:58.30ms
step:1645/2330 train_time:95898ms step_avg:58.30ms
step:1646/2330 train_time:95959ms step_avg:58.30ms
step:1647/2330 train_time:96016ms step_avg:58.30ms
step:1648/2330 train_time:96077ms step_avg:58.30ms
step:1649/2330 train_time:96135ms step_avg:58.30ms
step:1650/2330 train_time:96196ms step_avg:58.30ms
step:1651/2330 train_time:96253ms step_avg:58.30ms
step:1652/2330 train_time:96313ms step_avg:58.30ms
step:1653/2330 train_time:96372ms step_avg:58.30ms
step:1654/2330 train_time:96432ms step_avg:58.30ms
step:1655/2330 train_time:96490ms step_avg:58.30ms
step:1656/2330 train_time:96550ms step_avg:58.30ms
step:1657/2330 train_time:96608ms step_avg:58.30ms
step:1658/2330 train_time:96669ms step_avg:58.30ms
step:1659/2330 train_time:96727ms step_avg:58.30ms
step:1660/2330 train_time:96787ms step_avg:58.31ms
step:1661/2330 train_time:96847ms step_avg:58.31ms
step:1662/2330 train_time:96907ms step_avg:58.31ms
step:1663/2330 train_time:96966ms step_avg:58.31ms
step:1664/2330 train_time:97025ms step_avg:58.31ms
step:1665/2330 train_time:97083ms step_avg:58.31ms
step:1666/2330 train_time:97144ms step_avg:58.31ms
step:1667/2330 train_time:97201ms step_avg:58.31ms
step:1668/2330 train_time:97262ms step_avg:58.31ms
step:1669/2330 train_time:97319ms step_avg:58.31ms
step:1670/2330 train_time:97380ms step_avg:58.31ms
step:1671/2330 train_time:97437ms step_avg:58.31ms
step:1672/2330 train_time:97499ms step_avg:58.31ms
step:1673/2330 train_time:97556ms step_avg:58.31ms
step:1674/2330 train_time:97618ms step_avg:58.31ms
step:1675/2330 train_time:97675ms step_avg:58.31ms
step:1676/2330 train_time:97736ms step_avg:58.32ms
step:1677/2330 train_time:97793ms step_avg:58.31ms
step:1678/2330 train_time:97855ms step_avg:58.32ms
step:1679/2330 train_time:97912ms step_avg:58.32ms
step:1680/2330 train_time:97975ms step_avg:58.32ms
step:1681/2330 train_time:98033ms step_avg:58.32ms
step:1682/2330 train_time:98093ms step_avg:58.32ms
step:1683/2330 train_time:98151ms step_avg:58.32ms
step:1684/2330 train_time:98212ms step_avg:58.32ms
step:1685/2330 train_time:98269ms step_avg:58.32ms
step:1686/2330 train_time:98329ms step_avg:58.32ms
step:1687/2330 train_time:98388ms step_avg:58.32ms
step:1688/2330 train_time:98449ms step_avg:58.32ms
step:1689/2330 train_time:98507ms step_avg:58.32ms
step:1690/2330 train_time:98567ms step_avg:58.32ms
step:1691/2330 train_time:98625ms step_avg:58.32ms
step:1692/2330 train_time:98684ms step_avg:58.32ms
step:1693/2330 train_time:98742ms step_avg:58.32ms
step:1694/2330 train_time:98803ms step_avg:58.33ms
step:1695/2330 train_time:98861ms step_avg:58.32ms
step:1696/2330 train_time:98921ms step_avg:58.33ms
step:1697/2330 train_time:98978ms step_avg:58.33ms
step:1698/2330 train_time:99040ms step_avg:58.33ms
step:1699/2330 train_time:99097ms step_avg:58.33ms
step:1700/2330 train_time:99159ms step_avg:58.33ms
step:1701/2330 train_time:99216ms step_avg:58.33ms
step:1702/2330 train_time:99278ms step_avg:58.33ms
step:1703/2330 train_time:99336ms step_avg:58.33ms
step:1704/2330 train_time:99397ms step_avg:58.33ms
step:1705/2330 train_time:99454ms step_avg:58.33ms
step:1706/2330 train_time:99515ms step_avg:58.33ms
step:1707/2330 train_time:99572ms step_avg:58.33ms
step:1708/2330 train_time:99633ms step_avg:58.33ms
step:1709/2330 train_time:99691ms step_avg:58.33ms
step:1710/2330 train_time:99752ms step_avg:58.33ms
step:1711/2330 train_time:99810ms step_avg:58.33ms
step:1712/2330 train_time:99870ms step_avg:58.34ms
step:1713/2330 train_time:99928ms step_avg:58.34ms
step:1714/2330 train_time:99989ms step_avg:58.34ms
step:1715/2330 train_time:100047ms step_avg:58.34ms
step:1716/2330 train_time:100108ms step_avg:58.34ms
step:1717/2330 train_time:100166ms step_avg:58.34ms
step:1718/2330 train_time:100226ms step_avg:58.34ms
step:1719/2330 train_time:100285ms step_avg:58.34ms
step:1720/2330 train_time:100345ms step_avg:58.34ms
step:1721/2330 train_time:100403ms step_avg:58.34ms
step:1722/2330 train_time:100463ms step_avg:58.34ms
step:1723/2330 train_time:100521ms step_avg:58.34ms
step:1724/2330 train_time:100583ms step_avg:58.34ms
step:1725/2330 train_time:100640ms step_avg:58.34ms
step:1726/2330 train_time:100701ms step_avg:58.34ms
step:1727/2330 train_time:100758ms step_avg:58.34ms
step:1728/2330 train_time:100819ms step_avg:58.34ms
step:1729/2330 train_time:100876ms step_avg:58.34ms
step:1730/2330 train_time:100938ms step_avg:58.35ms
step:1731/2330 train_time:100995ms step_avg:58.34ms
step:1732/2330 train_time:101057ms step_avg:58.35ms
step:1733/2330 train_time:101113ms step_avg:58.35ms
step:1734/2330 train_time:101175ms step_avg:58.35ms
step:1735/2330 train_time:101232ms step_avg:58.35ms
step:1736/2330 train_time:101293ms step_avg:58.35ms
step:1737/2330 train_time:101351ms step_avg:58.35ms
step:1738/2330 train_time:101411ms step_avg:58.35ms
step:1739/2330 train_time:101469ms step_avg:58.35ms
step:1740/2330 train_time:101530ms step_avg:58.35ms
step:1741/2330 train_time:101588ms step_avg:58.35ms
step:1742/2330 train_time:101649ms step_avg:58.35ms
step:1743/2330 train_time:101708ms step_avg:58.35ms
step:1744/2330 train_time:101768ms step_avg:58.35ms
step:1745/2330 train_time:101826ms step_avg:58.35ms
step:1746/2330 train_time:101885ms step_avg:58.35ms
step:1747/2330 train_time:101944ms step_avg:58.35ms
step:1748/2330 train_time:102003ms step_avg:58.35ms
step:1749/2330 train_time:102060ms step_avg:58.35ms
step:1750/2330 train_time:102121ms step_avg:58.35ms
step:1750/2330 val_loss:3.8292 train_time:102204ms step_avg:58.40ms
step:1751/2330 train_time:102224ms step_avg:58.38ms
step:1752/2330 train_time:102244ms step_avg:58.36ms
step:1753/2330 train_time:102298ms step_avg:58.36ms
step:1754/2330 train_time:102364ms step_avg:58.36ms
step:1755/2330 train_time:102420ms step_avg:58.36ms
step:1756/2330 train_time:102484ms step_avg:58.36ms
step:1757/2330 train_time:102540ms step_avg:58.36ms
step:1758/2330 train_time:102600ms step_avg:58.36ms
step:1759/2330 train_time:102656ms step_avg:58.36ms
step:1760/2330 train_time:102716ms step_avg:58.36ms
step:1761/2330 train_time:102773ms step_avg:58.36ms
step:1762/2330 train_time:102832ms step_avg:58.36ms
step:1763/2330 train_time:102889ms step_avg:58.36ms
step:1764/2330 train_time:102949ms step_avg:58.36ms
step:1765/2330 train_time:103006ms step_avg:58.36ms
step:1766/2330 train_time:103065ms step_avg:58.36ms
step:1767/2330 train_time:103126ms step_avg:58.36ms
step:1768/2330 train_time:103191ms step_avg:58.37ms
step:1769/2330 train_time:103250ms step_avg:58.37ms
step:1770/2330 train_time:103312ms step_avg:58.37ms
step:1771/2330 train_time:103370ms step_avg:58.37ms
step:1772/2330 train_time:103431ms step_avg:58.37ms
step:1773/2330 train_time:103488ms step_avg:58.37ms
step:1774/2330 train_time:103548ms step_avg:58.37ms
step:1775/2330 train_time:103606ms step_avg:58.37ms
step:1776/2330 train_time:103666ms step_avg:58.37ms
step:1777/2330 train_time:103723ms step_avg:58.37ms
step:1778/2330 train_time:103784ms step_avg:58.37ms
step:1779/2330 train_time:103841ms step_avg:58.37ms
step:1780/2330 train_time:103901ms step_avg:58.37ms
step:1781/2330 train_time:103958ms step_avg:58.37ms
step:1782/2330 train_time:104018ms step_avg:58.37ms
step:1783/2330 train_time:104076ms step_avg:58.37ms
step:1784/2330 train_time:104137ms step_avg:58.37ms
step:1785/2330 train_time:104196ms step_avg:58.37ms
step:1786/2330 train_time:104257ms step_avg:58.37ms
step:1787/2330 train_time:104315ms step_avg:58.37ms
step:1788/2330 train_time:104377ms step_avg:58.38ms
step:1789/2330 train_time:104433ms step_avg:58.38ms
step:1790/2330 train_time:104496ms step_avg:58.38ms
step:1791/2330 train_time:104553ms step_avg:58.38ms
step:1792/2330 train_time:104614ms step_avg:58.38ms
step:1793/2330 train_time:104670ms step_avg:58.38ms
step:1794/2330 train_time:104732ms step_avg:58.38ms
step:1795/2330 train_time:104788ms step_avg:58.38ms
step:1796/2330 train_time:104850ms step_avg:58.38ms
step:1797/2330 train_time:104906ms step_avg:58.38ms
step:1798/2330 train_time:104967ms step_avg:58.38ms
step:1799/2330 train_time:105024ms step_avg:58.38ms
step:1800/2330 train_time:105086ms step_avg:58.38ms
step:1801/2330 train_time:105145ms step_avg:58.38ms
step:1802/2330 train_time:105207ms step_avg:58.38ms
step:1803/2330 train_time:105266ms step_avg:58.38ms
step:1804/2330 train_time:105328ms step_avg:58.39ms
step:1805/2330 train_time:105387ms step_avg:58.39ms
step:1806/2330 train_time:105447ms step_avg:58.39ms
step:1807/2330 train_time:105506ms step_avg:58.39ms
step:1808/2330 train_time:105566ms step_avg:58.39ms
step:1809/2330 train_time:105623ms step_avg:58.39ms
step:1810/2330 train_time:105683ms step_avg:58.39ms
step:1811/2330 train_time:105741ms step_avg:58.39ms
step:1812/2330 train_time:105800ms step_avg:58.39ms
step:1813/2330 train_time:105857ms step_avg:58.39ms
step:1814/2330 train_time:105918ms step_avg:58.39ms
step:1815/2330 train_time:105975ms step_avg:58.39ms
step:1816/2330 train_time:106036ms step_avg:58.39ms
step:1817/2330 train_time:106093ms step_avg:58.39ms
step:1818/2330 train_time:106154ms step_avg:58.39ms
step:1819/2330 train_time:106212ms step_avg:58.39ms
step:1820/2330 train_time:106274ms step_avg:58.39ms
step:1821/2330 train_time:106332ms step_avg:58.39ms
step:1822/2330 train_time:106394ms step_avg:58.39ms
step:1823/2330 train_time:106451ms step_avg:58.39ms
step:1824/2330 train_time:106512ms step_avg:58.39ms
step:1825/2330 train_time:106570ms step_avg:58.39ms
step:1826/2330 train_time:106630ms step_avg:58.40ms
step:1827/2330 train_time:106687ms step_avg:58.39ms
step:1828/2330 train_time:106747ms step_avg:58.40ms
step:1829/2330 train_time:106805ms step_avg:58.40ms
step:1830/2330 train_time:106866ms step_avg:58.40ms
step:1831/2330 train_time:106924ms step_avg:58.40ms
step:1832/2330 train_time:106985ms step_avg:58.40ms
step:1833/2330 train_time:107042ms step_avg:58.40ms
step:1834/2330 train_time:107102ms step_avg:58.40ms
step:1835/2330 train_time:107159ms step_avg:58.40ms
step:1836/2330 train_time:107221ms step_avg:58.40ms
step:1837/2330 train_time:107279ms step_avg:58.40ms
step:1838/2330 train_time:107339ms step_avg:58.40ms
step:1839/2330 train_time:107396ms step_avg:58.40ms
step:1840/2330 train_time:107458ms step_avg:58.40ms
step:1841/2330 train_time:107515ms step_avg:58.40ms
step:1842/2330 train_time:107577ms step_avg:58.40ms
step:1843/2330 train_time:107634ms step_avg:58.40ms
step:1844/2330 train_time:107696ms step_avg:58.40ms
step:1845/2330 train_time:107753ms step_avg:58.40ms
step:1846/2330 train_time:107815ms step_avg:58.40ms
step:1847/2330 train_time:107871ms step_avg:58.40ms
step:1848/2330 train_time:107933ms step_avg:58.41ms
step:1849/2330 train_time:107990ms step_avg:58.40ms
step:1850/2330 train_time:108052ms step_avg:58.41ms
step:1851/2330 train_time:108109ms step_avg:58.41ms
step:1852/2330 train_time:108171ms step_avg:58.41ms
step:1853/2330 train_time:108228ms step_avg:58.41ms
step:1854/2330 train_time:108291ms step_avg:58.41ms
step:1855/2330 train_time:108349ms step_avg:58.41ms
step:1856/2330 train_time:108410ms step_avg:58.41ms
step:1857/2330 train_time:108467ms step_avg:58.41ms
step:1858/2330 train_time:108529ms step_avg:58.41ms
step:1859/2330 train_time:108587ms step_avg:58.41ms
step:1860/2330 train_time:108648ms step_avg:58.41ms
step:1861/2330 train_time:108705ms step_avg:58.41ms
step:1862/2330 train_time:108766ms step_avg:58.41ms
step:1863/2330 train_time:108824ms step_avg:58.41ms
step:1864/2330 train_time:108885ms step_avg:58.41ms
step:1865/2330 train_time:108942ms step_avg:58.41ms
step:1866/2330 train_time:109003ms step_avg:58.42ms
step:1867/2330 train_time:109060ms step_avg:58.41ms
step:1868/2330 train_time:109121ms step_avg:58.42ms
step:1869/2330 train_time:109178ms step_avg:58.42ms
step:1870/2330 train_time:109240ms step_avg:58.42ms
step:1871/2330 train_time:109297ms step_avg:58.42ms
step:1872/2330 train_time:109358ms step_avg:58.42ms
step:1873/2330 train_time:109416ms step_avg:58.42ms
step:1874/2330 train_time:109477ms step_avg:58.42ms
step:1875/2330 train_time:109535ms step_avg:58.42ms
step:1876/2330 train_time:109596ms step_avg:58.42ms
step:1877/2330 train_time:109653ms step_avg:58.42ms
step:1878/2330 train_time:109715ms step_avg:58.42ms
step:1879/2330 train_time:109773ms step_avg:58.42ms
step:1880/2330 train_time:109833ms step_avg:58.42ms
step:1881/2330 train_time:109890ms step_avg:58.42ms
step:1882/2330 train_time:109951ms step_avg:58.42ms
step:1883/2330 train_time:110008ms step_avg:58.42ms
step:1884/2330 train_time:110070ms step_avg:58.42ms
step:1885/2330 train_time:110128ms step_avg:58.42ms
step:1886/2330 train_time:110189ms step_avg:58.42ms
step:1887/2330 train_time:110246ms step_avg:58.42ms
step:1888/2330 train_time:110309ms step_avg:58.43ms
step:1889/2330 train_time:110367ms step_avg:58.43ms
step:1890/2330 train_time:110429ms step_avg:58.43ms
step:1891/2330 train_time:110487ms step_avg:58.43ms
step:1892/2330 train_time:110548ms step_avg:58.43ms
step:1893/2330 train_time:110606ms step_avg:58.43ms
step:1894/2330 train_time:110666ms step_avg:58.43ms
step:1895/2330 train_time:110724ms step_avg:58.43ms
step:1896/2330 train_time:110784ms step_avg:58.43ms
step:1897/2330 train_time:110842ms step_avg:58.43ms
step:1898/2330 train_time:110902ms step_avg:58.43ms
step:1899/2330 train_time:110960ms step_avg:58.43ms
step:1900/2330 train_time:111020ms step_avg:58.43ms
step:1901/2330 train_time:111095ms step_avg:58.44ms
step:1902/2330 train_time:111139ms step_avg:58.43ms
step:1903/2330 train_time:111196ms step_avg:58.43ms
step:1904/2330 train_time:111257ms step_avg:58.43ms
step:1905/2330 train_time:111315ms step_avg:58.43ms
step:1906/2330 train_time:111375ms step_avg:58.43ms
step:1907/2330 train_time:111432ms step_avg:58.43ms
step:1908/2330 train_time:111494ms step_avg:58.43ms
step:1909/2330 train_time:111551ms step_avg:58.43ms
step:1910/2330 train_time:111613ms step_avg:58.44ms
step:1911/2330 train_time:111670ms step_avg:58.44ms
step:1912/2330 train_time:111731ms step_avg:58.44ms
step:1913/2330 train_time:111789ms step_avg:58.44ms
step:1914/2330 train_time:111850ms step_avg:58.44ms
step:1915/2330 train_time:111907ms step_avg:58.44ms
step:1916/2330 train_time:111968ms step_avg:58.44ms
step:1917/2330 train_time:112027ms step_avg:58.44ms
step:1918/2330 train_time:112088ms step_avg:58.44ms
step:1919/2330 train_time:112146ms step_avg:58.44ms
step:1920/2330 train_time:112207ms step_avg:58.44ms
step:1921/2330 train_time:112265ms step_avg:58.44ms
step:1922/2330 train_time:112326ms step_avg:58.44ms
step:1923/2330 train_time:112384ms step_avg:58.44ms
step:1924/2330 train_time:112444ms step_avg:58.44ms
step:1925/2330 train_time:112502ms step_avg:58.44ms
step:1926/2330 train_time:112562ms step_avg:58.44ms
step:1927/2330 train_time:112620ms step_avg:58.44ms
step:1928/2330 train_time:112680ms step_avg:58.44ms
step:1929/2330 train_time:112737ms step_avg:58.44ms
step:1930/2330 train_time:112798ms step_avg:58.44ms
step:1931/2330 train_time:112855ms step_avg:58.44ms
step:1932/2330 train_time:112917ms step_avg:58.45ms
step:1933/2330 train_time:112974ms step_avg:58.45ms
step:1934/2330 train_time:113036ms step_avg:58.45ms
step:1935/2330 train_time:113092ms step_avg:58.45ms
step:1936/2330 train_time:113154ms step_avg:58.45ms
step:1937/2330 train_time:113211ms step_avg:58.45ms
step:1938/2330 train_time:113272ms step_avg:58.45ms
step:1939/2330 train_time:113330ms step_avg:58.45ms
step:1940/2330 train_time:113392ms step_avg:58.45ms
step:1941/2330 train_time:113450ms step_avg:58.45ms
step:1942/2330 train_time:113510ms step_avg:58.45ms
step:1943/2330 train_time:113567ms step_avg:58.45ms
step:1944/2330 train_time:113631ms step_avg:58.45ms
step:1945/2330 train_time:113688ms step_avg:58.45ms
step:1946/2330 train_time:113749ms step_avg:58.45ms
step:1947/2330 train_time:113807ms step_avg:58.45ms
step:1948/2330 train_time:113868ms step_avg:58.45ms
step:1949/2330 train_time:113925ms step_avg:58.45ms
step:1950/2330 train_time:113986ms step_avg:58.45ms
step:1951/2330 train_time:114043ms step_avg:58.45ms
step:1952/2330 train_time:114105ms step_avg:58.46ms
step:1953/2330 train_time:114162ms step_avg:58.45ms
step:1954/2330 train_time:114222ms step_avg:58.46ms
step:1955/2330 train_time:114280ms step_avg:58.46ms
step:1956/2330 train_time:114340ms step_avg:58.46ms
step:1957/2330 train_time:114397ms step_avg:58.46ms
step:1958/2330 train_time:114457ms step_avg:58.46ms
step:1959/2330 train_time:114515ms step_avg:58.46ms
step:1960/2330 train_time:114576ms step_avg:58.46ms
step:1961/2330 train_time:114633ms step_avg:58.46ms
step:1962/2330 train_time:114695ms step_avg:58.46ms
step:1963/2330 train_time:114752ms step_avg:58.46ms
step:1964/2330 train_time:114815ms step_avg:58.46ms
step:1965/2330 train_time:114872ms step_avg:58.46ms
step:1966/2330 train_time:114934ms step_avg:58.46ms
step:1967/2330 train_time:114990ms step_avg:58.46ms
step:1968/2330 train_time:115053ms step_avg:58.46ms
step:1969/2330 train_time:115110ms step_avg:58.46ms
step:1970/2330 train_time:115172ms step_avg:58.46ms
step:1971/2330 train_time:115229ms step_avg:58.46ms
step:1972/2330 train_time:115290ms step_avg:58.46ms
step:1973/2330 train_time:115348ms step_avg:58.46ms
step:1974/2330 train_time:115409ms step_avg:58.46ms
step:1975/2330 train_time:115468ms step_avg:58.46ms
step:1976/2330 train_time:115528ms step_avg:58.47ms
step:1977/2330 train_time:115586ms step_avg:58.47ms
step:1978/2330 train_time:115646ms step_avg:58.47ms
step:1979/2330 train_time:115704ms step_avg:58.47ms
step:1980/2330 train_time:115764ms step_avg:58.47ms
step:1981/2330 train_time:115822ms step_avg:58.47ms
step:1982/2330 train_time:115882ms step_avg:58.47ms
step:1983/2330 train_time:115940ms step_avg:58.47ms
step:1984/2330 train_time:116000ms step_avg:58.47ms
step:1985/2330 train_time:116057ms step_avg:58.47ms
step:1986/2330 train_time:116118ms step_avg:58.47ms
step:1987/2330 train_time:116175ms step_avg:58.47ms
step:1988/2330 train_time:116236ms step_avg:58.47ms
step:1989/2330 train_time:116293ms step_avg:58.47ms
step:1990/2330 train_time:116355ms step_avg:58.47ms
step:1991/2330 train_time:116412ms step_avg:58.47ms
step:1992/2330 train_time:116474ms step_avg:58.47ms
step:1993/2330 train_time:116532ms step_avg:58.47ms
step:1994/2330 train_time:116593ms step_avg:58.47ms
step:1995/2330 train_time:116650ms step_avg:58.47ms
step:1996/2330 train_time:116712ms step_avg:58.47ms
step:1997/2330 train_time:116769ms step_avg:58.47ms
step:1998/2330 train_time:116830ms step_avg:58.47ms
step:1999/2330 train_time:116888ms step_avg:58.47ms
step:2000/2330 train_time:116949ms step_avg:58.47ms
step:2000/2330 val_loss:3.7670 train_time:117031ms step_avg:58.52ms
step:2001/2330 train_time:117050ms step_avg:58.50ms
step:2002/2330 train_time:117070ms step_avg:58.48ms
step:2003/2330 train_time:117129ms step_avg:58.48ms
step:2004/2330 train_time:117196ms step_avg:58.48ms
step:2005/2330 train_time:117253ms step_avg:58.48ms
step:2006/2330 train_time:117314ms step_avg:58.48ms
step:2007/2330 train_time:117371ms step_avg:58.48ms
step:2008/2330 train_time:117432ms step_avg:58.48ms
step:2009/2330 train_time:117488ms step_avg:58.48ms
step:2010/2330 train_time:117549ms step_avg:58.48ms
step:2011/2330 train_time:117606ms step_avg:58.48ms
step:2012/2330 train_time:117665ms step_avg:58.48ms
step:2013/2330 train_time:117722ms step_avg:58.48ms
step:2014/2330 train_time:117782ms step_avg:58.48ms
step:2015/2330 train_time:117839ms step_avg:58.48ms
step:2016/2330 train_time:117899ms step_avg:58.48ms
step:2017/2330 train_time:117957ms step_avg:58.48ms
step:2018/2330 train_time:118018ms step_avg:58.48ms
step:2019/2330 train_time:118078ms step_avg:58.48ms
step:2020/2330 train_time:118141ms step_avg:58.49ms
step:2021/2330 train_time:118200ms step_avg:58.49ms
step:2022/2330 train_time:118262ms step_avg:58.49ms
step:2023/2330 train_time:118320ms step_avg:58.49ms
step:2024/2330 train_time:118381ms step_avg:58.49ms
step:2025/2330 train_time:118439ms step_avg:58.49ms
step:2026/2330 train_time:118501ms step_avg:58.49ms
step:2027/2330 train_time:118559ms step_avg:58.49ms
step:2028/2330 train_time:118619ms step_avg:58.49ms
step:2029/2330 train_time:118676ms step_avg:58.49ms
step:2030/2330 train_time:118735ms step_avg:58.49ms
step:2031/2330 train_time:118792ms step_avg:58.49ms
step:2032/2330 train_time:118852ms step_avg:58.49ms
step:2033/2330 train_time:118909ms step_avg:58.49ms
step:2034/2330 train_time:118969ms step_avg:58.49ms
step:2035/2330 train_time:119028ms step_avg:58.49ms
step:2036/2330 train_time:119089ms step_avg:58.49ms
step:2037/2330 train_time:119148ms step_avg:58.49ms
step:2038/2330 train_time:119210ms step_avg:58.49ms
step:2039/2330 train_time:119267ms step_avg:58.49ms
step:2040/2330 train_time:119329ms step_avg:58.49ms
step:2041/2330 train_time:119386ms step_avg:58.49ms
step:2042/2330 train_time:119448ms step_avg:58.50ms
step:2043/2330 train_time:119504ms step_avg:58.49ms
step:2044/2330 train_time:119566ms step_avg:58.50ms
step:2045/2330 train_time:119622ms step_avg:58.49ms
step:2046/2330 train_time:119684ms step_avg:58.50ms
step:2047/2330 train_time:119741ms step_avg:58.50ms
step:2048/2330 train_time:119802ms step_avg:58.50ms
step:2049/2330 train_time:119859ms step_avg:58.50ms
step:2050/2330 train_time:119920ms step_avg:58.50ms
step:2051/2330 train_time:119978ms step_avg:58.50ms
step:2052/2330 train_time:120040ms step_avg:58.50ms
step:2053/2330 train_time:120099ms step_avg:58.50ms
step:2054/2330 train_time:120160ms step_avg:58.50ms
step:2055/2330 train_time:120219ms step_avg:58.50ms
step:2056/2330 train_time:120281ms step_avg:58.50ms
step:2057/2330 train_time:120339ms step_avg:58.50ms
step:2058/2330 train_time:120401ms step_avg:58.50ms
step:2059/2330 train_time:120458ms step_avg:58.50ms
step:2060/2330 train_time:120520ms step_avg:58.50ms
step:2061/2330 train_time:120577ms step_avg:58.50ms
step:2062/2330 train_time:120637ms step_avg:58.51ms
step:2063/2330 train_time:120694ms step_avg:58.50ms
step:2064/2330 train_time:120755ms step_avg:58.51ms
step:2065/2330 train_time:120813ms step_avg:58.51ms
step:2066/2330 train_time:120873ms step_avg:58.51ms
step:2067/2330 train_time:120930ms step_avg:58.50ms
step:2068/2330 train_time:120990ms step_avg:58.51ms
step:2069/2330 train_time:121048ms step_avg:58.51ms
step:2070/2330 train_time:121108ms step_avg:58.51ms
step:2071/2330 train_time:121165ms step_avg:58.51ms
step:2072/2330 train_time:121227ms step_avg:58.51ms
step:2073/2330 train_time:121286ms step_avg:58.51ms
step:2074/2330 train_time:121346ms step_avg:58.51ms
step:2075/2330 train_time:121404ms step_avg:58.51ms
step:2076/2330 train_time:121465ms step_avg:58.51ms
step:2077/2330 train_time:121523ms step_avg:58.51ms
step:2078/2330 train_time:121584ms step_avg:58.51ms
step:2079/2330 train_time:121641ms step_avg:58.51ms
step:2080/2330 train_time:121702ms step_avg:58.51ms
step:2081/2330 train_time:121759ms step_avg:58.51ms
step:2082/2330 train_time:121821ms step_avg:58.51ms
step:2083/2330 train_time:121879ms step_avg:58.51ms
step:2084/2330 train_time:121940ms step_avg:58.51ms
step:2085/2330 train_time:121998ms step_avg:58.51ms
step:2086/2330 train_time:122058ms step_avg:58.51ms
step:2087/2330 train_time:122117ms step_avg:58.51ms
step:2088/2330 train_time:122177ms step_avg:58.51ms
step:2089/2330 train_time:122235ms step_avg:58.51ms
step:2090/2330 train_time:122296ms step_avg:58.51ms
step:2091/2330 train_time:122355ms step_avg:58.52ms
step:2092/2330 train_time:122416ms step_avg:58.52ms
step:2093/2330 train_time:122474ms step_avg:58.52ms
step:2094/2330 train_time:122535ms step_avg:58.52ms
step:2095/2330 train_time:122593ms step_avg:58.52ms
step:2096/2330 train_time:122653ms step_avg:58.52ms
step:2097/2330 train_time:122710ms step_avg:58.52ms
step:2098/2330 train_time:122772ms step_avg:58.52ms
step:2099/2330 train_time:122828ms step_avg:58.52ms
step:2100/2330 train_time:122890ms step_avg:58.52ms
step:2101/2330 train_time:122946ms step_avg:58.52ms
step:2102/2330 train_time:123008ms step_avg:58.52ms
step:2103/2330 train_time:123065ms step_avg:58.52ms
step:2104/2330 train_time:123127ms step_avg:58.52ms
step:2105/2330 train_time:123184ms step_avg:58.52ms
step:2106/2330 train_time:123245ms step_avg:58.52ms
step:2107/2330 train_time:123303ms step_avg:58.52ms
step:2108/2330 train_time:123364ms step_avg:58.52ms
step:2109/2330 train_time:123422ms step_avg:58.52ms
step:2110/2330 train_time:123483ms step_avg:58.52ms
step:2111/2330 train_time:123542ms step_avg:58.52ms
step:2112/2330 train_time:123602ms step_avg:58.52ms
step:2113/2330 train_time:123660ms step_avg:58.52ms
step:2114/2330 train_time:123720ms step_avg:58.52ms
step:2115/2330 train_time:123779ms step_avg:58.52ms
step:2116/2330 train_time:123839ms step_avg:58.53ms
step:2117/2330 train_time:123897ms step_avg:58.52ms
step:2118/2330 train_time:123957ms step_avg:58.53ms
step:2119/2330 train_time:124014ms step_avg:58.52ms
step:2120/2330 train_time:124074ms step_avg:58.53ms
step:2121/2330 train_time:124132ms step_avg:58.53ms
step:2122/2330 train_time:124192ms step_avg:58.53ms
step:2123/2330 train_time:124250ms step_avg:58.53ms
step:2124/2330 train_time:124311ms step_avg:58.53ms
step:2125/2330 train_time:124368ms step_avg:58.53ms
step:2126/2330 train_time:124429ms step_avg:58.53ms
step:2127/2330 train_time:124486ms step_avg:58.53ms
step:2128/2330 train_time:124548ms step_avg:58.53ms
step:2129/2330 train_time:124605ms step_avg:58.53ms
step:2130/2330 train_time:124668ms step_avg:58.53ms
step:2131/2330 train_time:124724ms step_avg:58.53ms
step:2132/2330 train_time:124787ms step_avg:58.53ms
step:2133/2330 train_time:124844ms step_avg:58.53ms
step:2134/2330 train_time:124906ms step_avg:58.53ms
step:2135/2330 train_time:124963ms step_avg:58.53ms
step:2136/2330 train_time:125025ms step_avg:58.53ms
step:2137/2330 train_time:125082ms step_avg:58.53ms
step:2138/2330 train_time:125144ms step_avg:58.53ms
step:2139/2330 train_time:125201ms step_avg:58.53ms
step:2140/2330 train_time:125263ms step_avg:58.53ms
step:2141/2330 train_time:125320ms step_avg:58.53ms
step:2142/2330 train_time:125381ms step_avg:58.53ms
step:2143/2330 train_time:125440ms step_avg:58.53ms
step:2144/2330 train_time:125501ms step_avg:58.54ms
step:2145/2330 train_time:125559ms step_avg:58.54ms
step:2146/2330 train_time:125619ms step_avg:58.54ms
step:2147/2330 train_time:125677ms step_avg:58.54ms
step:2148/2330 train_time:125737ms step_avg:58.54ms
step:2149/2330 train_time:125795ms step_avg:58.54ms
step:2150/2330 train_time:125856ms step_avg:58.54ms
step:2151/2330 train_time:125913ms step_avg:58.54ms
step:2152/2330 train_time:125973ms step_avg:58.54ms
step:2153/2330 train_time:126030ms step_avg:58.54ms
step:2154/2330 train_time:126090ms step_avg:58.54ms
step:2155/2330 train_time:126147ms step_avg:58.54ms
step:2156/2330 train_time:126210ms step_avg:58.54ms
step:2157/2330 train_time:126267ms step_avg:58.54ms
step:2158/2330 train_time:126329ms step_avg:58.54ms
step:2159/2330 train_time:126386ms step_avg:58.54ms
step:2160/2330 train_time:126447ms step_avg:58.54ms
step:2161/2330 train_time:126505ms step_avg:58.54ms
step:2162/2330 train_time:126566ms step_avg:58.54ms
step:2163/2330 train_time:126624ms step_avg:58.54ms
step:2164/2330 train_time:126685ms step_avg:58.54ms
step:2165/2330 train_time:126743ms step_avg:58.54ms
step:2166/2330 train_time:126804ms step_avg:58.54ms
step:2167/2330 train_time:126862ms step_avg:58.54ms
step:2168/2330 train_time:126922ms step_avg:58.54ms
step:2169/2330 train_time:126981ms step_avg:58.54ms
step:2170/2330 train_time:127042ms step_avg:58.54ms
step:2171/2330 train_time:127101ms step_avg:58.54ms
step:2172/2330 train_time:127161ms step_avg:58.55ms
step:2173/2330 train_time:127219ms step_avg:58.55ms
step:2174/2330 train_time:127281ms step_avg:58.55ms
step:2175/2330 train_time:127338ms step_avg:58.55ms
step:2176/2330 train_time:127401ms step_avg:58.55ms
step:2177/2330 train_time:127458ms step_avg:58.55ms
step:2178/2330 train_time:127519ms step_avg:58.55ms
step:2179/2330 train_time:127576ms step_avg:58.55ms
step:2180/2330 train_time:127637ms step_avg:58.55ms
step:2181/2330 train_time:127695ms step_avg:58.55ms
step:2182/2330 train_time:127755ms step_avg:58.55ms
step:2183/2330 train_time:127812ms step_avg:58.55ms
step:2184/2330 train_time:127873ms step_avg:58.55ms
step:2185/2330 train_time:127930ms step_avg:58.55ms
step:2186/2330 train_time:127992ms step_avg:58.55ms
step:2187/2330 train_time:128049ms step_avg:58.55ms
step:2188/2330 train_time:128110ms step_avg:58.55ms
step:2189/2330 train_time:128167ms step_avg:58.55ms
step:2190/2330 train_time:128228ms step_avg:58.55ms
step:2191/2330 train_time:128286ms step_avg:58.55ms
step:2192/2330 train_time:128347ms step_avg:58.55ms
step:2193/2330 train_time:128404ms step_avg:58.55ms
step:2194/2330 train_time:128466ms step_avg:58.55ms
step:2195/2330 train_time:128523ms step_avg:58.55ms
step:2196/2330 train_time:128585ms step_avg:58.55ms
step:2197/2330 train_time:128642ms step_avg:58.55ms
step:2198/2330 train_time:128705ms step_avg:58.56ms
step:2199/2330 train_time:128762ms step_avg:58.55ms
step:2200/2330 train_time:128824ms step_avg:58.56ms
step:2201/2330 train_time:128880ms step_avg:58.56ms
step:2202/2330 train_time:128944ms step_avg:58.56ms
step:2203/2330 train_time:129001ms step_avg:58.56ms
step:2204/2330 train_time:129062ms step_avg:58.56ms
step:2205/2330 train_time:129121ms step_avg:58.56ms
step:2206/2330 train_time:129181ms step_avg:58.56ms
step:2207/2330 train_time:129239ms step_avg:58.56ms
step:2208/2330 train_time:129299ms step_avg:58.56ms
step:2209/2330 train_time:129358ms step_avg:58.56ms
step:2210/2330 train_time:129419ms step_avg:58.56ms
step:2211/2330 train_time:129477ms step_avg:58.56ms
step:2212/2330 train_time:129537ms step_avg:58.56ms
step:2213/2330 train_time:129595ms step_avg:58.56ms
step:2214/2330 train_time:129656ms step_avg:58.56ms
step:2215/2330 train_time:129714ms step_avg:58.56ms
step:2216/2330 train_time:129774ms step_avg:58.56ms
step:2217/2330 train_time:129832ms step_avg:58.56ms
step:2218/2330 train_time:129893ms step_avg:58.56ms
step:2219/2330 train_time:129950ms step_avg:58.56ms
step:2220/2330 train_time:130011ms step_avg:58.56ms
step:2221/2330 train_time:130068ms step_avg:58.56ms
step:2222/2330 train_time:130129ms step_avg:58.56ms
step:2223/2330 train_time:130186ms step_avg:58.56ms
step:2224/2330 train_time:130248ms step_avg:58.56ms
step:2225/2330 train_time:130305ms step_avg:58.56ms
step:2226/2330 train_time:130367ms step_avg:58.57ms
step:2227/2330 train_time:130424ms step_avg:58.56ms
step:2228/2330 train_time:130485ms step_avg:58.57ms
step:2229/2330 train_time:130542ms step_avg:58.57ms
step:2230/2330 train_time:130604ms step_avg:58.57ms
step:2231/2330 train_time:130661ms step_avg:58.57ms
step:2232/2330 train_time:130723ms step_avg:58.57ms
step:2233/2330 train_time:130780ms step_avg:58.57ms
step:2234/2330 train_time:130842ms step_avg:58.57ms
step:2235/2330 train_time:130900ms step_avg:58.57ms
step:2236/2330 train_time:130961ms step_avg:58.57ms
step:2237/2330 train_time:131020ms step_avg:58.57ms
step:2238/2330 train_time:131080ms step_avg:58.57ms
step:2239/2330 train_time:131138ms step_avg:58.57ms
step:2240/2330 train_time:131200ms step_avg:58.57ms
step:2241/2330 train_time:131258ms step_avg:58.57ms
step:2242/2330 train_time:131319ms step_avg:58.57ms
step:2243/2330 train_time:131377ms step_avg:58.57ms
step:2244/2330 train_time:131437ms step_avg:58.57ms
step:2245/2330 train_time:131496ms step_avg:58.57ms
step:2246/2330 train_time:131556ms step_avg:58.57ms
step:2247/2330 train_time:131614ms step_avg:58.57ms
step:2248/2330 train_time:131674ms step_avg:58.57ms
step:2249/2330 train_time:131731ms step_avg:58.57ms
step:2250/2330 train_time:131793ms step_avg:58.57ms
step:2250/2330 val_loss:3.7193 train_time:131875ms step_avg:58.61ms
step:2251/2330 train_time:131894ms step_avg:58.59ms
step:2252/2330 train_time:131914ms step_avg:58.58ms
step:2253/2330 train_time:131973ms step_avg:58.58ms
step:2254/2330 train_time:132039ms step_avg:58.58ms
step:2255/2330 train_time:132095ms step_avg:58.58ms
step:2256/2330 train_time:132158ms step_avg:58.58ms
step:2257/2330 train_time:132214ms step_avg:58.58ms
step:2258/2330 train_time:132276ms step_avg:58.58ms
step:2259/2330 train_time:132333ms step_avg:58.58ms
step:2260/2330 train_time:132393ms step_avg:58.58ms
step:2261/2330 train_time:132450ms step_avg:58.58ms
step:2262/2330 train_time:132510ms step_avg:58.58ms
step:2263/2330 train_time:132567ms step_avg:58.58ms
step:2264/2330 train_time:132627ms step_avg:58.58ms
step:2265/2330 train_time:132684ms step_avg:58.58ms
step:2266/2330 train_time:132744ms step_avg:58.58ms
step:2267/2330 train_time:132803ms step_avg:58.58ms
step:2268/2330 train_time:132864ms step_avg:58.58ms
step:2269/2330 train_time:132924ms step_avg:58.58ms
step:2270/2330 train_time:132987ms step_avg:58.58ms
step:2271/2330 train_time:133046ms step_avg:58.58ms
step:2272/2330 train_time:133107ms step_avg:58.59ms
step:2273/2330 train_time:133166ms step_avg:58.59ms
step:2274/2330 train_time:133226ms step_avg:58.59ms
step:2275/2330 train_time:133284ms step_avg:58.59ms
step:2276/2330 train_time:133343ms step_avg:58.59ms
step:2277/2330 train_time:133400ms step_avg:58.59ms
step:2278/2330 train_time:133461ms step_avg:58.59ms
step:2279/2330 train_time:133517ms step_avg:58.59ms
step:2280/2330 train_time:133579ms step_avg:58.59ms
step:2281/2330 train_time:133636ms step_avg:58.59ms
step:2282/2330 train_time:133696ms step_avg:58.59ms
step:2283/2330 train_time:133754ms step_avg:58.59ms
step:2284/2330 train_time:133816ms step_avg:58.59ms
step:2285/2330 train_time:133874ms step_avg:58.59ms
step:2286/2330 train_time:133936ms step_avg:58.59ms
step:2287/2330 train_time:133993ms step_avg:58.59ms
step:2288/2330 train_time:134056ms step_avg:58.59ms
step:2289/2330 train_time:134114ms step_avg:58.59ms
step:2290/2330 train_time:134177ms step_avg:58.59ms
step:2291/2330 train_time:134233ms step_avg:58.59ms
step:2292/2330 train_time:134294ms step_avg:58.59ms
step:2293/2330 train_time:134351ms step_avg:58.59ms
step:2294/2330 train_time:134413ms step_avg:58.59ms
step:2295/2330 train_time:134471ms step_avg:58.59ms
step:2296/2330 train_time:134532ms step_avg:58.59ms
step:2297/2330 train_time:134588ms step_avg:58.59ms
step:2298/2330 train_time:134649ms step_avg:58.59ms
step:2299/2330 train_time:134706ms step_avg:58.59ms
step:2300/2330 train_time:134767ms step_avg:58.59ms
step:2301/2330 train_time:134825ms step_avg:58.59ms
step:2302/2330 train_time:134886ms step_avg:58.60ms
step:2303/2330 train_time:134944ms step_avg:58.59ms
step:2304/2330 train_time:135006ms step_avg:58.60ms
step:2305/2330 train_time:135064ms step_avg:58.60ms
step:2306/2330 train_time:135126ms step_avg:58.60ms
step:2307/2330 train_time:135184ms step_avg:58.60ms
step:2308/2330 train_time:135245ms step_avg:58.60ms
step:2309/2330 train_time:135304ms step_avg:58.60ms
step:2310/2330 train_time:135364ms step_avg:58.60ms
step:2311/2330 train_time:135423ms step_avg:58.60ms
step:2312/2330 train_time:135483ms step_avg:58.60ms
step:2313/2330 train_time:135540ms step_avg:58.60ms
step:2314/2330 train_time:135601ms step_avg:58.60ms
step:2315/2330 train_time:135658ms step_avg:58.60ms
step:2316/2330 train_time:135719ms step_avg:58.60ms
step:2317/2330 train_time:135777ms step_avg:58.60ms
step:2318/2330 train_time:135838ms step_avg:58.60ms
step:2319/2330 train_time:135895ms step_avg:58.60ms
step:2320/2330 train_time:135957ms step_avg:58.60ms
step:2321/2330 train_time:136014ms step_avg:58.60ms
step:2322/2330 train_time:136077ms step_avg:58.60ms
step:2323/2330 train_time:136134ms step_avg:58.60ms
step:2324/2330 train_time:136195ms step_avg:58.60ms
step:2325/2330 train_time:136252ms step_avg:58.60ms
step:2326/2330 train_time:136314ms step_avg:58.60ms
step:2327/2330 train_time:136371ms step_avg:58.60ms
step:2328/2330 train_time:136434ms step_avg:58.61ms
step:2329/2330 train_time:136490ms step_avg:58.60ms
step:2330/2330 train_time:136552ms step_avg:58.61ms
step:2330/2330 val_loss:3.7040 train_time:136634ms step_avg:58.64ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
