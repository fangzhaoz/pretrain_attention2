import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--adamw_lr', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_lr_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_lr_tuning/{args2.adamw_lr}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=float(args2.adamw_lr),  betas=(0.85, 0.85), eps=1e-10, weight_decay=0.0, fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_lr_tuning/{args2.adamw_lr}", exist_ok=True)
            torch.save(log, f"logs_adamw_small_lr_tuning/{args2.adamw_lr}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 01:00:28 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   27C    P0             113W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   26C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   25C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   25C    P0             115W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   24C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   25C    P0             112W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   24C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   24C    P0             109W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:90ms step_avg:90.43ms
step:2/2330 train_time:184ms step_avg:91.90ms
step:3/2330 train_time:202ms step_avg:67.31ms
step:4/2330 train_time:221ms step_avg:55.19ms
step:5/2330 train_time:275ms step_avg:54.92ms
step:6/2330 train_time:333ms step_avg:55.42ms
step:7/2330 train_time:388ms step_avg:55.48ms
step:8/2330 train_time:447ms step_avg:55.84ms
step:9/2330 train_time:503ms step_avg:55.84ms
step:10/2330 train_time:561ms step_avg:56.12ms
step:11/2330 train_time:616ms step_avg:56.04ms
step:12/2330 train_time:676ms step_avg:56.36ms
step:13/2330 train_time:733ms step_avg:56.35ms
step:14/2330 train_time:791ms step_avg:56.50ms
step:15/2330 train_time:847ms step_avg:56.45ms
step:16/2330 train_time:905ms step_avg:56.59ms
step:17/2330 train_time:962ms step_avg:56.56ms
step:18/2330 train_time:1021ms step_avg:56.74ms
step:19/2330 train_time:1080ms step_avg:56.87ms
step:20/2330 train_time:1142ms step_avg:57.12ms
step:21/2330 train_time:1200ms step_avg:57.15ms
step:22/2330 train_time:1263ms step_avg:57.39ms
step:23/2330 train_time:1319ms step_avg:57.33ms
step:24/2330 train_time:1381ms step_avg:57.52ms
step:25/2330 train_time:1437ms step_avg:57.47ms
step:26/2330 train_time:1498ms step_avg:57.60ms
step:27/2330 train_time:1554ms step_avg:57.55ms
step:28/2330 train_time:1613ms step_avg:57.59ms
step:29/2330 train_time:1669ms step_avg:57.55ms
step:30/2330 train_time:1727ms step_avg:57.58ms
step:31/2330 train_time:1784ms step_avg:57.54ms
step:32/2330 train_time:1842ms step_avg:57.57ms
step:33/2330 train_time:1898ms step_avg:57.52ms
step:34/2330 train_time:1957ms step_avg:57.56ms
step:35/2330 train_time:2015ms step_avg:57.56ms
step:36/2330 train_time:2074ms step_avg:57.61ms
step:37/2330 train_time:2131ms step_avg:57.60ms
step:38/2330 train_time:2192ms step_avg:57.69ms
step:39/2330 train_time:2249ms step_avg:57.68ms
step:40/2330 train_time:2311ms step_avg:57.78ms
step:41/2330 train_time:2368ms step_avg:57.75ms
step:42/2330 train_time:2428ms step_avg:57.81ms
step:43/2330 train_time:2485ms step_avg:57.80ms
step:44/2330 train_time:2545ms step_avg:57.84ms
step:45/2330 train_time:2602ms step_avg:57.81ms
step:46/2330 train_time:2660ms step_avg:57.84ms
step:47/2330 train_time:2716ms step_avg:57.80ms
step:48/2330 train_time:2776ms step_avg:57.83ms
step:49/2330 train_time:2833ms step_avg:57.81ms
step:50/2330 train_time:2891ms step_avg:57.83ms
step:51/2330 train_time:2947ms step_avg:57.79ms
step:52/2330 train_time:3008ms step_avg:57.85ms
step:53/2330 train_time:3065ms step_avg:57.84ms
step:54/2330 train_time:3125ms step_avg:57.86ms
step:55/2330 train_time:3182ms step_avg:57.85ms
step:56/2330 train_time:3242ms step_avg:57.90ms
step:57/2330 train_time:3299ms step_avg:57.88ms
step:58/2330 train_time:3359ms step_avg:57.92ms
step:59/2330 train_time:3416ms step_avg:57.89ms
step:60/2330 train_time:3477ms step_avg:57.95ms
step:61/2330 train_time:3533ms step_avg:57.92ms
step:62/2330 train_time:3593ms step_avg:57.96ms
step:63/2330 train_time:3649ms step_avg:57.93ms
step:64/2330 train_time:3709ms step_avg:57.95ms
step:65/2330 train_time:3765ms step_avg:57.92ms
step:66/2330 train_time:3825ms step_avg:57.95ms
step:67/2330 train_time:3881ms step_avg:57.93ms
step:68/2330 train_time:3940ms step_avg:57.95ms
step:69/2330 train_time:3996ms step_avg:57.92ms
step:70/2330 train_time:4057ms step_avg:57.95ms
step:71/2330 train_time:4115ms step_avg:57.95ms
step:72/2330 train_time:4174ms step_avg:57.97ms
step:73/2330 train_time:4231ms step_avg:57.96ms
step:74/2330 train_time:4291ms step_avg:57.99ms
step:75/2330 train_time:4348ms step_avg:57.98ms
step:76/2330 train_time:4408ms step_avg:58.00ms
step:77/2330 train_time:4465ms step_avg:57.99ms
step:78/2330 train_time:4524ms step_avg:58.01ms
step:79/2330 train_time:4581ms step_avg:57.99ms
step:80/2330 train_time:4640ms step_avg:58.00ms
step:81/2330 train_time:4697ms step_avg:57.99ms
step:82/2330 train_time:4756ms step_avg:58.00ms
step:83/2330 train_time:4812ms step_avg:57.98ms
step:84/2330 train_time:4872ms step_avg:58.00ms
step:85/2330 train_time:4929ms step_avg:57.98ms
step:86/2330 train_time:4988ms step_avg:58.00ms
step:87/2330 train_time:5045ms step_avg:57.99ms
step:88/2330 train_time:5104ms step_avg:58.00ms
step:89/2330 train_time:5161ms step_avg:57.98ms
step:90/2330 train_time:5221ms step_avg:58.02ms
step:91/2330 train_time:5278ms step_avg:58.00ms
step:92/2330 train_time:5339ms step_avg:58.03ms
step:93/2330 train_time:5395ms step_avg:58.01ms
step:94/2330 train_time:5456ms step_avg:58.04ms
step:95/2330 train_time:5512ms step_avg:58.02ms
step:96/2330 train_time:5571ms step_avg:58.03ms
step:97/2330 train_time:5628ms step_avg:58.02ms
step:98/2330 train_time:5688ms step_avg:58.04ms
step:99/2330 train_time:5745ms step_avg:58.03ms
step:100/2330 train_time:5805ms step_avg:58.05ms
step:101/2330 train_time:5861ms step_avg:58.03ms
step:102/2330 train_time:5920ms step_avg:58.03ms
step:103/2330 train_time:5976ms step_avg:58.02ms
step:104/2330 train_time:6037ms step_avg:58.05ms
step:105/2330 train_time:6094ms step_avg:58.04ms
step:106/2330 train_time:6153ms step_avg:58.05ms
step:107/2330 train_time:6210ms step_avg:58.04ms
step:108/2330 train_time:6270ms step_avg:58.05ms
step:109/2330 train_time:6327ms step_avg:58.05ms
step:110/2330 train_time:6387ms step_avg:58.06ms
step:111/2330 train_time:6444ms step_avg:58.06ms
step:112/2330 train_time:6504ms step_avg:58.07ms
step:113/2330 train_time:6560ms step_avg:58.06ms
step:114/2330 train_time:6621ms step_avg:58.08ms
step:115/2330 train_time:6677ms step_avg:58.06ms
step:116/2330 train_time:6737ms step_avg:58.08ms
step:117/2330 train_time:6794ms step_avg:58.07ms
step:118/2330 train_time:6852ms step_avg:58.07ms
step:119/2330 train_time:6910ms step_avg:58.06ms
step:120/2330 train_time:6969ms step_avg:58.07ms
step:121/2330 train_time:7026ms step_avg:58.07ms
step:122/2330 train_time:7085ms step_avg:58.08ms
step:123/2330 train_time:7142ms step_avg:58.06ms
step:124/2330 train_time:7203ms step_avg:58.09ms
step:125/2330 train_time:7259ms step_avg:58.07ms
step:126/2330 train_time:7319ms step_avg:58.09ms
step:127/2330 train_time:7376ms step_avg:58.08ms
step:128/2330 train_time:7435ms step_avg:58.09ms
step:129/2330 train_time:7493ms step_avg:58.08ms
step:130/2330 train_time:7552ms step_avg:58.09ms
step:131/2330 train_time:7609ms step_avg:58.08ms
step:132/2330 train_time:7669ms step_avg:58.10ms
step:133/2330 train_time:7726ms step_avg:58.09ms
step:134/2330 train_time:7785ms step_avg:58.10ms
step:135/2330 train_time:7841ms step_avg:58.08ms
step:136/2330 train_time:7901ms step_avg:58.09ms
step:137/2330 train_time:7957ms step_avg:58.08ms
step:138/2330 train_time:8018ms step_avg:58.10ms
step:139/2330 train_time:8075ms step_avg:58.09ms
step:140/2330 train_time:8135ms step_avg:58.10ms
step:141/2330 train_time:8191ms step_avg:58.09ms
step:142/2330 train_time:8252ms step_avg:58.11ms
step:143/2330 train_time:8308ms step_avg:58.10ms
step:144/2330 train_time:8368ms step_avg:58.11ms
step:145/2330 train_time:8424ms step_avg:58.10ms
step:146/2330 train_time:8484ms step_avg:58.11ms
step:147/2330 train_time:8541ms step_avg:58.10ms
step:148/2330 train_time:8601ms step_avg:58.11ms
step:149/2330 train_time:8657ms step_avg:58.10ms
step:150/2330 train_time:8716ms step_avg:58.11ms
step:151/2330 train_time:8773ms step_avg:58.10ms
step:152/2330 train_time:8832ms step_avg:58.11ms
step:153/2330 train_time:8890ms step_avg:58.10ms
step:154/2330 train_time:8949ms step_avg:58.11ms
step:155/2330 train_time:9007ms step_avg:58.11ms
step:156/2330 train_time:9067ms step_avg:58.12ms
step:157/2330 train_time:9124ms step_avg:58.11ms
step:158/2330 train_time:9183ms step_avg:58.12ms
step:159/2330 train_time:9239ms step_avg:58.11ms
step:160/2330 train_time:9299ms step_avg:58.12ms
step:161/2330 train_time:9355ms step_avg:58.11ms
step:162/2330 train_time:9415ms step_avg:58.12ms
step:163/2330 train_time:9471ms step_avg:58.10ms
step:164/2330 train_time:9534ms step_avg:58.13ms
step:165/2330 train_time:9590ms step_avg:58.12ms
step:166/2330 train_time:9649ms step_avg:58.13ms
step:167/2330 train_time:9706ms step_avg:58.12ms
step:168/2330 train_time:9765ms step_avg:58.13ms
step:169/2330 train_time:9821ms step_avg:58.12ms
step:170/2330 train_time:9881ms step_avg:58.13ms
step:171/2330 train_time:9939ms step_avg:58.12ms
step:172/2330 train_time:9998ms step_avg:58.13ms
step:173/2330 train_time:10054ms step_avg:58.12ms
step:174/2330 train_time:10115ms step_avg:58.13ms
step:175/2330 train_time:10171ms step_avg:58.12ms
step:176/2330 train_time:10232ms step_avg:58.14ms
step:177/2330 train_time:10289ms step_avg:58.13ms
step:178/2330 train_time:10348ms step_avg:58.14ms
step:179/2330 train_time:10405ms step_avg:58.13ms
step:180/2330 train_time:10465ms step_avg:58.14ms
step:181/2330 train_time:10522ms step_avg:58.13ms
step:182/2330 train_time:10581ms step_avg:58.14ms
step:183/2330 train_time:10637ms step_avg:58.13ms
step:184/2330 train_time:10697ms step_avg:58.14ms
step:185/2330 train_time:10753ms step_avg:58.13ms
step:186/2330 train_time:10814ms step_avg:58.14ms
step:187/2330 train_time:10869ms step_avg:58.12ms
step:188/2330 train_time:10930ms step_avg:58.14ms
step:189/2330 train_time:10987ms step_avg:58.13ms
step:190/2330 train_time:11046ms step_avg:58.14ms
step:191/2330 train_time:11103ms step_avg:58.13ms
step:192/2330 train_time:11163ms step_avg:58.14ms
step:193/2330 train_time:11219ms step_avg:58.13ms
step:194/2330 train_time:11280ms step_avg:58.14ms
step:195/2330 train_time:11336ms step_avg:58.14ms
step:196/2330 train_time:11397ms step_avg:58.15ms
step:197/2330 train_time:11452ms step_avg:58.13ms
step:198/2330 train_time:11513ms step_avg:58.15ms
step:199/2330 train_time:11570ms step_avg:58.14ms
step:200/2330 train_time:11629ms step_avg:58.15ms
step:201/2330 train_time:11686ms step_avg:58.14ms
step:202/2330 train_time:11745ms step_avg:58.15ms
step:203/2330 train_time:11802ms step_avg:58.14ms
step:204/2330 train_time:11861ms step_avg:58.14ms
step:205/2330 train_time:11918ms step_avg:58.14ms
step:206/2330 train_time:11978ms step_avg:58.15ms
step:207/2330 train_time:12035ms step_avg:58.14ms
step:208/2330 train_time:12095ms step_avg:58.15ms
step:209/2330 train_time:12152ms step_avg:58.14ms
step:210/2330 train_time:12211ms step_avg:58.15ms
step:211/2330 train_time:12268ms step_avg:58.14ms
step:212/2330 train_time:12329ms step_avg:58.16ms
step:213/2330 train_time:12387ms step_avg:58.15ms
step:214/2330 train_time:12446ms step_avg:58.16ms
step:215/2330 train_time:12502ms step_avg:58.15ms
step:216/2330 train_time:12562ms step_avg:58.16ms
step:217/2330 train_time:12618ms step_avg:58.15ms
step:218/2330 train_time:12678ms step_avg:58.16ms
step:219/2330 train_time:12735ms step_avg:58.15ms
step:220/2330 train_time:12795ms step_avg:58.16ms
step:221/2330 train_time:12851ms step_avg:58.15ms
step:222/2330 train_time:12911ms step_avg:58.16ms
step:223/2330 train_time:12968ms step_avg:58.15ms
step:224/2330 train_time:13028ms step_avg:58.16ms
step:225/2330 train_time:13085ms step_avg:58.16ms
step:226/2330 train_time:13145ms step_avg:58.16ms
step:227/2330 train_time:13201ms step_avg:58.15ms
step:228/2330 train_time:13261ms step_avg:58.16ms
step:229/2330 train_time:13317ms step_avg:58.15ms
step:230/2330 train_time:13380ms step_avg:58.17ms
step:231/2330 train_time:13436ms step_avg:58.17ms
step:232/2330 train_time:13497ms step_avg:58.18ms
step:233/2330 train_time:13553ms step_avg:58.17ms
step:234/2330 train_time:13613ms step_avg:58.18ms
step:235/2330 train_time:13669ms step_avg:58.17ms
step:236/2330 train_time:13730ms step_avg:58.18ms
step:237/2330 train_time:13787ms step_avg:58.17ms
step:238/2330 train_time:13846ms step_avg:58.18ms
step:239/2330 train_time:13903ms step_avg:58.17ms
step:240/2330 train_time:13962ms step_avg:58.18ms
step:241/2330 train_time:14019ms step_avg:58.17ms
step:242/2330 train_time:14079ms step_avg:58.18ms
step:243/2330 train_time:14136ms step_avg:58.17ms
step:244/2330 train_time:14196ms step_avg:58.18ms
step:245/2330 train_time:14252ms step_avg:58.17ms
step:246/2330 train_time:14312ms step_avg:58.18ms
step:247/2330 train_time:14369ms step_avg:58.17ms
step:248/2330 train_time:14429ms step_avg:58.18ms
step:249/2330 train_time:14486ms step_avg:58.18ms
step:250/2330 train_time:14545ms step_avg:58.18ms
step:250/2330 val_loss:5.0880 train_time:14626ms step_avg:58.50ms
step:251/2330 train_time:14643ms step_avg:58.34ms
step:252/2330 train_time:14665ms step_avg:58.19ms
step:253/2330 train_time:14722ms step_avg:58.19ms
step:254/2330 train_time:14788ms step_avg:58.22ms
step:255/2330 train_time:14848ms step_avg:58.23ms
step:256/2330 train_time:14908ms step_avg:58.24ms
step:257/2330 train_time:14965ms step_avg:58.23ms
step:258/2330 train_time:15024ms step_avg:58.23ms
step:259/2330 train_time:15081ms step_avg:58.23ms
step:260/2330 train_time:15140ms step_avg:58.23ms
step:261/2330 train_time:15196ms step_avg:58.22ms
step:262/2330 train_time:15255ms step_avg:58.23ms
step:263/2330 train_time:15312ms step_avg:58.22ms
step:264/2330 train_time:15370ms step_avg:58.22ms
step:265/2330 train_time:15426ms step_avg:58.21ms
step:266/2330 train_time:15485ms step_avg:58.21ms
step:267/2330 train_time:15540ms step_avg:58.20ms
step:268/2330 train_time:15601ms step_avg:58.21ms
step:269/2330 train_time:15657ms step_avg:58.21ms
step:270/2330 train_time:15719ms step_avg:58.22ms
step:271/2330 train_time:15777ms step_avg:58.22ms
step:272/2330 train_time:15838ms step_avg:58.23ms
step:273/2330 train_time:15896ms step_avg:58.23ms
step:274/2330 train_time:15957ms step_avg:58.24ms
step:275/2330 train_time:16014ms step_avg:58.23ms
step:276/2330 train_time:16073ms step_avg:58.24ms
step:277/2330 train_time:16130ms step_avg:58.23ms
step:278/2330 train_time:16188ms step_avg:58.23ms
step:279/2330 train_time:16245ms step_avg:58.23ms
step:280/2330 train_time:16304ms step_avg:58.23ms
step:281/2330 train_time:16360ms step_avg:58.22ms
step:282/2330 train_time:16419ms step_avg:58.22ms
step:283/2330 train_time:16475ms step_avg:58.21ms
step:284/2330 train_time:16534ms step_avg:58.22ms
step:285/2330 train_time:16591ms step_avg:58.21ms
step:286/2330 train_time:16651ms step_avg:58.22ms
step:287/2330 train_time:16708ms step_avg:58.22ms
step:288/2330 train_time:16769ms step_avg:58.22ms
step:289/2330 train_time:16827ms step_avg:58.22ms
step:290/2330 train_time:16888ms step_avg:58.23ms
step:291/2330 train_time:16945ms step_avg:58.23ms
step:292/2330 train_time:17005ms step_avg:58.24ms
step:293/2330 train_time:17062ms step_avg:58.23ms
step:294/2330 train_time:17122ms step_avg:58.24ms
step:295/2330 train_time:17178ms step_avg:58.23ms
step:296/2330 train_time:17237ms step_avg:58.23ms
step:297/2330 train_time:17294ms step_avg:58.23ms
step:298/2330 train_time:17353ms step_avg:58.23ms
step:299/2330 train_time:17409ms step_avg:58.23ms
step:300/2330 train_time:17468ms step_avg:58.23ms
step:301/2330 train_time:17524ms step_avg:58.22ms
step:302/2330 train_time:17584ms step_avg:58.23ms
step:303/2330 train_time:17640ms step_avg:58.22ms
step:304/2330 train_time:17701ms step_avg:58.23ms
step:305/2330 train_time:17757ms step_avg:58.22ms
step:306/2330 train_time:17819ms step_avg:58.23ms
step:307/2330 train_time:17875ms step_avg:58.23ms
step:308/2330 train_time:17938ms step_avg:58.24ms
step:309/2330 train_time:17995ms step_avg:58.24ms
step:310/2330 train_time:18054ms step_avg:58.24ms
step:311/2330 train_time:18111ms step_avg:58.24ms
step:312/2330 train_time:18171ms step_avg:58.24ms
step:313/2330 train_time:18228ms step_avg:58.24ms
step:314/2330 train_time:18287ms step_avg:58.24ms
step:315/2330 train_time:18343ms step_avg:58.23ms
step:316/2330 train_time:18402ms step_avg:58.24ms
step:317/2330 train_time:18460ms step_avg:58.23ms
step:318/2330 train_time:18518ms step_avg:58.23ms
step:319/2330 train_time:18574ms step_avg:58.23ms
step:320/2330 train_time:18634ms step_avg:58.23ms
step:321/2330 train_time:18692ms step_avg:58.23ms
step:322/2330 train_time:18753ms step_avg:58.24ms
step:323/2330 train_time:18810ms step_avg:58.23ms
step:324/2330 train_time:18871ms step_avg:58.24ms
step:325/2330 train_time:18927ms step_avg:58.24ms
step:326/2330 train_time:18987ms step_avg:58.24ms
step:327/2330 train_time:19043ms step_avg:58.24ms
step:328/2330 train_time:19104ms step_avg:58.24ms
step:329/2330 train_time:19160ms step_avg:58.24ms
step:330/2330 train_time:19220ms step_avg:58.24ms
step:331/2330 train_time:19277ms step_avg:58.24ms
step:332/2330 train_time:19336ms step_avg:58.24ms
step:333/2330 train_time:19393ms step_avg:58.24ms
step:334/2330 train_time:19452ms step_avg:58.24ms
step:335/2330 train_time:19509ms step_avg:58.23ms
step:336/2330 train_time:19568ms step_avg:58.24ms
step:337/2330 train_time:19625ms step_avg:58.23ms
step:338/2330 train_time:19685ms step_avg:58.24ms
step:339/2330 train_time:19742ms step_avg:58.23ms
step:340/2330 train_time:19803ms step_avg:58.24ms
step:341/2330 train_time:19860ms step_avg:58.24ms
step:342/2330 train_time:19921ms step_avg:58.25ms
step:343/2330 train_time:19977ms step_avg:58.24ms
step:344/2330 train_time:20037ms step_avg:58.25ms
step:345/2330 train_time:20094ms step_avg:58.24ms
step:346/2330 train_time:20154ms step_avg:58.25ms
step:347/2330 train_time:20211ms step_avg:58.24ms
step:348/2330 train_time:20271ms step_avg:58.25ms
step:349/2330 train_time:20327ms step_avg:58.24ms
step:350/2330 train_time:20386ms step_avg:58.25ms
step:351/2330 train_time:20442ms step_avg:58.24ms
step:352/2330 train_time:20502ms step_avg:58.24ms
step:353/2330 train_time:20559ms step_avg:58.24ms
step:354/2330 train_time:20618ms step_avg:58.24ms
step:355/2330 train_time:20675ms step_avg:58.24ms
step:356/2330 train_time:20736ms step_avg:58.25ms
step:357/2330 train_time:20793ms step_avg:58.24ms
step:358/2330 train_time:20852ms step_avg:58.25ms
step:359/2330 train_time:20909ms step_avg:58.24ms
step:360/2330 train_time:20969ms step_avg:58.25ms
step:361/2330 train_time:21026ms step_avg:58.24ms
step:362/2330 train_time:21085ms step_avg:58.25ms
step:363/2330 train_time:21142ms step_avg:58.24ms
step:364/2330 train_time:21203ms step_avg:58.25ms
step:365/2330 train_time:21259ms step_avg:58.24ms
step:366/2330 train_time:21320ms step_avg:58.25ms
step:367/2330 train_time:21376ms step_avg:58.25ms
step:368/2330 train_time:21435ms step_avg:58.25ms
step:369/2330 train_time:21492ms step_avg:58.24ms
step:370/2330 train_time:21551ms step_avg:58.25ms
step:371/2330 train_time:21608ms step_avg:58.24ms
step:372/2330 train_time:21668ms step_avg:58.25ms
step:373/2330 train_time:21724ms step_avg:58.24ms
step:374/2330 train_time:21784ms step_avg:58.25ms
step:375/2330 train_time:21841ms step_avg:58.24ms
step:376/2330 train_time:21901ms step_avg:58.25ms
step:377/2330 train_time:21957ms step_avg:58.24ms
step:378/2330 train_time:22017ms step_avg:58.25ms
step:379/2330 train_time:22075ms step_avg:58.24ms
step:380/2330 train_time:22134ms step_avg:58.25ms
step:381/2330 train_time:22191ms step_avg:58.24ms
step:382/2330 train_time:22251ms step_avg:58.25ms
step:383/2330 train_time:22309ms step_avg:58.25ms
step:384/2330 train_time:22368ms step_avg:58.25ms
step:385/2330 train_time:22425ms step_avg:58.25ms
step:386/2330 train_time:22484ms step_avg:58.25ms
step:387/2330 train_time:22541ms step_avg:58.25ms
step:388/2330 train_time:22600ms step_avg:58.25ms
step:389/2330 train_time:22657ms step_avg:58.24ms
step:390/2330 train_time:22716ms step_avg:58.25ms
step:391/2330 train_time:22774ms step_avg:58.25ms
step:392/2330 train_time:22833ms step_avg:58.25ms
step:393/2330 train_time:22890ms step_avg:58.25ms
step:394/2330 train_time:22949ms step_avg:58.25ms
step:395/2330 train_time:23006ms step_avg:58.24ms
step:396/2330 train_time:23066ms step_avg:58.25ms
step:397/2330 train_time:23123ms step_avg:58.24ms
step:398/2330 train_time:23184ms step_avg:58.25ms
step:399/2330 train_time:23240ms step_avg:58.25ms
step:400/2330 train_time:23300ms step_avg:58.25ms
step:401/2330 train_time:23356ms step_avg:58.24ms
step:402/2330 train_time:23416ms step_avg:58.25ms
step:403/2330 train_time:23473ms step_avg:58.25ms
step:404/2330 train_time:23532ms step_avg:58.25ms
step:405/2330 train_time:23589ms step_avg:58.24ms
step:406/2330 train_time:23649ms step_avg:58.25ms
step:407/2330 train_time:23705ms step_avg:58.24ms
step:408/2330 train_time:23765ms step_avg:58.25ms
step:409/2330 train_time:23821ms step_avg:58.24ms
step:410/2330 train_time:23880ms step_avg:58.24ms
step:411/2330 train_time:23937ms step_avg:58.24ms
step:412/2330 train_time:23998ms step_avg:58.25ms
step:413/2330 train_time:24055ms step_avg:58.24ms
step:414/2330 train_time:24115ms step_avg:58.25ms
step:415/2330 train_time:24172ms step_avg:58.25ms
step:416/2330 train_time:24232ms step_avg:58.25ms
step:417/2330 train_time:24289ms step_avg:58.25ms
step:418/2330 train_time:24348ms step_avg:58.25ms
step:419/2330 train_time:24404ms step_avg:58.24ms
step:420/2330 train_time:24464ms step_avg:58.25ms
step:421/2330 train_time:24520ms step_avg:58.24ms
step:422/2330 train_time:24580ms step_avg:58.25ms
step:423/2330 train_time:24637ms step_avg:58.24ms
step:424/2330 train_time:24697ms step_avg:58.25ms
step:425/2330 train_time:24754ms step_avg:58.25ms
step:426/2330 train_time:24813ms step_avg:58.25ms
step:427/2330 train_time:24870ms step_avg:58.24ms
step:428/2330 train_time:24929ms step_avg:58.25ms
step:429/2330 train_time:24986ms step_avg:58.24ms
step:430/2330 train_time:25046ms step_avg:58.25ms
step:431/2330 train_time:25102ms step_avg:58.24ms
step:432/2330 train_time:25163ms step_avg:58.25ms
step:433/2330 train_time:25219ms step_avg:58.24ms
step:434/2330 train_time:25279ms step_avg:58.25ms
step:435/2330 train_time:25336ms step_avg:58.24ms
step:436/2330 train_time:25396ms step_avg:58.25ms
step:437/2330 train_time:25453ms step_avg:58.24ms
step:438/2330 train_time:25512ms step_avg:58.25ms
step:439/2330 train_time:25568ms step_avg:58.24ms
step:440/2330 train_time:25629ms step_avg:58.25ms
step:441/2330 train_time:25686ms step_avg:58.25ms
step:442/2330 train_time:25746ms step_avg:58.25ms
step:443/2330 train_time:25803ms step_avg:58.25ms
step:444/2330 train_time:25862ms step_avg:58.25ms
step:445/2330 train_time:25919ms step_avg:58.24ms
step:446/2330 train_time:25979ms step_avg:58.25ms
step:447/2330 train_time:26036ms step_avg:58.25ms
step:448/2330 train_time:26096ms step_avg:58.25ms
step:449/2330 train_time:26154ms step_avg:58.25ms
step:450/2330 train_time:26213ms step_avg:58.25ms
step:451/2330 train_time:26270ms step_avg:58.25ms
step:452/2330 train_time:26329ms step_avg:58.25ms
step:453/2330 train_time:26386ms step_avg:58.25ms
step:454/2330 train_time:26446ms step_avg:58.25ms
step:455/2330 train_time:26502ms step_avg:58.25ms
step:456/2330 train_time:26562ms step_avg:58.25ms
step:457/2330 train_time:26619ms step_avg:58.25ms
step:458/2330 train_time:26679ms step_avg:58.25ms
step:459/2330 train_time:26736ms step_avg:58.25ms
step:460/2330 train_time:26796ms step_avg:58.25ms
step:461/2330 train_time:26852ms step_avg:58.25ms
step:462/2330 train_time:26912ms step_avg:58.25ms
step:463/2330 train_time:26968ms step_avg:58.25ms
step:464/2330 train_time:27028ms step_avg:58.25ms
step:465/2330 train_time:27086ms step_avg:58.25ms
step:466/2330 train_time:27146ms step_avg:58.25ms
step:467/2330 train_time:27203ms step_avg:58.25ms
step:468/2330 train_time:27263ms step_avg:58.25ms
step:469/2330 train_time:27319ms step_avg:58.25ms
step:470/2330 train_time:27380ms step_avg:58.26ms
step:471/2330 train_time:27437ms step_avg:58.25ms
step:472/2330 train_time:27496ms step_avg:58.25ms
step:473/2330 train_time:27553ms step_avg:58.25ms
step:474/2330 train_time:27612ms step_avg:58.25ms
step:475/2330 train_time:27669ms step_avg:58.25ms
step:476/2330 train_time:27729ms step_avg:58.25ms
step:477/2330 train_time:27785ms step_avg:58.25ms
step:478/2330 train_time:27845ms step_avg:58.25ms
step:479/2330 train_time:27901ms step_avg:58.25ms
step:480/2330 train_time:27961ms step_avg:58.25ms
step:481/2330 train_time:28017ms step_avg:58.25ms
step:482/2330 train_time:28078ms step_avg:58.25ms
step:483/2330 train_time:28135ms step_avg:58.25ms
step:484/2330 train_time:28196ms step_avg:58.26ms
step:485/2330 train_time:28252ms step_avg:58.25ms
step:486/2330 train_time:28313ms step_avg:58.26ms
step:487/2330 train_time:28369ms step_avg:58.25ms
step:488/2330 train_time:28428ms step_avg:58.25ms
step:489/2330 train_time:28485ms step_avg:58.25ms
step:490/2330 train_time:28545ms step_avg:58.26ms
step:491/2330 train_time:28601ms step_avg:58.25ms
step:492/2330 train_time:28661ms step_avg:58.25ms
step:493/2330 train_time:28717ms step_avg:58.25ms
step:494/2330 train_time:28778ms step_avg:58.25ms
step:495/2330 train_time:28834ms step_avg:58.25ms
step:496/2330 train_time:28895ms step_avg:58.26ms
step:497/2330 train_time:28953ms step_avg:58.25ms
step:498/2330 train_time:29012ms step_avg:58.26ms
step:499/2330 train_time:29070ms step_avg:58.26ms
step:500/2330 train_time:29129ms step_avg:58.26ms
step:500/2330 val_loss:4.6810 train_time:29209ms step_avg:58.42ms
step:501/2330 train_time:29227ms step_avg:58.34ms
step:502/2330 train_time:29248ms step_avg:58.26ms
step:503/2330 train_time:29305ms step_avg:58.26ms
step:504/2330 train_time:29371ms step_avg:58.28ms
step:505/2330 train_time:29428ms step_avg:58.27ms
step:506/2330 train_time:29490ms step_avg:58.28ms
step:507/2330 train_time:29546ms step_avg:58.28ms
step:508/2330 train_time:29607ms step_avg:58.28ms
step:509/2330 train_time:29663ms step_avg:58.28ms
step:510/2330 train_time:29723ms step_avg:58.28ms
step:511/2330 train_time:29779ms step_avg:58.28ms
step:512/2330 train_time:29838ms step_avg:58.28ms
step:513/2330 train_time:29894ms step_avg:58.27ms
step:514/2330 train_time:29954ms step_avg:58.28ms
step:515/2330 train_time:30009ms step_avg:58.27ms
step:516/2330 train_time:30068ms step_avg:58.27ms
step:517/2330 train_time:30125ms step_avg:58.27ms
step:518/2330 train_time:30184ms step_avg:58.27ms
step:519/2330 train_time:30241ms step_avg:58.27ms
step:520/2330 train_time:30303ms step_avg:58.28ms
step:521/2330 train_time:30360ms step_avg:58.27ms
step:522/2330 train_time:30424ms step_avg:58.28ms
step:523/2330 train_time:30481ms step_avg:58.28ms
step:524/2330 train_time:30543ms step_avg:58.29ms
step:525/2330 train_time:30600ms step_avg:58.29ms
step:526/2330 train_time:30659ms step_avg:58.29ms
step:527/2330 train_time:30715ms step_avg:58.28ms
step:528/2330 train_time:30775ms step_avg:58.29ms
step:529/2330 train_time:30831ms step_avg:58.28ms
step:530/2330 train_time:30890ms step_avg:58.28ms
step:531/2330 train_time:30946ms step_avg:58.28ms
step:532/2330 train_time:31005ms step_avg:58.28ms
step:533/2330 train_time:31061ms step_avg:58.28ms
step:534/2330 train_time:31121ms step_avg:58.28ms
step:535/2330 train_time:31177ms step_avg:58.27ms
step:536/2330 train_time:31238ms step_avg:58.28ms
step:537/2330 train_time:31296ms step_avg:58.28ms
step:538/2330 train_time:31356ms step_avg:58.28ms
step:539/2330 train_time:31415ms step_avg:58.28ms
step:540/2330 train_time:31476ms step_avg:58.29ms
step:541/2330 train_time:31534ms step_avg:58.29ms
step:542/2330 train_time:31593ms step_avg:58.29ms
step:543/2330 train_time:31650ms step_avg:58.29ms
step:544/2330 train_time:31709ms step_avg:58.29ms
step:545/2330 train_time:31765ms step_avg:58.28ms
step:546/2330 train_time:31825ms step_avg:58.29ms
step:547/2330 train_time:31881ms step_avg:58.28ms
step:548/2330 train_time:31941ms step_avg:58.29ms
step:549/2330 train_time:31997ms step_avg:58.28ms
step:550/2330 train_time:32056ms step_avg:58.28ms
step:551/2330 train_time:32113ms step_avg:58.28ms
step:552/2330 train_time:32172ms step_avg:58.28ms
step:553/2330 train_time:32229ms step_avg:58.28ms
step:554/2330 train_time:32289ms step_avg:58.28ms
step:555/2330 train_time:32346ms step_avg:58.28ms
step:556/2330 train_time:32406ms step_avg:58.28ms
step:557/2330 train_time:32463ms step_avg:58.28ms
step:558/2330 train_time:32524ms step_avg:58.29ms
step:559/2330 train_time:32580ms step_avg:58.28ms
step:560/2330 train_time:32641ms step_avg:58.29ms
step:561/2330 train_time:32697ms step_avg:58.28ms
step:562/2330 train_time:32757ms step_avg:58.29ms
step:563/2330 train_time:32814ms step_avg:58.28ms
step:564/2330 train_time:32874ms step_avg:58.29ms
step:565/2330 train_time:32930ms step_avg:58.28ms
step:566/2330 train_time:32989ms step_avg:58.28ms
step:567/2330 train_time:33046ms step_avg:58.28ms
step:568/2330 train_time:33105ms step_avg:58.28ms
step:569/2330 train_time:33162ms step_avg:58.28ms
step:570/2330 train_time:33221ms step_avg:58.28ms
step:571/2330 train_time:33278ms step_avg:58.28ms
step:572/2330 train_time:33339ms step_avg:58.28ms
step:573/2330 train_time:33396ms step_avg:58.28ms
step:574/2330 train_time:33456ms step_avg:58.29ms
step:575/2330 train_time:33513ms step_avg:58.28ms
step:576/2330 train_time:33572ms step_avg:58.29ms
step:577/2330 train_time:33629ms step_avg:58.28ms
step:578/2330 train_time:33689ms step_avg:58.29ms
step:579/2330 train_time:33745ms step_avg:58.28ms
step:580/2330 train_time:33805ms step_avg:58.28ms
step:581/2330 train_time:33862ms step_avg:58.28ms
step:582/2330 train_time:33922ms step_avg:58.28ms
step:583/2330 train_time:33978ms step_avg:58.28ms
step:584/2330 train_time:34037ms step_avg:58.28ms
step:585/2330 train_time:34095ms step_avg:58.28ms
step:586/2330 train_time:34154ms step_avg:58.28ms
step:587/2330 train_time:34211ms step_avg:58.28ms
step:588/2330 train_time:34271ms step_avg:58.28ms
step:589/2330 train_time:34329ms step_avg:58.28ms
step:590/2330 train_time:34389ms step_avg:58.29ms
step:591/2330 train_time:34446ms step_avg:58.28ms
step:592/2330 train_time:34507ms step_avg:58.29ms
step:593/2330 train_time:34563ms step_avg:58.28ms
step:594/2330 train_time:34624ms step_avg:58.29ms
step:595/2330 train_time:34681ms step_avg:58.29ms
step:596/2330 train_time:34741ms step_avg:58.29ms
step:597/2330 train_time:34797ms step_avg:58.29ms
step:598/2330 train_time:34857ms step_avg:58.29ms
step:599/2330 train_time:34914ms step_avg:58.29ms
step:600/2330 train_time:34973ms step_avg:58.29ms
step:601/2330 train_time:35030ms step_avg:58.29ms
step:602/2330 train_time:35089ms step_avg:58.29ms
step:603/2330 train_time:35145ms step_avg:58.28ms
step:604/2330 train_time:35205ms step_avg:58.29ms
step:605/2330 train_time:35262ms step_avg:58.28ms
step:606/2330 train_time:35322ms step_avg:58.29ms
step:607/2330 train_time:35379ms step_avg:58.28ms
step:608/2330 train_time:35439ms step_avg:58.29ms
step:609/2330 train_time:35497ms step_avg:58.29ms
step:610/2330 train_time:35556ms step_avg:58.29ms
step:611/2330 train_time:35613ms step_avg:58.29ms
step:612/2330 train_time:35673ms step_avg:58.29ms
step:613/2330 train_time:35730ms step_avg:58.29ms
step:614/2330 train_time:35790ms step_avg:58.29ms
step:615/2330 train_time:35846ms step_avg:58.29ms
step:616/2330 train_time:35906ms step_avg:58.29ms
step:617/2330 train_time:35963ms step_avg:58.29ms
step:618/2330 train_time:36023ms step_avg:58.29ms
step:619/2330 train_time:36079ms step_avg:58.29ms
step:620/2330 train_time:36140ms step_avg:58.29ms
step:621/2330 train_time:36197ms step_avg:58.29ms
step:622/2330 train_time:36257ms step_avg:58.29ms
step:623/2330 train_time:36313ms step_avg:58.29ms
step:624/2330 train_time:36373ms step_avg:58.29ms
step:625/2330 train_time:36430ms step_avg:58.29ms
step:626/2330 train_time:36490ms step_avg:58.29ms
step:627/2330 train_time:36547ms step_avg:58.29ms
step:628/2330 train_time:36607ms step_avg:58.29ms
step:629/2330 train_time:36662ms step_avg:58.29ms
step:630/2330 train_time:36724ms step_avg:58.29ms
step:631/2330 train_time:36780ms step_avg:58.29ms
step:632/2330 train_time:36840ms step_avg:58.29ms
step:633/2330 train_time:36896ms step_avg:58.29ms
step:634/2330 train_time:36956ms step_avg:58.29ms
step:635/2330 train_time:37012ms step_avg:58.29ms
step:636/2330 train_time:37072ms step_avg:58.29ms
step:637/2330 train_time:37128ms step_avg:58.29ms
step:638/2330 train_time:37188ms step_avg:58.29ms
step:639/2330 train_time:37245ms step_avg:58.29ms
step:640/2330 train_time:37304ms step_avg:58.29ms
step:641/2330 train_time:37360ms step_avg:58.28ms
step:642/2330 train_time:37420ms step_avg:58.29ms
step:643/2330 train_time:37476ms step_avg:58.28ms
step:644/2330 train_time:37538ms step_avg:58.29ms
step:645/2330 train_time:37595ms step_avg:58.29ms
step:646/2330 train_time:37656ms step_avg:58.29ms
step:647/2330 train_time:37713ms step_avg:58.29ms
step:648/2330 train_time:37772ms step_avg:58.29ms
step:649/2330 train_time:37830ms step_avg:58.29ms
step:650/2330 train_time:37889ms step_avg:58.29ms
step:651/2330 train_time:37945ms step_avg:58.29ms
step:652/2330 train_time:38005ms step_avg:58.29ms
step:653/2330 train_time:38061ms step_avg:58.29ms
step:654/2330 train_time:38121ms step_avg:58.29ms
step:655/2330 train_time:38178ms step_avg:58.29ms
step:656/2330 train_time:38238ms step_avg:58.29ms
step:657/2330 train_time:38294ms step_avg:58.29ms
step:658/2330 train_time:38353ms step_avg:58.29ms
step:659/2330 train_time:38410ms step_avg:58.29ms
step:660/2330 train_time:38469ms step_avg:58.29ms
step:661/2330 train_time:38526ms step_avg:58.28ms
step:662/2330 train_time:38585ms step_avg:58.29ms
step:663/2330 train_time:38642ms step_avg:58.28ms
step:664/2330 train_time:38702ms step_avg:58.29ms
step:665/2330 train_time:38758ms step_avg:58.28ms
step:666/2330 train_time:38819ms step_avg:58.29ms
step:667/2330 train_time:38875ms step_avg:58.28ms
step:668/2330 train_time:38935ms step_avg:58.29ms
step:669/2330 train_time:38993ms step_avg:58.29ms
step:670/2330 train_time:39053ms step_avg:58.29ms
step:671/2330 train_time:39111ms step_avg:58.29ms
step:672/2330 train_time:39170ms step_avg:58.29ms
step:673/2330 train_time:39227ms step_avg:58.29ms
step:674/2330 train_time:39286ms step_avg:58.29ms
step:675/2330 train_time:39342ms step_avg:58.28ms
step:676/2330 train_time:39404ms step_avg:58.29ms
step:677/2330 train_time:39460ms step_avg:58.29ms
step:678/2330 train_time:39520ms step_avg:58.29ms
step:679/2330 train_time:39576ms step_avg:58.29ms
step:680/2330 train_time:39638ms step_avg:58.29ms
step:681/2330 train_time:39694ms step_avg:58.29ms
step:682/2330 train_time:39753ms step_avg:58.29ms
step:683/2330 train_time:39810ms step_avg:58.29ms
step:684/2330 train_time:39869ms step_avg:58.29ms
step:685/2330 train_time:39926ms step_avg:58.29ms
step:686/2330 train_time:39985ms step_avg:58.29ms
step:687/2330 train_time:40043ms step_avg:58.29ms
step:688/2330 train_time:40102ms step_avg:58.29ms
step:689/2330 train_time:40158ms step_avg:58.29ms
step:690/2330 train_time:40220ms step_avg:58.29ms
step:691/2330 train_time:40277ms step_avg:58.29ms
step:692/2330 train_time:40336ms step_avg:58.29ms
step:693/2330 train_time:40393ms step_avg:58.29ms
step:694/2330 train_time:40454ms step_avg:58.29ms
step:695/2330 train_time:40511ms step_avg:58.29ms
step:696/2330 train_time:40571ms step_avg:58.29ms
step:697/2330 train_time:40627ms step_avg:58.29ms
step:698/2330 train_time:40688ms step_avg:58.29ms
step:699/2330 train_time:40745ms step_avg:58.29ms
step:700/2330 train_time:40805ms step_avg:58.29ms
step:701/2330 train_time:40861ms step_avg:58.29ms
step:702/2330 train_time:40921ms step_avg:58.29ms
step:703/2330 train_time:40977ms step_avg:58.29ms
step:704/2330 train_time:41038ms step_avg:58.29ms
step:705/2330 train_time:41095ms step_avg:58.29ms
step:706/2330 train_time:41154ms step_avg:58.29ms
step:707/2330 train_time:41211ms step_avg:58.29ms
step:708/2330 train_time:41270ms step_avg:58.29ms
step:709/2330 train_time:41327ms step_avg:58.29ms
step:710/2330 train_time:41386ms step_avg:58.29ms
step:711/2330 train_time:41443ms step_avg:58.29ms
step:712/2330 train_time:41503ms step_avg:58.29ms
step:713/2330 train_time:41559ms step_avg:58.29ms
step:714/2330 train_time:41620ms step_avg:58.29ms
step:715/2330 train_time:41676ms step_avg:58.29ms
step:716/2330 train_time:41736ms step_avg:58.29ms
step:717/2330 train_time:41793ms step_avg:58.29ms
step:718/2330 train_time:41853ms step_avg:58.29ms
step:719/2330 train_time:41909ms step_avg:58.29ms
step:720/2330 train_time:41969ms step_avg:58.29ms
step:721/2330 train_time:42026ms step_avg:58.29ms
step:722/2330 train_time:42085ms step_avg:58.29ms
step:723/2330 train_time:42142ms step_avg:58.29ms
step:724/2330 train_time:42202ms step_avg:58.29ms
step:725/2330 train_time:42257ms step_avg:58.29ms
step:726/2330 train_time:42318ms step_avg:58.29ms
step:727/2330 train_time:42376ms step_avg:58.29ms
step:728/2330 train_time:42436ms step_avg:58.29ms
step:729/2330 train_time:42493ms step_avg:58.29ms
step:730/2330 train_time:42552ms step_avg:58.29ms
step:731/2330 train_time:42608ms step_avg:58.29ms
step:732/2330 train_time:42668ms step_avg:58.29ms
step:733/2330 train_time:42725ms step_avg:58.29ms
step:734/2330 train_time:42785ms step_avg:58.29ms
step:735/2330 train_time:42841ms step_avg:58.29ms
step:736/2330 train_time:42901ms step_avg:58.29ms
step:737/2330 train_time:42957ms step_avg:58.29ms
step:738/2330 train_time:43018ms step_avg:58.29ms
step:739/2330 train_time:43074ms step_avg:58.29ms
step:740/2330 train_time:43134ms step_avg:58.29ms
step:741/2330 train_time:43192ms step_avg:58.29ms
step:742/2330 train_time:43251ms step_avg:58.29ms
step:743/2330 train_time:43308ms step_avg:58.29ms
step:744/2330 train_time:43368ms step_avg:58.29ms
step:745/2330 train_time:43424ms step_avg:58.29ms
step:746/2330 train_time:43485ms step_avg:58.29ms
step:747/2330 train_time:43541ms step_avg:58.29ms
step:748/2330 train_time:43601ms step_avg:58.29ms
step:749/2330 train_time:43657ms step_avg:58.29ms
step:750/2330 train_time:43719ms step_avg:58.29ms
step:750/2330 val_loss:4.3787 train_time:43799ms step_avg:58.40ms
step:751/2330 train_time:43816ms step_avg:58.34ms
step:752/2330 train_time:43837ms step_avg:58.29ms
step:753/2330 train_time:43895ms step_avg:58.29ms
step:754/2330 train_time:43961ms step_avg:58.30ms
step:755/2330 train_time:44018ms step_avg:58.30ms
step:756/2330 train_time:44080ms step_avg:58.31ms
step:757/2330 train_time:44136ms step_avg:58.30ms
step:758/2330 train_time:44195ms step_avg:58.30ms
step:759/2330 train_time:44252ms step_avg:58.30ms
step:760/2330 train_time:44311ms step_avg:58.30ms
step:761/2330 train_time:44367ms step_avg:58.30ms
step:762/2330 train_time:44426ms step_avg:58.30ms
step:763/2330 train_time:44482ms step_avg:58.30ms
step:764/2330 train_time:44540ms step_avg:58.30ms
step:765/2330 train_time:44597ms step_avg:58.30ms
step:766/2330 train_time:44655ms step_avg:58.30ms
step:767/2330 train_time:44712ms step_avg:58.29ms
step:768/2330 train_time:44773ms step_avg:58.30ms
step:769/2330 train_time:44832ms step_avg:58.30ms
step:770/2330 train_time:44894ms step_avg:58.30ms
step:771/2330 train_time:44953ms step_avg:58.30ms
step:772/2330 train_time:45014ms step_avg:58.31ms
step:773/2330 train_time:45073ms step_avg:58.31ms
step:774/2330 train_time:45134ms step_avg:58.31ms
step:775/2330 train_time:45192ms step_avg:58.31ms
step:776/2330 train_time:45252ms step_avg:58.31ms
step:777/2330 train_time:45309ms step_avg:58.31ms
step:778/2330 train_time:45370ms step_avg:58.32ms
step:779/2330 train_time:45428ms step_avg:58.32ms
step:780/2330 train_time:45488ms step_avg:58.32ms
step:781/2330 train_time:45544ms step_avg:58.32ms
step:782/2330 train_time:45605ms step_avg:58.32ms
step:783/2330 train_time:45661ms step_avg:58.32ms
step:784/2330 train_time:45723ms step_avg:58.32ms
step:785/2330 train_time:45780ms step_avg:58.32ms
step:786/2330 train_time:45841ms step_avg:58.32ms
step:787/2330 train_time:45898ms step_avg:58.32ms
step:788/2330 train_time:45961ms step_avg:58.33ms
step:789/2330 train_time:46019ms step_avg:58.33ms
step:790/2330 train_time:46082ms step_avg:58.33ms
step:791/2330 train_time:46139ms step_avg:58.33ms
step:792/2330 train_time:46199ms step_avg:58.33ms
step:793/2330 train_time:46256ms step_avg:58.33ms
step:794/2330 train_time:46317ms step_avg:58.33ms
step:795/2330 train_time:46375ms step_avg:58.33ms
step:796/2330 train_time:46436ms step_avg:58.34ms
step:797/2330 train_time:46494ms step_avg:58.34ms
step:798/2330 train_time:46554ms step_avg:58.34ms
step:799/2330 train_time:46611ms step_avg:58.34ms
step:800/2330 train_time:46672ms step_avg:58.34ms
step:801/2330 train_time:46730ms step_avg:58.34ms
step:802/2330 train_time:46790ms step_avg:58.34ms
step:803/2330 train_time:46848ms step_avg:58.34ms
step:804/2330 train_time:46909ms step_avg:58.34ms
step:805/2330 train_time:46967ms step_avg:58.34ms
step:806/2330 train_time:47029ms step_avg:58.35ms
step:807/2330 train_time:47086ms step_avg:58.35ms
step:808/2330 train_time:47148ms step_avg:58.35ms
step:809/2330 train_time:47204ms step_avg:58.35ms
step:810/2330 train_time:47266ms step_avg:58.35ms
step:811/2330 train_time:47323ms step_avg:58.35ms
step:812/2330 train_time:47385ms step_avg:58.36ms
step:813/2330 train_time:47441ms step_avg:58.35ms
step:814/2330 train_time:47502ms step_avg:58.36ms
step:815/2330 train_time:47560ms step_avg:58.36ms
step:816/2330 train_time:47619ms step_avg:58.36ms
step:817/2330 train_time:47676ms step_avg:58.36ms
step:818/2330 train_time:47738ms step_avg:58.36ms
step:819/2330 train_time:47794ms step_avg:58.36ms
step:820/2330 train_time:47856ms step_avg:58.36ms
step:821/2330 train_time:47913ms step_avg:58.36ms
step:822/2330 train_time:47976ms step_avg:58.36ms
step:823/2330 train_time:48034ms step_avg:58.36ms
step:824/2330 train_time:48095ms step_avg:58.37ms
step:825/2330 train_time:48153ms step_avg:58.37ms
step:826/2330 train_time:48213ms step_avg:58.37ms
step:827/2330 train_time:48272ms step_avg:58.37ms
step:828/2330 train_time:48332ms step_avg:58.37ms
step:829/2330 train_time:48390ms step_avg:58.37ms
step:830/2330 train_time:48450ms step_avg:58.37ms
step:831/2330 train_time:48507ms step_avg:58.37ms
step:832/2330 train_time:48567ms step_avg:58.37ms
step:833/2330 train_time:48625ms step_avg:58.37ms
step:834/2330 train_time:48685ms step_avg:58.38ms
step:835/2330 train_time:48742ms step_avg:58.37ms
step:836/2330 train_time:48803ms step_avg:58.38ms
step:837/2330 train_time:48860ms step_avg:58.37ms
step:838/2330 train_time:48921ms step_avg:58.38ms
step:839/2330 train_time:48979ms step_avg:58.38ms
step:840/2330 train_time:49039ms step_avg:58.38ms
step:841/2330 train_time:49096ms step_avg:58.38ms
step:842/2330 train_time:49158ms step_avg:58.38ms
step:843/2330 train_time:49216ms step_avg:58.38ms
step:844/2330 train_time:49276ms step_avg:58.38ms
step:845/2330 train_time:49334ms step_avg:58.38ms
step:846/2330 train_time:49395ms step_avg:58.39ms
step:847/2330 train_time:49452ms step_avg:58.39ms
step:848/2330 train_time:49514ms step_avg:58.39ms
step:849/2330 train_time:49571ms step_avg:58.39ms
step:850/2330 train_time:49632ms step_avg:58.39ms
step:851/2330 train_time:49690ms step_avg:58.39ms
step:852/2330 train_time:49750ms step_avg:58.39ms
step:853/2330 train_time:49808ms step_avg:58.39ms
step:854/2330 train_time:49867ms step_avg:58.39ms
step:855/2330 train_time:49925ms step_avg:58.39ms
step:856/2330 train_time:49986ms step_avg:58.39ms
step:857/2330 train_time:50042ms step_avg:58.39ms
step:858/2330 train_time:50104ms step_avg:58.40ms
step:859/2330 train_time:50162ms step_avg:58.40ms
step:860/2330 train_time:50223ms step_avg:58.40ms
step:861/2330 train_time:50280ms step_avg:58.40ms
step:862/2330 train_time:50342ms step_avg:58.40ms
step:863/2330 train_time:50398ms step_avg:58.40ms
step:864/2330 train_time:50460ms step_avg:58.40ms
step:865/2330 train_time:50517ms step_avg:58.40ms
step:866/2330 train_time:50579ms step_avg:58.41ms
step:867/2330 train_time:50636ms step_avg:58.40ms
step:868/2330 train_time:50697ms step_avg:58.41ms
step:869/2330 train_time:50754ms step_avg:58.41ms
step:870/2330 train_time:50814ms step_avg:58.41ms
step:871/2330 train_time:50872ms step_avg:58.41ms
step:872/2330 train_time:50932ms step_avg:58.41ms
step:873/2330 train_time:50990ms step_avg:58.41ms
step:874/2330 train_time:51051ms step_avg:58.41ms
step:875/2330 train_time:51109ms step_avg:58.41ms
step:876/2330 train_time:51169ms step_avg:58.41ms
step:877/2330 train_time:51226ms step_avg:58.41ms
step:878/2330 train_time:51286ms step_avg:58.41ms
step:879/2330 train_time:51343ms step_avg:58.41ms
step:880/2330 train_time:51405ms step_avg:58.41ms
step:881/2330 train_time:51462ms step_avg:58.41ms
step:882/2330 train_time:51524ms step_avg:58.42ms
step:883/2330 train_time:51581ms step_avg:58.42ms
step:884/2330 train_time:51643ms step_avg:58.42ms
step:885/2330 train_time:51699ms step_avg:58.42ms
step:886/2330 train_time:51761ms step_avg:58.42ms
step:887/2330 train_time:51817ms step_avg:58.42ms
step:888/2330 train_time:51878ms step_avg:58.42ms
step:889/2330 train_time:51934ms step_avg:58.42ms
step:890/2330 train_time:51996ms step_avg:58.42ms
step:891/2330 train_time:52053ms step_avg:58.42ms
step:892/2330 train_time:52115ms step_avg:58.42ms
step:893/2330 train_time:52173ms step_avg:58.42ms
step:894/2330 train_time:52233ms step_avg:58.43ms
step:895/2330 train_time:52292ms step_avg:58.43ms
step:896/2330 train_time:52352ms step_avg:58.43ms
step:897/2330 train_time:52410ms step_avg:58.43ms
step:898/2330 train_time:52470ms step_avg:58.43ms
step:899/2330 train_time:52528ms step_avg:58.43ms
step:900/2330 train_time:52588ms step_avg:58.43ms
step:901/2330 train_time:52646ms step_avg:58.43ms
step:902/2330 train_time:52706ms step_avg:58.43ms
step:903/2330 train_time:52763ms step_avg:58.43ms
step:904/2330 train_time:52824ms step_avg:58.43ms
step:905/2330 train_time:52881ms step_avg:58.43ms
step:906/2330 train_time:52942ms step_avg:58.43ms
step:907/2330 train_time:52999ms step_avg:58.43ms
step:908/2330 train_time:53060ms step_avg:58.44ms
step:909/2330 train_time:53117ms step_avg:58.43ms
step:910/2330 train_time:53178ms step_avg:58.44ms
step:911/2330 train_time:53235ms step_avg:58.44ms
step:912/2330 train_time:53297ms step_avg:58.44ms
step:913/2330 train_time:53354ms step_avg:58.44ms
step:914/2330 train_time:53416ms step_avg:58.44ms
step:915/2330 train_time:53475ms step_avg:58.44ms
step:916/2330 train_time:53536ms step_avg:58.45ms
step:917/2330 train_time:53593ms step_avg:58.44ms
step:918/2330 train_time:53655ms step_avg:58.45ms
step:919/2330 train_time:53712ms step_avg:58.45ms
step:920/2330 train_time:53773ms step_avg:58.45ms
step:921/2330 train_time:53831ms step_avg:58.45ms
step:922/2330 train_time:53891ms step_avg:58.45ms
step:923/2330 train_time:53949ms step_avg:58.45ms
step:924/2330 train_time:54009ms step_avg:58.45ms
step:925/2330 train_time:54066ms step_avg:58.45ms
step:926/2330 train_time:54127ms step_avg:58.45ms
step:927/2330 train_time:54184ms step_avg:58.45ms
step:928/2330 train_time:54246ms step_avg:58.46ms
step:929/2330 train_time:54303ms step_avg:58.45ms
step:930/2330 train_time:54365ms step_avg:58.46ms
step:931/2330 train_time:54421ms step_avg:58.45ms
step:932/2330 train_time:54482ms step_avg:58.46ms
step:933/2330 train_time:54539ms step_avg:58.46ms
step:934/2330 train_time:54601ms step_avg:58.46ms
step:935/2330 train_time:54658ms step_avg:58.46ms
step:936/2330 train_time:54719ms step_avg:58.46ms
step:937/2330 train_time:54777ms step_avg:58.46ms
step:938/2330 train_time:54837ms step_avg:58.46ms
step:939/2330 train_time:54894ms step_avg:58.46ms
step:940/2330 train_time:54954ms step_avg:58.46ms
step:941/2330 train_time:55012ms step_avg:58.46ms
step:942/2330 train_time:55072ms step_avg:58.46ms
step:943/2330 train_time:55131ms step_avg:58.46ms
step:944/2330 train_time:55191ms step_avg:58.46ms
step:945/2330 train_time:55248ms step_avg:58.46ms
step:946/2330 train_time:55308ms step_avg:58.47ms
step:947/2330 train_time:55366ms step_avg:58.46ms
step:948/2330 train_time:55427ms step_avg:58.47ms
step:949/2330 train_time:55484ms step_avg:58.47ms
step:950/2330 train_time:55545ms step_avg:58.47ms
step:951/2330 train_time:55601ms step_avg:58.47ms
step:952/2330 train_time:55663ms step_avg:58.47ms
step:953/2330 train_time:55720ms step_avg:58.47ms
step:954/2330 train_time:55782ms step_avg:58.47ms
step:955/2330 train_time:55838ms step_avg:58.47ms
step:956/2330 train_time:55899ms step_avg:58.47ms
step:957/2330 train_time:55956ms step_avg:58.47ms
step:958/2330 train_time:56018ms step_avg:58.47ms
step:959/2330 train_time:56075ms step_avg:58.47ms
step:960/2330 train_time:56136ms step_avg:58.47ms
step:961/2330 train_time:56194ms step_avg:58.47ms
step:962/2330 train_time:56255ms step_avg:58.48ms
step:963/2330 train_time:56312ms step_avg:58.48ms
step:964/2330 train_time:56373ms step_avg:58.48ms
step:965/2330 train_time:56432ms step_avg:58.48ms
step:966/2330 train_time:56492ms step_avg:58.48ms
step:967/2330 train_time:56550ms step_avg:58.48ms
step:968/2330 train_time:56610ms step_avg:58.48ms
step:969/2330 train_time:56666ms step_avg:58.48ms
step:970/2330 train_time:56727ms step_avg:58.48ms
step:971/2330 train_time:56784ms step_avg:58.48ms
step:972/2330 train_time:56846ms step_avg:58.48ms
step:973/2330 train_time:56903ms step_avg:58.48ms
step:974/2330 train_time:56965ms step_avg:58.49ms
step:975/2330 train_time:57021ms step_avg:58.48ms
step:976/2330 train_time:57084ms step_avg:58.49ms
step:977/2330 train_time:57140ms step_avg:58.49ms
step:978/2330 train_time:57202ms step_avg:58.49ms
step:979/2330 train_time:57258ms step_avg:58.49ms
step:980/2330 train_time:57320ms step_avg:58.49ms
step:981/2330 train_time:57377ms step_avg:58.49ms
step:982/2330 train_time:57439ms step_avg:58.49ms
step:983/2330 train_time:57496ms step_avg:58.49ms
step:984/2330 train_time:57557ms step_avg:58.49ms
step:985/2330 train_time:57615ms step_avg:58.49ms
step:986/2330 train_time:57675ms step_avg:58.49ms
step:987/2330 train_time:57733ms step_avg:58.49ms
step:988/2330 train_time:57793ms step_avg:58.50ms
step:989/2330 train_time:57851ms step_avg:58.49ms
step:990/2330 train_time:57911ms step_avg:58.50ms
step:991/2330 train_time:57969ms step_avg:58.50ms
step:992/2330 train_time:58029ms step_avg:58.50ms
step:993/2330 train_time:58087ms step_avg:58.50ms
step:994/2330 train_time:58147ms step_avg:58.50ms
step:995/2330 train_time:58204ms step_avg:58.50ms
step:996/2330 train_time:58266ms step_avg:58.50ms
step:997/2330 train_time:58322ms step_avg:58.50ms
step:998/2330 train_time:58385ms step_avg:58.50ms
step:999/2330 train_time:58441ms step_avg:58.50ms
step:1000/2330 train_time:58504ms step_avg:58.50ms
step:1000/2330 val_loss:4.1866 train_time:58585ms step_avg:58.59ms
step:1001/2330 train_time:58603ms step_avg:58.54ms
step:1002/2330 train_time:58623ms step_avg:58.51ms
step:1003/2330 train_time:58681ms step_avg:58.51ms
step:1004/2330 train_time:58749ms step_avg:58.51ms
step:1005/2330 train_time:58807ms step_avg:58.51ms
step:1006/2330 train_time:58872ms step_avg:58.52ms
step:1007/2330 train_time:58929ms step_avg:58.52ms
step:1008/2330 train_time:58988ms step_avg:58.52ms
step:1009/2330 train_time:59044ms step_avg:58.52ms
step:1010/2330 train_time:59105ms step_avg:58.52ms
step:1011/2330 train_time:59162ms step_avg:58.52ms
step:1012/2330 train_time:59222ms step_avg:58.52ms
step:1013/2330 train_time:59278ms step_avg:58.52ms
step:1014/2330 train_time:59337ms step_avg:58.52ms
step:1015/2330 train_time:59393ms step_avg:58.52ms
step:1016/2330 train_time:59453ms step_avg:58.52ms
step:1017/2330 train_time:59511ms step_avg:58.52ms
step:1018/2330 train_time:59572ms step_avg:58.52ms
step:1019/2330 train_time:59631ms step_avg:58.52ms
step:1020/2330 train_time:59693ms step_avg:58.52ms
step:1021/2330 train_time:59752ms step_avg:58.52ms
step:1022/2330 train_time:59814ms step_avg:58.53ms
step:1023/2330 train_time:59872ms step_avg:58.53ms
step:1024/2330 train_time:59933ms step_avg:58.53ms
step:1025/2330 train_time:59990ms step_avg:58.53ms
step:1026/2330 train_time:60051ms step_avg:58.53ms
step:1027/2330 train_time:60108ms step_avg:58.53ms
step:1028/2330 train_time:60169ms step_avg:58.53ms
step:1029/2330 train_time:60226ms step_avg:58.53ms
step:1030/2330 train_time:60285ms step_avg:58.53ms
step:1031/2330 train_time:60342ms step_avg:58.53ms
step:1032/2330 train_time:60403ms step_avg:58.53ms
step:1033/2330 train_time:60460ms step_avg:58.53ms
step:1034/2330 train_time:60521ms step_avg:58.53ms
step:1035/2330 train_time:60579ms step_avg:58.53ms
step:1036/2330 train_time:60640ms step_avg:58.53ms
step:1037/2330 train_time:60699ms step_avg:58.53ms
step:1038/2330 train_time:60760ms step_avg:58.54ms
step:1039/2330 train_time:60820ms step_avg:58.54ms
step:1040/2330 train_time:60880ms step_avg:58.54ms
step:1041/2330 train_time:60938ms step_avg:58.54ms
step:1042/2330 train_time:60998ms step_avg:58.54ms
step:1043/2330 train_time:61056ms step_avg:58.54ms
step:1044/2330 train_time:61117ms step_avg:58.54ms
step:1045/2330 train_time:61174ms step_avg:58.54ms
step:1046/2330 train_time:61235ms step_avg:58.54ms
step:1047/2330 train_time:61292ms step_avg:58.54ms
step:1048/2330 train_time:61353ms step_avg:58.54ms
step:1049/2330 train_time:61410ms step_avg:58.54ms
step:1050/2330 train_time:61471ms step_avg:58.54ms
step:1051/2330 train_time:61528ms step_avg:58.54ms
step:1052/2330 train_time:61588ms step_avg:58.54ms
step:1053/2330 train_time:61645ms step_avg:58.54ms
step:1054/2330 train_time:61707ms step_avg:58.55ms
step:1055/2330 train_time:61765ms step_avg:58.54ms
step:1056/2330 train_time:61828ms step_avg:58.55ms
step:1057/2330 train_time:61886ms step_avg:58.55ms
step:1058/2330 train_time:61948ms step_avg:58.55ms
step:1059/2330 train_time:62005ms step_avg:58.55ms
step:1060/2330 train_time:62067ms step_avg:58.55ms
step:1061/2330 train_time:62124ms step_avg:58.55ms
step:1062/2330 train_time:62185ms step_avg:58.55ms
step:1063/2330 train_time:62242ms step_avg:58.55ms
step:1064/2330 train_time:62303ms step_avg:58.56ms
step:1065/2330 train_time:62360ms step_avg:58.55ms
step:1066/2330 train_time:62420ms step_avg:58.56ms
step:1067/2330 train_time:62477ms step_avg:58.55ms
step:1068/2330 train_time:62538ms step_avg:58.56ms
step:1069/2330 train_time:62595ms step_avg:58.55ms
step:1070/2330 train_time:62656ms step_avg:58.56ms
step:1071/2330 train_time:62713ms step_avg:58.56ms
step:1072/2330 train_time:62774ms step_avg:58.56ms
step:1073/2330 train_time:62831ms step_avg:58.56ms
step:1074/2330 train_time:62894ms step_avg:58.56ms
step:1075/2330 train_time:62952ms step_avg:58.56ms
step:1076/2330 train_time:63014ms step_avg:58.56ms
step:1077/2330 train_time:63071ms step_avg:58.56ms
step:1078/2330 train_time:63132ms step_avg:58.56ms
step:1079/2330 train_time:63188ms step_avg:58.56ms
step:1080/2330 train_time:63249ms step_avg:58.56ms
step:1081/2330 train_time:63306ms step_avg:58.56ms
step:1082/2330 train_time:63367ms step_avg:58.57ms
step:1083/2330 train_time:63424ms step_avg:58.56ms
step:1084/2330 train_time:63486ms step_avg:58.57ms
step:1085/2330 train_time:63543ms step_avg:58.56ms
step:1086/2330 train_time:63605ms step_avg:58.57ms
step:1087/2330 train_time:63662ms step_avg:58.57ms
step:1088/2330 train_time:63724ms step_avg:58.57ms
step:1089/2330 train_time:63782ms step_avg:58.57ms
step:1090/2330 train_time:63844ms step_avg:58.57ms
step:1091/2330 train_time:63903ms step_avg:58.57ms
step:1092/2330 train_time:63964ms step_avg:58.58ms
step:1093/2330 train_time:64023ms step_avg:58.58ms
step:1094/2330 train_time:64084ms step_avg:58.58ms
step:1095/2330 train_time:64142ms step_avg:58.58ms
step:1096/2330 train_time:64202ms step_avg:58.58ms
step:1097/2330 train_time:64260ms step_avg:58.58ms
step:1098/2330 train_time:64320ms step_avg:58.58ms
step:1099/2330 train_time:64377ms step_avg:58.58ms
step:1100/2330 train_time:64437ms step_avg:58.58ms
step:1101/2330 train_time:64494ms step_avg:58.58ms
step:1102/2330 train_time:64555ms step_avg:58.58ms
step:1103/2330 train_time:64613ms step_avg:58.58ms
step:1104/2330 train_time:64675ms step_avg:58.58ms
step:1105/2330 train_time:64731ms step_avg:58.58ms
step:1106/2330 train_time:64793ms step_avg:58.58ms
step:1107/2330 train_time:64851ms step_avg:58.58ms
step:1108/2330 train_time:64913ms step_avg:58.59ms
step:1109/2330 train_time:64970ms step_avg:58.58ms
step:1110/2330 train_time:65032ms step_avg:58.59ms
step:1111/2330 train_time:65088ms step_avg:58.59ms
step:1112/2330 train_time:65150ms step_avg:58.59ms
step:1113/2330 train_time:65207ms step_avg:58.59ms
step:1114/2330 train_time:65267ms step_avg:58.59ms
step:1115/2330 train_time:65325ms step_avg:58.59ms
step:1116/2330 train_time:65386ms step_avg:58.59ms
step:1117/2330 train_time:65443ms step_avg:58.59ms
step:1118/2330 train_time:65504ms step_avg:58.59ms
step:1119/2330 train_time:65563ms step_avg:58.59ms
step:1120/2330 train_time:65623ms step_avg:58.59ms
step:1121/2330 train_time:65682ms step_avg:58.59ms
step:1122/2330 train_time:65742ms step_avg:58.59ms
step:1123/2330 train_time:65800ms step_avg:58.59ms
step:1124/2330 train_time:65861ms step_avg:58.60ms
step:1125/2330 train_time:65920ms step_avg:58.60ms
step:1126/2330 train_time:65980ms step_avg:58.60ms
step:1127/2330 train_time:66038ms step_avg:58.60ms
step:1128/2330 train_time:66098ms step_avg:58.60ms
step:1129/2330 train_time:66157ms step_avg:58.60ms
step:1130/2330 train_time:66217ms step_avg:58.60ms
step:1131/2330 train_time:66274ms step_avg:58.60ms
step:1132/2330 train_time:66336ms step_avg:58.60ms
step:1133/2330 train_time:66392ms step_avg:58.60ms
step:1134/2330 train_time:66454ms step_avg:58.60ms
step:1135/2330 train_time:66511ms step_avg:58.60ms
step:1136/2330 train_time:66572ms step_avg:58.60ms
step:1137/2330 train_time:66629ms step_avg:58.60ms
step:1138/2330 train_time:66690ms step_avg:58.60ms
step:1139/2330 train_time:66747ms step_avg:58.60ms
step:1140/2330 train_time:66809ms step_avg:58.60ms
step:1141/2330 train_time:66866ms step_avg:58.60ms
step:1142/2330 train_time:66927ms step_avg:58.61ms
step:1143/2330 train_time:66984ms step_avg:58.60ms
step:1144/2330 train_time:67046ms step_avg:58.61ms
step:1145/2330 train_time:67104ms step_avg:58.61ms
step:1146/2330 train_time:67584ms step_avg:58.97ms
step:1147/2330 train_time:67640ms step_avg:58.97ms
step:1148/2330 train_time:67699ms step_avg:58.97ms
step:1149/2330 train_time:67756ms step_avg:58.97ms
step:1150/2330 train_time:67815ms step_avg:58.97ms
step:1151/2330 train_time:67872ms step_avg:58.97ms
step:1152/2330 train_time:67932ms step_avg:58.97ms
step:1153/2330 train_time:67987ms step_avg:58.97ms
step:1154/2330 train_time:68048ms step_avg:58.97ms
step:1155/2330 train_time:68105ms step_avg:58.96ms
step:1156/2330 train_time:68165ms step_avg:58.97ms
step:1157/2330 train_time:68222ms step_avg:58.96ms
step:1158/2330 train_time:68281ms step_avg:58.96ms
step:1159/2330 train_time:68337ms step_avg:58.96ms
step:1160/2330 train_time:68397ms step_avg:58.96ms
step:1161/2330 train_time:68461ms step_avg:58.97ms
step:1162/2330 train_time:68525ms step_avg:58.97ms
step:1163/2330 train_time:68584ms step_avg:58.97ms
step:1164/2330 train_time:68644ms step_avg:58.97ms
step:1165/2330 train_time:68701ms step_avg:58.97ms
step:1166/2330 train_time:68763ms step_avg:58.97ms
step:1167/2330 train_time:68821ms step_avg:58.97ms
step:1168/2330 train_time:68881ms step_avg:58.97ms
step:1169/2330 train_time:68938ms step_avg:58.97ms
step:1170/2330 train_time:68998ms step_avg:58.97ms
step:1171/2330 train_time:69056ms step_avg:58.97ms
step:1172/2330 train_time:69115ms step_avg:58.97ms
step:1173/2330 train_time:69172ms step_avg:58.97ms
step:1174/2330 train_time:69232ms step_avg:58.97ms
step:1175/2330 train_time:69288ms step_avg:58.97ms
step:1176/2330 train_time:69348ms step_avg:58.97ms
step:1177/2330 train_time:69406ms step_avg:58.97ms
step:1178/2330 train_time:69469ms step_avg:58.97ms
step:1179/2330 train_time:69527ms step_avg:58.97ms
step:1180/2330 train_time:69588ms step_avg:58.97ms
step:1181/2330 train_time:69646ms step_avg:58.97ms
step:1182/2330 train_time:69706ms step_avg:58.97ms
step:1183/2330 train_time:69764ms step_avg:58.97ms
step:1184/2330 train_time:69825ms step_avg:58.97ms
step:1185/2330 train_time:69882ms step_avg:58.97ms
step:1186/2330 train_time:69943ms step_avg:58.97ms
step:1187/2330 train_time:70000ms step_avg:58.97ms
step:1188/2330 train_time:70061ms step_avg:58.97ms
step:1189/2330 train_time:70118ms step_avg:58.97ms
step:1190/2330 train_time:70178ms step_avg:58.97ms
step:1191/2330 train_time:70235ms step_avg:58.97ms
step:1192/2330 train_time:70294ms step_avg:58.97ms
step:1193/2330 train_time:70352ms step_avg:58.97ms
step:1194/2330 train_time:70413ms step_avg:58.97ms
step:1195/2330 train_time:70471ms step_avg:58.97ms
step:1196/2330 train_time:70533ms step_avg:58.97ms
step:1197/2330 train_time:70591ms step_avg:58.97ms
step:1198/2330 train_time:70654ms step_avg:58.98ms
step:1199/2330 train_time:70710ms step_avg:58.97ms
step:1200/2330 train_time:70773ms step_avg:58.98ms
step:1201/2330 train_time:70829ms step_avg:58.98ms
step:1202/2330 train_time:70891ms step_avg:58.98ms
step:1203/2330 train_time:70948ms step_avg:58.98ms
step:1204/2330 train_time:71008ms step_avg:58.98ms
step:1205/2330 train_time:71065ms step_avg:58.98ms
step:1206/2330 train_time:71126ms step_avg:58.98ms
step:1207/2330 train_time:71183ms step_avg:58.97ms
step:1208/2330 train_time:71243ms step_avg:58.98ms
step:1209/2330 train_time:71301ms step_avg:58.97ms
step:1210/2330 train_time:71362ms step_avg:58.98ms
step:1211/2330 train_time:71420ms step_avg:58.98ms
step:1212/2330 train_time:71481ms step_avg:58.98ms
step:1213/2330 train_time:71540ms step_avg:58.98ms
step:1214/2330 train_time:71601ms step_avg:58.98ms
step:1215/2330 train_time:71659ms step_avg:58.98ms
step:1216/2330 train_time:71719ms step_avg:58.98ms
step:1217/2330 train_time:71778ms step_avg:58.98ms
step:1218/2330 train_time:71838ms step_avg:58.98ms
step:1219/2330 train_time:71896ms step_avg:58.98ms
step:1220/2330 train_time:71956ms step_avg:58.98ms
step:1221/2330 train_time:72014ms step_avg:58.98ms
step:1222/2330 train_time:72074ms step_avg:58.98ms
step:1223/2330 train_time:72131ms step_avg:58.98ms
step:1224/2330 train_time:72191ms step_avg:58.98ms
step:1225/2330 train_time:72249ms step_avg:58.98ms
step:1226/2330 train_time:72308ms step_avg:58.98ms
step:1227/2330 train_time:72366ms step_avg:58.98ms
step:1228/2330 train_time:72427ms step_avg:58.98ms
step:1229/2330 train_time:72484ms step_avg:58.98ms
step:1230/2330 train_time:72544ms step_avg:58.98ms
step:1231/2330 train_time:72603ms step_avg:58.98ms
step:1232/2330 train_time:72664ms step_avg:58.98ms
step:1233/2330 train_time:72722ms step_avg:58.98ms
step:1234/2330 train_time:72783ms step_avg:58.98ms
step:1235/2330 train_time:72840ms step_avg:58.98ms
step:1236/2330 train_time:72901ms step_avg:58.98ms
step:1237/2330 train_time:72960ms step_avg:58.98ms
step:1238/2330 train_time:73020ms step_avg:58.98ms
step:1239/2330 train_time:73079ms step_avg:58.98ms
step:1240/2330 train_time:73139ms step_avg:58.98ms
step:1241/2330 train_time:73197ms step_avg:58.98ms
step:1242/2330 train_time:73256ms step_avg:58.98ms
step:1243/2330 train_time:73313ms step_avg:58.98ms
step:1244/2330 train_time:73374ms step_avg:58.98ms
step:1245/2330 train_time:73431ms step_avg:58.98ms
step:1246/2330 train_time:73493ms step_avg:58.98ms
step:1247/2330 train_time:73550ms step_avg:58.98ms
step:1248/2330 train_time:73611ms step_avg:58.98ms
step:1249/2330 train_time:73669ms step_avg:58.98ms
step:1250/2330 train_time:73730ms step_avg:58.98ms
step:1250/2330 val_loss:4.0978 train_time:73811ms step_avg:59.05ms
step:1251/2330 train_time:73828ms step_avg:59.02ms
step:1252/2330 train_time:73850ms step_avg:58.99ms
step:1253/2330 train_time:73909ms step_avg:58.99ms
step:1254/2330 train_time:73977ms step_avg:58.99ms
step:1255/2330 train_time:74034ms step_avg:58.99ms
step:1256/2330 train_time:74097ms step_avg:58.99ms
step:1257/2330 train_time:74154ms step_avg:58.99ms
step:1258/2330 train_time:74215ms step_avg:58.99ms
step:1259/2330 train_time:74272ms step_avg:58.99ms
step:1260/2330 train_time:74332ms step_avg:58.99ms
step:1261/2330 train_time:74389ms step_avg:58.99ms
step:1262/2330 train_time:74448ms step_avg:58.99ms
step:1263/2330 train_time:74505ms step_avg:58.99ms
step:1264/2330 train_time:74565ms step_avg:58.99ms
step:1265/2330 train_time:74621ms step_avg:58.99ms
step:1266/2330 train_time:74682ms step_avg:58.99ms
step:1267/2330 train_time:74739ms step_avg:58.99ms
step:1268/2330 train_time:74802ms step_avg:58.99ms
step:1269/2330 train_time:74862ms step_avg:58.99ms
step:1270/2330 train_time:74923ms step_avg:58.99ms
step:1271/2330 train_time:74981ms step_avg:58.99ms
step:1272/2330 train_time:75044ms step_avg:59.00ms
step:1273/2330 train_time:75103ms step_avg:59.00ms
step:1274/2330 train_time:75165ms step_avg:59.00ms
step:1275/2330 train_time:75222ms step_avg:59.00ms
step:1276/2330 train_time:75283ms step_avg:59.00ms
step:1277/2330 train_time:75340ms step_avg:59.00ms
step:1278/2330 train_time:75400ms step_avg:59.00ms
step:1279/2330 train_time:75458ms step_avg:59.00ms
step:1280/2330 train_time:75517ms step_avg:59.00ms
step:1281/2330 train_time:75574ms step_avg:59.00ms
step:1282/2330 train_time:75635ms step_avg:59.00ms
step:1283/2330 train_time:75692ms step_avg:59.00ms
step:1284/2330 train_time:75753ms step_avg:59.00ms
step:1285/2330 train_time:75810ms step_avg:59.00ms
step:1286/2330 train_time:75874ms step_avg:59.00ms
step:1287/2330 train_time:75931ms step_avg:59.00ms
step:1288/2330 train_time:75994ms step_avg:59.00ms
step:1289/2330 train_time:76050ms step_avg:59.00ms
step:1290/2330 train_time:76114ms step_avg:59.00ms
step:1291/2330 train_time:76170ms step_avg:59.00ms
step:1292/2330 train_time:76233ms step_avg:59.00ms
step:1293/2330 train_time:76290ms step_avg:59.00ms
step:1294/2330 train_time:76351ms step_avg:59.00ms
step:1295/2330 train_time:76407ms step_avg:59.00ms
step:1296/2330 train_time:76469ms step_avg:59.00ms
step:1297/2330 train_time:76526ms step_avg:59.00ms
step:1298/2330 train_time:76588ms step_avg:59.00ms
step:1299/2330 train_time:76644ms step_avg:59.00ms
step:1300/2330 train_time:76707ms step_avg:59.01ms
step:1301/2330 train_time:76764ms step_avg:59.00ms
step:1302/2330 train_time:76825ms step_avg:59.01ms
step:1303/2330 train_time:76884ms step_avg:59.01ms
step:1304/2330 train_time:76944ms step_avg:59.01ms
step:1305/2330 train_time:77002ms step_avg:59.01ms
step:1306/2330 train_time:77064ms step_avg:59.01ms
step:1307/2330 train_time:77122ms step_avg:59.01ms
step:1308/2330 train_time:77184ms step_avg:59.01ms
step:1309/2330 train_time:77242ms step_avg:59.01ms
step:1310/2330 train_time:77302ms step_avg:59.01ms
step:1311/2330 train_time:77360ms step_avg:59.01ms
step:1312/2330 train_time:77420ms step_avg:59.01ms
step:1313/2330 train_time:77477ms step_avg:59.01ms
step:1314/2330 train_time:77537ms step_avg:59.01ms
step:1315/2330 train_time:77594ms step_avg:59.01ms
step:1316/2330 train_time:77654ms step_avg:59.01ms
step:1317/2330 train_time:77711ms step_avg:59.01ms
step:1318/2330 train_time:77773ms step_avg:59.01ms
step:1319/2330 train_time:77829ms step_avg:59.01ms
step:1320/2330 train_time:77892ms step_avg:59.01ms
step:1321/2330 train_time:77950ms step_avg:59.01ms
step:1322/2330 train_time:78011ms step_avg:59.01ms
step:1323/2330 train_time:78068ms step_avg:59.01ms
step:1324/2330 train_time:78130ms step_avg:59.01ms
step:1325/2330 train_time:78188ms step_avg:59.01ms
step:1326/2330 train_time:78249ms step_avg:59.01ms
step:1327/2330 train_time:78305ms step_avg:59.01ms
step:1328/2330 train_time:78368ms step_avg:59.01ms
step:1329/2330 train_time:78425ms step_avg:59.01ms
step:1330/2330 train_time:78487ms step_avg:59.01ms
step:1331/2330 train_time:78544ms step_avg:59.01ms
step:1332/2330 train_time:78604ms step_avg:59.01ms
step:1333/2330 train_time:78663ms step_avg:59.01ms
step:1334/2330 train_time:78723ms step_avg:59.01ms
step:1335/2330 train_time:78781ms step_avg:59.01ms
step:1336/2330 train_time:78841ms step_avg:59.01ms
step:1337/2330 train_time:78899ms step_avg:59.01ms
step:1338/2330 train_time:78959ms step_avg:59.01ms
step:1339/2330 train_time:79017ms step_avg:59.01ms
step:1340/2330 train_time:79077ms step_avg:59.01ms
step:1341/2330 train_time:79134ms step_avg:59.01ms
step:1342/2330 train_time:79196ms step_avg:59.01ms
step:1343/2330 train_time:79253ms step_avg:59.01ms
step:1344/2330 train_time:79315ms step_avg:59.01ms
step:1345/2330 train_time:79371ms step_avg:59.01ms
step:1346/2330 train_time:79434ms step_avg:59.01ms
step:1347/2330 train_time:79490ms step_avg:59.01ms
step:1348/2330 train_time:79552ms step_avg:59.01ms
step:1349/2330 train_time:79608ms step_avg:59.01ms
step:1350/2330 train_time:79669ms step_avg:59.01ms
step:1351/2330 train_time:79725ms step_avg:59.01ms
step:1352/2330 train_time:79788ms step_avg:59.01ms
step:1353/2330 train_time:79845ms step_avg:59.01ms
step:1354/2330 train_time:79906ms step_avg:59.01ms
step:1355/2330 train_time:79964ms step_avg:59.01ms
step:1356/2330 train_time:80026ms step_avg:59.02ms
step:1357/2330 train_time:80084ms step_avg:59.02ms
step:1358/2330 train_time:80145ms step_avg:59.02ms
step:1359/2330 train_time:80202ms step_avg:59.02ms
step:1360/2330 train_time:80264ms step_avg:59.02ms
step:1361/2330 train_time:80321ms step_avg:59.02ms
step:1362/2330 train_time:80383ms step_avg:59.02ms
step:1363/2330 train_time:80441ms step_avg:59.02ms
step:1364/2330 train_time:80500ms step_avg:59.02ms
step:1365/2330 train_time:80558ms step_avg:59.02ms
step:1366/2330 train_time:80618ms step_avg:59.02ms
step:1367/2330 train_time:80675ms step_avg:59.02ms
step:1368/2330 train_time:80736ms step_avg:59.02ms
step:1369/2330 train_time:80792ms step_avg:59.02ms
step:1370/2330 train_time:80854ms step_avg:59.02ms
step:1371/2330 train_time:80910ms step_avg:59.02ms
step:1372/2330 train_time:80974ms step_avg:59.02ms
step:1373/2330 train_time:81031ms step_avg:59.02ms
step:1374/2330 train_time:81091ms step_avg:59.02ms
step:1375/2330 train_time:81148ms step_avg:59.02ms
step:1376/2330 train_time:81209ms step_avg:59.02ms
step:1377/2330 train_time:81267ms step_avg:59.02ms
step:1378/2330 train_time:81328ms step_avg:59.02ms
step:1379/2330 train_time:81386ms step_avg:59.02ms
step:1380/2330 train_time:81446ms step_avg:59.02ms
step:1381/2330 train_time:81503ms step_avg:59.02ms
step:1382/2330 train_time:81564ms step_avg:59.02ms
step:1383/2330 train_time:81622ms step_avg:59.02ms
step:1384/2330 train_time:81683ms step_avg:59.02ms
step:1385/2330 train_time:81741ms step_avg:59.02ms
step:1386/2330 train_time:81802ms step_avg:59.02ms
step:1387/2330 train_time:81860ms step_avg:59.02ms
step:1388/2330 train_time:81920ms step_avg:59.02ms
step:1389/2330 train_time:81979ms step_avg:59.02ms
step:1390/2330 train_time:82039ms step_avg:59.02ms
step:1391/2330 train_time:82095ms step_avg:59.02ms
step:1392/2330 train_time:82157ms step_avg:59.02ms
step:1393/2330 train_time:82214ms step_avg:59.02ms
step:1394/2330 train_time:82276ms step_avg:59.02ms
step:1395/2330 train_time:82333ms step_avg:59.02ms
step:1396/2330 train_time:82395ms step_avg:59.02ms
step:1397/2330 train_time:82451ms step_avg:59.02ms
step:1398/2330 train_time:82513ms step_avg:59.02ms
step:1399/2330 train_time:82569ms step_avg:59.02ms
step:1400/2330 train_time:82630ms step_avg:59.02ms
step:1401/2330 train_time:82687ms step_avg:59.02ms
step:1402/2330 train_time:82748ms step_avg:59.02ms
step:1403/2330 train_time:82805ms step_avg:59.02ms
step:1404/2330 train_time:82868ms step_avg:59.02ms
step:1405/2330 train_time:82925ms step_avg:59.02ms
step:1406/2330 train_time:82987ms step_avg:59.02ms
step:1407/2330 train_time:83045ms step_avg:59.02ms
step:1408/2330 train_time:83105ms step_avg:59.02ms
step:1409/2330 train_time:83163ms step_avg:59.02ms
step:1410/2330 train_time:83224ms step_avg:59.02ms
step:1411/2330 train_time:83283ms step_avg:59.02ms
step:1412/2330 train_time:83343ms step_avg:59.03ms
step:1413/2330 train_time:83401ms step_avg:59.02ms
step:1414/2330 train_time:83461ms step_avg:59.02ms
step:1415/2330 train_time:83519ms step_avg:59.02ms
step:1416/2330 train_time:83579ms step_avg:59.02ms
step:1417/2330 train_time:83636ms step_avg:59.02ms
step:1418/2330 train_time:83696ms step_avg:59.02ms
step:1419/2330 train_time:83753ms step_avg:59.02ms
step:1420/2330 train_time:83815ms step_avg:59.02ms
step:1421/2330 train_time:83872ms step_avg:59.02ms
step:1422/2330 train_time:83935ms step_avg:59.03ms
step:1423/2330 train_time:83992ms step_avg:59.02ms
step:1424/2330 train_time:84053ms step_avg:59.03ms
step:1425/2330 train_time:84109ms step_avg:59.02ms
step:1426/2330 train_time:84171ms step_avg:59.03ms
step:1427/2330 train_time:84228ms step_avg:59.02ms
step:1428/2330 train_time:84289ms step_avg:59.03ms
step:1429/2330 train_time:84346ms step_avg:59.02ms
step:1430/2330 train_time:84407ms step_avg:59.03ms
step:1431/2330 train_time:84464ms step_avg:59.02ms
step:1432/2330 train_time:84526ms step_avg:59.03ms
step:1433/2330 train_time:84583ms step_avg:59.03ms
step:1434/2330 train_time:84645ms step_avg:59.03ms
step:1435/2330 train_time:84702ms step_avg:59.03ms
step:1436/2330 train_time:84765ms step_avg:59.03ms
step:1437/2330 train_time:84823ms step_avg:59.03ms
step:1438/2330 train_time:84885ms step_avg:59.03ms
step:1439/2330 train_time:84942ms step_avg:59.03ms
step:1440/2330 train_time:85004ms step_avg:59.03ms
step:1441/2330 train_time:85062ms step_avg:59.03ms
step:1442/2330 train_time:85122ms step_avg:59.03ms
step:1443/2330 train_time:85179ms step_avg:59.03ms
step:1444/2330 train_time:85239ms step_avg:59.03ms
step:1445/2330 train_time:85297ms step_avg:59.03ms
step:1446/2330 train_time:85356ms step_avg:59.03ms
step:1447/2330 train_time:85413ms step_avg:59.03ms
step:1448/2330 train_time:85474ms step_avg:59.03ms
step:1449/2330 train_time:85531ms step_avg:59.03ms
step:1450/2330 train_time:85592ms step_avg:59.03ms
step:1451/2330 train_time:85650ms step_avg:59.03ms
step:1452/2330 train_time:85710ms step_avg:59.03ms
step:1453/2330 train_time:85766ms step_avg:59.03ms
step:1454/2330 train_time:85828ms step_avg:59.03ms
step:1455/2330 train_time:85886ms step_avg:59.03ms
step:1456/2330 train_time:85947ms step_avg:59.03ms
step:1457/2330 train_time:86005ms step_avg:59.03ms
step:1458/2330 train_time:86065ms step_avg:59.03ms
step:1459/2330 train_time:86123ms step_avg:59.03ms
step:1460/2330 train_time:86183ms step_avg:59.03ms
step:1461/2330 train_time:86241ms step_avg:59.03ms
step:1462/2330 train_time:86303ms step_avg:59.03ms
step:1463/2330 train_time:86360ms step_avg:59.03ms
step:1464/2330 train_time:86423ms step_avg:59.03ms
step:1465/2330 train_time:86481ms step_avg:59.03ms
step:1466/2330 train_time:86541ms step_avg:59.03ms
step:1467/2330 train_time:86598ms step_avg:59.03ms
step:1468/2330 train_time:86659ms step_avg:59.03ms
step:1469/2330 train_time:86716ms step_avg:59.03ms
step:1470/2330 train_time:86777ms step_avg:59.03ms
step:1471/2330 train_time:86834ms step_avg:59.03ms
step:1472/2330 train_time:86897ms step_avg:59.03ms
step:1473/2330 train_time:86954ms step_avg:59.03ms
step:1474/2330 train_time:87015ms step_avg:59.03ms
step:1475/2330 train_time:87072ms step_avg:59.03ms
step:1476/2330 train_time:87133ms step_avg:59.03ms
step:1477/2330 train_time:87189ms step_avg:59.03ms
step:1478/2330 train_time:87250ms step_avg:59.03ms
step:1479/2330 train_time:87307ms step_avg:59.03ms
step:1480/2330 train_time:87368ms step_avg:59.03ms
step:1481/2330 train_time:87425ms step_avg:59.03ms
step:1482/2330 train_time:87488ms step_avg:59.03ms
step:1483/2330 train_time:87545ms step_avg:59.03ms
step:1484/2330 train_time:87605ms step_avg:59.03ms
step:1485/2330 train_time:87663ms step_avg:59.03ms
step:1486/2330 train_time:87724ms step_avg:59.03ms
step:1487/2330 train_time:87781ms step_avg:59.03ms
step:1488/2330 train_time:87842ms step_avg:59.03ms
step:1489/2330 train_time:87901ms step_avg:59.03ms
step:1490/2330 train_time:87961ms step_avg:59.03ms
step:1491/2330 train_time:88018ms step_avg:59.03ms
step:1492/2330 train_time:88078ms step_avg:59.03ms
step:1493/2330 train_time:88135ms step_avg:59.03ms
step:1494/2330 train_time:88195ms step_avg:59.03ms
step:1495/2330 train_time:88252ms step_avg:59.03ms
step:1496/2330 train_time:88313ms step_avg:59.03ms
step:1497/2330 train_time:88371ms step_avg:59.03ms
step:1498/2330 train_time:88432ms step_avg:59.03ms
step:1499/2330 train_time:88489ms step_avg:59.03ms
step:1500/2330 train_time:88550ms step_avg:59.03ms
step:1500/2330 val_loss:4.0170 train_time:88632ms step_avg:59.09ms
step:1501/2330 train_time:88649ms step_avg:59.06ms
step:1502/2330 train_time:88670ms step_avg:59.03ms
step:1503/2330 train_time:88728ms step_avg:59.03ms
step:1504/2330 train_time:88794ms step_avg:59.04ms
step:1505/2330 train_time:88850ms step_avg:59.04ms
step:1506/2330 train_time:88912ms step_avg:59.04ms
step:1507/2330 train_time:88969ms step_avg:59.04ms
step:1508/2330 train_time:89029ms step_avg:59.04ms
step:1509/2330 train_time:89086ms step_avg:59.04ms
step:1510/2330 train_time:89146ms step_avg:59.04ms
step:1511/2330 train_time:89203ms step_avg:59.04ms
step:1512/2330 train_time:89262ms step_avg:59.04ms
step:1513/2330 train_time:89319ms step_avg:59.03ms
step:1514/2330 train_time:89379ms step_avg:59.03ms
step:1515/2330 train_time:89436ms step_avg:59.03ms
step:1516/2330 train_time:89496ms step_avg:59.03ms
step:1517/2330 train_time:89554ms step_avg:59.03ms
step:1518/2330 train_time:89616ms step_avg:59.04ms
step:1519/2330 train_time:89675ms step_avg:59.04ms
step:1520/2330 train_time:89737ms step_avg:59.04ms
step:1521/2330 train_time:89795ms step_avg:59.04ms
step:1522/2330 train_time:89858ms step_avg:59.04ms
step:1523/2330 train_time:89915ms step_avg:59.04ms
step:1524/2330 train_time:89977ms step_avg:59.04ms
step:1525/2330 train_time:90034ms step_avg:59.04ms
step:1526/2330 train_time:90095ms step_avg:59.04ms
step:1527/2330 train_time:90153ms step_avg:59.04ms
step:1528/2330 train_time:90214ms step_avg:59.04ms
step:1529/2330 train_time:90273ms step_avg:59.04ms
step:1530/2330 train_time:90332ms step_avg:59.04ms
step:1531/2330 train_time:90390ms step_avg:59.04ms
step:1532/2330 train_time:90450ms step_avg:59.04ms
step:1533/2330 train_time:90508ms step_avg:59.04ms
step:1534/2330 train_time:90568ms step_avg:59.04ms
step:1535/2330 train_time:90627ms step_avg:59.04ms
step:1536/2330 train_time:90688ms step_avg:59.04ms
step:1537/2330 train_time:90745ms step_avg:59.04ms
step:1538/2330 train_time:90809ms step_avg:59.04ms
step:1539/2330 train_time:90866ms step_avg:59.04ms
step:1540/2330 train_time:90929ms step_avg:59.04ms
step:1541/2330 train_time:90986ms step_avg:59.04ms
step:1542/2330 train_time:91048ms step_avg:59.05ms
step:1543/2330 train_time:91106ms step_avg:59.04ms
step:1544/2330 train_time:91168ms step_avg:59.05ms
step:1545/2330 train_time:91225ms step_avg:59.05ms
step:1546/2330 train_time:91287ms step_avg:59.05ms
step:1547/2330 train_time:91344ms step_avg:59.05ms
step:1548/2330 train_time:91405ms step_avg:59.05ms
step:1549/2330 train_time:91462ms step_avg:59.05ms
step:1550/2330 train_time:91524ms step_avg:59.05ms
step:1551/2330 train_time:91581ms step_avg:59.05ms
step:1552/2330 train_time:91642ms step_avg:59.05ms
step:1553/2330 train_time:91700ms step_avg:59.05ms
step:1554/2330 train_time:91761ms step_avg:59.05ms
step:1555/2330 train_time:91819ms step_avg:59.05ms
step:1556/2330 train_time:91881ms step_avg:59.05ms
step:1557/2330 train_time:91938ms step_avg:59.05ms
step:1558/2330 train_time:92002ms step_avg:59.05ms
step:1559/2330 train_time:92058ms step_avg:59.05ms
step:1560/2330 train_time:92121ms step_avg:59.05ms
step:1561/2330 train_time:92179ms step_avg:59.05ms
step:1562/2330 train_time:92240ms step_avg:59.05ms
step:1563/2330 train_time:92297ms step_avg:59.05ms
step:1564/2330 train_time:92358ms step_avg:59.05ms
step:1565/2330 train_time:92417ms step_avg:59.05ms
step:1566/2330 train_time:92477ms step_avg:59.05ms
step:1567/2330 train_time:92535ms step_avg:59.05ms
step:1568/2330 train_time:92596ms step_avg:59.05ms
step:1569/2330 train_time:92654ms step_avg:59.05ms
step:1570/2330 train_time:92716ms step_avg:59.05ms
step:1571/2330 train_time:92774ms step_avg:59.05ms
step:1572/2330 train_time:92836ms step_avg:59.06ms
step:1573/2330 train_time:92895ms step_avg:59.06ms
step:1574/2330 train_time:92957ms step_avg:59.06ms
step:1575/2330 train_time:93015ms step_avg:59.06ms
step:1576/2330 train_time:93077ms step_avg:59.06ms
step:1577/2330 train_time:93134ms step_avg:59.06ms
step:1578/2330 train_time:93196ms step_avg:59.06ms
step:1579/2330 train_time:93254ms step_avg:59.06ms
step:1580/2330 train_time:93315ms step_avg:59.06ms
step:1581/2330 train_time:93373ms step_avg:59.06ms
step:1582/2330 train_time:93434ms step_avg:59.06ms
step:1583/2330 train_time:93492ms step_avg:59.06ms
step:1584/2330 train_time:93553ms step_avg:59.06ms
step:1585/2330 train_time:93611ms step_avg:59.06ms
step:1586/2330 train_time:93671ms step_avg:59.06ms
step:1587/2330 train_time:93729ms step_avg:59.06ms
step:1588/2330 train_time:93791ms step_avg:59.06ms
step:1589/2330 train_time:93848ms step_avg:59.06ms
step:1590/2330 train_time:93911ms step_avg:59.06ms
step:1591/2330 train_time:93969ms step_avg:59.06ms
step:1592/2330 train_time:94030ms step_avg:59.06ms
step:1593/2330 train_time:94088ms step_avg:59.06ms
step:1594/2330 train_time:94149ms step_avg:59.06ms
step:1595/2330 train_time:94206ms step_avg:59.06ms
step:1596/2330 train_time:94268ms step_avg:59.07ms
step:1597/2330 train_time:94326ms step_avg:59.06ms
step:1598/2330 train_time:94387ms step_avg:59.07ms
step:1599/2330 train_time:94444ms step_avg:59.06ms
step:1600/2330 train_time:94507ms step_avg:59.07ms
step:1601/2330 train_time:94564ms step_avg:59.07ms
step:1602/2330 train_time:94624ms step_avg:59.07ms
step:1603/2330 train_time:94682ms step_avg:59.07ms
step:1604/2330 train_time:94743ms step_avg:59.07ms
step:1605/2330 train_time:94800ms step_avg:59.07ms
step:1606/2330 train_time:94861ms step_avg:59.07ms
step:1607/2330 train_time:94919ms step_avg:59.07ms
step:1608/2330 train_time:94981ms step_avg:59.07ms
step:1609/2330 train_time:95038ms step_avg:59.07ms
step:1610/2330 train_time:95100ms step_avg:59.07ms
step:1611/2330 train_time:95157ms step_avg:59.07ms
step:1612/2330 train_time:95219ms step_avg:59.07ms
step:1613/2330 train_time:95276ms step_avg:59.07ms
step:1614/2330 train_time:95339ms step_avg:59.07ms
step:1615/2330 train_time:95397ms step_avg:59.07ms
step:1616/2330 train_time:95459ms step_avg:59.07ms
step:1617/2330 train_time:95517ms step_avg:59.07ms
step:1618/2330 train_time:95578ms step_avg:59.07ms
step:1619/2330 train_time:95636ms step_avg:59.07ms
step:1620/2330 train_time:95697ms step_avg:59.07ms
step:1621/2330 train_time:95755ms step_avg:59.07ms
step:1622/2330 train_time:95818ms step_avg:59.07ms
step:1623/2330 train_time:95875ms step_avg:59.07ms
step:1624/2330 train_time:95936ms step_avg:59.07ms
step:1625/2330 train_time:95995ms step_avg:59.07ms
step:1626/2330 train_time:96057ms step_avg:59.08ms
step:1627/2330 train_time:96115ms step_avg:59.07ms
step:1628/2330 train_time:96177ms step_avg:59.08ms
step:1629/2330 train_time:96235ms step_avg:59.08ms
step:1630/2330 train_time:96296ms step_avg:59.08ms
step:1631/2330 train_time:96353ms step_avg:59.08ms
step:1632/2330 train_time:96414ms step_avg:59.08ms
step:1633/2330 train_time:96472ms step_avg:59.08ms
step:1634/2330 train_time:96533ms step_avg:59.08ms
step:1635/2330 train_time:96590ms step_avg:59.08ms
step:1636/2330 train_time:96653ms step_avg:59.08ms
step:1637/2330 train_time:96711ms step_avg:59.08ms
step:1638/2330 train_time:96772ms step_avg:59.08ms
step:1639/2330 train_time:96830ms step_avg:59.08ms
step:1640/2330 train_time:96891ms step_avg:59.08ms
step:1641/2330 train_time:96949ms step_avg:59.08ms
step:1642/2330 train_time:97010ms step_avg:59.08ms
step:1643/2330 train_time:97069ms step_avg:59.08ms
step:1644/2330 train_time:97129ms step_avg:59.08ms
step:1645/2330 train_time:97186ms step_avg:59.08ms
step:1646/2330 train_time:97248ms step_avg:59.08ms
step:1647/2330 train_time:97306ms step_avg:59.08ms
step:1648/2330 train_time:97367ms step_avg:59.08ms
step:1649/2330 train_time:97424ms step_avg:59.08ms
step:1650/2330 train_time:97486ms step_avg:59.08ms
step:1651/2330 train_time:97543ms step_avg:59.08ms
step:1652/2330 train_time:97605ms step_avg:59.08ms
step:1653/2330 train_time:97662ms step_avg:59.08ms
step:1654/2330 train_time:97725ms step_avg:59.08ms
step:1655/2330 train_time:97782ms step_avg:59.08ms
step:1656/2330 train_time:97844ms step_avg:59.08ms
step:1657/2330 train_time:97901ms step_avg:59.08ms
step:1658/2330 train_time:97963ms step_avg:59.08ms
step:1659/2330 train_time:98019ms step_avg:59.08ms
step:1660/2330 train_time:98082ms step_avg:59.09ms
step:1661/2330 train_time:98139ms step_avg:59.08ms
step:1662/2330 train_time:98202ms step_avg:59.09ms
step:1663/2330 train_time:98259ms step_avg:59.09ms
step:1664/2330 train_time:98321ms step_avg:59.09ms
step:1665/2330 train_time:98378ms step_avg:59.09ms
step:1666/2330 train_time:98440ms step_avg:59.09ms
step:1667/2330 train_time:98498ms step_avg:59.09ms
step:1668/2330 train_time:98558ms step_avg:59.09ms
step:1669/2330 train_time:98616ms step_avg:59.09ms
step:1670/2330 train_time:98678ms step_avg:59.09ms
step:1671/2330 train_time:98736ms step_avg:59.09ms
step:1672/2330 train_time:98796ms step_avg:59.09ms
step:1673/2330 train_time:98854ms step_avg:59.09ms
step:1674/2330 train_time:98916ms step_avg:59.09ms
step:1675/2330 train_time:98974ms step_avg:59.09ms
step:1676/2330 train_time:99035ms step_avg:59.09ms
step:1677/2330 train_time:99092ms step_avg:59.09ms
step:1678/2330 train_time:99154ms step_avg:59.09ms
step:1679/2330 train_time:99212ms step_avg:59.09ms
step:1680/2330 train_time:99274ms step_avg:59.09ms
step:1681/2330 train_time:99333ms step_avg:59.09ms
step:1682/2330 train_time:99394ms step_avg:59.09ms
step:1683/2330 train_time:99451ms step_avg:59.09ms
step:1684/2330 train_time:99514ms step_avg:59.09ms
step:1685/2330 train_time:99573ms step_avg:59.09ms
step:1686/2330 train_time:99634ms step_avg:59.09ms
step:1687/2330 train_time:99691ms step_avg:59.09ms
step:1688/2330 train_time:99753ms step_avg:59.10ms
step:1689/2330 train_time:99810ms step_avg:59.09ms
step:1690/2330 train_time:99871ms step_avg:59.10ms
step:1691/2330 train_time:99929ms step_avg:59.09ms
step:1692/2330 train_time:99990ms step_avg:59.10ms
step:1693/2330 train_time:100048ms step_avg:59.09ms
step:1694/2330 train_time:100109ms step_avg:59.10ms
step:1695/2330 train_time:100168ms step_avg:59.10ms
step:1696/2330 train_time:100229ms step_avg:59.10ms
step:1697/2330 train_time:100286ms step_avg:59.10ms
step:1698/2330 train_time:100349ms step_avg:59.10ms
step:1699/2330 train_time:100407ms step_avg:59.10ms
step:1700/2330 train_time:100469ms step_avg:59.10ms
step:1701/2330 train_time:100527ms step_avg:59.10ms
step:1702/2330 train_time:100588ms step_avg:59.10ms
step:1703/2330 train_time:100646ms step_avg:59.10ms
step:1704/2330 train_time:100708ms step_avg:59.10ms
step:1705/2330 train_time:100765ms step_avg:59.10ms
step:1706/2330 train_time:100826ms step_avg:59.10ms
step:1707/2330 train_time:100883ms step_avg:59.10ms
step:1708/2330 train_time:100945ms step_avg:59.10ms
step:1709/2330 train_time:101002ms step_avg:59.10ms
step:1710/2330 train_time:101064ms step_avg:59.10ms
step:1711/2330 train_time:101122ms step_avg:59.10ms
step:1712/2330 train_time:101184ms step_avg:59.10ms
step:1713/2330 train_time:101241ms step_avg:59.10ms
step:1714/2330 train_time:101303ms step_avg:59.10ms
step:1715/2330 train_time:101360ms step_avg:59.10ms
step:1716/2330 train_time:101423ms step_avg:59.10ms
step:1717/2330 train_time:101480ms step_avg:59.10ms
step:1718/2330 train_time:101541ms step_avg:59.10ms
step:1719/2330 train_time:101598ms step_avg:59.10ms
step:1720/2330 train_time:101660ms step_avg:59.10ms
step:1721/2330 train_time:101718ms step_avg:59.10ms
step:1722/2330 train_time:101779ms step_avg:59.11ms
step:1723/2330 train_time:101837ms step_avg:59.10ms
step:1724/2330 train_time:101898ms step_avg:59.11ms
step:1725/2330 train_time:101955ms step_avg:59.10ms
step:1726/2330 train_time:102018ms step_avg:59.11ms
step:1727/2330 train_time:102076ms step_avg:59.11ms
step:1728/2330 train_time:102137ms step_avg:59.11ms
step:1729/2330 train_time:102195ms step_avg:59.11ms
step:1730/2330 train_time:102257ms step_avg:59.11ms
step:1731/2330 train_time:102315ms step_avg:59.11ms
step:1732/2330 train_time:102376ms step_avg:59.11ms
step:1733/2330 train_time:102434ms step_avg:59.11ms
step:1734/2330 train_time:102497ms step_avg:59.11ms
step:1735/2330 train_time:102555ms step_avg:59.11ms
step:1736/2330 train_time:102616ms step_avg:59.11ms
step:1737/2330 train_time:102674ms step_avg:59.11ms
step:1738/2330 train_time:102735ms step_avg:59.11ms
step:1739/2330 train_time:102792ms step_avg:59.11ms
step:1740/2330 train_time:102854ms step_avg:59.11ms
step:1741/2330 train_time:102912ms step_avg:59.11ms
step:1742/2330 train_time:102973ms step_avg:59.11ms
step:1743/2330 train_time:103031ms step_avg:59.11ms
step:1744/2330 train_time:103092ms step_avg:59.11ms
step:1745/2330 train_time:103150ms step_avg:59.11ms
step:1746/2330 train_time:103211ms step_avg:59.11ms
step:1747/2330 train_time:103270ms step_avg:59.11ms
step:1748/2330 train_time:103331ms step_avg:59.11ms
step:1749/2330 train_time:103390ms step_avg:59.11ms
step:1750/2330 train_time:103450ms step_avg:59.11ms
step:1750/2330 val_loss:3.9129 train_time:103533ms step_avg:59.16ms
step:1751/2330 train_time:103552ms step_avg:59.14ms
step:1752/2330 train_time:103572ms step_avg:59.12ms
step:1753/2330 train_time:103629ms step_avg:59.12ms
step:1754/2330 train_time:103697ms step_avg:59.12ms
step:1755/2330 train_time:103754ms step_avg:59.12ms
step:1756/2330 train_time:103818ms step_avg:59.12ms
step:1757/2330 train_time:103876ms step_avg:59.12ms
step:1758/2330 train_time:103936ms step_avg:59.12ms
step:1759/2330 train_time:103993ms step_avg:59.12ms
step:1760/2330 train_time:104054ms step_avg:59.12ms
step:1761/2330 train_time:104111ms step_avg:59.12ms
step:1762/2330 train_time:104171ms step_avg:59.12ms
step:1763/2330 train_time:104227ms step_avg:59.12ms
step:1764/2330 train_time:104289ms step_avg:59.12ms
step:1765/2330 train_time:104346ms step_avg:59.12ms
step:1766/2330 train_time:104406ms step_avg:59.12ms
step:1767/2330 train_time:104465ms step_avg:59.12ms
step:1768/2330 train_time:104529ms step_avg:59.12ms
step:1769/2330 train_time:104588ms step_avg:59.12ms
step:1770/2330 train_time:104652ms step_avg:59.13ms
step:1771/2330 train_time:104709ms step_avg:59.12ms
step:1772/2330 train_time:104771ms step_avg:59.13ms
step:1773/2330 train_time:104828ms step_avg:59.12ms
step:1774/2330 train_time:104891ms step_avg:59.13ms
step:1775/2330 train_time:104948ms step_avg:59.13ms
step:1776/2330 train_time:105010ms step_avg:59.13ms
step:1777/2330 train_time:105067ms step_avg:59.13ms
step:1778/2330 train_time:105128ms step_avg:59.13ms
step:1779/2330 train_time:105185ms step_avg:59.13ms
step:1780/2330 train_time:105246ms step_avg:59.13ms
step:1781/2330 train_time:105303ms step_avg:59.13ms
step:1782/2330 train_time:105364ms step_avg:59.13ms
step:1783/2330 train_time:105422ms step_avg:59.13ms
step:1784/2330 train_time:105484ms step_avg:59.13ms
step:1785/2330 train_time:105542ms step_avg:59.13ms
step:1786/2330 train_time:105605ms step_avg:59.13ms
step:1787/2330 train_time:105663ms step_avg:59.13ms
step:1788/2330 train_time:105726ms step_avg:59.13ms
step:1789/2330 train_time:105783ms step_avg:59.13ms
step:1790/2330 train_time:105844ms step_avg:59.13ms
step:1791/2330 train_time:105903ms step_avg:59.13ms
step:1792/2330 train_time:105964ms step_avg:59.13ms
step:1793/2330 train_time:106022ms step_avg:59.13ms
step:1794/2330 train_time:106083ms step_avg:59.13ms
step:1795/2330 train_time:106140ms step_avg:59.13ms
step:1796/2330 train_time:106201ms step_avg:59.13ms
step:1797/2330 train_time:106258ms step_avg:59.13ms
step:1798/2330 train_time:106318ms step_avg:59.13ms
step:1799/2330 train_time:106376ms step_avg:59.13ms
step:1800/2330 train_time:106437ms step_avg:59.13ms
step:1801/2330 train_time:106496ms step_avg:59.13ms
step:1802/2330 train_time:106559ms step_avg:59.13ms
step:1803/2330 train_time:106618ms step_avg:59.13ms
step:1804/2330 train_time:106679ms step_avg:59.13ms
step:1805/2330 train_time:106738ms step_avg:59.13ms
step:1806/2330 train_time:106798ms step_avg:59.14ms
step:1807/2330 train_time:106857ms step_avg:59.14ms
step:1808/2330 train_time:106919ms step_avg:59.14ms
step:1809/2330 train_time:106977ms step_avg:59.14ms
step:1810/2330 train_time:107037ms step_avg:59.14ms
step:1811/2330 train_time:107094ms step_avg:59.14ms
step:1812/2330 train_time:107155ms step_avg:59.14ms
step:1813/2330 train_time:107211ms step_avg:59.13ms
step:1814/2330 train_time:107273ms step_avg:59.14ms
step:1815/2330 train_time:107330ms step_avg:59.14ms
step:1816/2330 train_time:107391ms step_avg:59.14ms
step:1817/2330 train_time:107450ms step_avg:59.14ms
step:1818/2330 train_time:107512ms step_avg:59.14ms
step:1819/2330 train_time:107569ms step_avg:59.14ms
step:1820/2330 train_time:107632ms step_avg:59.14ms
step:1821/2330 train_time:107690ms step_avg:59.14ms
step:1822/2330 train_time:107751ms step_avg:59.14ms
step:1823/2330 train_time:107808ms step_avg:59.14ms
step:1824/2330 train_time:107870ms step_avg:59.14ms
step:1825/2330 train_time:107928ms step_avg:59.14ms
step:1826/2330 train_time:107989ms step_avg:59.14ms
step:1827/2330 train_time:108046ms step_avg:59.14ms
step:1828/2330 train_time:108109ms step_avg:59.14ms
step:1829/2330 train_time:108166ms step_avg:59.14ms
step:1830/2330 train_time:108227ms step_avg:59.14ms
step:1831/2330 train_time:108283ms step_avg:59.14ms
step:1832/2330 train_time:108346ms step_avg:59.14ms
step:1833/2330 train_time:108404ms step_avg:59.14ms
step:1834/2330 train_time:108466ms step_avg:59.14ms
step:1835/2330 train_time:108523ms step_avg:59.14ms
step:1836/2330 train_time:108586ms step_avg:59.14ms
step:1837/2330 train_time:108644ms step_avg:59.14ms
step:1838/2330 train_time:108706ms step_avg:59.14ms
step:1839/2330 train_time:108764ms step_avg:59.14ms
step:1840/2330 train_time:108826ms step_avg:59.14ms
step:1841/2330 train_time:108883ms step_avg:59.14ms
step:1842/2330 train_time:108944ms step_avg:59.14ms
step:1843/2330 train_time:109003ms step_avg:59.14ms
step:1844/2330 train_time:109064ms step_avg:59.15ms
step:1845/2330 train_time:109122ms step_avg:59.14ms
step:1846/2330 train_time:109183ms step_avg:59.15ms
step:1847/2330 train_time:109240ms step_avg:59.14ms
step:1848/2330 train_time:109302ms step_avg:59.15ms
step:1849/2330 train_time:109360ms step_avg:59.15ms
step:1850/2330 train_time:109422ms step_avg:59.15ms
step:1851/2330 train_time:109481ms step_avg:59.15ms
step:1852/2330 train_time:109543ms step_avg:59.15ms
step:1853/2330 train_time:109600ms step_avg:59.15ms
step:1854/2330 train_time:109662ms step_avg:59.15ms
step:1855/2330 train_time:109720ms step_avg:59.15ms
step:1856/2330 train_time:109782ms step_avg:59.15ms
step:1857/2330 train_time:109841ms step_avg:59.15ms
step:1858/2330 train_time:109901ms step_avg:59.15ms
step:1859/2330 train_time:109959ms step_avg:59.15ms
step:1860/2330 train_time:110020ms step_avg:59.15ms
step:1861/2330 train_time:110078ms step_avg:59.15ms
step:1862/2330 train_time:110139ms step_avg:59.15ms
step:1863/2330 train_time:110197ms step_avg:59.15ms
step:1864/2330 train_time:110258ms step_avg:59.15ms
step:1865/2330 train_time:110316ms step_avg:59.15ms
step:1866/2330 train_time:110376ms step_avg:59.15ms
step:1867/2330 train_time:110435ms step_avg:59.15ms
step:1868/2330 train_time:110496ms step_avg:59.15ms
step:1869/2330 train_time:110555ms step_avg:59.15ms
step:1870/2330 train_time:110615ms step_avg:59.15ms
step:1871/2330 train_time:110673ms step_avg:59.15ms
step:1872/2330 train_time:110734ms step_avg:59.15ms
step:1873/2330 train_time:110792ms step_avg:59.15ms
step:1874/2330 train_time:110853ms step_avg:59.15ms
step:1875/2330 train_time:110911ms step_avg:59.15ms
step:1876/2330 train_time:110971ms step_avg:59.15ms
step:1877/2330 train_time:111029ms step_avg:59.15ms
step:1878/2330 train_time:111091ms step_avg:59.15ms
step:1879/2330 train_time:111148ms step_avg:59.15ms
step:1880/2330 train_time:111209ms step_avg:59.15ms
step:1881/2330 train_time:111265ms step_avg:59.15ms
step:1882/2330 train_time:111328ms step_avg:59.15ms
step:1883/2330 train_time:111385ms step_avg:59.15ms
step:1884/2330 train_time:111448ms step_avg:59.16ms
step:1885/2330 train_time:111506ms step_avg:59.15ms
step:1886/2330 train_time:111567ms step_avg:59.16ms
step:1887/2330 train_time:111625ms step_avg:59.15ms
step:1888/2330 train_time:111686ms step_avg:59.16ms
step:1889/2330 train_time:111743ms step_avg:59.15ms
step:1890/2330 train_time:111806ms step_avg:59.16ms
step:1891/2330 train_time:111864ms step_avg:59.16ms
step:1892/2330 train_time:111925ms step_avg:59.16ms
step:1893/2330 train_time:111983ms step_avg:59.16ms
step:1894/2330 train_time:112044ms step_avg:59.16ms
step:1895/2330 train_time:112103ms step_avg:59.16ms
step:1896/2330 train_time:112163ms step_avg:59.16ms
step:1897/2330 train_time:112222ms step_avg:59.16ms
step:1898/2330 train_time:112283ms step_avg:59.16ms
step:1899/2330 train_time:112341ms step_avg:59.16ms
step:1900/2330 train_time:112402ms step_avg:59.16ms
step:1901/2330 train_time:112460ms step_avg:59.16ms
step:1902/2330 train_time:112522ms step_avg:59.16ms
step:1903/2330 train_time:112580ms step_avg:59.16ms
step:1904/2330 train_time:112641ms step_avg:59.16ms
step:1905/2330 train_time:112700ms step_avg:59.16ms
step:1906/2330 train_time:112761ms step_avg:59.16ms
step:1907/2330 train_time:112819ms step_avg:59.16ms
step:1908/2330 train_time:112880ms step_avg:59.16ms
step:1909/2330 train_time:112938ms step_avg:59.16ms
step:1910/2330 train_time:112999ms step_avg:59.16ms
step:1911/2330 train_time:113058ms step_avg:59.16ms
step:1912/2330 train_time:113119ms step_avg:59.16ms
step:1913/2330 train_time:113177ms step_avg:59.16ms
step:1914/2330 train_time:113238ms step_avg:59.16ms
step:1915/2330 train_time:113296ms step_avg:59.16ms
step:1916/2330 train_time:113357ms step_avg:59.16ms
step:1917/2330 train_time:113416ms step_avg:59.16ms
step:1918/2330 train_time:113477ms step_avg:59.16ms
step:1919/2330 train_time:113535ms step_avg:59.16ms
step:1920/2330 train_time:113596ms step_avg:59.16ms
step:1921/2330 train_time:113653ms step_avg:59.16ms
step:1922/2330 train_time:113716ms step_avg:59.17ms
step:1923/2330 train_time:113773ms step_avg:59.16ms
step:1924/2330 train_time:113834ms step_avg:59.17ms
step:1925/2330 train_time:113891ms step_avg:59.16ms
step:1926/2330 train_time:113953ms step_avg:59.17ms
step:1927/2330 train_time:114011ms step_avg:59.16ms
step:1928/2330 train_time:114072ms step_avg:59.17ms
step:1929/2330 train_time:114128ms step_avg:59.16ms
step:1930/2330 train_time:114190ms step_avg:59.17ms
step:1931/2330 train_time:114248ms step_avg:59.17ms
step:1932/2330 train_time:114309ms step_avg:59.17ms
step:1933/2330 train_time:114366ms step_avg:59.17ms
step:1934/2330 train_time:114428ms step_avg:59.17ms
step:1935/2330 train_time:114486ms step_avg:59.17ms
step:1936/2330 train_time:114548ms step_avg:59.17ms
step:1937/2330 train_time:114605ms step_avg:59.17ms
step:1938/2330 train_time:114667ms step_avg:59.17ms
step:1939/2330 train_time:114724ms step_avg:59.17ms
step:1940/2330 train_time:114787ms step_avg:59.17ms
step:1941/2330 train_time:114844ms step_avg:59.17ms
step:1942/2330 train_time:114907ms step_avg:59.17ms
step:1943/2330 train_time:114964ms step_avg:59.17ms
step:1944/2330 train_time:115025ms step_avg:59.17ms
step:1945/2330 train_time:115082ms step_avg:59.17ms
step:1946/2330 train_time:115144ms step_avg:59.17ms
step:1947/2330 train_time:115201ms step_avg:59.17ms
step:1948/2330 train_time:115263ms step_avg:59.17ms
step:1949/2330 train_time:115320ms step_avg:59.17ms
step:1950/2330 train_time:115382ms step_avg:59.17ms
step:1951/2330 train_time:115439ms step_avg:59.17ms
step:1952/2330 train_time:115502ms step_avg:59.17ms
step:1953/2330 train_time:115561ms step_avg:59.17ms
step:1954/2330 train_time:115621ms step_avg:59.17ms
step:1955/2330 train_time:115680ms step_avg:59.17ms
step:1956/2330 train_time:115742ms step_avg:59.17ms
step:1957/2330 train_time:115801ms step_avg:59.17ms
step:1958/2330 train_time:115862ms step_avg:59.17ms
step:1959/2330 train_time:115920ms step_avg:59.17ms
step:1960/2330 train_time:115981ms step_avg:59.17ms
step:1961/2330 train_time:116038ms step_avg:59.17ms
step:1962/2330 train_time:116101ms step_avg:59.17ms
step:1963/2330 train_time:116158ms step_avg:59.17ms
step:1964/2330 train_time:116220ms step_avg:59.18ms
step:1965/2330 train_time:116278ms step_avg:59.17ms
step:1966/2330 train_time:116339ms step_avg:59.18ms
step:1967/2330 train_time:116397ms step_avg:59.18ms
step:1968/2330 train_time:116458ms step_avg:59.18ms
step:1969/2330 train_time:116517ms step_avg:59.18ms
step:1970/2330 train_time:116577ms step_avg:59.18ms
step:1971/2330 train_time:116635ms step_avg:59.18ms
step:1972/2330 train_time:116697ms step_avg:59.18ms
step:1973/2330 train_time:116755ms step_avg:59.18ms
step:1974/2330 train_time:116817ms step_avg:59.18ms
step:1975/2330 train_time:116876ms step_avg:59.18ms
step:1976/2330 train_time:116937ms step_avg:59.18ms
step:1977/2330 train_time:116995ms step_avg:59.18ms
step:1978/2330 train_time:117056ms step_avg:59.18ms
step:1979/2330 train_time:117114ms step_avg:59.18ms
step:1980/2330 train_time:117174ms step_avg:59.18ms
step:1981/2330 train_time:117232ms step_avg:59.18ms
step:1982/2330 train_time:117293ms step_avg:59.18ms
step:1983/2330 train_time:117351ms step_avg:59.18ms
step:1984/2330 train_time:117411ms step_avg:59.18ms
step:1985/2330 train_time:117469ms step_avg:59.18ms
step:1986/2330 train_time:117531ms step_avg:59.18ms
step:1987/2330 train_time:117588ms step_avg:59.18ms
step:1988/2330 train_time:117651ms step_avg:59.18ms
step:1989/2330 train_time:117709ms step_avg:59.18ms
step:1990/2330 train_time:117769ms step_avg:59.18ms
step:1991/2330 train_time:117826ms step_avg:59.18ms
step:1992/2330 train_time:117889ms step_avg:59.18ms
step:1993/2330 train_time:117946ms step_avg:59.18ms
step:1994/2330 train_time:118009ms step_avg:59.18ms
step:1995/2330 train_time:118065ms step_avg:59.18ms
step:1996/2330 train_time:118127ms step_avg:59.18ms
step:1997/2330 train_time:118185ms step_avg:59.18ms
step:1998/2330 train_time:118247ms step_avg:59.18ms
step:1999/2330 train_time:118304ms step_avg:59.18ms
step:2000/2330 train_time:118366ms step_avg:59.18ms
step:2000/2330 val_loss:3.8515 train_time:118449ms step_avg:59.22ms
step:2001/2330 train_time:118469ms step_avg:59.20ms
step:2002/2330 train_time:118490ms step_avg:59.19ms
step:2003/2330 train_time:118550ms step_avg:59.19ms
step:2004/2330 train_time:118615ms step_avg:59.19ms
step:2005/2330 train_time:118674ms step_avg:59.19ms
step:2006/2330 train_time:118736ms step_avg:59.19ms
step:2007/2330 train_time:118793ms step_avg:59.19ms
step:2008/2330 train_time:118853ms step_avg:59.19ms
step:2009/2330 train_time:118910ms step_avg:59.19ms
step:2010/2330 train_time:118971ms step_avg:59.19ms
step:2011/2330 train_time:119027ms step_avg:59.19ms
step:2012/2330 train_time:119088ms step_avg:59.19ms
step:2013/2330 train_time:119144ms step_avg:59.19ms
step:2014/2330 train_time:119205ms step_avg:59.19ms
step:2015/2330 train_time:119262ms step_avg:59.19ms
step:2016/2330 train_time:119323ms step_avg:59.19ms
step:2017/2330 train_time:119380ms step_avg:59.19ms
step:2018/2330 train_time:119442ms step_avg:59.19ms
step:2019/2330 train_time:119502ms step_avg:59.19ms
step:2020/2330 train_time:119568ms step_avg:59.19ms
step:2021/2330 train_time:119626ms step_avg:59.19ms
step:2022/2330 train_time:119689ms step_avg:59.19ms
step:2023/2330 train_time:119747ms step_avg:59.19ms
step:2024/2330 train_time:119808ms step_avg:59.19ms
step:2025/2330 train_time:119865ms step_avg:59.19ms
step:2026/2330 train_time:119927ms step_avg:59.19ms
step:2027/2330 train_time:119983ms step_avg:59.19ms
step:2028/2330 train_time:120043ms step_avg:59.19ms
step:2029/2330 train_time:120100ms step_avg:59.19ms
step:2030/2330 train_time:120160ms step_avg:59.19ms
step:2031/2330 train_time:120217ms step_avg:59.19ms
step:2032/2330 train_time:120278ms step_avg:59.19ms
step:2033/2330 train_time:120335ms step_avg:59.19ms
step:2034/2330 train_time:120396ms step_avg:59.19ms
step:2035/2330 train_time:120456ms step_avg:59.19ms
step:2036/2330 train_time:120518ms step_avg:59.19ms
step:2037/2330 train_time:120577ms step_avg:59.19ms
step:2038/2330 train_time:120638ms step_avg:59.19ms
step:2039/2330 train_time:120696ms step_avg:59.19ms
step:2040/2330 train_time:120757ms step_avg:59.19ms
step:2041/2330 train_time:120815ms step_avg:59.19ms
step:2042/2330 train_time:120876ms step_avg:59.19ms
step:2043/2330 train_time:120934ms step_avg:59.19ms
step:2044/2330 train_time:120995ms step_avg:59.20ms
step:2045/2330 train_time:121053ms step_avg:59.19ms
step:2046/2330 train_time:121113ms step_avg:59.19ms
step:2047/2330 train_time:121171ms step_avg:59.19ms
step:2048/2330 train_time:121231ms step_avg:59.19ms
step:2049/2330 train_time:121289ms step_avg:59.19ms
step:2050/2330 train_time:121350ms step_avg:59.20ms
step:2051/2330 train_time:121410ms step_avg:59.20ms
step:2052/2330 train_time:121471ms step_avg:59.20ms
step:2053/2330 train_time:121530ms step_avg:59.20ms
step:2054/2330 train_time:121591ms step_avg:59.20ms
step:2055/2330 train_time:121649ms step_avg:59.20ms
step:2056/2330 train_time:121710ms step_avg:59.20ms
step:2057/2330 train_time:121768ms step_avg:59.20ms
step:2058/2330 train_time:121829ms step_avg:59.20ms
step:2059/2330 train_time:121888ms step_avg:59.20ms
step:2060/2330 train_time:121949ms step_avg:59.20ms
step:2061/2330 train_time:122006ms step_avg:59.20ms
step:2062/2330 train_time:122068ms step_avg:59.20ms
step:2063/2330 train_time:122126ms step_avg:59.20ms
step:2064/2330 train_time:122186ms step_avg:59.20ms
step:2065/2330 train_time:122243ms step_avg:59.20ms
step:2066/2330 train_time:122304ms step_avg:59.20ms
step:2067/2330 train_time:122361ms step_avg:59.20ms
step:2068/2330 train_time:122422ms step_avg:59.20ms
step:2069/2330 train_time:122481ms step_avg:59.20ms
step:2070/2330 train_time:122543ms step_avg:59.20ms
step:2071/2330 train_time:122600ms step_avg:59.20ms
step:2072/2330 train_time:122663ms step_avg:59.20ms
step:2073/2330 train_time:122720ms step_avg:59.20ms
step:2074/2330 train_time:122784ms step_avg:59.20ms
step:2075/2330 train_time:122841ms step_avg:59.20ms
step:2076/2330 train_time:122904ms step_avg:59.20ms
step:2077/2330 train_time:122961ms step_avg:59.20ms
step:2078/2330 train_time:123023ms step_avg:59.20ms
step:2079/2330 train_time:123080ms step_avg:59.20ms
step:2080/2330 train_time:123140ms step_avg:59.20ms
step:2081/2330 train_time:123197ms step_avg:59.20ms
step:2082/2330 train_time:123258ms step_avg:59.20ms
step:2083/2330 train_time:123316ms step_avg:59.20ms
step:2084/2330 train_time:123377ms step_avg:59.20ms
step:2085/2330 train_time:123435ms step_avg:59.20ms
step:2086/2330 train_time:123496ms step_avg:59.20ms
step:2087/2330 train_time:123554ms step_avg:59.20ms
step:2088/2330 train_time:123616ms step_avg:59.20ms
step:2089/2330 train_time:123674ms step_avg:59.20ms
step:2090/2330 train_time:123735ms step_avg:59.20ms
step:2091/2330 train_time:123793ms step_avg:59.20ms
step:2092/2330 train_time:123853ms step_avg:59.20ms
step:2093/2330 train_time:123913ms step_avg:59.20ms
step:2094/2330 train_time:123974ms step_avg:59.20ms
step:2095/2330 train_time:124032ms step_avg:59.20ms
step:2096/2330 train_time:124092ms step_avg:59.20ms
step:2097/2330 train_time:124150ms step_avg:59.20ms
step:2098/2330 train_time:124210ms step_avg:59.20ms
step:2099/2330 train_time:124269ms step_avg:59.20ms
step:2100/2330 train_time:124330ms step_avg:59.20ms
step:2101/2330 train_time:124388ms step_avg:59.20ms
step:2102/2330 train_time:124449ms step_avg:59.21ms
step:2103/2330 train_time:124508ms step_avg:59.21ms
step:2104/2330 train_time:124570ms step_avg:59.21ms
step:2105/2330 train_time:124628ms step_avg:59.21ms
step:2106/2330 train_time:124689ms step_avg:59.21ms
step:2107/2330 train_time:124748ms step_avg:59.21ms
step:2108/2330 train_time:124808ms step_avg:59.21ms
step:2109/2330 train_time:124866ms step_avg:59.21ms
step:2110/2330 train_time:124927ms step_avg:59.21ms
step:2111/2330 train_time:124985ms step_avg:59.21ms
step:2112/2330 train_time:125045ms step_avg:59.21ms
step:2113/2330 train_time:125103ms step_avg:59.21ms
step:2114/2330 train_time:125165ms step_avg:59.21ms
step:2115/2330 train_time:125223ms step_avg:59.21ms
step:2116/2330 train_time:125282ms step_avg:59.21ms
step:2117/2330 train_time:125340ms step_avg:59.21ms
step:2118/2330 train_time:125401ms step_avg:59.21ms
step:2119/2330 train_time:125459ms step_avg:59.21ms
step:2120/2330 train_time:125521ms step_avg:59.21ms
step:2121/2330 train_time:125579ms step_avg:59.21ms
step:2122/2330 train_time:125640ms step_avg:59.21ms
step:2123/2330 train_time:125697ms step_avg:59.21ms
step:2124/2330 train_time:125759ms step_avg:59.21ms
step:2125/2330 train_time:125816ms step_avg:59.21ms
step:2126/2330 train_time:125877ms step_avg:59.21ms
step:2127/2330 train_time:125936ms step_avg:59.21ms
step:2128/2330 train_time:125997ms step_avg:59.21ms
step:2129/2330 train_time:126055ms step_avg:59.21ms
step:2130/2330 train_time:126116ms step_avg:59.21ms
step:2131/2330 train_time:126175ms step_avg:59.21ms
step:2132/2330 train_time:126235ms step_avg:59.21ms
step:2133/2330 train_time:126294ms step_avg:59.21ms
step:2134/2330 train_time:126355ms step_avg:59.21ms
step:2135/2330 train_time:126414ms step_avg:59.21ms
step:2136/2330 train_time:126474ms step_avg:59.21ms
step:2137/2330 train_time:126533ms step_avg:59.21ms
step:2138/2330 train_time:126593ms step_avg:59.21ms
step:2139/2330 train_time:126651ms step_avg:59.21ms
step:2140/2330 train_time:126712ms step_avg:59.21ms
step:2141/2330 train_time:126771ms step_avg:59.21ms
step:2142/2330 train_time:126832ms step_avg:59.21ms
step:2143/2330 train_time:126889ms step_avg:59.21ms
step:2144/2330 train_time:126951ms step_avg:59.21ms
step:2145/2330 train_time:127010ms step_avg:59.21ms
step:2146/2330 train_time:127070ms step_avg:59.21ms
step:2147/2330 train_time:127128ms step_avg:59.21ms
step:2148/2330 train_time:127189ms step_avg:59.21ms
step:2149/2330 train_time:127248ms step_avg:59.21ms
step:2150/2330 train_time:127308ms step_avg:59.21ms
step:2151/2330 train_time:127366ms step_avg:59.21ms
step:2152/2330 train_time:127427ms step_avg:59.21ms
step:2153/2330 train_time:127485ms step_avg:59.21ms
step:2154/2330 train_time:127546ms step_avg:59.21ms
step:2155/2330 train_time:127604ms step_avg:59.21ms
step:2156/2330 train_time:127665ms step_avg:59.21ms
step:2157/2330 train_time:127723ms step_avg:59.21ms
step:2158/2330 train_time:127784ms step_avg:59.21ms
step:2159/2330 train_time:127841ms step_avg:59.21ms
step:2160/2330 train_time:127904ms step_avg:59.22ms
step:2161/2330 train_time:127962ms step_avg:59.21ms
step:2162/2330 train_time:128025ms step_avg:59.22ms
step:2163/2330 train_time:128081ms step_avg:59.21ms
step:2164/2330 train_time:128143ms step_avg:59.22ms
step:2165/2330 train_time:128200ms step_avg:59.21ms
step:2166/2330 train_time:128262ms step_avg:59.22ms
step:2167/2330 train_time:128320ms step_avg:59.22ms
step:2168/2330 train_time:128382ms step_avg:59.22ms
step:2169/2330 train_time:128438ms step_avg:59.22ms
step:2170/2330 train_time:128501ms step_avg:59.22ms
step:2171/2330 train_time:128559ms step_avg:59.22ms
step:2172/2330 train_time:128620ms step_avg:59.22ms
step:2173/2330 train_time:128677ms step_avg:59.22ms
step:2174/2330 train_time:128739ms step_avg:59.22ms
step:2175/2330 train_time:128797ms step_avg:59.22ms
step:2176/2330 train_time:128858ms step_avg:59.22ms
step:2177/2330 train_time:128916ms step_avg:59.22ms
step:2178/2330 train_time:128977ms step_avg:59.22ms
step:2179/2330 train_time:129035ms step_avg:59.22ms
step:2180/2330 train_time:129095ms step_avg:59.22ms
step:2181/2330 train_time:129154ms step_avg:59.22ms
step:2182/2330 train_time:129215ms step_avg:59.22ms
step:2183/2330 train_time:129272ms step_avg:59.22ms
step:2184/2330 train_time:129333ms step_avg:59.22ms
step:2185/2330 train_time:129392ms step_avg:59.22ms
step:2186/2330 train_time:129453ms step_avg:59.22ms
step:2187/2330 train_time:129511ms step_avg:59.22ms
step:2188/2330 train_time:129572ms step_avg:59.22ms
step:2189/2330 train_time:129630ms step_avg:59.22ms
step:2190/2330 train_time:129690ms step_avg:59.22ms
step:2191/2330 train_time:129749ms step_avg:59.22ms
step:2192/2330 train_time:129809ms step_avg:59.22ms
step:2193/2330 train_time:129869ms step_avg:59.22ms
step:2194/2330 train_time:129929ms step_avg:59.22ms
step:2195/2330 train_time:129988ms step_avg:59.22ms
step:2196/2330 train_time:130048ms step_avg:59.22ms
step:2197/2330 train_time:130105ms step_avg:59.22ms
step:2198/2330 train_time:130166ms step_avg:59.22ms
step:2199/2330 train_time:130225ms step_avg:59.22ms
step:2200/2330 train_time:130285ms step_avg:59.22ms
step:2201/2330 train_time:130342ms step_avg:59.22ms
step:2202/2330 train_time:130405ms step_avg:59.22ms
step:2203/2330 train_time:130462ms step_avg:59.22ms
step:2204/2330 train_time:130524ms step_avg:59.22ms
step:2205/2330 train_time:130582ms step_avg:59.22ms
step:2206/2330 train_time:130642ms step_avg:59.22ms
step:2207/2330 train_time:130699ms step_avg:59.22ms
step:2208/2330 train_time:130761ms step_avg:59.22ms
step:2209/2330 train_time:130819ms step_avg:59.22ms
step:2210/2330 train_time:130882ms step_avg:59.22ms
step:2211/2330 train_time:130939ms step_avg:59.22ms
step:2212/2330 train_time:131001ms step_avg:59.22ms
step:2213/2330 train_time:131058ms step_avg:59.22ms
step:2214/2330 train_time:131120ms step_avg:59.22ms
step:2215/2330 train_time:131178ms step_avg:59.22ms
step:2216/2330 train_time:131239ms step_avg:59.22ms
step:2217/2330 train_time:131296ms step_avg:59.22ms
step:2218/2330 train_time:131358ms step_avg:59.22ms
step:2219/2330 train_time:131415ms step_avg:59.22ms
step:2220/2330 train_time:131477ms step_avg:59.22ms
step:2221/2330 train_time:131535ms step_avg:59.22ms
step:2222/2330 train_time:131596ms step_avg:59.22ms
step:2223/2330 train_time:131653ms step_avg:59.22ms
step:2224/2330 train_time:131714ms step_avg:59.22ms
step:2225/2330 train_time:131774ms step_avg:59.22ms
step:2226/2330 train_time:131834ms step_avg:59.22ms
step:2227/2330 train_time:131891ms step_avg:59.22ms
step:2228/2330 train_time:131952ms step_avg:59.22ms
step:2229/2330 train_time:132011ms step_avg:59.22ms
step:2230/2330 train_time:132071ms step_avg:59.22ms
step:2231/2330 train_time:132129ms step_avg:59.22ms
step:2232/2330 train_time:132190ms step_avg:59.22ms
step:2233/2330 train_time:132247ms step_avg:59.22ms
step:2234/2330 train_time:132309ms step_avg:59.22ms
step:2235/2330 train_time:132367ms step_avg:59.22ms
step:2236/2330 train_time:132428ms step_avg:59.23ms
step:2237/2330 train_time:132485ms step_avg:59.22ms
step:2238/2330 train_time:132546ms step_avg:59.23ms
step:2239/2330 train_time:132604ms step_avg:59.22ms
step:2240/2330 train_time:132664ms step_avg:59.23ms
step:2241/2330 train_time:132722ms step_avg:59.22ms
step:2242/2330 train_time:132783ms step_avg:59.23ms
step:2243/2330 train_time:132841ms step_avg:59.22ms
step:2244/2330 train_time:132903ms step_avg:59.23ms
step:2245/2330 train_time:132960ms step_avg:59.22ms
step:2246/2330 train_time:133022ms step_avg:59.23ms
step:2247/2330 train_time:133080ms step_avg:59.23ms
step:2248/2330 train_time:133141ms step_avg:59.23ms
step:2249/2330 train_time:133198ms step_avg:59.23ms
step:2250/2330 train_time:133260ms step_avg:59.23ms
step:2250/2330 val_loss:3.8061 train_time:133342ms step_avg:59.26ms
step:2251/2330 train_time:133362ms step_avg:59.25ms
step:2252/2330 train_time:133382ms step_avg:59.23ms
step:2253/2330 train_time:133441ms step_avg:59.23ms
step:2254/2330 train_time:133506ms step_avg:59.23ms
step:2255/2330 train_time:133563ms step_avg:59.23ms
step:2256/2330 train_time:133626ms step_avg:59.23ms
step:2257/2330 train_time:133683ms step_avg:59.23ms
step:2258/2330 train_time:133745ms step_avg:59.23ms
step:2259/2330 train_time:133802ms step_avg:59.23ms
step:2260/2330 train_time:133863ms step_avg:59.23ms
step:2261/2330 train_time:133920ms step_avg:59.23ms
step:2262/2330 train_time:133982ms step_avg:59.23ms
step:2263/2330 train_time:134039ms step_avg:59.23ms
step:2264/2330 train_time:134100ms step_avg:59.23ms
step:2265/2330 train_time:134157ms step_avg:59.23ms
step:2266/2330 train_time:134217ms step_avg:59.23ms
step:2267/2330 train_time:134274ms step_avg:59.23ms
step:2268/2330 train_time:134336ms step_avg:59.23ms
step:2269/2330 train_time:134395ms step_avg:59.23ms
step:2270/2330 train_time:134458ms step_avg:59.23ms
step:2271/2330 train_time:134515ms step_avg:59.23ms
step:2272/2330 train_time:134578ms step_avg:59.23ms
step:2273/2330 train_time:134636ms step_avg:59.23ms
step:2274/2330 train_time:134697ms step_avg:59.23ms
step:2275/2330 train_time:134754ms step_avg:59.23ms
step:2276/2330 train_time:134815ms step_avg:59.23ms
step:2277/2330 train_time:134873ms step_avg:59.23ms
step:2278/2330 train_time:134934ms step_avg:59.23ms
step:2279/2330 train_time:134991ms step_avg:59.23ms
step:2280/2330 train_time:135051ms step_avg:59.23ms
step:2281/2330 train_time:135108ms step_avg:59.23ms
step:2282/2330 train_time:135168ms step_avg:59.23ms
step:2283/2330 train_time:135226ms step_avg:59.23ms
step:2284/2330 train_time:135288ms step_avg:59.23ms
step:2285/2330 train_time:135345ms step_avg:59.23ms
step:2286/2330 train_time:135409ms step_avg:59.23ms
step:2287/2330 train_time:135467ms step_avg:59.23ms
step:2288/2330 train_time:135529ms step_avg:59.23ms
step:2289/2330 train_time:135587ms step_avg:59.23ms
step:2290/2330 train_time:135649ms step_avg:59.24ms
step:2291/2330 train_time:135707ms step_avg:59.23ms
step:2292/2330 train_time:135767ms step_avg:59.24ms
step:2293/2330 train_time:135825ms step_avg:59.23ms
step:2294/2330 train_time:135887ms step_avg:59.24ms
step:2295/2330 train_time:135944ms step_avg:59.23ms
step:2296/2330 train_time:136004ms step_avg:59.24ms
step:2297/2330 train_time:136061ms step_avg:59.23ms
step:2298/2330 train_time:136122ms step_avg:59.23ms
step:2299/2330 train_time:136179ms step_avg:59.23ms
step:2300/2330 train_time:136240ms step_avg:59.23ms
step:2301/2330 train_time:136298ms step_avg:59.23ms
step:2302/2330 train_time:136359ms step_avg:59.23ms
step:2303/2330 train_time:136417ms step_avg:59.23ms
step:2304/2330 train_time:136479ms step_avg:59.24ms
step:2305/2330 train_time:136537ms step_avg:59.24ms
step:2306/2330 train_time:136599ms step_avg:59.24ms
step:2307/2330 train_time:136656ms step_avg:59.24ms
step:2308/2330 train_time:136720ms step_avg:59.24ms
step:2309/2330 train_time:136777ms step_avg:59.24ms
step:2310/2330 train_time:136839ms step_avg:59.24ms
step:2311/2330 train_time:136896ms step_avg:59.24ms
step:2312/2330 train_time:136958ms step_avg:59.24ms
step:2313/2330 train_time:137014ms step_avg:59.24ms
step:2314/2330 train_time:137076ms step_avg:59.24ms
step:2315/2330 train_time:137134ms step_avg:59.24ms
step:2316/2330 train_time:137194ms step_avg:59.24ms
step:2317/2330 train_time:137252ms step_avg:59.24ms
step:2318/2330 train_time:137313ms step_avg:59.24ms
step:2319/2330 train_time:137370ms step_avg:59.24ms
step:2320/2330 train_time:137432ms step_avg:59.24ms
step:2321/2330 train_time:137491ms step_avg:59.24ms
step:2322/2330 train_time:137552ms step_avg:59.24ms
step:2323/2330 train_time:137610ms step_avg:59.24ms
step:2324/2330 train_time:137671ms step_avg:59.24ms
step:2325/2330 train_time:137730ms step_avg:59.24ms
step:2326/2330 train_time:137791ms step_avg:59.24ms
step:2327/2330 train_time:137850ms step_avg:59.24ms
step:2328/2330 train_time:137910ms step_avg:59.24ms
step:2329/2330 train_time:137969ms step_avg:59.24ms
step:2330/2330 train_time:138029ms step_avg:59.24ms
step:2330/2330 val_loss:3.7918 train_time:138111ms step_avg:59.28ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
