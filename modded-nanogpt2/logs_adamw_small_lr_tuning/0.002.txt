import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--adamw_lr', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_lr_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_lr_tuning/{args2.adamw_lr}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=float(args2.adamw_lr),  betas=(0.85, 0.85), eps=1e-10, weight_decay=0.0, fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_lr_tuning/{args2.adamw_lr}", exist_ok=True)
            torch.save(log, f"logs_adamw_small_lr_tuning/{args2.adamw_lr}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 01:43:13 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   28C    P0             114W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   27C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   26C    P0             115W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   24C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   27C    P0             113W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   25C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   26C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:84ms step_avg:84.32ms
step:2/2330 train_time:184ms step_avg:92.10ms
step:3/2330 train_time:204ms step_avg:67.88ms
step:4/2330 train_time:223ms step_avg:55.77ms
step:5/2330 train_time:276ms step_avg:55.12ms
step:6/2330 train_time:333ms step_avg:55.50ms
step:7/2330 train_time:388ms step_avg:55.38ms
step:8/2330 train_time:446ms step_avg:55.70ms
step:9/2330 train_time:500ms step_avg:55.57ms
step:10/2330 train_time:558ms step_avg:55.80ms
step:11/2330 train_time:613ms step_avg:55.71ms
step:12/2330 train_time:670ms step_avg:55.85ms
step:13/2330 train_time:724ms step_avg:55.73ms
step:14/2330 train_time:782ms step_avg:55.88ms
step:15/2330 train_time:837ms step_avg:55.80ms
step:16/2330 train_time:895ms step_avg:55.93ms
step:17/2330 train_time:950ms step_avg:55.88ms
step:18/2330 train_time:1007ms step_avg:55.97ms
step:19/2330 train_time:1062ms step_avg:55.92ms
step:20/2330 train_time:1123ms step_avg:56.16ms
step:21/2330 train_time:1181ms step_avg:56.22ms
step:22/2330 train_time:1241ms step_avg:56.42ms
step:23/2330 train_time:1297ms step_avg:56.38ms
step:24/2330 train_time:1357ms step_avg:56.54ms
step:25/2330 train_time:1412ms step_avg:56.47ms
step:26/2330 train_time:1471ms step_avg:56.58ms
step:27/2330 train_time:1526ms step_avg:56.51ms
step:28/2330 train_time:1584ms step_avg:56.58ms
step:29/2330 train_time:1640ms step_avg:56.54ms
step:30/2330 train_time:1697ms step_avg:56.58ms
step:31/2330 train_time:1752ms step_avg:56.52ms
step:32/2330 train_time:1810ms step_avg:56.56ms
step:33/2330 train_time:1865ms step_avg:56.51ms
step:34/2330 train_time:1923ms step_avg:56.55ms
step:35/2330 train_time:1978ms step_avg:56.52ms
step:36/2330 train_time:2037ms step_avg:56.58ms
step:37/2330 train_time:2092ms step_avg:56.54ms
step:38/2330 train_time:2151ms step_avg:56.61ms
step:39/2330 train_time:2207ms step_avg:56.58ms
step:40/2330 train_time:2266ms step_avg:56.66ms
step:41/2330 train_time:2322ms step_avg:56.64ms
step:42/2330 train_time:2381ms step_avg:56.68ms
step:43/2330 train_time:2436ms step_avg:56.65ms
step:44/2330 train_time:2496ms step_avg:56.73ms
step:45/2330 train_time:2551ms step_avg:56.69ms
step:46/2330 train_time:2610ms step_avg:56.75ms
step:47/2330 train_time:2665ms step_avg:56.71ms
step:48/2330 train_time:2724ms step_avg:56.75ms
step:49/2330 train_time:2779ms step_avg:56.71ms
step:50/2330 train_time:2837ms step_avg:56.74ms
step:51/2330 train_time:2892ms step_avg:56.71ms
step:52/2330 train_time:2951ms step_avg:56.75ms
step:53/2330 train_time:3006ms step_avg:56.72ms
step:54/2330 train_time:3065ms step_avg:56.76ms
step:55/2330 train_time:3121ms step_avg:56.74ms
step:56/2330 train_time:3179ms step_avg:56.78ms
step:57/2330 train_time:3235ms step_avg:56.75ms
step:58/2330 train_time:3295ms step_avg:56.81ms
step:59/2330 train_time:3350ms step_avg:56.78ms
step:60/2330 train_time:3410ms step_avg:56.83ms
step:61/2330 train_time:3465ms step_avg:56.81ms
step:62/2330 train_time:3524ms step_avg:56.84ms
step:63/2330 train_time:3579ms step_avg:56.81ms
step:64/2330 train_time:3638ms step_avg:56.84ms
step:65/2330 train_time:3693ms step_avg:56.81ms
step:66/2330 train_time:3752ms step_avg:56.84ms
step:67/2330 train_time:3807ms step_avg:56.82ms
step:68/2330 train_time:3865ms step_avg:56.84ms
step:69/2330 train_time:3920ms step_avg:56.82ms
step:70/2330 train_time:3979ms step_avg:56.85ms
step:71/2330 train_time:4035ms step_avg:56.83ms
step:72/2330 train_time:4093ms step_avg:56.85ms
step:73/2330 train_time:4148ms step_avg:56.83ms
step:74/2330 train_time:4208ms step_avg:56.87ms
step:75/2330 train_time:4264ms step_avg:56.85ms
step:76/2330 train_time:4322ms step_avg:56.87ms
step:77/2330 train_time:4379ms step_avg:56.86ms
step:78/2330 train_time:4437ms step_avg:56.88ms
step:79/2330 train_time:4492ms step_avg:56.86ms
step:80/2330 train_time:4551ms step_avg:56.89ms
step:81/2330 train_time:4606ms step_avg:56.87ms
step:82/2330 train_time:4665ms step_avg:56.89ms
step:83/2330 train_time:4721ms step_avg:56.88ms
step:84/2330 train_time:4779ms step_avg:56.90ms
step:85/2330 train_time:4835ms step_avg:56.88ms
step:86/2330 train_time:4893ms step_avg:56.90ms
step:87/2330 train_time:4948ms step_avg:56.87ms
step:88/2330 train_time:5008ms step_avg:56.91ms
step:89/2330 train_time:5064ms step_avg:56.90ms
step:90/2330 train_time:5122ms step_avg:56.91ms
step:91/2330 train_time:5177ms step_avg:56.89ms
step:92/2330 train_time:5236ms step_avg:56.91ms
step:93/2330 train_time:5291ms step_avg:56.89ms
step:94/2330 train_time:5351ms step_avg:56.92ms
step:95/2330 train_time:5406ms step_avg:56.91ms
step:96/2330 train_time:5465ms step_avg:56.93ms
step:97/2330 train_time:5521ms step_avg:56.92ms
step:98/2330 train_time:5580ms step_avg:56.94ms
step:99/2330 train_time:5635ms step_avg:56.92ms
step:100/2330 train_time:5694ms step_avg:56.94ms
step:101/2330 train_time:5749ms step_avg:56.92ms
step:102/2330 train_time:5808ms step_avg:56.94ms
step:103/2330 train_time:5864ms step_avg:56.94ms
step:104/2330 train_time:5923ms step_avg:56.95ms
step:105/2330 train_time:5978ms step_avg:56.93ms
step:106/2330 train_time:6037ms step_avg:56.96ms
step:107/2330 train_time:6093ms step_avg:56.95ms
step:108/2330 train_time:6152ms step_avg:56.96ms
step:109/2330 train_time:6207ms step_avg:56.94ms
step:110/2330 train_time:6266ms step_avg:56.96ms
step:111/2330 train_time:6321ms step_avg:56.95ms
step:112/2330 train_time:6381ms step_avg:56.97ms
step:113/2330 train_time:6436ms step_avg:56.95ms
step:114/2330 train_time:6495ms step_avg:56.97ms
step:115/2330 train_time:6549ms step_avg:56.95ms
step:116/2330 train_time:6609ms step_avg:56.97ms
step:117/2330 train_time:6665ms step_avg:56.96ms
step:118/2330 train_time:6723ms step_avg:56.97ms
step:119/2330 train_time:6779ms step_avg:56.96ms
step:120/2330 train_time:6837ms step_avg:56.97ms
step:121/2330 train_time:6892ms step_avg:56.96ms
step:122/2330 train_time:6951ms step_avg:56.98ms
step:123/2330 train_time:7007ms step_avg:56.97ms
step:124/2330 train_time:7066ms step_avg:56.98ms
step:125/2330 train_time:7121ms step_avg:56.97ms
step:126/2330 train_time:7180ms step_avg:56.98ms
step:127/2330 train_time:7236ms step_avg:56.97ms
step:128/2330 train_time:7295ms step_avg:56.99ms
step:129/2330 train_time:7351ms step_avg:56.98ms
step:130/2330 train_time:7409ms step_avg:57.00ms
step:131/2330 train_time:7465ms step_avg:56.99ms
step:132/2330 train_time:7523ms step_avg:56.99ms
step:133/2330 train_time:7579ms step_avg:56.99ms
step:134/2330 train_time:7638ms step_avg:57.00ms
step:135/2330 train_time:7694ms step_avg:56.99ms
step:136/2330 train_time:7752ms step_avg:57.00ms
step:137/2330 train_time:7807ms step_avg:56.99ms
step:138/2330 train_time:7866ms step_avg:57.00ms
step:139/2330 train_time:7922ms step_avg:56.99ms
step:140/2330 train_time:7981ms step_avg:57.01ms
step:141/2330 train_time:8036ms step_avg:56.99ms
step:142/2330 train_time:8094ms step_avg:57.00ms
step:143/2330 train_time:8150ms step_avg:56.99ms
step:144/2330 train_time:8209ms step_avg:57.01ms
step:145/2330 train_time:8265ms step_avg:57.00ms
step:146/2330 train_time:8323ms step_avg:57.01ms
step:147/2330 train_time:8379ms step_avg:57.00ms
step:148/2330 train_time:8437ms step_avg:57.01ms
step:149/2330 train_time:8493ms step_avg:57.00ms
step:150/2330 train_time:8552ms step_avg:57.01ms
step:151/2330 train_time:8607ms step_avg:57.00ms
step:152/2330 train_time:8666ms step_avg:57.01ms
step:153/2330 train_time:8721ms step_avg:57.00ms
step:154/2330 train_time:8781ms step_avg:57.02ms
step:155/2330 train_time:8836ms step_avg:57.01ms
step:156/2330 train_time:8896ms step_avg:57.03ms
step:157/2330 train_time:8951ms step_avg:57.02ms
step:158/2330 train_time:9010ms step_avg:57.03ms
step:159/2330 train_time:9065ms step_avg:57.02ms
step:160/2330 train_time:9124ms step_avg:57.03ms
step:161/2330 train_time:9180ms step_avg:57.02ms
step:162/2330 train_time:9239ms step_avg:57.03ms
step:163/2330 train_time:9296ms step_avg:57.03ms
step:164/2330 train_time:9354ms step_avg:57.04ms
step:165/2330 train_time:9409ms step_avg:57.02ms
step:166/2330 train_time:9467ms step_avg:57.03ms
step:167/2330 train_time:9523ms step_avg:57.02ms
step:168/2330 train_time:9581ms step_avg:57.03ms
step:169/2330 train_time:9636ms step_avg:57.02ms
step:170/2330 train_time:9695ms step_avg:57.03ms
step:171/2330 train_time:9751ms step_avg:57.02ms
step:172/2330 train_time:9810ms step_avg:57.04ms
step:173/2330 train_time:9866ms step_avg:57.03ms
step:174/2330 train_time:9924ms step_avg:57.04ms
step:175/2330 train_time:9980ms step_avg:57.03ms
step:176/2330 train_time:10038ms step_avg:57.03ms
step:177/2330 train_time:10093ms step_avg:57.02ms
step:178/2330 train_time:10153ms step_avg:57.04ms
step:179/2330 train_time:10208ms step_avg:57.03ms
step:180/2330 train_time:10267ms step_avg:57.04ms
step:181/2330 train_time:10323ms step_avg:57.03ms
step:182/2330 train_time:10381ms step_avg:57.04ms
step:183/2330 train_time:10437ms step_avg:57.03ms
step:184/2330 train_time:10495ms step_avg:57.04ms
step:185/2330 train_time:10551ms step_avg:57.03ms
step:186/2330 train_time:10610ms step_avg:57.04ms
step:187/2330 train_time:10665ms step_avg:57.03ms
step:188/2330 train_time:10724ms step_avg:57.04ms
step:189/2330 train_time:10780ms step_avg:57.03ms
step:190/2330 train_time:10838ms step_avg:57.04ms
step:191/2330 train_time:10894ms step_avg:57.04ms
step:192/2330 train_time:10952ms step_avg:57.04ms
step:193/2330 train_time:11007ms step_avg:57.03ms
step:194/2330 train_time:11067ms step_avg:57.05ms
step:195/2330 train_time:11122ms step_avg:57.04ms
step:196/2330 train_time:11181ms step_avg:57.05ms
step:197/2330 train_time:11237ms step_avg:57.04ms
step:198/2330 train_time:11296ms step_avg:57.05ms
step:199/2330 train_time:11351ms step_avg:57.04ms
step:200/2330 train_time:11410ms step_avg:57.05ms
step:201/2330 train_time:11466ms step_avg:57.04ms
step:202/2330 train_time:11524ms step_avg:57.05ms
step:203/2330 train_time:11579ms step_avg:57.04ms
step:204/2330 train_time:11638ms step_avg:57.05ms
step:205/2330 train_time:11694ms step_avg:57.04ms
step:206/2330 train_time:11752ms step_avg:57.05ms
step:207/2330 train_time:11807ms step_avg:57.04ms
step:208/2330 train_time:11866ms step_avg:57.05ms
step:209/2330 train_time:11923ms step_avg:57.05ms
step:210/2330 train_time:11981ms step_avg:57.05ms
step:211/2330 train_time:12037ms step_avg:57.05ms
step:212/2330 train_time:12096ms step_avg:57.06ms
step:213/2330 train_time:12152ms step_avg:57.05ms
step:214/2330 train_time:12210ms step_avg:57.05ms
step:215/2330 train_time:12265ms step_avg:57.05ms
step:216/2330 train_time:12324ms step_avg:57.05ms
step:217/2330 train_time:12379ms step_avg:57.05ms
step:218/2330 train_time:12438ms step_avg:57.06ms
step:219/2330 train_time:12494ms step_avg:57.05ms
step:220/2330 train_time:12553ms step_avg:57.06ms
step:221/2330 train_time:12608ms step_avg:57.05ms
step:222/2330 train_time:12668ms step_avg:57.06ms
step:223/2330 train_time:12723ms step_avg:57.05ms
step:224/2330 train_time:12782ms step_avg:57.06ms
step:225/2330 train_time:12837ms step_avg:57.05ms
step:226/2330 train_time:12896ms step_avg:57.06ms
step:227/2330 train_time:12951ms step_avg:57.05ms
step:228/2330 train_time:13010ms step_avg:57.06ms
step:229/2330 train_time:13065ms step_avg:57.05ms
step:230/2330 train_time:13124ms step_avg:57.06ms
step:231/2330 train_time:13179ms step_avg:57.05ms
step:232/2330 train_time:13238ms step_avg:57.06ms
step:233/2330 train_time:13294ms step_avg:57.05ms
step:234/2330 train_time:13352ms step_avg:57.06ms
step:235/2330 train_time:13407ms step_avg:57.05ms
step:236/2330 train_time:13466ms step_avg:57.06ms
step:237/2330 train_time:13522ms step_avg:57.05ms
step:238/2330 train_time:13580ms step_avg:57.06ms
step:239/2330 train_time:13636ms step_avg:57.05ms
step:240/2330 train_time:13695ms step_avg:57.06ms
step:241/2330 train_time:13751ms step_avg:57.06ms
step:242/2330 train_time:13810ms step_avg:57.07ms
step:243/2330 train_time:13865ms step_avg:57.06ms
step:244/2330 train_time:13924ms step_avg:57.07ms
step:245/2330 train_time:13980ms step_avg:57.06ms
step:246/2330 train_time:14038ms step_avg:57.07ms
step:247/2330 train_time:14094ms step_avg:57.06ms
step:248/2330 train_time:14152ms step_avg:57.06ms
step:249/2330 train_time:14207ms step_avg:57.06ms
step:250/2330 train_time:14267ms step_avg:57.07ms
step:250/2330 val_loss:4.9135 train_time:14345ms step_avg:57.38ms
step:251/2330 train_time:14363ms step_avg:57.22ms
step:252/2330 train_time:14383ms step_avg:57.07ms
step:253/2330 train_time:14437ms step_avg:57.06ms
step:254/2330 train_time:14502ms step_avg:57.09ms
step:255/2330 train_time:14556ms step_avg:57.08ms
step:256/2330 train_time:14622ms step_avg:57.12ms
step:257/2330 train_time:14676ms step_avg:57.11ms
step:258/2330 train_time:14737ms step_avg:57.12ms
step:259/2330 train_time:14791ms step_avg:57.11ms
step:260/2330 train_time:14850ms step_avg:57.11ms
step:261/2330 train_time:14905ms step_avg:57.11ms
step:262/2330 train_time:14963ms step_avg:57.11ms
step:263/2330 train_time:15018ms step_avg:57.10ms
step:264/2330 train_time:15076ms step_avg:57.11ms
step:265/2330 train_time:15131ms step_avg:57.10ms
step:266/2330 train_time:15189ms step_avg:57.10ms
step:267/2330 train_time:15245ms step_avg:57.10ms
step:268/2330 train_time:15303ms step_avg:57.10ms
step:269/2330 train_time:15358ms step_avg:57.09ms
step:270/2330 train_time:15418ms step_avg:57.11ms
step:271/2330 train_time:15474ms step_avg:57.10ms
step:272/2330 train_time:15535ms step_avg:57.11ms
step:273/2330 train_time:15590ms step_avg:57.11ms
step:274/2330 train_time:15651ms step_avg:57.12ms
step:275/2330 train_time:15707ms step_avg:57.12ms
step:276/2330 train_time:15765ms step_avg:57.12ms
step:277/2330 train_time:15821ms step_avg:57.11ms
step:278/2330 train_time:15880ms step_avg:57.12ms
step:279/2330 train_time:15935ms step_avg:57.12ms
step:280/2330 train_time:15993ms step_avg:57.12ms
step:281/2330 train_time:16048ms step_avg:57.11ms
step:282/2330 train_time:16107ms step_avg:57.12ms
step:283/2330 train_time:16162ms step_avg:57.11ms
step:284/2330 train_time:16221ms step_avg:57.11ms
step:285/2330 train_time:16276ms step_avg:57.11ms
step:286/2330 train_time:16334ms step_avg:57.11ms
step:287/2330 train_time:16390ms step_avg:57.11ms
step:288/2330 train_time:16450ms step_avg:57.12ms
step:289/2330 train_time:16506ms step_avg:57.12ms
step:290/2330 train_time:16565ms step_avg:57.12ms
step:291/2330 train_time:16621ms step_avg:57.12ms
step:292/2330 train_time:16680ms step_avg:57.12ms
step:293/2330 train_time:16736ms step_avg:57.12ms
step:294/2330 train_time:16795ms step_avg:57.13ms
step:295/2330 train_time:16851ms step_avg:57.12ms
step:296/2330 train_time:16910ms step_avg:57.13ms
step:297/2330 train_time:16966ms step_avg:57.12ms
step:298/2330 train_time:17024ms step_avg:57.13ms
step:299/2330 train_time:17079ms step_avg:57.12ms
step:300/2330 train_time:17138ms step_avg:57.13ms
step:301/2330 train_time:17192ms step_avg:57.12ms
step:302/2330 train_time:17251ms step_avg:57.12ms
step:303/2330 train_time:17306ms step_avg:57.12ms
step:304/2330 train_time:17365ms step_avg:57.12ms
step:305/2330 train_time:17420ms step_avg:57.12ms
step:306/2330 train_time:17479ms step_avg:57.12ms
step:307/2330 train_time:17535ms step_avg:57.12ms
step:308/2330 train_time:17593ms step_avg:57.12ms
step:309/2330 train_time:17649ms step_avg:57.12ms
step:310/2330 train_time:17708ms step_avg:57.12ms
step:311/2330 train_time:17764ms step_avg:57.12ms
step:312/2330 train_time:17823ms step_avg:57.12ms
step:313/2330 train_time:17879ms step_avg:57.12ms
step:314/2330 train_time:17937ms step_avg:57.13ms
step:315/2330 train_time:17993ms step_avg:57.12ms
step:316/2330 train_time:18051ms step_avg:57.12ms
step:317/2330 train_time:18106ms step_avg:57.12ms
step:318/2330 train_time:18165ms step_avg:57.12ms
step:319/2330 train_time:18220ms step_avg:57.12ms
step:320/2330 train_time:18279ms step_avg:57.12ms
step:321/2330 train_time:18334ms step_avg:57.11ms
step:322/2330 train_time:18393ms step_avg:57.12ms
step:323/2330 train_time:18448ms step_avg:57.11ms
step:324/2330 train_time:18508ms step_avg:57.12ms
step:325/2330 train_time:18563ms step_avg:57.12ms
step:326/2330 train_time:18622ms step_avg:57.12ms
step:327/2330 train_time:18677ms step_avg:57.12ms
step:328/2330 train_time:18736ms step_avg:57.12ms
step:329/2330 train_time:18791ms step_avg:57.12ms
step:330/2330 train_time:18851ms step_avg:57.12ms
step:331/2330 train_time:18906ms step_avg:57.12ms
step:332/2330 train_time:18965ms step_avg:57.12ms
step:333/2330 train_time:19021ms step_avg:57.12ms
step:334/2330 train_time:19079ms step_avg:57.12ms
step:335/2330 train_time:19135ms step_avg:57.12ms
step:336/2330 train_time:19193ms step_avg:57.12ms
step:337/2330 train_time:19247ms step_avg:57.11ms
step:338/2330 train_time:19307ms step_avg:57.12ms
step:339/2330 train_time:19362ms step_avg:57.12ms
step:340/2330 train_time:19421ms step_avg:57.12ms
step:341/2330 train_time:19476ms step_avg:57.11ms
step:342/2330 train_time:19534ms step_avg:57.12ms
step:343/2330 train_time:19590ms step_avg:57.11ms
step:344/2330 train_time:19649ms step_avg:57.12ms
step:345/2330 train_time:19705ms step_avg:57.12ms
step:346/2330 train_time:19763ms step_avg:57.12ms
step:347/2330 train_time:19819ms step_avg:57.11ms
step:348/2330 train_time:19877ms step_avg:57.12ms
step:349/2330 train_time:19932ms step_avg:57.11ms
step:350/2330 train_time:19991ms step_avg:57.12ms
step:351/2330 train_time:20047ms step_avg:57.11ms
step:352/2330 train_time:20105ms step_avg:57.12ms
step:353/2330 train_time:20161ms step_avg:57.11ms
step:354/2330 train_time:20219ms step_avg:57.12ms
step:355/2330 train_time:20275ms step_avg:57.11ms
step:356/2330 train_time:20333ms step_avg:57.12ms
step:357/2330 train_time:20388ms step_avg:57.11ms
step:358/2330 train_time:20447ms step_avg:57.12ms
step:359/2330 train_time:20503ms step_avg:57.11ms
step:360/2330 train_time:20561ms step_avg:57.11ms
step:361/2330 train_time:20617ms step_avg:57.11ms
step:362/2330 train_time:20675ms step_avg:57.11ms
step:363/2330 train_time:20731ms step_avg:57.11ms
step:364/2330 train_time:20789ms step_avg:57.11ms
step:365/2330 train_time:20845ms step_avg:57.11ms
step:366/2330 train_time:20903ms step_avg:57.11ms
step:367/2330 train_time:20958ms step_avg:57.11ms
step:368/2330 train_time:21017ms step_avg:57.11ms
step:369/2330 train_time:21073ms step_avg:57.11ms
step:370/2330 train_time:21131ms step_avg:57.11ms
step:371/2330 train_time:21187ms step_avg:57.11ms
step:372/2330 train_time:21246ms step_avg:57.11ms
step:373/2330 train_time:21302ms step_avg:57.11ms
step:374/2330 train_time:21360ms step_avg:57.11ms
step:375/2330 train_time:21416ms step_avg:57.11ms
step:376/2330 train_time:21475ms step_avg:57.11ms
step:377/2330 train_time:21530ms step_avg:57.11ms
step:378/2330 train_time:21589ms step_avg:57.11ms
step:379/2330 train_time:21645ms step_avg:57.11ms
step:380/2330 train_time:21703ms step_avg:57.11ms
step:381/2330 train_time:21759ms step_avg:57.11ms
step:382/2330 train_time:21818ms step_avg:57.11ms
step:383/2330 train_time:21873ms step_avg:57.11ms
step:384/2330 train_time:21932ms step_avg:57.11ms
step:385/2330 train_time:21988ms step_avg:57.11ms
step:386/2330 train_time:22046ms step_avg:57.11ms
step:387/2330 train_time:22102ms step_avg:57.11ms
step:388/2330 train_time:22160ms step_avg:57.11ms
step:389/2330 train_time:22216ms step_avg:57.11ms
step:390/2330 train_time:22275ms step_avg:57.12ms
step:391/2330 train_time:22330ms step_avg:57.11ms
step:392/2330 train_time:22389ms step_avg:57.11ms
step:393/2330 train_time:22445ms step_avg:57.11ms
step:394/2330 train_time:22503ms step_avg:57.12ms
step:395/2330 train_time:22559ms step_avg:57.11ms
step:396/2330 train_time:22617ms step_avg:57.11ms
step:397/2330 train_time:22673ms step_avg:57.11ms
step:398/2330 train_time:22732ms step_avg:57.11ms
step:399/2330 train_time:22787ms step_avg:57.11ms
step:400/2330 train_time:22846ms step_avg:57.12ms
step:401/2330 train_time:22903ms step_avg:57.11ms
step:402/2330 train_time:22962ms step_avg:57.12ms
step:403/2330 train_time:23018ms step_avg:57.12ms
step:404/2330 train_time:23076ms step_avg:57.12ms
step:405/2330 train_time:23132ms step_avg:57.12ms
step:406/2330 train_time:23190ms step_avg:57.12ms
step:407/2330 train_time:23246ms step_avg:57.11ms
step:408/2330 train_time:23305ms step_avg:57.12ms
step:409/2330 train_time:23360ms step_avg:57.12ms
step:410/2330 train_time:23419ms step_avg:57.12ms
step:411/2330 train_time:23475ms step_avg:57.12ms
step:412/2330 train_time:23533ms step_avg:57.12ms
step:413/2330 train_time:23588ms step_avg:57.11ms
step:414/2330 train_time:23647ms step_avg:57.12ms
step:415/2330 train_time:23703ms step_avg:57.12ms
step:416/2330 train_time:23762ms step_avg:57.12ms
step:417/2330 train_time:23817ms step_avg:57.12ms
step:418/2330 train_time:23877ms step_avg:57.12ms
step:419/2330 train_time:23932ms step_avg:57.12ms
step:420/2330 train_time:23991ms step_avg:57.12ms
step:421/2330 train_time:24047ms step_avg:57.12ms
step:422/2330 train_time:24105ms step_avg:57.12ms
step:423/2330 train_time:24161ms step_avg:57.12ms
step:424/2330 train_time:24220ms step_avg:57.12ms
step:425/2330 train_time:24275ms step_avg:57.12ms
step:426/2330 train_time:24335ms step_avg:57.12ms
step:427/2330 train_time:24390ms step_avg:57.12ms
step:428/2330 train_time:24449ms step_avg:57.12ms
step:429/2330 train_time:24504ms step_avg:57.12ms
step:430/2330 train_time:24563ms step_avg:57.12ms
step:431/2330 train_time:24619ms step_avg:57.12ms
step:432/2330 train_time:24678ms step_avg:57.12ms
step:433/2330 train_time:24733ms step_avg:57.12ms
step:434/2330 train_time:24793ms step_avg:57.13ms
step:435/2330 train_time:24848ms step_avg:57.12ms
step:436/2330 train_time:24907ms step_avg:57.13ms
step:437/2330 train_time:24963ms step_avg:57.12ms
step:438/2330 train_time:25022ms step_avg:57.13ms
step:439/2330 train_time:25077ms step_avg:57.12ms
step:440/2330 train_time:25136ms step_avg:57.13ms
step:441/2330 train_time:25192ms step_avg:57.12ms
step:442/2330 train_time:25251ms step_avg:57.13ms
step:443/2330 train_time:25306ms step_avg:57.13ms
step:444/2330 train_time:25365ms step_avg:57.13ms
step:445/2330 train_time:25421ms step_avg:57.12ms
step:446/2330 train_time:25479ms step_avg:57.13ms
step:447/2330 train_time:25535ms step_avg:57.12ms
step:448/2330 train_time:25592ms step_avg:57.13ms
step:449/2330 train_time:25648ms step_avg:57.12ms
step:450/2330 train_time:25708ms step_avg:57.13ms
step:451/2330 train_time:25763ms step_avg:57.13ms
step:452/2330 train_time:25822ms step_avg:57.13ms
step:453/2330 train_time:25878ms step_avg:57.13ms
step:454/2330 train_time:25936ms step_avg:57.13ms
step:455/2330 train_time:25992ms step_avg:57.13ms
step:456/2330 train_time:26051ms step_avg:57.13ms
step:457/2330 train_time:26106ms step_avg:57.13ms
step:458/2330 train_time:26165ms step_avg:57.13ms
step:459/2330 train_time:26221ms step_avg:57.13ms
step:460/2330 train_time:26279ms step_avg:57.13ms
step:461/2330 train_time:26336ms step_avg:57.13ms
step:462/2330 train_time:26394ms step_avg:57.13ms
step:463/2330 train_time:26450ms step_avg:57.13ms
step:464/2330 train_time:26509ms step_avg:57.13ms
step:465/2330 train_time:26564ms step_avg:57.13ms
step:466/2330 train_time:26623ms step_avg:57.13ms
step:467/2330 train_time:26679ms step_avg:57.13ms
step:468/2330 train_time:26737ms step_avg:57.13ms
step:469/2330 train_time:26793ms step_avg:57.13ms
step:470/2330 train_time:26851ms step_avg:57.13ms
step:471/2330 train_time:26907ms step_avg:57.13ms
step:472/2330 train_time:26967ms step_avg:57.13ms
step:473/2330 train_time:27023ms step_avg:57.13ms
step:474/2330 train_time:27081ms step_avg:57.13ms
step:475/2330 train_time:27137ms step_avg:57.13ms
step:476/2330 train_time:27195ms step_avg:57.13ms
step:477/2330 train_time:27250ms step_avg:57.13ms
step:478/2330 train_time:27309ms step_avg:57.13ms
step:479/2330 train_time:27365ms step_avg:57.13ms
step:480/2330 train_time:27424ms step_avg:57.13ms
step:481/2330 train_time:27481ms step_avg:57.13ms
step:482/2330 train_time:27540ms step_avg:57.14ms
step:483/2330 train_time:27595ms step_avg:57.13ms
step:484/2330 train_time:27653ms step_avg:57.13ms
step:485/2330 train_time:27708ms step_avg:57.13ms
step:486/2330 train_time:27768ms step_avg:57.14ms
step:487/2330 train_time:27824ms step_avg:57.13ms
step:488/2330 train_time:27883ms step_avg:57.14ms
step:489/2330 train_time:27939ms step_avg:57.13ms
step:490/2330 train_time:27997ms step_avg:57.14ms
step:491/2330 train_time:28053ms step_avg:57.13ms
step:492/2330 train_time:28112ms step_avg:57.14ms
step:493/2330 train_time:28167ms step_avg:57.13ms
step:494/2330 train_time:28227ms step_avg:57.14ms
step:495/2330 train_time:28284ms step_avg:57.14ms
step:496/2330 train_time:28342ms step_avg:57.14ms
step:497/2330 train_time:28398ms step_avg:57.14ms
step:498/2330 train_time:28457ms step_avg:57.14ms
step:499/2330 train_time:28512ms step_avg:57.14ms
step:500/2330 train_time:28572ms step_avg:57.14ms
step:500/2330 val_loss:4.4334 train_time:28651ms step_avg:57.30ms
step:501/2330 train_time:28669ms step_avg:57.22ms
step:502/2330 train_time:28689ms step_avg:57.15ms
step:503/2330 train_time:28746ms step_avg:57.15ms
step:504/2330 train_time:28808ms step_avg:57.16ms
step:505/2330 train_time:28865ms step_avg:57.16ms
step:506/2330 train_time:28925ms step_avg:57.16ms
step:507/2330 train_time:28980ms step_avg:57.16ms
step:508/2330 train_time:29039ms step_avg:57.16ms
step:509/2330 train_time:29094ms step_avg:57.16ms
step:510/2330 train_time:29153ms step_avg:57.16ms
step:511/2330 train_time:29208ms step_avg:57.16ms
step:512/2330 train_time:29266ms step_avg:57.16ms
step:513/2330 train_time:29321ms step_avg:57.16ms
step:514/2330 train_time:29380ms step_avg:57.16ms
step:515/2330 train_time:29434ms step_avg:57.15ms
step:516/2330 train_time:29493ms step_avg:57.16ms
step:517/2330 train_time:29548ms step_avg:57.15ms
step:518/2330 train_time:29607ms step_avg:57.16ms
step:519/2330 train_time:29664ms step_avg:57.16ms
step:520/2330 train_time:29723ms step_avg:57.16ms
step:521/2330 train_time:29780ms step_avg:57.16ms
step:522/2330 train_time:29840ms step_avg:57.16ms
step:523/2330 train_time:29895ms step_avg:57.16ms
step:524/2330 train_time:29956ms step_avg:57.17ms
step:525/2330 train_time:30011ms step_avg:57.16ms
step:526/2330 train_time:30071ms step_avg:57.17ms
step:527/2330 train_time:30127ms step_avg:57.17ms
step:528/2330 train_time:30185ms step_avg:57.17ms
step:529/2330 train_time:30240ms step_avg:57.17ms
step:530/2330 train_time:30299ms step_avg:57.17ms
step:531/2330 train_time:30354ms step_avg:57.16ms
step:532/2330 train_time:30412ms step_avg:57.17ms
step:533/2330 train_time:30468ms step_avg:57.16ms
step:534/2330 train_time:30526ms step_avg:57.16ms
step:535/2330 train_time:30582ms step_avg:57.16ms
step:536/2330 train_time:30640ms step_avg:57.17ms
step:537/2330 train_time:30696ms step_avg:57.16ms
step:538/2330 train_time:30756ms step_avg:57.17ms
step:539/2330 train_time:30812ms step_avg:57.17ms
step:540/2330 train_time:30873ms step_avg:57.17ms
step:541/2330 train_time:30929ms step_avg:57.17ms
step:542/2330 train_time:30989ms step_avg:57.17ms
step:543/2330 train_time:31045ms step_avg:57.17ms
step:544/2330 train_time:31103ms step_avg:57.17ms
step:545/2330 train_time:31159ms step_avg:57.17ms
step:546/2330 train_time:31217ms step_avg:57.17ms
step:547/2330 train_time:31273ms step_avg:57.17ms
step:548/2330 train_time:31332ms step_avg:57.17ms
step:549/2330 train_time:31387ms step_avg:57.17ms
step:550/2330 train_time:31446ms step_avg:57.17ms
step:551/2330 train_time:31501ms step_avg:57.17ms
step:552/2330 train_time:31560ms step_avg:57.17ms
step:553/2330 train_time:31615ms step_avg:57.17ms
step:554/2330 train_time:31674ms step_avg:57.17ms
step:555/2330 train_time:31730ms step_avg:57.17ms
step:556/2330 train_time:31789ms step_avg:57.17ms
step:557/2330 train_time:31845ms step_avg:57.17ms
step:558/2330 train_time:31904ms step_avg:57.18ms
step:559/2330 train_time:31960ms step_avg:57.17ms
step:560/2330 train_time:32019ms step_avg:57.18ms
step:561/2330 train_time:32075ms step_avg:57.18ms
step:562/2330 train_time:32134ms step_avg:57.18ms
step:563/2330 train_time:32190ms step_avg:57.18ms
step:564/2330 train_time:32249ms step_avg:57.18ms
step:565/2330 train_time:32304ms step_avg:57.18ms
step:566/2330 train_time:32363ms step_avg:57.18ms
step:567/2330 train_time:32419ms step_avg:57.18ms
step:568/2330 train_time:32477ms step_avg:57.18ms
step:569/2330 train_time:32533ms step_avg:57.18ms
step:570/2330 train_time:32592ms step_avg:57.18ms
step:571/2330 train_time:32648ms step_avg:57.18ms
step:572/2330 train_time:32707ms step_avg:57.18ms
step:573/2330 train_time:32764ms step_avg:57.18ms
step:574/2330 train_time:32822ms step_avg:57.18ms
step:575/2330 train_time:32878ms step_avg:57.18ms
step:576/2330 train_time:32937ms step_avg:57.18ms
step:577/2330 train_time:32992ms step_avg:57.18ms
step:578/2330 train_time:33052ms step_avg:57.18ms
step:579/2330 train_time:33108ms step_avg:57.18ms
step:580/2330 train_time:33167ms step_avg:57.18ms
step:581/2330 train_time:33223ms step_avg:57.18ms
step:582/2330 train_time:33282ms step_avg:57.19ms
step:583/2330 train_time:33338ms step_avg:57.18ms
step:584/2330 train_time:33395ms step_avg:57.18ms
step:585/2330 train_time:33451ms step_avg:57.18ms
step:586/2330 train_time:33512ms step_avg:57.19ms
step:587/2330 train_time:33568ms step_avg:57.19ms
step:588/2330 train_time:33626ms step_avg:57.19ms
step:589/2330 train_time:33682ms step_avg:57.19ms
step:590/2330 train_time:33741ms step_avg:57.19ms
step:591/2330 train_time:33796ms step_avg:57.18ms
step:592/2330 train_time:33855ms step_avg:57.19ms
step:593/2330 train_time:33911ms step_avg:57.19ms
step:594/2330 train_time:33971ms step_avg:57.19ms
step:595/2330 train_time:34026ms step_avg:57.19ms
step:596/2330 train_time:34086ms step_avg:57.19ms
step:597/2330 train_time:34142ms step_avg:57.19ms
step:598/2330 train_time:34200ms step_avg:57.19ms
step:599/2330 train_time:34256ms step_avg:57.19ms
step:600/2330 train_time:34315ms step_avg:57.19ms
step:601/2330 train_time:34370ms step_avg:57.19ms
step:602/2330 train_time:34429ms step_avg:57.19ms
step:603/2330 train_time:34485ms step_avg:57.19ms
step:604/2330 train_time:34543ms step_avg:57.19ms
step:605/2330 train_time:34598ms step_avg:57.19ms
step:606/2330 train_time:34658ms step_avg:57.19ms
step:607/2330 train_time:34713ms step_avg:57.19ms
step:608/2330 train_time:34773ms step_avg:57.19ms
step:609/2330 train_time:34829ms step_avg:57.19ms
step:610/2330 train_time:34888ms step_avg:57.19ms
step:611/2330 train_time:34944ms step_avg:57.19ms
step:612/2330 train_time:35002ms step_avg:57.19ms
step:613/2330 train_time:35057ms step_avg:57.19ms
step:614/2330 train_time:35117ms step_avg:57.19ms
step:615/2330 train_time:35173ms step_avg:57.19ms
step:616/2330 train_time:35233ms step_avg:57.20ms
step:617/2330 train_time:35288ms step_avg:57.19ms
step:618/2330 train_time:35347ms step_avg:57.20ms
step:619/2330 train_time:35403ms step_avg:57.19ms
step:620/2330 train_time:35462ms step_avg:57.20ms
step:621/2330 train_time:35518ms step_avg:57.19ms
step:622/2330 train_time:35576ms step_avg:57.20ms
step:623/2330 train_time:35632ms step_avg:57.19ms
step:624/2330 train_time:35691ms step_avg:57.20ms
step:625/2330 train_time:35746ms step_avg:57.19ms
step:626/2330 train_time:35806ms step_avg:57.20ms
step:627/2330 train_time:35862ms step_avg:57.20ms
step:628/2330 train_time:35920ms step_avg:57.20ms
step:629/2330 train_time:35977ms step_avg:57.20ms
step:630/2330 train_time:36035ms step_avg:57.20ms
step:631/2330 train_time:36090ms step_avg:57.20ms
step:632/2330 train_time:36150ms step_avg:57.20ms
step:633/2330 train_time:36206ms step_avg:57.20ms
step:634/2330 train_time:36265ms step_avg:57.20ms
step:635/2330 train_time:36320ms step_avg:57.20ms
step:636/2330 train_time:36379ms step_avg:57.20ms
step:637/2330 train_time:36435ms step_avg:57.20ms
step:638/2330 train_time:36494ms step_avg:57.20ms
step:639/2330 train_time:36550ms step_avg:57.20ms
step:640/2330 train_time:36608ms step_avg:57.20ms
step:641/2330 train_time:36664ms step_avg:57.20ms
step:642/2330 train_time:36723ms step_avg:57.20ms
step:643/2330 train_time:36779ms step_avg:57.20ms
step:644/2330 train_time:36838ms step_avg:57.20ms
step:645/2330 train_time:36894ms step_avg:57.20ms
step:646/2330 train_time:36953ms step_avg:57.20ms
step:647/2330 train_time:37009ms step_avg:57.20ms
step:648/2330 train_time:37069ms step_avg:57.20ms
step:649/2330 train_time:37125ms step_avg:57.20ms
step:650/2330 train_time:37183ms step_avg:57.21ms
step:651/2330 train_time:37239ms step_avg:57.20ms
step:652/2330 train_time:37299ms step_avg:57.21ms
step:653/2330 train_time:37354ms step_avg:57.20ms
step:654/2330 train_time:37412ms step_avg:57.21ms
step:655/2330 train_time:37468ms step_avg:57.20ms
step:656/2330 train_time:37527ms step_avg:57.21ms
step:657/2330 train_time:37583ms step_avg:57.20ms
step:658/2330 train_time:37642ms step_avg:57.21ms
step:659/2330 train_time:37697ms step_avg:57.20ms
step:660/2330 train_time:37756ms step_avg:57.21ms
step:661/2330 train_time:37811ms step_avg:57.20ms
step:662/2330 train_time:37871ms step_avg:57.21ms
step:663/2330 train_time:37927ms step_avg:57.21ms
step:664/2330 train_time:37986ms step_avg:57.21ms
step:665/2330 train_time:38042ms step_avg:57.21ms
step:666/2330 train_time:38101ms step_avg:57.21ms
step:667/2330 train_time:38157ms step_avg:57.21ms
step:668/2330 train_time:38215ms step_avg:57.21ms
step:669/2330 train_time:38271ms step_avg:57.21ms
step:670/2330 train_time:38331ms step_avg:57.21ms
step:671/2330 train_time:38387ms step_avg:57.21ms
step:672/2330 train_time:38445ms step_avg:57.21ms
step:673/2330 train_time:38501ms step_avg:57.21ms
step:674/2330 train_time:38560ms step_avg:57.21ms
step:675/2330 train_time:38616ms step_avg:57.21ms
step:676/2330 train_time:38675ms step_avg:57.21ms
step:677/2330 train_time:38731ms step_avg:57.21ms
step:678/2330 train_time:38790ms step_avg:57.21ms
step:679/2330 train_time:38846ms step_avg:57.21ms
step:680/2330 train_time:38905ms step_avg:57.21ms
step:681/2330 train_time:38960ms step_avg:57.21ms
step:682/2330 train_time:39020ms step_avg:57.21ms
step:683/2330 train_time:39076ms step_avg:57.21ms
step:684/2330 train_time:39135ms step_avg:57.21ms
step:685/2330 train_time:39190ms step_avg:57.21ms
step:686/2330 train_time:39249ms step_avg:57.21ms
step:687/2330 train_time:39306ms step_avg:57.21ms
step:688/2330 train_time:39364ms step_avg:57.22ms
step:689/2330 train_time:39420ms step_avg:57.21ms
step:690/2330 train_time:39479ms step_avg:57.22ms
step:691/2330 train_time:39534ms step_avg:57.21ms
step:692/2330 train_time:39593ms step_avg:57.22ms
step:693/2330 train_time:39649ms step_avg:57.21ms
step:694/2330 train_time:39708ms step_avg:57.22ms
step:695/2330 train_time:39763ms step_avg:57.21ms
step:696/2330 train_time:39823ms step_avg:57.22ms
step:697/2330 train_time:39879ms step_avg:57.21ms
step:698/2330 train_time:39938ms step_avg:57.22ms
step:699/2330 train_time:39993ms step_avg:57.22ms
step:700/2330 train_time:40052ms step_avg:57.22ms
step:701/2330 train_time:40108ms step_avg:57.22ms
step:702/2330 train_time:40168ms step_avg:57.22ms
step:703/2330 train_time:40224ms step_avg:57.22ms
step:704/2330 train_time:40284ms step_avg:57.22ms
step:705/2330 train_time:40340ms step_avg:57.22ms
step:706/2330 train_time:40398ms step_avg:57.22ms
step:707/2330 train_time:40454ms step_avg:57.22ms
step:708/2330 train_time:40512ms step_avg:57.22ms
step:709/2330 train_time:40568ms step_avg:57.22ms
step:710/2330 train_time:40628ms step_avg:57.22ms
step:711/2330 train_time:40684ms step_avg:57.22ms
step:712/2330 train_time:40742ms step_avg:57.22ms
step:713/2330 train_time:40798ms step_avg:57.22ms
step:714/2330 train_time:40858ms step_avg:57.22ms
step:715/2330 train_time:40914ms step_avg:57.22ms
step:716/2330 train_time:40974ms step_avg:57.23ms
step:717/2330 train_time:41029ms step_avg:57.22ms
step:718/2330 train_time:41088ms step_avg:57.23ms
step:719/2330 train_time:41144ms step_avg:57.22ms
step:720/2330 train_time:41203ms step_avg:57.23ms
step:721/2330 train_time:41259ms step_avg:57.22ms
step:722/2330 train_time:41317ms step_avg:57.23ms
step:723/2330 train_time:41373ms step_avg:57.22ms
step:724/2330 train_time:41432ms step_avg:57.23ms
step:725/2330 train_time:41488ms step_avg:57.22ms
step:726/2330 train_time:41546ms step_avg:57.23ms
step:727/2330 train_time:41602ms step_avg:57.22ms
step:728/2330 train_time:41661ms step_avg:57.23ms
step:729/2330 train_time:41717ms step_avg:57.23ms
step:730/2330 train_time:41777ms step_avg:57.23ms
step:731/2330 train_time:41832ms step_avg:57.23ms
step:732/2330 train_time:41891ms step_avg:57.23ms
step:733/2330 train_time:41948ms step_avg:57.23ms
step:734/2330 train_time:42006ms step_avg:57.23ms
step:735/2330 train_time:42062ms step_avg:57.23ms
step:736/2330 train_time:42121ms step_avg:57.23ms
step:737/2330 train_time:42177ms step_avg:57.23ms
step:738/2330 train_time:42236ms step_avg:57.23ms
step:739/2330 train_time:42291ms step_avg:57.23ms
step:740/2330 train_time:42351ms step_avg:57.23ms
step:741/2330 train_time:42407ms step_avg:57.23ms
step:742/2330 train_time:42466ms step_avg:57.23ms
step:743/2330 train_time:42522ms step_avg:57.23ms
step:744/2330 train_time:42581ms step_avg:57.23ms
step:745/2330 train_time:42637ms step_avg:57.23ms
step:746/2330 train_time:42695ms step_avg:57.23ms
step:747/2330 train_time:42750ms step_avg:57.23ms
step:748/2330 train_time:42810ms step_avg:57.23ms
step:749/2330 train_time:42866ms step_avg:57.23ms
step:750/2330 train_time:42925ms step_avg:57.23ms
step:750/2330 val_loss:4.2449 train_time:43004ms step_avg:57.34ms
step:751/2330 train_time:43023ms step_avg:57.29ms
step:752/2330 train_time:43044ms step_avg:57.24ms
step:753/2330 train_time:43101ms step_avg:57.24ms
step:754/2330 train_time:43161ms step_avg:57.24ms
step:755/2330 train_time:43218ms step_avg:57.24ms
step:756/2330 train_time:43280ms step_avg:57.25ms
step:757/2330 train_time:43335ms step_avg:57.25ms
step:758/2330 train_time:43394ms step_avg:57.25ms
step:759/2330 train_time:43450ms step_avg:57.25ms
step:760/2330 train_time:43508ms step_avg:57.25ms
step:761/2330 train_time:43563ms step_avg:57.24ms
step:762/2330 train_time:43622ms step_avg:57.25ms
step:763/2330 train_time:43677ms step_avg:57.24ms
step:764/2330 train_time:43736ms step_avg:57.25ms
step:765/2330 train_time:43793ms step_avg:57.25ms
step:766/2330 train_time:43850ms step_avg:57.25ms
step:767/2330 train_time:43906ms step_avg:57.24ms
step:768/2330 train_time:43966ms step_avg:57.25ms
step:769/2330 train_time:44024ms step_avg:57.25ms
step:770/2330 train_time:44083ms step_avg:57.25ms
step:771/2330 train_time:44141ms step_avg:57.25ms
step:772/2330 train_time:44202ms step_avg:57.26ms
step:773/2330 train_time:44259ms step_avg:57.26ms
step:774/2330 train_time:44319ms step_avg:57.26ms
step:775/2330 train_time:44375ms step_avg:57.26ms
step:776/2330 train_time:44436ms step_avg:57.26ms
step:777/2330 train_time:44492ms step_avg:57.26ms
step:778/2330 train_time:44552ms step_avg:57.26ms
step:779/2330 train_time:44608ms step_avg:57.26ms
step:780/2330 train_time:44668ms step_avg:57.27ms
step:781/2330 train_time:44724ms step_avg:57.27ms
step:782/2330 train_time:44783ms step_avg:57.27ms
step:783/2330 train_time:44839ms step_avg:57.27ms
step:784/2330 train_time:44898ms step_avg:57.27ms
step:785/2330 train_time:44955ms step_avg:57.27ms
step:786/2330 train_time:45015ms step_avg:57.27ms
step:787/2330 train_time:45072ms step_avg:57.27ms
step:788/2330 train_time:45133ms step_avg:57.28ms
step:789/2330 train_time:45191ms step_avg:57.28ms
step:790/2330 train_time:45251ms step_avg:57.28ms
step:791/2330 train_time:45309ms step_avg:57.28ms
step:792/2330 train_time:45369ms step_avg:57.28ms
step:793/2330 train_time:45425ms step_avg:57.28ms
step:794/2330 train_time:45485ms step_avg:57.29ms
step:795/2330 train_time:45542ms step_avg:57.28ms
step:796/2330 train_time:45601ms step_avg:57.29ms
step:797/2330 train_time:45657ms step_avg:57.29ms
step:798/2330 train_time:45716ms step_avg:57.29ms
step:799/2330 train_time:45772ms step_avg:57.29ms
step:800/2330 train_time:45832ms step_avg:57.29ms
step:801/2330 train_time:45888ms step_avg:57.29ms
step:802/2330 train_time:45948ms step_avg:57.29ms
step:803/2330 train_time:46005ms step_avg:57.29ms
step:804/2330 train_time:46064ms step_avg:57.29ms
step:805/2330 train_time:46121ms step_avg:57.29ms
step:806/2330 train_time:46181ms step_avg:57.30ms
step:807/2330 train_time:46237ms step_avg:57.30ms
step:808/2330 train_time:46298ms step_avg:57.30ms
step:809/2330 train_time:46355ms step_avg:57.30ms
step:810/2330 train_time:46416ms step_avg:57.30ms
step:811/2330 train_time:46473ms step_avg:57.30ms
step:812/2330 train_time:46533ms step_avg:57.31ms
step:813/2330 train_time:46589ms step_avg:57.31ms
step:814/2330 train_time:46648ms step_avg:57.31ms
step:815/2330 train_time:46705ms step_avg:57.31ms
step:816/2330 train_time:46764ms step_avg:57.31ms
step:817/2330 train_time:46820ms step_avg:57.31ms
step:818/2330 train_time:46879ms step_avg:57.31ms
step:819/2330 train_time:46935ms step_avg:57.31ms
step:820/2330 train_time:46996ms step_avg:57.31ms
step:821/2330 train_time:47053ms step_avg:57.31ms
step:822/2330 train_time:47114ms step_avg:57.32ms
step:823/2330 train_time:47170ms step_avg:57.32ms
step:824/2330 train_time:47230ms step_avg:57.32ms
step:825/2330 train_time:47287ms step_avg:57.32ms
step:826/2330 train_time:47346ms step_avg:57.32ms
step:827/2330 train_time:47404ms step_avg:57.32ms
step:828/2330 train_time:47464ms step_avg:57.32ms
step:829/2330 train_time:47521ms step_avg:57.32ms
step:830/2330 train_time:47580ms step_avg:57.33ms
step:831/2330 train_time:47636ms step_avg:57.32ms
step:832/2330 train_time:47696ms step_avg:57.33ms
step:833/2330 train_time:47752ms step_avg:57.33ms
step:834/2330 train_time:47814ms step_avg:57.33ms
step:835/2330 train_time:47870ms step_avg:57.33ms
step:836/2330 train_time:47929ms step_avg:57.33ms
step:837/2330 train_time:47986ms step_avg:57.33ms
step:838/2330 train_time:48046ms step_avg:57.33ms
step:839/2330 train_time:48103ms step_avg:57.33ms
step:840/2330 train_time:48162ms step_avg:57.34ms
step:841/2330 train_time:48219ms step_avg:57.34ms
step:842/2330 train_time:48279ms step_avg:57.34ms
step:843/2330 train_time:48335ms step_avg:57.34ms
step:844/2330 train_time:48396ms step_avg:57.34ms
step:845/2330 train_time:48453ms step_avg:57.34ms
step:846/2330 train_time:48514ms step_avg:57.35ms
step:847/2330 train_time:48572ms step_avg:57.35ms
step:848/2330 train_time:48632ms step_avg:57.35ms
step:849/2330 train_time:48688ms step_avg:57.35ms
step:850/2330 train_time:48748ms step_avg:57.35ms
step:851/2330 train_time:48805ms step_avg:57.35ms
step:852/2330 train_time:48864ms step_avg:57.35ms
step:853/2330 train_time:48920ms step_avg:57.35ms
step:854/2330 train_time:48980ms step_avg:57.35ms
step:855/2330 train_time:49036ms step_avg:57.35ms
step:856/2330 train_time:49095ms step_avg:57.35ms
step:857/2330 train_time:49152ms step_avg:57.35ms
step:858/2330 train_time:49213ms step_avg:57.36ms
step:859/2330 train_time:49270ms step_avg:57.36ms
step:860/2330 train_time:49330ms step_avg:57.36ms
step:861/2330 train_time:49387ms step_avg:57.36ms
step:862/2330 train_time:49448ms step_avg:57.36ms
step:863/2330 train_time:49504ms step_avg:57.36ms
step:864/2330 train_time:49566ms step_avg:57.37ms
step:865/2330 train_time:49622ms step_avg:57.37ms
step:866/2330 train_time:49682ms step_avg:57.37ms
step:867/2330 train_time:49738ms step_avg:57.37ms
step:868/2330 train_time:49798ms step_avg:57.37ms
step:869/2330 train_time:49854ms step_avg:57.37ms
step:870/2330 train_time:49914ms step_avg:57.37ms
step:871/2330 train_time:49971ms step_avg:57.37ms
step:872/2330 train_time:50031ms step_avg:57.37ms
step:873/2330 train_time:50087ms step_avg:57.37ms
step:874/2330 train_time:50148ms step_avg:57.38ms
step:875/2330 train_time:50204ms step_avg:57.38ms
step:876/2330 train_time:50265ms step_avg:57.38ms
step:877/2330 train_time:50322ms step_avg:57.38ms
step:878/2330 train_time:50382ms step_avg:57.38ms
step:879/2330 train_time:50438ms step_avg:57.38ms
step:880/2330 train_time:50498ms step_avg:57.38ms
step:881/2330 train_time:50555ms step_avg:57.38ms
step:882/2330 train_time:50615ms step_avg:57.39ms
step:883/2330 train_time:50672ms step_avg:57.39ms
step:884/2330 train_time:50732ms step_avg:57.39ms
step:885/2330 train_time:50788ms step_avg:57.39ms
step:886/2330 train_time:50848ms step_avg:57.39ms
step:887/2330 train_time:50905ms step_avg:57.39ms
step:888/2330 train_time:50964ms step_avg:57.39ms
step:889/2330 train_time:51021ms step_avg:57.39ms
step:890/2330 train_time:51081ms step_avg:57.39ms
step:891/2330 train_time:51137ms step_avg:57.39ms
step:892/2330 train_time:51198ms step_avg:57.40ms
step:893/2330 train_time:51254ms step_avg:57.40ms
step:894/2330 train_time:51315ms step_avg:57.40ms
step:895/2330 train_time:51373ms step_avg:57.40ms
step:896/2330 train_time:51432ms step_avg:57.40ms
step:897/2330 train_time:51489ms step_avg:57.40ms
step:898/2330 train_time:51549ms step_avg:57.40ms
step:899/2330 train_time:51606ms step_avg:57.40ms
step:900/2330 train_time:51667ms step_avg:57.41ms
step:901/2330 train_time:51724ms step_avg:57.41ms
step:902/2330 train_time:51783ms step_avg:57.41ms
step:903/2330 train_time:51839ms step_avg:57.41ms
step:904/2330 train_time:51899ms step_avg:57.41ms
step:905/2330 train_time:51956ms step_avg:57.41ms
step:906/2330 train_time:52017ms step_avg:57.41ms
step:907/2330 train_time:52074ms step_avg:57.41ms
step:908/2330 train_time:52133ms step_avg:57.42ms
step:909/2330 train_time:52190ms step_avg:57.41ms
step:910/2330 train_time:52249ms step_avg:57.42ms
step:911/2330 train_time:52306ms step_avg:57.42ms
step:912/2330 train_time:52366ms step_avg:57.42ms
step:913/2330 train_time:52423ms step_avg:57.42ms
step:914/2330 train_time:52482ms step_avg:57.42ms
step:915/2330 train_time:52538ms step_avg:57.42ms
step:916/2330 train_time:52599ms step_avg:57.42ms
step:917/2330 train_time:52655ms step_avg:57.42ms
step:918/2330 train_time:52716ms step_avg:57.43ms
step:919/2330 train_time:52773ms step_avg:57.42ms
step:920/2330 train_time:52832ms step_avg:57.43ms
step:921/2330 train_time:52889ms step_avg:57.43ms
step:922/2330 train_time:52949ms step_avg:57.43ms
step:923/2330 train_time:53006ms step_avg:57.43ms
step:924/2330 train_time:53066ms step_avg:57.43ms
step:925/2330 train_time:53123ms step_avg:57.43ms
step:926/2330 train_time:53182ms step_avg:57.43ms
step:927/2330 train_time:53238ms step_avg:57.43ms
step:928/2330 train_time:53299ms step_avg:57.43ms
step:929/2330 train_time:53356ms step_avg:57.43ms
step:930/2330 train_time:53416ms step_avg:57.44ms
step:931/2330 train_time:53473ms step_avg:57.44ms
step:932/2330 train_time:53532ms step_avg:57.44ms
step:933/2330 train_time:53589ms step_avg:57.44ms
step:934/2330 train_time:53649ms step_avg:57.44ms
step:935/2330 train_time:53706ms step_avg:57.44ms
step:936/2330 train_time:53766ms step_avg:57.44ms
step:937/2330 train_time:53823ms step_avg:57.44ms
step:938/2330 train_time:53882ms step_avg:57.44ms
step:939/2330 train_time:53938ms step_avg:57.44ms
step:940/2330 train_time:53997ms step_avg:57.44ms
step:941/2330 train_time:54054ms step_avg:57.44ms
step:942/2330 train_time:54114ms step_avg:57.45ms
step:943/2330 train_time:54171ms step_avg:57.45ms
step:944/2330 train_time:54230ms step_avg:57.45ms
step:945/2330 train_time:54287ms step_avg:57.45ms
step:946/2330 train_time:54346ms step_avg:57.45ms
step:947/2330 train_time:54402ms step_avg:57.45ms
step:948/2330 train_time:54463ms step_avg:57.45ms
step:949/2330 train_time:54519ms step_avg:57.45ms
step:950/2330 train_time:54579ms step_avg:57.45ms
step:951/2330 train_time:54635ms step_avg:57.45ms
step:952/2330 train_time:54696ms step_avg:57.45ms
step:953/2330 train_time:54753ms step_avg:57.45ms
step:954/2330 train_time:54813ms step_avg:57.46ms
step:955/2330 train_time:54870ms step_avg:57.46ms
step:956/2330 train_time:54930ms step_avg:57.46ms
step:957/2330 train_time:54986ms step_avg:57.46ms
step:958/2330 train_time:55047ms step_avg:57.46ms
step:959/2330 train_time:55103ms step_avg:57.46ms
step:960/2330 train_time:55163ms step_avg:57.46ms
step:961/2330 train_time:55220ms step_avg:57.46ms
step:962/2330 train_time:55279ms step_avg:57.46ms
step:963/2330 train_time:55335ms step_avg:57.46ms
step:964/2330 train_time:55395ms step_avg:57.46ms
step:965/2330 train_time:55452ms step_avg:57.46ms
step:966/2330 train_time:55512ms step_avg:57.47ms
step:967/2330 train_time:55569ms step_avg:57.47ms
step:968/2330 train_time:55629ms step_avg:57.47ms
step:969/2330 train_time:55685ms step_avg:57.47ms
step:970/2330 train_time:55745ms step_avg:57.47ms
step:971/2330 train_time:55802ms step_avg:57.47ms
step:972/2330 train_time:55862ms step_avg:57.47ms
step:973/2330 train_time:55919ms step_avg:57.47ms
step:974/2330 train_time:55979ms step_avg:57.47ms
step:975/2330 train_time:56035ms step_avg:57.47ms
step:976/2330 train_time:56095ms step_avg:57.47ms
step:977/2330 train_time:56152ms step_avg:57.47ms
step:978/2330 train_time:56212ms step_avg:57.48ms
step:979/2330 train_time:56269ms step_avg:57.48ms
step:980/2330 train_time:56329ms step_avg:57.48ms
step:981/2330 train_time:56386ms step_avg:57.48ms
step:982/2330 train_time:56445ms step_avg:57.48ms
step:983/2330 train_time:56502ms step_avg:57.48ms
step:984/2330 train_time:56562ms step_avg:57.48ms
step:985/2330 train_time:56618ms step_avg:57.48ms
step:986/2330 train_time:56678ms step_avg:57.48ms
step:987/2330 train_time:56734ms step_avg:57.48ms
step:988/2330 train_time:56795ms step_avg:57.48ms
step:989/2330 train_time:56851ms step_avg:57.48ms
step:990/2330 train_time:56912ms step_avg:57.49ms
step:991/2330 train_time:56969ms step_avg:57.49ms
step:992/2330 train_time:57029ms step_avg:57.49ms
step:993/2330 train_time:57086ms step_avg:57.49ms
step:994/2330 train_time:57145ms step_avg:57.49ms
step:995/2330 train_time:57202ms step_avg:57.49ms
step:996/2330 train_time:57261ms step_avg:57.49ms
step:997/2330 train_time:57318ms step_avg:57.49ms
step:998/2330 train_time:57377ms step_avg:57.49ms
step:999/2330 train_time:57434ms step_avg:57.49ms
step:1000/2330 train_time:57494ms step_avg:57.49ms
step:1000/2330 val_loss:4.1040 train_time:57574ms step_avg:57.57ms
step:1001/2330 train_time:57592ms step_avg:57.53ms
step:1002/2330 train_time:57613ms step_avg:57.50ms
step:1003/2330 train_time:57670ms step_avg:57.50ms
step:1004/2330 train_time:57732ms step_avg:57.50ms
step:1005/2330 train_time:57789ms step_avg:57.50ms
step:1006/2330 train_time:57849ms step_avg:57.50ms
step:1007/2330 train_time:57906ms step_avg:57.50ms
step:1008/2330 train_time:57965ms step_avg:57.50ms
step:1009/2330 train_time:58021ms step_avg:57.50ms
step:1010/2330 train_time:58080ms step_avg:57.51ms
step:1011/2330 train_time:58136ms step_avg:57.50ms
step:1012/2330 train_time:58195ms step_avg:57.51ms
step:1013/2330 train_time:58251ms step_avg:57.50ms
step:1014/2330 train_time:58310ms step_avg:57.50ms
step:1015/2330 train_time:58366ms step_avg:57.50ms
step:1016/2330 train_time:58425ms step_avg:57.50ms
step:1017/2330 train_time:58481ms step_avg:57.50ms
step:1018/2330 train_time:58545ms step_avg:57.51ms
step:1019/2330 train_time:58602ms step_avg:57.51ms
step:1020/2330 train_time:58664ms step_avg:57.51ms
step:1021/2330 train_time:58722ms step_avg:57.51ms
step:1022/2330 train_time:58782ms step_avg:57.52ms
step:1023/2330 train_time:58839ms step_avg:57.52ms
step:1024/2330 train_time:58900ms step_avg:57.52ms
step:1025/2330 train_time:58957ms step_avg:57.52ms
step:1026/2330 train_time:59017ms step_avg:57.52ms
step:1027/2330 train_time:59073ms step_avg:57.52ms
step:1028/2330 train_time:59131ms step_avg:57.52ms
step:1029/2330 train_time:59187ms step_avg:57.52ms
step:1030/2330 train_time:59246ms step_avg:57.52ms
step:1031/2330 train_time:59302ms step_avg:57.52ms
step:1032/2330 train_time:59362ms step_avg:57.52ms
step:1033/2330 train_time:59418ms step_avg:57.52ms
step:1034/2330 train_time:59478ms step_avg:57.52ms
step:1035/2330 train_time:59534ms step_avg:57.52ms
step:1036/2330 train_time:59596ms step_avg:57.52ms
step:1037/2330 train_time:59652ms step_avg:57.52ms
step:1038/2330 train_time:59714ms step_avg:57.53ms
step:1039/2330 train_time:59771ms step_avg:57.53ms
step:1040/2330 train_time:59831ms step_avg:57.53ms
step:1041/2330 train_time:59888ms step_avg:57.53ms
step:1042/2330 train_time:59948ms step_avg:57.53ms
step:1043/2330 train_time:60005ms step_avg:57.53ms
step:1044/2330 train_time:60064ms step_avg:57.53ms
step:1045/2330 train_time:60121ms step_avg:57.53ms
step:1046/2330 train_time:60180ms step_avg:57.53ms
step:1047/2330 train_time:60237ms step_avg:57.53ms
step:1048/2330 train_time:60296ms step_avg:57.53ms
step:1049/2330 train_time:60353ms step_avg:57.53ms
step:1050/2330 train_time:60412ms step_avg:57.53ms
step:1051/2330 train_time:60468ms step_avg:57.53ms
step:1052/2330 train_time:60528ms step_avg:57.54ms
step:1053/2330 train_time:60584ms step_avg:57.53ms
step:1054/2330 train_time:60646ms step_avg:57.54ms
step:1055/2330 train_time:60702ms step_avg:57.54ms
step:1056/2330 train_time:60763ms step_avg:57.54ms
step:1057/2330 train_time:60820ms step_avg:57.54ms
step:1058/2330 train_time:60880ms step_avg:57.54ms
step:1059/2330 train_time:60937ms step_avg:57.54ms
step:1060/2330 train_time:60997ms step_avg:57.54ms
step:1061/2330 train_time:61054ms step_avg:57.54ms
step:1062/2330 train_time:61114ms step_avg:57.55ms
step:1063/2330 train_time:61171ms step_avg:57.55ms
step:1064/2330 train_time:61229ms step_avg:57.55ms
step:1065/2330 train_time:61285ms step_avg:57.54ms
step:1066/2330 train_time:61345ms step_avg:57.55ms
step:1067/2330 train_time:61402ms step_avg:57.55ms
step:1068/2330 train_time:61462ms step_avg:57.55ms
step:1069/2330 train_time:61519ms step_avg:57.55ms
step:1070/2330 train_time:61579ms step_avg:57.55ms
step:1071/2330 train_time:61636ms step_avg:57.55ms
step:1072/2330 train_time:61697ms step_avg:57.55ms
step:1073/2330 train_time:61753ms step_avg:57.55ms
step:1074/2330 train_time:61814ms step_avg:57.55ms
step:1075/2330 train_time:61869ms step_avg:57.55ms
step:1076/2330 train_time:61930ms step_avg:57.56ms
step:1077/2330 train_time:61986ms step_avg:57.55ms
step:1078/2330 train_time:62046ms step_avg:57.56ms
step:1079/2330 train_time:62102ms step_avg:57.56ms
step:1080/2330 train_time:62162ms step_avg:57.56ms
step:1081/2330 train_time:62218ms step_avg:57.56ms
step:1082/2330 train_time:62278ms step_avg:57.56ms
step:1083/2330 train_time:62335ms step_avg:57.56ms
step:1084/2330 train_time:62396ms step_avg:57.56ms
step:1085/2330 train_time:62452ms step_avg:57.56ms
step:1086/2330 train_time:62512ms step_avg:57.56ms
step:1087/2330 train_time:62568ms step_avg:57.56ms
step:1088/2330 train_time:62628ms step_avg:57.56ms
step:1089/2330 train_time:62684ms step_avg:57.56ms
step:1090/2330 train_time:62745ms step_avg:57.56ms
step:1091/2330 train_time:62802ms step_avg:57.56ms
step:1092/2330 train_time:62862ms step_avg:57.57ms
step:1093/2330 train_time:62919ms step_avg:57.57ms
step:1094/2330 train_time:62978ms step_avg:57.57ms
step:1095/2330 train_time:63035ms step_avg:57.57ms
step:1096/2330 train_time:63094ms step_avg:57.57ms
step:1097/2330 train_time:63151ms step_avg:57.57ms
step:1098/2330 train_time:63210ms step_avg:57.57ms
step:1099/2330 train_time:63267ms step_avg:57.57ms
step:1100/2330 train_time:63327ms step_avg:57.57ms
step:1101/2330 train_time:63383ms step_avg:57.57ms
step:1102/2330 train_time:63444ms step_avg:57.57ms
step:1103/2330 train_time:63501ms step_avg:57.57ms
step:1104/2330 train_time:63561ms step_avg:57.57ms
step:1105/2330 train_time:63618ms step_avg:57.57ms
step:1106/2330 train_time:63678ms step_avg:57.58ms
step:1107/2330 train_time:63736ms step_avg:57.57ms
step:1108/2330 train_time:63795ms step_avg:57.58ms
step:1109/2330 train_time:63852ms step_avg:57.58ms
step:1110/2330 train_time:63912ms step_avg:57.58ms
step:1111/2330 train_time:63969ms step_avg:57.58ms
step:1112/2330 train_time:64028ms step_avg:57.58ms
step:1113/2330 train_time:64085ms step_avg:57.58ms
step:1114/2330 train_time:64145ms step_avg:57.58ms
step:1115/2330 train_time:64202ms step_avg:57.58ms
step:1116/2330 train_time:64261ms step_avg:57.58ms
step:1117/2330 train_time:64318ms step_avg:57.58ms
step:1118/2330 train_time:64378ms step_avg:57.58ms
step:1119/2330 train_time:64434ms step_avg:57.58ms
step:1120/2330 train_time:64495ms step_avg:57.58ms
step:1121/2330 train_time:64551ms step_avg:57.58ms
step:1122/2330 train_time:64610ms step_avg:57.58ms
step:1123/2330 train_time:64667ms step_avg:57.58ms
step:1124/2330 train_time:64726ms step_avg:57.59ms
step:1125/2330 train_time:64783ms step_avg:57.58ms
step:1126/2330 train_time:64844ms step_avg:57.59ms
step:1127/2330 train_time:64900ms step_avg:57.59ms
step:1128/2330 train_time:64961ms step_avg:57.59ms
step:1129/2330 train_time:65017ms step_avg:57.59ms
step:1130/2330 train_time:65077ms step_avg:57.59ms
step:1131/2330 train_time:65134ms step_avg:57.59ms
step:1132/2330 train_time:65194ms step_avg:57.59ms
step:1133/2330 train_time:65251ms step_avg:57.59ms
step:1134/2330 train_time:65310ms step_avg:57.59ms
step:1135/2330 train_time:65366ms step_avg:57.59ms
step:1136/2330 train_time:65427ms step_avg:57.59ms
step:1137/2330 train_time:65484ms step_avg:57.59ms
step:1138/2330 train_time:65544ms step_avg:57.60ms
step:1139/2330 train_time:65601ms step_avg:57.59ms
step:1140/2330 train_time:65660ms step_avg:57.60ms
step:1141/2330 train_time:65717ms step_avg:57.60ms
step:1142/2330 train_time:65777ms step_avg:57.60ms
step:1143/2330 train_time:65834ms step_avg:57.60ms
step:1144/2330 train_time:65893ms step_avg:57.60ms
step:1145/2330 train_time:65950ms step_avg:57.60ms
step:1146/2330 train_time:66010ms step_avg:57.60ms
step:1147/2330 train_time:66066ms step_avg:57.60ms
step:1148/2330 train_time:66127ms step_avg:57.60ms
step:1149/2330 train_time:66183ms step_avg:57.60ms
step:1150/2330 train_time:66243ms step_avg:57.60ms
step:1151/2330 train_time:66301ms step_avg:57.60ms
step:1152/2330 train_time:66360ms step_avg:57.60ms
step:1153/2330 train_time:66417ms step_avg:57.60ms
step:1154/2330 train_time:66476ms step_avg:57.61ms
step:1155/2330 train_time:66533ms step_avg:57.60ms
step:1156/2330 train_time:66593ms step_avg:57.61ms
step:1157/2330 train_time:66649ms step_avg:57.61ms
step:1158/2330 train_time:66710ms step_avg:57.61ms
step:1159/2330 train_time:66766ms step_avg:57.61ms
step:1160/2330 train_time:66826ms step_avg:57.61ms
step:1161/2330 train_time:66882ms step_avg:57.61ms
step:1162/2330 train_time:66943ms step_avg:57.61ms
step:1163/2330 train_time:67000ms step_avg:57.61ms
step:1164/2330 train_time:67059ms step_avg:57.61ms
step:1165/2330 train_time:67117ms step_avg:57.61ms
step:1166/2330 train_time:67176ms step_avg:57.61ms
step:1167/2330 train_time:67233ms step_avg:57.61ms
step:1168/2330 train_time:67292ms step_avg:57.61ms
step:1169/2330 train_time:67349ms step_avg:57.61ms
step:1170/2330 train_time:67409ms step_avg:57.61ms
step:1171/2330 train_time:67465ms step_avg:57.61ms
step:1172/2330 train_time:67525ms step_avg:57.62ms
step:1173/2330 train_time:67582ms step_avg:57.61ms
step:1174/2330 train_time:67641ms step_avg:57.62ms
step:1175/2330 train_time:67699ms step_avg:57.62ms
step:1176/2330 train_time:67759ms step_avg:57.62ms
step:1177/2330 train_time:67817ms step_avg:57.62ms
step:1178/2330 train_time:67876ms step_avg:57.62ms
step:1179/2330 train_time:67933ms step_avg:57.62ms
step:1180/2330 train_time:67993ms step_avg:57.62ms
step:1181/2330 train_time:68049ms step_avg:57.62ms
step:1182/2330 train_time:68108ms step_avg:57.62ms
step:1183/2330 train_time:68165ms step_avg:57.62ms
step:1184/2330 train_time:68225ms step_avg:57.62ms
step:1185/2330 train_time:68282ms step_avg:57.62ms
step:1186/2330 train_time:68342ms step_avg:57.62ms
step:1187/2330 train_time:68399ms step_avg:57.62ms
step:1188/2330 train_time:68458ms step_avg:57.62ms
step:1189/2330 train_time:68514ms step_avg:57.62ms
step:1190/2330 train_time:68574ms step_avg:57.62ms
step:1191/2330 train_time:68630ms step_avg:57.62ms
step:1192/2330 train_time:68690ms step_avg:57.63ms
step:1193/2330 train_time:68748ms step_avg:57.63ms
step:1194/2330 train_time:68807ms step_avg:57.63ms
step:1195/2330 train_time:68863ms step_avg:57.63ms
step:1196/2330 train_time:68924ms step_avg:57.63ms
step:1197/2330 train_time:68981ms step_avg:57.63ms
step:1198/2330 train_time:69041ms step_avg:57.63ms
step:1199/2330 train_time:69098ms step_avg:57.63ms
step:1200/2330 train_time:69158ms step_avg:57.63ms
step:1201/2330 train_time:69215ms step_avg:57.63ms
step:1202/2330 train_time:69274ms step_avg:57.63ms
step:1203/2330 train_time:69331ms step_avg:57.63ms
step:1204/2330 train_time:69390ms step_avg:57.63ms
step:1205/2330 train_time:69447ms step_avg:57.63ms
step:1206/2330 train_time:69507ms step_avg:57.63ms
step:1207/2330 train_time:69563ms step_avg:57.63ms
step:1208/2330 train_time:69623ms step_avg:57.64ms
step:1209/2330 train_time:69680ms step_avg:57.63ms
step:1210/2330 train_time:69740ms step_avg:57.64ms
step:1211/2330 train_time:69798ms step_avg:57.64ms
step:1212/2330 train_time:69857ms step_avg:57.64ms
step:1213/2330 train_time:69914ms step_avg:57.64ms
step:1214/2330 train_time:69973ms step_avg:57.64ms
step:1215/2330 train_time:70030ms step_avg:57.64ms
step:1216/2330 train_time:70090ms step_avg:57.64ms
step:1217/2330 train_time:70146ms step_avg:57.64ms
step:1218/2330 train_time:70207ms step_avg:57.64ms
step:1219/2330 train_time:70262ms step_avg:57.64ms
step:1220/2330 train_time:70323ms step_avg:57.64ms
step:1221/2330 train_time:70380ms step_avg:57.64ms
step:1222/2330 train_time:70439ms step_avg:57.64ms
step:1223/2330 train_time:70497ms step_avg:57.64ms
step:1224/2330 train_time:70557ms step_avg:57.64ms
step:1225/2330 train_time:70613ms step_avg:57.64ms
step:1226/2330 train_time:70672ms step_avg:57.64ms
step:1227/2330 train_time:70728ms step_avg:57.64ms
step:1228/2330 train_time:70788ms step_avg:57.65ms
step:1229/2330 train_time:70845ms step_avg:57.64ms
step:1230/2330 train_time:70905ms step_avg:57.65ms
step:1231/2330 train_time:70962ms step_avg:57.65ms
step:1232/2330 train_time:71022ms step_avg:57.65ms
step:1233/2330 train_time:71079ms step_avg:57.65ms
step:1234/2330 train_time:71138ms step_avg:57.65ms
step:1235/2330 train_time:71195ms step_avg:57.65ms
step:1236/2330 train_time:71255ms step_avg:57.65ms
step:1237/2330 train_time:71311ms step_avg:57.65ms
step:1238/2330 train_time:71371ms step_avg:57.65ms
step:1239/2330 train_time:71427ms step_avg:57.65ms
step:1240/2330 train_time:71487ms step_avg:57.65ms
step:1241/2330 train_time:71544ms step_avg:57.65ms
step:1242/2330 train_time:71604ms step_avg:57.65ms
step:1243/2330 train_time:71661ms step_avg:57.65ms
step:1244/2330 train_time:71720ms step_avg:57.65ms
step:1245/2330 train_time:71777ms step_avg:57.65ms
step:1246/2330 train_time:71836ms step_avg:57.65ms
step:1247/2330 train_time:71893ms step_avg:57.65ms
step:1248/2330 train_time:71953ms step_avg:57.65ms
step:1249/2330 train_time:72009ms step_avg:57.65ms
step:1250/2330 train_time:72068ms step_avg:57.65ms
step:1250/2330 val_loss:4.0263 train_time:72149ms step_avg:57.72ms
step:1251/2330 train_time:72168ms step_avg:57.69ms
step:1252/2330 train_time:72189ms step_avg:57.66ms
step:1253/2330 train_time:72247ms step_avg:57.66ms
step:1254/2330 train_time:72310ms step_avg:57.66ms
step:1255/2330 train_time:72366ms step_avg:57.66ms
step:1256/2330 train_time:72428ms step_avg:57.67ms
step:1257/2330 train_time:72484ms step_avg:57.66ms
step:1258/2330 train_time:72544ms step_avg:57.67ms
step:1259/2330 train_time:72600ms step_avg:57.66ms
step:1260/2330 train_time:72660ms step_avg:57.67ms
step:1261/2330 train_time:72715ms step_avg:57.66ms
step:1262/2330 train_time:72775ms step_avg:57.67ms
step:1263/2330 train_time:72831ms step_avg:57.66ms
step:1264/2330 train_time:72889ms step_avg:57.67ms
step:1265/2330 train_time:72945ms step_avg:57.66ms
step:1266/2330 train_time:73004ms step_avg:57.67ms
step:1267/2330 train_time:73060ms step_avg:57.66ms
step:1268/2330 train_time:73120ms step_avg:57.67ms
step:1269/2330 train_time:73178ms step_avg:57.67ms
step:1270/2330 train_time:73239ms step_avg:57.67ms
step:1271/2330 train_time:73296ms step_avg:57.67ms
step:1272/2330 train_time:73359ms step_avg:57.67ms
step:1273/2330 train_time:73415ms step_avg:57.67ms
step:1274/2330 train_time:73477ms step_avg:57.67ms
step:1275/2330 train_time:73533ms step_avg:57.67ms
step:1276/2330 train_time:73593ms step_avg:57.67ms
step:1277/2330 train_time:73649ms step_avg:57.67ms
step:1278/2330 train_time:73708ms step_avg:57.67ms
step:1279/2330 train_time:73764ms step_avg:57.67ms
step:1280/2330 train_time:73824ms step_avg:57.67ms
step:1281/2330 train_time:73880ms step_avg:57.67ms
step:1282/2330 train_time:73940ms step_avg:57.68ms
step:1283/2330 train_time:73996ms step_avg:57.67ms
step:1284/2330 train_time:74054ms step_avg:57.67ms
step:1285/2330 train_time:74111ms step_avg:57.67ms
step:1286/2330 train_time:74172ms step_avg:57.68ms
step:1287/2330 train_time:74229ms step_avg:57.68ms
step:1288/2330 train_time:74289ms step_avg:57.68ms
step:1289/2330 train_time:74346ms step_avg:57.68ms
step:1290/2330 train_time:74407ms step_avg:57.68ms
step:1291/2330 train_time:74463ms step_avg:57.68ms
step:1292/2330 train_time:74524ms step_avg:57.68ms
step:1293/2330 train_time:74580ms step_avg:57.68ms
step:1294/2330 train_time:74640ms step_avg:57.68ms
step:1295/2330 train_time:74696ms step_avg:57.68ms
step:1296/2330 train_time:74756ms step_avg:57.68ms
step:1297/2330 train_time:74813ms step_avg:57.68ms
step:1298/2330 train_time:74872ms step_avg:57.68ms
step:1299/2330 train_time:74928ms step_avg:57.68ms
step:1300/2330 train_time:74988ms step_avg:57.68ms
step:1301/2330 train_time:75044ms step_avg:57.68ms
step:1302/2330 train_time:75104ms step_avg:57.68ms
step:1303/2330 train_time:75160ms step_avg:57.68ms
step:1304/2330 train_time:75220ms step_avg:57.68ms
step:1305/2330 train_time:75277ms step_avg:57.68ms
step:1306/2330 train_time:75338ms step_avg:57.69ms
step:1307/2330 train_time:75395ms step_avg:57.69ms
step:1308/2330 train_time:75456ms step_avg:57.69ms
step:1309/2330 train_time:75513ms step_avg:57.69ms
step:1310/2330 train_time:75573ms step_avg:57.69ms
step:1311/2330 train_time:75630ms step_avg:57.69ms
step:1312/2330 train_time:75689ms step_avg:57.69ms
step:1313/2330 train_time:75746ms step_avg:57.69ms
step:1314/2330 train_time:75805ms step_avg:57.69ms
step:1315/2330 train_time:75860ms step_avg:57.69ms
step:1316/2330 train_time:75920ms step_avg:57.69ms
step:1317/2330 train_time:75976ms step_avg:57.69ms
step:1318/2330 train_time:76036ms step_avg:57.69ms
step:1319/2330 train_time:76093ms step_avg:57.69ms
step:1320/2330 train_time:76153ms step_avg:57.69ms
step:1321/2330 train_time:76210ms step_avg:57.69ms
step:1322/2330 train_time:76270ms step_avg:57.69ms
step:1323/2330 train_time:76327ms step_avg:57.69ms
step:1324/2330 train_time:76387ms step_avg:57.69ms
step:1325/2330 train_time:76443ms step_avg:57.69ms
step:1326/2330 train_time:76503ms step_avg:57.69ms
step:1327/2330 train_time:76559ms step_avg:57.69ms
step:1328/2330 train_time:76620ms step_avg:57.70ms
step:1329/2330 train_time:76677ms step_avg:57.70ms
step:1330/2330 train_time:76737ms step_avg:57.70ms
step:1331/2330 train_time:76793ms step_avg:57.70ms
step:1332/2330 train_time:76853ms step_avg:57.70ms
step:1333/2330 train_time:76909ms step_avg:57.70ms
step:1334/2330 train_time:76969ms step_avg:57.70ms
step:1335/2330 train_time:77025ms step_avg:57.70ms
step:1336/2330 train_time:77084ms step_avg:57.70ms
step:1337/2330 train_time:77140ms step_avg:57.70ms
step:1338/2330 train_time:77201ms step_avg:57.70ms
step:1339/2330 train_time:77257ms step_avg:57.70ms
step:1340/2330 train_time:77318ms step_avg:57.70ms
step:1341/2330 train_time:77374ms step_avg:57.70ms
step:1342/2330 train_time:77434ms step_avg:57.70ms
step:1343/2330 train_time:77491ms step_avg:57.70ms
step:1344/2330 train_time:77551ms step_avg:57.70ms
step:1345/2330 train_time:77607ms step_avg:57.70ms
step:1346/2330 train_time:77667ms step_avg:57.70ms
step:1347/2330 train_time:77723ms step_avg:57.70ms
step:1348/2330 train_time:77783ms step_avg:57.70ms
step:1349/2330 train_time:77839ms step_avg:57.70ms
step:1350/2330 train_time:77899ms step_avg:57.70ms
step:1351/2330 train_time:77955ms step_avg:57.70ms
step:1352/2330 train_time:78015ms step_avg:57.70ms
step:1353/2330 train_time:78072ms step_avg:57.70ms
step:1354/2330 train_time:78132ms step_avg:57.70ms
step:1355/2330 train_time:78189ms step_avg:57.70ms
step:1356/2330 train_time:78249ms step_avg:57.71ms
step:1357/2330 train_time:78305ms step_avg:57.70ms
step:1358/2330 train_time:78364ms step_avg:57.71ms
step:1359/2330 train_time:78420ms step_avg:57.70ms
step:1360/2330 train_time:78480ms step_avg:57.71ms
step:1361/2330 train_time:78537ms step_avg:57.71ms
step:1362/2330 train_time:78598ms step_avg:57.71ms
step:1363/2330 train_time:78654ms step_avg:57.71ms
step:1364/2330 train_time:78714ms step_avg:57.71ms
step:1365/2330 train_time:78770ms step_avg:57.71ms
step:1366/2330 train_time:78830ms step_avg:57.71ms
step:1367/2330 train_time:78886ms step_avg:57.71ms
step:1368/2330 train_time:78947ms step_avg:57.71ms
step:1369/2330 train_time:79003ms step_avg:57.71ms
step:1370/2330 train_time:79063ms step_avg:57.71ms
step:1371/2330 train_time:79119ms step_avg:57.71ms
step:1372/2330 train_time:79180ms step_avg:57.71ms
step:1373/2330 train_time:79236ms step_avg:57.71ms
step:1374/2330 train_time:79296ms step_avg:57.71ms
step:1375/2330 train_time:79353ms step_avg:57.71ms
step:1376/2330 train_time:79413ms step_avg:57.71ms
step:1377/2330 train_time:79469ms step_avg:57.71ms
step:1378/2330 train_time:79529ms step_avg:57.71ms
step:1379/2330 train_time:79586ms step_avg:57.71ms
step:1380/2330 train_time:79646ms step_avg:57.71ms
step:1381/2330 train_time:79701ms step_avg:57.71ms
step:1382/2330 train_time:79762ms step_avg:57.71ms
step:1383/2330 train_time:79818ms step_avg:57.71ms
step:1384/2330 train_time:79878ms step_avg:57.72ms
step:1385/2330 train_time:79935ms step_avg:57.71ms
step:1386/2330 train_time:79994ms step_avg:57.72ms
step:1387/2330 train_time:80051ms step_avg:57.71ms
step:1388/2330 train_time:80110ms step_avg:57.72ms
step:1389/2330 train_time:80167ms step_avg:57.72ms
step:1390/2330 train_time:80227ms step_avg:57.72ms
step:1391/2330 train_time:80283ms step_avg:57.72ms
step:1392/2330 train_time:80343ms step_avg:57.72ms
step:1393/2330 train_time:80399ms step_avg:57.72ms
step:1394/2330 train_time:80459ms step_avg:57.72ms
step:1395/2330 train_time:80516ms step_avg:57.72ms
step:1396/2330 train_time:80576ms step_avg:57.72ms
step:1397/2330 train_time:80633ms step_avg:57.72ms
step:1398/2330 train_time:80693ms step_avg:57.72ms
step:1399/2330 train_time:80749ms step_avg:57.72ms
step:1400/2330 train_time:80809ms step_avg:57.72ms
step:1401/2330 train_time:80865ms step_avg:57.72ms
step:1402/2330 train_time:80924ms step_avg:57.72ms
step:1403/2330 train_time:80981ms step_avg:57.72ms
step:1404/2330 train_time:81040ms step_avg:57.72ms
step:1405/2330 train_time:81097ms step_avg:57.72ms
step:1406/2330 train_time:81157ms step_avg:57.72ms
step:1407/2330 train_time:81214ms step_avg:57.72ms
step:1408/2330 train_time:81274ms step_avg:57.72ms
step:1409/2330 train_time:81331ms step_avg:57.72ms
step:1410/2330 train_time:81390ms step_avg:57.72ms
step:1411/2330 train_time:81447ms step_avg:57.72ms
step:1412/2330 train_time:81507ms step_avg:57.72ms
step:1413/2330 train_time:81563ms step_avg:57.72ms
step:1414/2330 train_time:81623ms step_avg:57.72ms
step:1415/2330 train_time:81680ms step_avg:57.72ms
step:1416/2330 train_time:81740ms step_avg:57.73ms
step:1417/2330 train_time:81797ms step_avg:57.73ms
step:1418/2330 train_time:81856ms step_avg:57.73ms
step:1419/2330 train_time:81913ms step_avg:57.73ms
step:1420/2330 train_time:81973ms step_avg:57.73ms
step:1421/2330 train_time:82030ms step_avg:57.73ms
step:1422/2330 train_time:82090ms step_avg:57.73ms
step:1423/2330 train_time:82146ms step_avg:57.73ms
step:1424/2330 train_time:82206ms step_avg:57.73ms
step:1425/2330 train_time:82261ms step_avg:57.73ms
step:1426/2330 train_time:82321ms step_avg:57.73ms
step:1427/2330 train_time:82378ms step_avg:57.73ms
step:1428/2330 train_time:82439ms step_avg:57.73ms
step:1429/2330 train_time:82495ms step_avg:57.73ms
step:1430/2330 train_time:82555ms step_avg:57.73ms
step:1431/2330 train_time:82612ms step_avg:57.73ms
step:1432/2330 train_time:82672ms step_avg:57.73ms
step:1433/2330 train_time:82729ms step_avg:57.73ms
step:1434/2330 train_time:82788ms step_avg:57.73ms
step:1435/2330 train_time:82844ms step_avg:57.73ms
step:1436/2330 train_time:82904ms step_avg:57.73ms
step:1437/2330 train_time:82960ms step_avg:57.73ms
step:1438/2330 train_time:83021ms step_avg:57.73ms
step:1439/2330 train_time:83077ms step_avg:57.73ms
step:1440/2330 train_time:83137ms step_avg:57.73ms
step:1441/2330 train_time:83194ms step_avg:57.73ms
step:1442/2330 train_time:83254ms step_avg:57.74ms
step:1443/2330 train_time:83311ms step_avg:57.73ms
step:1444/2330 train_time:83371ms step_avg:57.74ms
step:1445/2330 train_time:83427ms step_avg:57.74ms
step:1446/2330 train_time:83487ms step_avg:57.74ms
step:1447/2330 train_time:83543ms step_avg:57.74ms
step:1448/2330 train_time:83603ms step_avg:57.74ms
step:1449/2330 train_time:83659ms step_avg:57.74ms
step:1450/2330 train_time:83719ms step_avg:57.74ms
step:1451/2330 train_time:83775ms step_avg:57.74ms
step:1452/2330 train_time:83836ms step_avg:57.74ms
step:1453/2330 train_time:83892ms step_avg:57.74ms
step:1454/2330 train_time:83952ms step_avg:57.74ms
step:1455/2330 train_time:84009ms step_avg:57.74ms
step:1456/2330 train_time:84069ms step_avg:57.74ms
step:1457/2330 train_time:84125ms step_avg:57.74ms
step:1458/2330 train_time:84185ms step_avg:57.74ms
step:1459/2330 train_time:84241ms step_avg:57.74ms
step:1460/2330 train_time:84301ms step_avg:57.74ms
step:1461/2330 train_time:84357ms step_avg:57.74ms
step:1462/2330 train_time:84418ms step_avg:57.74ms
step:1463/2330 train_time:84474ms step_avg:57.74ms
step:1464/2330 train_time:84534ms step_avg:57.74ms
step:1465/2330 train_time:84591ms step_avg:57.74ms
step:1466/2330 train_time:84651ms step_avg:57.74ms
step:1467/2330 train_time:84708ms step_avg:57.74ms
step:1468/2330 train_time:84767ms step_avg:57.74ms
step:1469/2330 train_time:84824ms step_avg:57.74ms
step:1470/2330 train_time:84883ms step_avg:57.74ms
step:1471/2330 train_time:84939ms step_avg:57.74ms
step:1472/2330 train_time:84999ms step_avg:57.74ms
step:1473/2330 train_time:85055ms step_avg:57.74ms
step:1474/2330 train_time:85116ms step_avg:57.74ms
step:1475/2330 train_time:85173ms step_avg:57.74ms
step:1476/2330 train_time:85232ms step_avg:57.75ms
step:1477/2330 train_time:85290ms step_avg:57.75ms
step:1478/2330 train_time:85348ms step_avg:57.75ms
step:1479/2330 train_time:85405ms step_avg:57.75ms
step:1480/2330 train_time:85465ms step_avg:57.75ms
step:1481/2330 train_time:85521ms step_avg:57.75ms
step:1482/2330 train_time:85581ms step_avg:57.75ms
step:1483/2330 train_time:85637ms step_avg:57.75ms
step:1484/2330 train_time:85698ms step_avg:57.75ms
step:1485/2330 train_time:85754ms step_avg:57.75ms
step:1486/2330 train_time:85816ms step_avg:57.75ms
step:1487/2330 train_time:85872ms step_avg:57.75ms
step:1488/2330 train_time:85931ms step_avg:57.75ms
step:1489/2330 train_time:85988ms step_avg:57.75ms
step:1490/2330 train_time:86047ms step_avg:57.75ms
step:1491/2330 train_time:86104ms step_avg:57.75ms
step:1492/2330 train_time:86164ms step_avg:57.75ms
step:1493/2330 train_time:86220ms step_avg:57.75ms
step:1494/2330 train_time:86280ms step_avg:57.75ms
step:1495/2330 train_time:86337ms step_avg:57.75ms
step:1496/2330 train_time:86397ms step_avg:57.75ms
step:1497/2330 train_time:86454ms step_avg:57.75ms
step:1498/2330 train_time:86514ms step_avg:57.75ms
step:1499/2330 train_time:86570ms step_avg:57.75ms
step:1500/2330 train_time:86630ms step_avg:57.75ms
step:1500/2330 val_loss:3.9478 train_time:86709ms step_avg:57.81ms
step:1501/2330 train_time:86729ms step_avg:57.78ms
step:1502/2330 train_time:86749ms step_avg:57.76ms
step:1503/2330 train_time:86807ms step_avg:57.76ms
step:1504/2330 train_time:86870ms step_avg:57.76ms
step:1505/2330 train_time:86926ms step_avg:57.76ms
step:1506/2330 train_time:86990ms step_avg:57.76ms
step:1507/2330 train_time:87046ms step_avg:57.76ms
step:1508/2330 train_time:87106ms step_avg:57.76ms
step:1509/2330 train_time:87162ms step_avg:57.76ms
step:1510/2330 train_time:87222ms step_avg:57.76ms
step:1511/2330 train_time:87277ms step_avg:57.76ms
step:1512/2330 train_time:87337ms step_avg:57.76ms
step:1513/2330 train_time:87393ms step_avg:57.76ms
step:1514/2330 train_time:87452ms step_avg:57.76ms
step:1515/2330 train_time:87508ms step_avg:57.76ms
step:1516/2330 train_time:87567ms step_avg:57.76ms
step:1517/2330 train_time:87623ms step_avg:57.76ms
step:1518/2330 train_time:87683ms step_avg:57.76ms
step:1519/2330 train_time:87739ms step_avg:57.76ms
step:1520/2330 train_time:87803ms step_avg:57.76ms
step:1521/2330 train_time:87861ms step_avg:57.77ms
step:1522/2330 train_time:87921ms step_avg:57.77ms
step:1523/2330 train_time:87980ms step_avg:57.77ms
step:1524/2330 train_time:88040ms step_avg:57.77ms
step:1525/2330 train_time:88097ms step_avg:57.77ms
step:1526/2330 train_time:88157ms step_avg:57.77ms
step:1527/2330 train_time:88213ms step_avg:57.77ms
step:1528/2330 train_time:88273ms step_avg:57.77ms
step:1529/2330 train_time:88330ms step_avg:57.77ms
step:1530/2330 train_time:88389ms step_avg:57.77ms
step:1531/2330 train_time:88445ms step_avg:57.77ms
step:1532/2330 train_time:88506ms step_avg:57.77ms
step:1533/2330 train_time:88562ms step_avg:57.77ms
step:1534/2330 train_time:88622ms step_avg:57.77ms
step:1535/2330 train_time:88679ms step_avg:57.77ms
step:1536/2330 train_time:88739ms step_avg:57.77ms
step:1537/2330 train_time:88797ms step_avg:57.77ms
step:1538/2330 train_time:88859ms step_avg:57.78ms
step:1539/2330 train_time:88918ms step_avg:57.78ms
step:1540/2330 train_time:88978ms step_avg:57.78ms
step:1541/2330 train_time:89036ms step_avg:57.78ms
step:1542/2330 train_time:89095ms step_avg:57.78ms
step:1543/2330 train_time:89152ms step_avg:57.78ms
step:1544/2330 train_time:89213ms step_avg:57.78ms
step:1545/2330 train_time:89270ms step_avg:57.78ms
step:1546/2330 train_time:89330ms step_avg:57.78ms
step:1547/2330 train_time:89387ms step_avg:57.78ms
step:1548/2330 train_time:89448ms step_avg:57.78ms
step:1549/2330 train_time:89505ms step_avg:57.78ms
step:1550/2330 train_time:89565ms step_avg:57.78ms
step:1551/2330 train_time:89621ms step_avg:57.78ms
step:1552/2330 train_time:89682ms step_avg:57.78ms
step:1553/2330 train_time:89738ms step_avg:57.78ms
step:1554/2330 train_time:89800ms step_avg:57.79ms
step:1555/2330 train_time:89858ms step_avg:57.79ms
step:1556/2330 train_time:89919ms step_avg:57.79ms
step:1557/2330 train_time:89976ms step_avg:57.79ms
step:1558/2330 train_time:90037ms step_avg:57.79ms
step:1559/2330 train_time:90095ms step_avg:57.79ms
step:1560/2330 train_time:90156ms step_avg:57.79ms
step:1561/2330 train_time:90214ms step_avg:57.79ms
step:1562/2330 train_time:90275ms step_avg:57.79ms
step:1563/2330 train_time:90333ms step_avg:57.79ms
step:1564/2330 train_time:90392ms step_avg:57.80ms
step:1565/2330 train_time:90449ms step_avg:57.80ms
step:1566/2330 train_time:90510ms step_avg:57.80ms
step:1567/2330 train_time:90566ms step_avg:57.80ms
step:1568/2330 train_time:90628ms step_avg:57.80ms
step:1569/2330 train_time:90684ms step_avg:57.80ms
step:1570/2330 train_time:90745ms step_avg:57.80ms
step:1571/2330 train_time:90801ms step_avg:57.80ms
step:1572/2330 train_time:90863ms step_avg:57.80ms
step:1573/2330 train_time:90920ms step_avg:57.80ms
step:1574/2330 train_time:90982ms step_avg:57.80ms
step:1575/2330 train_time:91039ms step_avg:57.80ms
step:1576/2330 train_time:91102ms step_avg:57.81ms
step:1577/2330 train_time:91159ms step_avg:57.81ms
step:1578/2330 train_time:91220ms step_avg:57.81ms
step:1579/2330 train_time:91278ms step_avg:57.81ms
step:1580/2330 train_time:91338ms step_avg:57.81ms
step:1581/2330 train_time:91397ms step_avg:57.81ms
step:1582/2330 train_time:91457ms step_avg:57.81ms
step:1583/2330 train_time:91515ms step_avg:57.81ms
step:1584/2330 train_time:91584ms step_avg:57.82ms
step:1585/2330 train_time:91633ms step_avg:57.81ms
step:1586/2330 train_time:91693ms step_avg:57.81ms
step:1587/2330 train_time:91750ms step_avg:57.81ms
step:1588/2330 train_time:91812ms step_avg:57.82ms
step:1589/2330 train_time:91868ms step_avg:57.81ms
step:1590/2330 train_time:91929ms step_avg:57.82ms
step:1591/2330 train_time:91985ms step_avg:57.82ms
step:1592/2330 train_time:92047ms step_avg:57.82ms
step:1593/2330 train_time:92103ms step_avg:57.82ms
step:1594/2330 train_time:92164ms step_avg:57.82ms
step:1595/2330 train_time:92221ms step_avg:57.82ms
step:1596/2330 train_time:92282ms step_avg:57.82ms
step:1597/2330 train_time:92339ms step_avg:57.82ms
step:1598/2330 train_time:92401ms step_avg:57.82ms
step:1599/2330 train_time:92458ms step_avg:57.82ms
step:1600/2330 train_time:92519ms step_avg:57.82ms
step:1601/2330 train_time:92577ms step_avg:57.82ms
step:1602/2330 train_time:92637ms step_avg:57.83ms
step:1603/2330 train_time:92696ms step_avg:57.83ms
step:1604/2330 train_time:92756ms step_avg:57.83ms
step:1605/2330 train_time:92815ms step_avg:57.83ms
step:1606/2330 train_time:92875ms step_avg:57.83ms
step:1607/2330 train_time:92932ms step_avg:57.83ms
step:1608/2330 train_time:92992ms step_avg:57.83ms
step:1609/2330 train_time:93049ms step_avg:57.83ms
step:1610/2330 train_time:93110ms step_avg:57.83ms
step:1611/2330 train_time:93167ms step_avg:57.83ms
step:1612/2330 train_time:93227ms step_avg:57.83ms
step:1613/2330 train_time:93283ms step_avg:57.83ms
step:1614/2330 train_time:93346ms step_avg:57.84ms
step:1615/2330 train_time:93403ms step_avg:57.83ms
step:1616/2330 train_time:93464ms step_avg:57.84ms
step:1617/2330 train_time:93521ms step_avg:57.84ms
step:1618/2330 train_time:93582ms step_avg:57.84ms
step:1619/2330 train_time:93639ms step_avg:57.84ms
step:1620/2330 train_time:93701ms step_avg:57.84ms
step:1621/2330 train_time:93758ms step_avg:57.84ms
step:1622/2330 train_time:93819ms step_avg:57.84ms
step:1623/2330 train_time:93877ms step_avg:57.84ms
step:1624/2330 train_time:93937ms step_avg:57.84ms
step:1625/2330 train_time:93995ms step_avg:57.84ms
step:1626/2330 train_time:94056ms step_avg:57.85ms
step:1627/2330 train_time:94114ms step_avg:57.85ms
step:1628/2330 train_time:94174ms step_avg:57.85ms
step:1629/2330 train_time:94231ms step_avg:57.85ms
step:1630/2330 train_time:94291ms step_avg:57.85ms
step:1631/2330 train_time:94348ms step_avg:57.85ms
step:1632/2330 train_time:94409ms step_avg:57.85ms
step:1633/2330 train_time:94466ms step_avg:57.85ms
step:1634/2330 train_time:94526ms step_avg:57.85ms
step:1635/2330 train_time:94583ms step_avg:57.85ms
step:1636/2330 train_time:94645ms step_avg:57.85ms
step:1637/2330 train_time:94702ms step_avg:57.85ms
step:1638/2330 train_time:94765ms step_avg:57.85ms
step:1639/2330 train_time:94821ms step_avg:57.85ms
step:1640/2330 train_time:94884ms step_avg:57.86ms
step:1641/2330 train_time:94941ms step_avg:57.86ms
step:1642/2330 train_time:95002ms step_avg:57.86ms
step:1643/2330 train_time:95059ms step_avg:57.86ms
step:1644/2330 train_time:95119ms step_avg:57.86ms
step:1645/2330 train_time:95177ms step_avg:57.86ms
step:1646/2330 train_time:95238ms step_avg:57.86ms
step:1647/2330 train_time:95296ms step_avg:57.86ms
step:1648/2330 train_time:95357ms step_avg:57.86ms
step:1649/2330 train_time:95416ms step_avg:57.86ms
step:1650/2330 train_time:95476ms step_avg:57.86ms
step:1651/2330 train_time:95533ms step_avg:57.86ms
step:1652/2330 train_time:95593ms step_avg:57.87ms
step:1653/2330 train_time:95650ms step_avg:57.86ms
step:1654/2330 train_time:95713ms step_avg:57.87ms
step:1655/2330 train_time:95771ms step_avg:57.87ms
step:1656/2330 train_time:95831ms step_avg:57.87ms
step:1657/2330 train_time:95887ms step_avg:57.87ms
step:1658/2330 train_time:95949ms step_avg:57.87ms
step:1659/2330 train_time:96005ms step_avg:57.87ms
step:1660/2330 train_time:96067ms step_avg:57.87ms
step:1661/2330 train_time:96123ms step_avg:57.87ms
step:1662/2330 train_time:96185ms step_avg:57.87ms
step:1663/2330 train_time:96241ms step_avg:57.87ms
step:1664/2330 train_time:96303ms step_avg:57.87ms
step:1665/2330 train_time:96360ms step_avg:57.87ms
step:1666/2330 train_time:96421ms step_avg:57.88ms
step:1667/2330 train_time:96478ms step_avg:57.88ms
step:1668/2330 train_time:96538ms step_avg:57.88ms
step:1669/2330 train_time:96596ms step_avg:57.88ms
step:1670/2330 train_time:96657ms step_avg:57.88ms
step:1671/2330 train_time:96714ms step_avg:57.88ms
step:1672/2330 train_time:96775ms step_avg:57.88ms
step:1673/2330 train_time:96833ms step_avg:57.88ms
step:1674/2330 train_time:96894ms step_avg:57.88ms
step:1675/2330 train_time:96952ms step_avg:57.88ms
step:1676/2330 train_time:97012ms step_avg:57.88ms
step:1677/2330 train_time:97068ms step_avg:57.88ms
step:1678/2330 train_time:97128ms step_avg:57.88ms
step:1679/2330 train_time:97186ms step_avg:57.88ms
step:1680/2330 train_time:97246ms step_avg:57.88ms
step:1681/2330 train_time:97303ms step_avg:57.88ms
step:1682/2330 train_time:97364ms step_avg:57.89ms
step:1683/2330 train_time:97420ms step_avg:57.88ms
step:1684/2330 train_time:97482ms step_avg:57.89ms
step:1685/2330 train_time:97539ms step_avg:57.89ms
step:1686/2330 train_time:97600ms step_avg:57.89ms
step:1687/2330 train_time:97658ms step_avg:57.89ms
step:1688/2330 train_time:97719ms step_avg:57.89ms
step:1689/2330 train_time:97776ms step_avg:57.89ms
step:1690/2330 train_time:97838ms step_avg:57.89ms
step:1691/2330 train_time:97896ms step_avg:57.89ms
step:1692/2330 train_time:97958ms step_avg:57.89ms
step:1693/2330 train_time:98016ms step_avg:57.89ms
step:1694/2330 train_time:98076ms step_avg:57.90ms
step:1695/2330 train_time:98133ms step_avg:57.90ms
step:1696/2330 train_time:98194ms step_avg:57.90ms
step:1697/2330 train_time:98251ms step_avg:57.90ms
step:1698/2330 train_time:98312ms step_avg:57.90ms
step:1699/2330 train_time:98369ms step_avg:57.90ms
step:1700/2330 train_time:98429ms step_avg:57.90ms
step:1701/2330 train_time:98486ms step_avg:57.90ms
step:1702/2330 train_time:98547ms step_avg:57.90ms
step:1703/2330 train_time:98604ms step_avg:57.90ms
step:1704/2330 train_time:98664ms step_avg:57.90ms
step:1705/2330 train_time:98720ms step_avg:57.90ms
step:1706/2330 train_time:98783ms step_avg:57.90ms
step:1707/2330 train_time:98840ms step_avg:57.90ms
step:1708/2330 train_time:98902ms step_avg:57.91ms
step:1709/2330 train_time:98960ms step_avg:57.90ms
step:1710/2330 train_time:99021ms step_avg:57.91ms
step:1711/2330 train_time:99078ms step_avg:57.91ms
step:1712/2330 train_time:99139ms step_avg:57.91ms
step:1713/2330 train_time:99197ms step_avg:57.91ms
step:1714/2330 train_time:99257ms step_avg:57.91ms
step:1715/2330 train_time:99315ms step_avg:57.91ms
step:1716/2330 train_time:99377ms step_avg:57.91ms
step:1717/2330 train_time:99435ms step_avg:57.91ms
step:1718/2330 train_time:99495ms step_avg:57.91ms
step:1719/2330 train_time:99552ms step_avg:57.91ms
step:1720/2330 train_time:99613ms step_avg:57.91ms
step:1721/2330 train_time:99670ms step_avg:57.91ms
step:1722/2330 train_time:99731ms step_avg:57.92ms
step:1723/2330 train_time:99787ms step_avg:57.91ms
step:1724/2330 train_time:99849ms step_avg:57.92ms
step:1725/2330 train_time:99905ms step_avg:57.92ms
step:1726/2330 train_time:99967ms step_avg:57.92ms
step:1727/2330 train_time:100023ms step_avg:57.92ms
step:1728/2330 train_time:100085ms step_avg:57.92ms
step:1729/2330 train_time:100141ms step_avg:57.92ms
step:1730/2330 train_time:100204ms step_avg:57.92ms
step:1731/2330 train_time:100260ms step_avg:57.92ms
step:1732/2330 train_time:100322ms step_avg:57.92ms
step:1733/2330 train_time:100379ms step_avg:57.92ms
step:1734/2330 train_time:100440ms step_avg:57.92ms
step:1735/2330 train_time:100498ms step_avg:57.92ms
step:1736/2330 train_time:100559ms step_avg:57.93ms
step:1737/2330 train_time:100616ms step_avg:57.93ms
step:1738/2330 train_time:100677ms step_avg:57.93ms
step:1739/2330 train_time:100734ms step_avg:57.93ms
step:1740/2330 train_time:100795ms step_avg:57.93ms
step:1741/2330 train_time:100853ms step_avg:57.93ms
step:1742/2330 train_time:100913ms step_avg:57.93ms
step:1743/2330 train_time:100970ms step_avg:57.93ms
step:1744/2330 train_time:101030ms step_avg:57.93ms
step:1745/2330 train_time:101088ms step_avg:57.93ms
step:1746/2330 train_time:101148ms step_avg:57.93ms
step:1747/2330 train_time:101205ms step_avg:57.93ms
step:1748/2330 train_time:101266ms step_avg:57.93ms
step:1749/2330 train_time:101323ms step_avg:57.93ms
step:1750/2330 train_time:101384ms step_avg:57.93ms
step:1750/2330 val_loss:3.8676 train_time:101467ms step_avg:57.98ms
step:1751/2330 train_time:101487ms step_avg:57.96ms
step:1752/2330 train_time:101508ms step_avg:57.94ms
step:1753/2330 train_time:101561ms step_avg:57.94ms
step:1754/2330 train_time:101632ms step_avg:57.94ms
step:1755/2330 train_time:101688ms step_avg:57.94ms
step:1756/2330 train_time:101752ms step_avg:57.95ms
step:1757/2330 train_time:101808ms step_avg:57.94ms
step:1758/2330 train_time:101869ms step_avg:57.95ms
step:1759/2330 train_time:101925ms step_avg:57.94ms
step:1760/2330 train_time:101985ms step_avg:57.95ms
step:1761/2330 train_time:102041ms step_avg:57.95ms
step:1762/2330 train_time:102101ms step_avg:57.95ms
step:1763/2330 train_time:102157ms step_avg:57.95ms
step:1764/2330 train_time:102217ms step_avg:57.95ms
step:1765/2330 train_time:102274ms step_avg:57.95ms
step:1766/2330 train_time:102333ms step_avg:57.95ms
step:1767/2330 train_time:102392ms step_avg:57.95ms
step:1768/2330 train_time:102453ms step_avg:57.95ms
step:1769/2330 train_time:102512ms step_avg:57.95ms
step:1770/2330 train_time:102574ms step_avg:57.95ms
step:1771/2330 train_time:102631ms step_avg:57.95ms
step:1772/2330 train_time:102693ms step_avg:57.95ms
step:1773/2330 train_time:102751ms step_avg:57.95ms
step:1774/2330 train_time:102811ms step_avg:57.95ms
step:1775/2330 train_time:102868ms step_avg:57.95ms
step:1776/2330 train_time:102929ms step_avg:57.96ms
step:1777/2330 train_time:102986ms step_avg:57.95ms
step:1778/2330 train_time:103047ms step_avg:57.96ms
step:1779/2330 train_time:103104ms step_avg:57.96ms
step:1780/2330 train_time:103163ms step_avg:57.96ms
step:1781/2330 train_time:103219ms step_avg:57.96ms
step:1782/2330 train_time:103279ms step_avg:57.96ms
step:1783/2330 train_time:103336ms step_avg:57.96ms
step:1784/2330 train_time:103396ms step_avg:57.96ms
step:1785/2330 train_time:103454ms step_avg:57.96ms
step:1786/2330 train_time:103515ms step_avg:57.96ms
step:1787/2330 train_time:103574ms step_avg:57.96ms
step:1788/2330 train_time:103634ms step_avg:57.96ms
step:1789/2330 train_time:103691ms step_avg:57.96ms
step:1790/2330 train_time:103752ms step_avg:57.96ms
step:1791/2330 train_time:103810ms step_avg:57.96ms
step:1792/2330 train_time:103870ms step_avg:57.96ms
step:1793/2330 train_time:103928ms step_avg:57.96ms
step:1794/2330 train_time:103988ms step_avg:57.96ms
step:1795/2330 train_time:104045ms step_avg:57.96ms
step:1796/2330 train_time:104105ms step_avg:57.97ms
step:1797/2330 train_time:104161ms step_avg:57.96ms
step:1798/2330 train_time:104223ms step_avg:57.97ms
step:1799/2330 train_time:104279ms step_avg:57.97ms
step:1800/2330 train_time:104338ms step_avg:57.97ms
step:1801/2330 train_time:104396ms step_avg:57.97ms
step:1802/2330 train_time:104456ms step_avg:57.97ms
step:1803/2330 train_time:104513ms step_avg:57.97ms
step:1804/2330 train_time:104574ms step_avg:57.97ms
step:1805/2330 train_time:104631ms step_avg:57.97ms
step:1806/2330 train_time:104692ms step_avg:57.97ms
step:1807/2330 train_time:104749ms step_avg:57.97ms
step:1808/2330 train_time:104810ms step_avg:57.97ms
step:1809/2330 train_time:104868ms step_avg:57.97ms
step:1810/2330 train_time:104929ms step_avg:57.97ms
step:1811/2330 train_time:104986ms step_avg:57.97ms
step:1812/2330 train_time:105047ms step_avg:57.97ms
step:1813/2330 train_time:105104ms step_avg:57.97ms
step:1814/2330 train_time:105165ms step_avg:57.97ms
step:1815/2330 train_time:105222ms step_avg:57.97ms
step:1816/2330 train_time:105282ms step_avg:57.97ms
step:1817/2330 train_time:105338ms step_avg:57.97ms
step:1818/2330 train_time:105398ms step_avg:57.97ms
step:1819/2330 train_time:105455ms step_avg:57.97ms
step:1820/2330 train_time:105517ms step_avg:57.98ms
step:1821/2330 train_time:105574ms step_avg:57.98ms
step:1822/2330 train_time:105635ms step_avg:57.98ms
step:1823/2330 train_time:105692ms step_avg:57.98ms
step:1824/2330 train_time:105752ms step_avg:57.98ms
step:1825/2330 train_time:105810ms step_avg:57.98ms
step:1826/2330 train_time:105871ms step_avg:57.98ms
step:1827/2330 train_time:105929ms step_avg:57.98ms
step:1828/2330 train_time:105990ms step_avg:57.98ms
step:1829/2330 train_time:106047ms step_avg:57.98ms
step:1830/2330 train_time:106108ms step_avg:57.98ms
step:1831/2330 train_time:106165ms step_avg:57.98ms
step:1832/2330 train_time:106225ms step_avg:57.98ms
step:1833/2330 train_time:106282ms step_avg:57.98ms
step:1834/2330 train_time:106342ms step_avg:57.98ms
step:1835/2330 train_time:106399ms step_avg:57.98ms
step:1836/2330 train_time:106460ms step_avg:57.98ms
step:1837/2330 train_time:106516ms step_avg:57.98ms
step:1838/2330 train_time:106577ms step_avg:57.99ms
step:1839/2330 train_time:106633ms step_avg:57.98ms
step:1840/2330 train_time:106695ms step_avg:57.99ms
step:1841/2330 train_time:106752ms step_avg:57.99ms
step:1842/2330 train_time:106814ms step_avg:57.99ms
step:1843/2330 train_time:106872ms step_avg:57.99ms
step:1844/2330 train_time:106933ms step_avg:57.99ms
step:1845/2330 train_time:106991ms step_avg:57.99ms
step:1846/2330 train_time:107052ms step_avg:57.99ms
step:1847/2330 train_time:107109ms step_avg:57.99ms
step:1848/2330 train_time:107169ms step_avg:57.99ms
step:1849/2330 train_time:107226ms step_avg:57.99ms
step:1850/2330 train_time:107286ms step_avg:57.99ms
step:1851/2330 train_time:107343ms step_avg:57.99ms
step:1852/2330 train_time:107403ms step_avg:57.99ms
step:1853/2330 train_time:107460ms step_avg:57.99ms
step:1854/2330 train_time:107520ms step_avg:57.99ms
step:1855/2330 train_time:107577ms step_avg:57.99ms
step:1856/2330 train_time:107638ms step_avg:57.99ms
step:1857/2330 train_time:107695ms step_avg:57.99ms
step:1858/2330 train_time:107756ms step_avg:58.00ms
step:1859/2330 train_time:107813ms step_avg:58.00ms
step:1860/2330 train_time:107876ms step_avg:58.00ms
step:1861/2330 train_time:107934ms step_avg:58.00ms
step:1862/2330 train_time:107995ms step_avg:58.00ms
step:1863/2330 train_time:108052ms step_avg:58.00ms
step:1864/2330 train_time:108113ms step_avg:58.00ms
step:1865/2330 train_time:108171ms step_avg:58.00ms
step:1866/2330 train_time:108232ms step_avg:58.00ms
step:1867/2330 train_time:108291ms step_avg:58.00ms
step:1868/2330 train_time:108350ms step_avg:58.00ms
step:1869/2330 train_time:108408ms step_avg:58.00ms
step:1870/2330 train_time:108468ms step_avg:58.00ms
step:1871/2330 train_time:108525ms step_avg:58.00ms
step:1872/2330 train_time:108585ms step_avg:58.00ms
step:1873/2330 train_time:108641ms step_avg:58.00ms
step:1874/2330 train_time:108702ms step_avg:58.01ms
step:1875/2330 train_time:108758ms step_avg:58.00ms
step:1876/2330 train_time:108820ms step_avg:58.01ms
step:1877/2330 train_time:108876ms step_avg:58.01ms
step:1878/2330 train_time:108940ms step_avg:58.01ms
step:1879/2330 train_time:108997ms step_avg:58.01ms
step:1880/2330 train_time:109059ms step_avg:58.01ms
step:1881/2330 train_time:109116ms step_avg:58.01ms
step:1882/2330 train_time:109177ms step_avg:58.01ms
step:1883/2330 train_time:109235ms step_avg:58.01ms
step:1884/2330 train_time:109295ms step_avg:58.01ms
step:1885/2330 train_time:109353ms step_avg:58.01ms
step:1886/2330 train_time:109413ms step_avg:58.01ms
step:1887/2330 train_time:109471ms step_avg:58.01ms
step:1888/2330 train_time:109532ms step_avg:58.01ms
step:1889/2330 train_time:109589ms step_avg:58.01ms
step:1890/2330 train_time:109649ms step_avg:58.02ms
step:1891/2330 train_time:109707ms step_avg:58.02ms
step:1892/2330 train_time:109767ms step_avg:58.02ms
step:1893/2330 train_time:109824ms step_avg:58.02ms
step:1894/2330 train_time:109885ms step_avg:58.02ms
step:1895/2330 train_time:109941ms step_avg:58.02ms
step:1896/2330 train_time:110001ms step_avg:58.02ms
step:1897/2330 train_time:110058ms step_avg:58.02ms
step:1898/2330 train_time:110119ms step_avg:58.02ms
step:1899/2330 train_time:110176ms step_avg:58.02ms
step:1900/2330 train_time:110238ms step_avg:58.02ms
step:1901/2330 train_time:110296ms step_avg:58.02ms
step:1902/2330 train_time:110357ms step_avg:58.02ms
step:1903/2330 train_time:110415ms step_avg:58.02ms
step:1904/2330 train_time:110476ms step_avg:58.02ms
step:1905/2330 train_time:110534ms step_avg:58.02ms
step:1906/2330 train_time:110594ms step_avg:58.02ms
step:1907/2330 train_time:110651ms step_avg:58.02ms
step:1908/2330 train_time:110712ms step_avg:58.02ms
step:1909/2330 train_time:110769ms step_avg:58.02ms
step:1910/2330 train_time:110830ms step_avg:58.03ms
step:1911/2330 train_time:110887ms step_avg:58.03ms
step:1912/2330 train_time:110947ms step_avg:58.03ms
step:1913/2330 train_time:111004ms step_avg:58.03ms
step:1914/2330 train_time:111066ms step_avg:58.03ms
step:1915/2330 train_time:111122ms step_avg:58.03ms
step:1916/2330 train_time:111183ms step_avg:58.03ms
step:1917/2330 train_time:111239ms step_avg:58.03ms
step:1918/2330 train_time:111301ms step_avg:58.03ms
step:1919/2330 train_time:111357ms step_avg:58.03ms
step:1920/2330 train_time:111419ms step_avg:58.03ms
step:1921/2330 train_time:111476ms step_avg:58.03ms
step:1922/2330 train_time:111536ms step_avg:58.03ms
step:1923/2330 train_time:111594ms step_avg:58.03ms
step:1924/2330 train_time:111654ms step_avg:58.03ms
step:1925/2330 train_time:111713ms step_avg:58.03ms
step:1926/2330 train_time:111772ms step_avg:58.03ms
step:1927/2330 train_time:111829ms step_avg:58.03ms
step:1928/2330 train_time:111891ms step_avg:58.03ms
step:1929/2330 train_time:111948ms step_avg:58.03ms
step:1930/2330 train_time:112010ms step_avg:58.04ms
step:1931/2330 train_time:112067ms step_avg:58.04ms
step:1932/2330 train_time:112127ms step_avg:58.04ms
step:1933/2330 train_time:112183ms step_avg:58.04ms
step:1934/2330 train_time:112245ms step_avg:58.04ms
step:1935/2330 train_time:112302ms step_avg:58.04ms
step:1936/2330 train_time:112363ms step_avg:58.04ms
step:1937/2330 train_time:112420ms step_avg:58.04ms
step:1938/2330 train_time:112482ms step_avg:58.04ms
step:1939/2330 train_time:112538ms step_avg:58.04ms
step:1940/2330 train_time:112599ms step_avg:58.04ms
step:1941/2330 train_time:112656ms step_avg:58.04ms
step:1942/2330 train_time:112717ms step_avg:58.04ms
step:1943/2330 train_time:112774ms step_avg:58.04ms
step:1944/2330 train_time:112835ms step_avg:58.04ms
step:1945/2330 train_time:112892ms step_avg:58.04ms
step:1946/2330 train_time:112954ms step_avg:58.04ms
step:1947/2330 train_time:113011ms step_avg:58.04ms
step:1948/2330 train_time:113072ms step_avg:58.04ms
step:1949/2330 train_time:113130ms step_avg:58.05ms
step:1950/2330 train_time:113191ms step_avg:58.05ms
step:1951/2330 train_time:113248ms step_avg:58.05ms
step:1952/2330 train_time:113308ms step_avg:58.05ms
step:1953/2330 train_time:113366ms step_avg:58.05ms
step:1954/2330 train_time:113426ms step_avg:58.05ms
step:1955/2330 train_time:113482ms step_avg:58.05ms
step:1956/2330 train_time:113543ms step_avg:58.05ms
step:1957/2330 train_time:113600ms step_avg:58.05ms
step:1958/2330 train_time:113660ms step_avg:58.05ms
step:1959/2330 train_time:113717ms step_avg:58.05ms
step:1960/2330 train_time:113780ms step_avg:58.05ms
step:1961/2330 train_time:113837ms step_avg:58.05ms
step:1962/2330 train_time:113898ms step_avg:58.05ms
step:1963/2330 train_time:113955ms step_avg:58.05ms
step:1964/2330 train_time:114018ms step_avg:58.05ms
step:1965/2330 train_time:114075ms step_avg:58.05ms
step:1966/2330 train_time:114135ms step_avg:58.05ms
step:1967/2330 train_time:114193ms step_avg:58.05ms
step:1968/2330 train_time:114253ms step_avg:58.06ms
step:1969/2330 train_time:114312ms step_avg:58.06ms
step:1970/2330 train_time:114374ms step_avg:58.06ms
step:1971/2330 train_time:114431ms step_avg:58.06ms
step:1972/2330 train_time:114491ms step_avg:58.06ms
step:1973/2330 train_time:114549ms step_avg:58.06ms
step:1974/2330 train_time:114609ms step_avg:58.06ms
step:1975/2330 train_time:114666ms step_avg:58.06ms
step:1976/2330 train_time:114726ms step_avg:58.06ms
step:1977/2330 train_time:114782ms step_avg:58.06ms
step:1978/2330 train_time:114844ms step_avg:58.06ms
step:1979/2330 train_time:114901ms step_avg:58.06ms
step:1980/2330 train_time:114961ms step_avg:58.06ms
step:1981/2330 train_time:115018ms step_avg:58.06ms
step:1982/2330 train_time:115080ms step_avg:58.06ms
step:1983/2330 train_time:115136ms step_avg:58.06ms
step:1984/2330 train_time:115198ms step_avg:58.06ms
step:1985/2330 train_time:115255ms step_avg:58.06ms
step:1986/2330 train_time:115317ms step_avg:58.07ms
step:1987/2330 train_time:115375ms step_avg:58.06ms
step:1988/2330 train_time:115435ms step_avg:58.07ms
step:1989/2330 train_time:115493ms step_avg:58.07ms
step:1990/2330 train_time:115552ms step_avg:58.07ms
step:1991/2330 train_time:115610ms step_avg:58.07ms
step:1992/2330 train_time:115670ms step_avg:58.07ms
step:1993/2330 train_time:115729ms step_avg:58.07ms
step:1994/2330 train_time:115789ms step_avg:58.07ms
step:1995/2330 train_time:115846ms step_avg:58.07ms
step:1996/2330 train_time:115906ms step_avg:58.07ms
step:1997/2330 train_time:115963ms step_avg:58.07ms
step:1998/2330 train_time:116024ms step_avg:58.07ms
step:1999/2330 train_time:116081ms step_avg:58.07ms
step:2000/2330 train_time:116141ms step_avg:58.07ms
step:2000/2330 val_loss:3.8074 train_time:116223ms step_avg:58.11ms
step:2001/2330 train_time:116244ms step_avg:58.09ms
step:2002/2330 train_time:116265ms step_avg:58.07ms
step:2003/2330 train_time:116325ms step_avg:58.08ms
step:2004/2330 train_time:116387ms step_avg:58.08ms
step:2005/2330 train_time:116445ms step_avg:58.08ms
step:2006/2330 train_time:116507ms step_avg:58.08ms
step:2007/2330 train_time:116563ms step_avg:58.08ms
step:2008/2330 train_time:116623ms step_avg:58.08ms
step:2009/2330 train_time:116679ms step_avg:58.08ms
step:2010/2330 train_time:116740ms step_avg:58.08ms
step:2011/2330 train_time:116797ms step_avg:58.08ms
step:2012/2330 train_time:116856ms step_avg:58.08ms
step:2013/2330 train_time:116912ms step_avg:58.08ms
step:2014/2330 train_time:116972ms step_avg:58.08ms
step:2015/2330 train_time:117028ms step_avg:58.08ms
step:2016/2330 train_time:117088ms step_avg:58.08ms
step:2017/2330 train_time:117145ms step_avg:58.08ms
step:2018/2330 train_time:117206ms step_avg:58.08ms
step:2019/2330 train_time:117264ms step_avg:58.08ms
step:2020/2330 train_time:117326ms step_avg:58.08ms
step:2021/2330 train_time:117383ms step_avg:58.08ms
step:2022/2330 train_time:117447ms step_avg:58.08ms
step:2023/2330 train_time:117505ms step_avg:58.08ms
step:2024/2330 train_time:117565ms step_avg:58.09ms
step:2025/2330 train_time:117622ms step_avg:58.08ms
step:2026/2330 train_time:117683ms step_avg:58.09ms
step:2027/2330 train_time:117740ms step_avg:58.09ms
step:2028/2330 train_time:117800ms step_avg:58.09ms
step:2029/2330 train_time:117857ms step_avg:58.09ms
step:2030/2330 train_time:117916ms step_avg:58.09ms
step:2031/2330 train_time:117974ms step_avg:58.09ms
step:2032/2330 train_time:118033ms step_avg:58.09ms
step:2033/2330 train_time:118089ms step_avg:58.09ms
step:2034/2330 train_time:118149ms step_avg:58.09ms
step:2035/2330 train_time:118207ms step_avg:58.09ms
step:2036/2330 train_time:118267ms step_avg:58.09ms
step:2037/2330 train_time:118325ms step_avg:58.09ms
step:2038/2330 train_time:118386ms step_avg:58.09ms
step:2039/2330 train_time:118444ms step_avg:58.09ms
step:2040/2330 train_time:118505ms step_avg:58.09ms
step:2041/2330 train_time:118563ms step_avg:58.09ms
step:2042/2330 train_time:118624ms step_avg:58.09ms
step:2043/2330 train_time:118681ms step_avg:58.09ms
step:2044/2330 train_time:118742ms step_avg:58.09ms
step:2045/2330 train_time:118799ms step_avg:58.09ms
step:2046/2330 train_time:118860ms step_avg:58.09ms
step:2047/2330 train_time:118917ms step_avg:58.09ms
step:2048/2330 train_time:118977ms step_avg:58.09ms
step:2049/2330 train_time:119035ms step_avg:58.09ms
step:2050/2330 train_time:119095ms step_avg:58.09ms
step:2051/2330 train_time:119152ms step_avg:58.09ms
step:2052/2330 train_time:119212ms step_avg:58.10ms
step:2053/2330 train_time:119269ms step_avg:58.09ms
step:2054/2330 train_time:119330ms step_avg:58.10ms
step:2055/2330 train_time:119387ms step_avg:58.10ms
step:2056/2330 train_time:119448ms step_avg:58.10ms
step:2057/2330 train_time:119505ms step_avg:58.10ms
step:2058/2330 train_time:119566ms step_avg:58.10ms
step:2059/2330 train_time:119622ms step_avg:58.10ms
step:2060/2330 train_time:119685ms step_avg:58.10ms
step:2061/2330 train_time:119742ms step_avg:58.10ms
step:2062/2330 train_time:119804ms step_avg:58.10ms
step:2063/2330 train_time:119861ms step_avg:58.10ms
step:2064/2330 train_time:119923ms step_avg:58.10ms
step:2065/2330 train_time:119980ms step_avg:58.10ms
step:2066/2330 train_time:120040ms step_avg:58.10ms
step:2067/2330 train_time:120098ms step_avg:58.10ms
step:2068/2330 train_time:120158ms step_avg:58.10ms
step:2069/2330 train_time:120215ms step_avg:58.10ms
step:2070/2330 train_time:120276ms step_avg:58.10ms
step:2071/2330 train_time:120334ms step_avg:58.10ms
step:2072/2330 train_time:120396ms step_avg:58.11ms
step:2073/2330 train_time:120454ms step_avg:58.11ms
step:2074/2330 train_time:120515ms step_avg:58.11ms
step:2075/2330 train_time:120572ms step_avg:58.11ms
step:2076/2330 train_time:120633ms step_avg:58.11ms
step:2077/2330 train_time:120690ms step_avg:58.11ms
step:2078/2330 train_time:120752ms step_avg:58.11ms
step:2079/2330 train_time:120808ms step_avg:58.11ms
step:2080/2330 train_time:120869ms step_avg:58.11ms
step:2081/2330 train_time:120926ms step_avg:58.11ms
step:2082/2330 train_time:120987ms step_avg:58.11ms
step:2083/2330 train_time:121043ms step_avg:58.11ms
step:2084/2330 train_time:121105ms step_avg:58.11ms
step:2085/2330 train_time:121162ms step_avg:58.11ms
step:2086/2330 train_time:121222ms step_avg:58.11ms
step:2087/2330 train_time:121279ms step_avg:58.11ms
step:2088/2330 train_time:121340ms step_avg:58.11ms
step:2089/2330 train_time:121398ms step_avg:58.11ms
step:2090/2330 train_time:121459ms step_avg:58.11ms
step:2091/2330 train_time:121517ms step_avg:58.11ms
step:2092/2330 train_time:121579ms step_avg:58.12ms
step:2093/2330 train_time:121637ms step_avg:58.12ms
step:2094/2330 train_time:121698ms step_avg:58.12ms
step:2095/2330 train_time:121756ms step_avg:58.12ms
step:2096/2330 train_time:121816ms step_avg:58.12ms
step:2097/2330 train_time:121874ms step_avg:58.12ms
step:2098/2330 train_time:121933ms step_avg:58.12ms
step:2099/2330 train_time:121991ms step_avg:58.12ms
step:2100/2330 train_time:122051ms step_avg:58.12ms
step:2101/2330 train_time:122108ms step_avg:58.12ms
step:2102/2330 train_time:122168ms step_avg:58.12ms
step:2103/2330 train_time:122225ms step_avg:58.12ms
step:2104/2330 train_time:122286ms step_avg:58.12ms
step:2105/2330 train_time:122343ms step_avg:58.12ms
step:2106/2330 train_time:122405ms step_avg:58.12ms
step:2107/2330 train_time:122462ms step_avg:58.12ms
step:2108/2330 train_time:122523ms step_avg:58.12ms
step:2109/2330 train_time:122580ms step_avg:58.12ms
step:2110/2330 train_time:122642ms step_avg:58.12ms
step:2111/2330 train_time:122700ms step_avg:58.12ms
step:2112/2330 train_time:122760ms step_avg:58.13ms
step:2113/2330 train_time:122818ms step_avg:58.12ms
step:2114/2330 train_time:122878ms step_avg:58.13ms
step:2115/2330 train_time:122935ms step_avg:58.13ms
step:2116/2330 train_time:122996ms step_avg:58.13ms
step:2117/2330 train_time:123053ms step_avg:58.13ms
step:2118/2330 train_time:123113ms step_avg:58.13ms
step:2119/2330 train_time:123169ms step_avg:58.13ms
step:2120/2330 train_time:123230ms step_avg:58.13ms
step:2121/2330 train_time:123286ms step_avg:58.13ms
step:2122/2330 train_time:123348ms step_avg:58.13ms
step:2123/2330 train_time:123405ms step_avg:58.13ms
step:2124/2330 train_time:123466ms step_avg:58.13ms
step:2125/2330 train_time:123523ms step_avg:58.13ms
step:2126/2330 train_time:123585ms step_avg:58.13ms
step:2127/2330 train_time:123642ms step_avg:58.13ms
step:2128/2330 train_time:123704ms step_avg:58.13ms
step:2129/2330 train_time:123760ms step_avg:58.13ms
step:2130/2330 train_time:123823ms step_avg:58.13ms
step:2131/2330 train_time:123879ms step_avg:58.13ms
step:2132/2330 train_time:123942ms step_avg:58.13ms
step:2133/2330 train_time:124000ms step_avg:58.13ms
step:2134/2330 train_time:124060ms step_avg:58.13ms
step:2135/2330 train_time:124117ms step_avg:58.13ms
step:2136/2330 train_time:124178ms step_avg:58.14ms
step:2137/2330 train_time:124236ms step_avg:58.14ms
step:2138/2330 train_time:124296ms step_avg:58.14ms
step:2139/2330 train_time:124355ms step_avg:58.14ms
step:2140/2330 train_time:124415ms step_avg:58.14ms
step:2141/2330 train_time:124472ms step_avg:58.14ms
step:2142/2330 train_time:124531ms step_avg:58.14ms
step:2143/2330 train_time:124589ms step_avg:58.14ms
step:2144/2330 train_time:124649ms step_avg:58.14ms
step:2145/2330 train_time:124706ms step_avg:58.14ms
step:2146/2330 train_time:124767ms step_avg:58.14ms
step:2147/2330 train_time:124824ms step_avg:58.14ms
step:2148/2330 train_time:124886ms step_avg:58.14ms
step:2149/2330 train_time:124943ms step_avg:58.14ms
step:2150/2330 train_time:125004ms step_avg:58.14ms
step:2151/2330 train_time:125061ms step_avg:58.14ms
step:2152/2330 train_time:125122ms step_avg:58.14ms
step:2153/2330 train_time:125179ms step_avg:58.14ms
step:2154/2330 train_time:125239ms step_avg:58.14ms
step:2155/2330 train_time:125296ms step_avg:58.14ms
step:2156/2330 train_time:125357ms step_avg:58.14ms
step:2157/2330 train_time:125415ms step_avg:58.14ms
step:2158/2330 train_time:125475ms step_avg:58.14ms
step:2159/2330 train_time:125533ms step_avg:58.14ms
step:2160/2330 train_time:125593ms step_avg:58.14ms
step:2161/2330 train_time:125651ms step_avg:58.14ms
step:2162/2330 train_time:125711ms step_avg:58.15ms
step:2163/2330 train_time:125767ms step_avg:58.14ms
step:2164/2330 train_time:125828ms step_avg:58.15ms
step:2165/2330 train_time:125885ms step_avg:58.15ms
step:2166/2330 train_time:125947ms step_avg:58.15ms
step:2167/2330 train_time:126003ms step_avg:58.15ms
step:2168/2330 train_time:126064ms step_avg:58.15ms
step:2169/2330 train_time:126121ms step_avg:58.15ms
step:2170/2330 train_time:126182ms step_avg:58.15ms
step:2171/2330 train_time:126239ms step_avg:58.15ms
step:2172/2330 train_time:126301ms step_avg:58.15ms
step:2173/2330 train_time:126358ms step_avg:58.15ms
step:2174/2330 train_time:126420ms step_avg:58.15ms
step:2175/2330 train_time:126477ms step_avg:58.15ms
step:2176/2330 train_time:126539ms step_avg:58.15ms
step:2177/2330 train_time:126597ms step_avg:58.15ms
step:2178/2330 train_time:126658ms step_avg:58.15ms
step:2179/2330 train_time:126715ms step_avg:58.15ms
step:2180/2330 train_time:126775ms step_avg:58.15ms
step:2181/2330 train_time:126833ms step_avg:58.15ms
step:2182/2330 train_time:126893ms step_avg:58.15ms
step:2183/2330 train_time:126951ms step_avg:58.15ms
step:2184/2330 train_time:127011ms step_avg:58.16ms
step:2185/2330 train_time:127068ms step_avg:58.15ms
step:2186/2330 train_time:127128ms step_avg:58.16ms
step:2187/2330 train_time:127185ms step_avg:58.15ms
step:2188/2330 train_time:127247ms step_avg:58.16ms
step:2189/2330 train_time:127303ms step_avg:58.16ms
step:2190/2330 train_time:127366ms step_avg:58.16ms
step:2191/2330 train_time:127422ms step_avg:58.16ms
step:2192/2330 train_time:127483ms step_avg:58.16ms
step:2193/2330 train_time:127540ms step_avg:58.16ms
step:2194/2330 train_time:127602ms step_avg:58.16ms
step:2195/2330 train_time:127659ms step_avg:58.16ms
step:2196/2330 train_time:127720ms step_avg:58.16ms
step:2197/2330 train_time:127777ms step_avg:58.16ms
step:2198/2330 train_time:127838ms step_avg:58.16ms
step:2199/2330 train_time:127895ms step_avg:58.16ms
step:2200/2330 train_time:127957ms step_avg:58.16ms
step:2201/2330 train_time:128015ms step_avg:58.16ms
step:2202/2330 train_time:128075ms step_avg:58.16ms
step:2203/2330 train_time:128132ms step_avg:58.16ms
step:2204/2330 train_time:128192ms step_avg:58.16ms
step:2205/2330 train_time:128250ms step_avg:58.16ms
step:2206/2330 train_time:128310ms step_avg:58.16ms
step:2207/2330 train_time:128367ms step_avg:58.16ms
step:2208/2330 train_time:128428ms step_avg:58.16ms
step:2209/2330 train_time:128485ms step_avg:58.16ms
step:2210/2330 train_time:128547ms step_avg:58.17ms
step:2211/2330 train_time:128603ms step_avg:58.17ms
step:2212/2330 train_time:128666ms step_avg:58.17ms
step:2213/2330 train_time:128722ms step_avg:58.17ms
step:2214/2330 train_time:128785ms step_avg:58.17ms
step:2215/2330 train_time:128842ms step_avg:58.17ms
step:2216/2330 train_time:128903ms step_avg:58.17ms
step:2217/2330 train_time:128961ms step_avg:58.17ms
step:2218/2330 train_time:129023ms step_avg:58.17ms
step:2219/2330 train_time:129080ms step_avg:58.17ms
step:2220/2330 train_time:129142ms step_avg:58.17ms
step:2221/2330 train_time:129200ms step_avg:58.17ms
step:2222/2330 train_time:129260ms step_avg:58.17ms
step:2223/2330 train_time:129318ms step_avg:58.17ms
step:2224/2330 train_time:129377ms step_avg:58.17ms
step:2225/2330 train_time:129436ms step_avg:58.17ms
step:2226/2330 train_time:129496ms step_avg:58.17ms
step:2227/2330 train_time:129554ms step_avg:58.17ms
step:2228/2330 train_time:129614ms step_avg:58.18ms
step:2229/2330 train_time:129671ms step_avg:58.17ms
step:2230/2330 train_time:129731ms step_avg:58.18ms
step:2231/2330 train_time:129787ms step_avg:58.17ms
step:2232/2330 train_time:129849ms step_avg:58.18ms
step:2233/2330 train_time:129905ms step_avg:58.18ms
step:2234/2330 train_time:129967ms step_avg:58.18ms
step:2235/2330 train_time:130023ms step_avg:58.18ms
step:2236/2330 train_time:130086ms step_avg:58.18ms
step:2237/2330 train_time:130143ms step_avg:58.18ms
step:2238/2330 train_time:130204ms step_avg:58.18ms
step:2239/2330 train_time:130261ms step_avg:58.18ms
step:2240/2330 train_time:130322ms step_avg:58.18ms
step:2241/2330 train_time:130379ms step_avg:58.18ms
step:2242/2330 train_time:130439ms step_avg:58.18ms
step:2243/2330 train_time:130497ms step_avg:58.18ms
step:2244/2330 train_time:130558ms step_avg:58.18ms
step:2245/2330 train_time:130616ms step_avg:58.18ms
step:2246/2330 train_time:130676ms step_avg:58.18ms
step:2247/2330 train_time:130734ms step_avg:58.18ms
step:2248/2330 train_time:130795ms step_avg:58.18ms
step:2249/2330 train_time:130852ms step_avg:58.18ms
step:2250/2330 train_time:130912ms step_avg:58.18ms
step:2250/2330 val_loss:3.7618 train_time:130993ms step_avg:58.22ms
step:2251/2330 train_time:131014ms step_avg:58.20ms
step:2252/2330 train_time:131034ms step_avg:58.19ms
step:2253/2330 train_time:131091ms step_avg:58.18ms
step:2254/2330 train_time:131156ms step_avg:58.19ms
step:2255/2330 train_time:131213ms step_avg:58.19ms
step:2256/2330 train_time:131275ms step_avg:58.19ms
step:2257/2330 train_time:131331ms step_avg:58.19ms
step:2258/2330 train_time:131391ms step_avg:58.19ms
step:2259/2330 train_time:131447ms step_avg:58.19ms
step:2260/2330 train_time:131508ms step_avg:58.19ms
step:2261/2330 train_time:131564ms step_avg:58.19ms
step:2262/2330 train_time:131624ms step_avg:58.19ms
step:2263/2330 train_time:131680ms step_avg:58.19ms
step:2264/2330 train_time:131740ms step_avg:58.19ms
step:2265/2330 train_time:131796ms step_avg:58.19ms
step:2266/2330 train_time:131856ms step_avg:58.19ms
step:2267/2330 train_time:131912ms step_avg:58.19ms
step:2268/2330 train_time:131973ms step_avg:58.19ms
step:2269/2330 train_time:132031ms step_avg:58.19ms
step:2270/2330 train_time:132093ms step_avg:58.19ms
step:2271/2330 train_time:132151ms step_avg:58.19ms
step:2272/2330 train_time:132213ms step_avg:58.19ms
step:2273/2330 train_time:132270ms step_avg:58.19ms
step:2274/2330 train_time:132330ms step_avg:58.19ms
step:2275/2330 train_time:132387ms step_avg:58.19ms
step:2276/2330 train_time:132448ms step_avg:58.19ms
step:2277/2330 train_time:132504ms step_avg:58.19ms
step:2278/2330 train_time:132564ms step_avg:58.19ms
step:2279/2330 train_time:132620ms step_avg:58.19ms
step:2280/2330 train_time:132681ms step_avg:58.19ms
step:2281/2330 train_time:132737ms step_avg:58.19ms
step:2282/2330 train_time:132798ms step_avg:58.19ms
step:2283/2330 train_time:132854ms step_avg:58.19ms
step:2284/2330 train_time:132914ms step_avg:58.19ms
step:2285/2330 train_time:132971ms step_avg:58.19ms
step:2286/2330 train_time:133032ms step_avg:58.19ms
step:2287/2330 train_time:133089ms step_avg:58.19ms
step:2288/2330 train_time:133152ms step_avg:58.20ms
step:2289/2330 train_time:133209ms step_avg:58.20ms
step:2290/2330 train_time:133270ms step_avg:58.20ms
step:2291/2330 train_time:133327ms step_avg:58.20ms
step:2292/2330 train_time:133388ms step_avg:58.20ms
step:2293/2330 train_time:133445ms step_avg:58.20ms
step:2294/2330 train_time:133506ms step_avg:58.20ms
step:2295/2330 train_time:133563ms step_avg:58.20ms
step:2296/2330 train_time:133623ms step_avg:58.20ms
step:2297/2330 train_time:133680ms step_avg:58.20ms
step:2298/2330 train_time:133740ms step_avg:58.20ms
step:2299/2330 train_time:133796ms step_avg:58.20ms
step:2300/2330 train_time:133856ms step_avg:58.20ms
step:2301/2330 train_time:133912ms step_avg:58.20ms
step:2302/2330 train_time:133974ms step_avg:58.20ms
step:2303/2330 train_time:134031ms step_avg:58.20ms
step:2304/2330 train_time:134091ms step_avg:58.20ms
step:2305/2330 train_time:134148ms step_avg:58.20ms
step:2306/2330 train_time:134209ms step_avg:58.20ms
step:2307/2330 train_time:134266ms step_avg:58.20ms
step:2308/2330 train_time:134327ms step_avg:58.20ms
step:2309/2330 train_time:134385ms step_avg:58.20ms
step:2310/2330 train_time:134445ms step_avg:58.20ms
step:2311/2330 train_time:134501ms step_avg:58.20ms
step:2312/2330 train_time:134563ms step_avg:58.20ms
step:2313/2330 train_time:134620ms step_avg:58.20ms
step:2314/2330 train_time:134680ms step_avg:58.20ms
step:2315/2330 train_time:134736ms step_avg:58.20ms
step:2316/2330 train_time:134796ms step_avg:58.20ms
step:2317/2330 train_time:134854ms step_avg:58.20ms
step:2318/2330 train_time:134914ms step_avg:58.20ms
step:2319/2330 train_time:134971ms step_avg:58.20ms
step:2320/2330 train_time:135031ms step_avg:58.20ms
step:2321/2330 train_time:135088ms step_avg:58.20ms
step:2322/2330 train_time:135149ms step_avg:58.20ms
step:2323/2330 train_time:135206ms step_avg:58.20ms
step:2324/2330 train_time:135267ms step_avg:58.20ms
step:2325/2330 train_time:135324ms step_avg:58.20ms
step:2326/2330 train_time:135384ms step_avg:58.20ms
step:2327/2330 train_time:135441ms step_avg:58.20ms
step:2328/2330 train_time:135501ms step_avg:58.20ms
step:2329/2330 train_time:135558ms step_avg:58.20ms
step:2330/2330 train_time:135618ms step_avg:58.20ms
step:2330/2330 val_loss:3.7469 train_time:135699ms step_avg:58.24ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
