import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--adamw_lr', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_lr_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_lr_tuning/{args2.adamw_lr}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=float(args2.adamw_lr),  betas=(0.85, 0.85), eps=1e-10, weight_decay=0.0, fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_lr_tuning/{args2.adamw_lr}", exist_ok=True)
            torch.save(log, f"logs_adamw_small_lr_tuning/{args2.adamw_lr}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 01:22:48 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   36C    P0             117W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   35C    P0             117W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             118W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             115W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   35C    P0             119W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             114W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:83ms step_avg:83.25ms
step:2/2330 train_time:176ms step_avg:88.12ms
step:3/2330 train_time:194ms step_avg:64.56ms
step:4/2330 train_time:213ms step_avg:53.28ms
step:5/2330 train_time:267ms step_avg:53.41ms
step:6/2330 train_time:325ms step_avg:54.20ms
step:7/2330 train_time:381ms step_avg:54.37ms
step:8/2330 train_time:439ms step_avg:54.88ms
step:9/2330 train_time:494ms step_avg:54.91ms
step:10/2330 train_time:552ms step_avg:55.23ms
step:11/2330 train_time:607ms step_avg:55.23ms
step:12/2330 train_time:667ms step_avg:55.56ms
step:13/2330 train_time:722ms step_avg:55.56ms
step:14/2330 train_time:780ms step_avg:55.75ms
step:15/2330 train_time:836ms step_avg:55.73ms
step:16/2330 train_time:894ms step_avg:55.86ms
step:17/2330 train_time:949ms step_avg:55.83ms
step:18/2330 train_time:1008ms step_avg:55.98ms
step:19/2330 train_time:1066ms step_avg:56.09ms
step:20/2330 train_time:1126ms step_avg:56.30ms
step:21/2330 train_time:1183ms step_avg:56.31ms
step:22/2330 train_time:1245ms step_avg:56.61ms
step:23/2330 train_time:1301ms step_avg:56.56ms
step:24/2330 train_time:1362ms step_avg:56.74ms
step:25/2330 train_time:1417ms step_avg:56.68ms
step:26/2330 train_time:1475ms step_avg:56.75ms
step:27/2330 train_time:1531ms step_avg:56.70ms
step:28/2330 train_time:1589ms step_avg:56.74ms
step:29/2330 train_time:1644ms step_avg:56.69ms
step:30/2330 train_time:1702ms step_avg:56.75ms
step:31/2330 train_time:1758ms step_avg:56.71ms
step:32/2330 train_time:1816ms step_avg:56.75ms
step:33/2330 train_time:1871ms step_avg:56.71ms
step:34/2330 train_time:1930ms step_avg:56.76ms
step:35/2330 train_time:1986ms step_avg:56.73ms
step:36/2330 train_time:2045ms step_avg:56.80ms
step:37/2330 train_time:2101ms step_avg:56.78ms
step:38/2330 train_time:2162ms step_avg:56.90ms
step:39/2330 train_time:2219ms step_avg:56.89ms
step:40/2330 train_time:2279ms step_avg:56.98ms
step:41/2330 train_time:2335ms step_avg:56.96ms
step:42/2330 train_time:2394ms step_avg:56.99ms
step:43/2330 train_time:2449ms step_avg:56.96ms
step:44/2330 train_time:2508ms step_avg:57.00ms
step:45/2330 train_time:2564ms step_avg:56.97ms
step:46/2330 train_time:2623ms step_avg:57.01ms
step:47/2330 train_time:2678ms step_avg:56.98ms
step:48/2330 train_time:2737ms step_avg:57.02ms
step:49/2330 train_time:2792ms step_avg:56.98ms
step:50/2330 train_time:2851ms step_avg:57.02ms
step:51/2330 train_time:2906ms step_avg:56.98ms
step:52/2330 train_time:2965ms step_avg:57.03ms
step:53/2330 train_time:3021ms step_avg:57.00ms
step:54/2330 train_time:3082ms step_avg:57.07ms
step:55/2330 train_time:3138ms step_avg:57.05ms
step:56/2330 train_time:3198ms step_avg:57.10ms
step:57/2330 train_time:3253ms step_avg:57.08ms
step:58/2330 train_time:3314ms step_avg:57.14ms
step:59/2330 train_time:3369ms step_avg:57.11ms
step:60/2330 train_time:3429ms step_avg:57.15ms
step:61/2330 train_time:3485ms step_avg:57.13ms
step:62/2330 train_time:3543ms step_avg:57.15ms
step:63/2330 train_time:3598ms step_avg:57.12ms
step:64/2330 train_time:3658ms step_avg:57.16ms
step:65/2330 train_time:3714ms step_avg:57.14ms
step:66/2330 train_time:3773ms step_avg:57.16ms
step:67/2330 train_time:3828ms step_avg:57.14ms
step:68/2330 train_time:3887ms step_avg:57.17ms
step:69/2330 train_time:3943ms step_avg:57.15ms
step:70/2330 train_time:4002ms step_avg:57.17ms
step:71/2330 train_time:4058ms step_avg:57.15ms
step:72/2330 train_time:4117ms step_avg:57.18ms
step:73/2330 train_time:4173ms step_avg:57.17ms
step:74/2330 train_time:4232ms step_avg:57.19ms
step:75/2330 train_time:4289ms step_avg:57.18ms
step:76/2330 train_time:4347ms step_avg:57.20ms
step:77/2330 train_time:4403ms step_avg:57.19ms
step:78/2330 train_time:4463ms step_avg:57.22ms
step:79/2330 train_time:4518ms step_avg:57.19ms
step:80/2330 train_time:4579ms step_avg:57.23ms
step:81/2330 train_time:4634ms step_avg:57.21ms
step:82/2330 train_time:4693ms step_avg:57.23ms
step:83/2330 train_time:4749ms step_avg:57.22ms
step:84/2330 train_time:4807ms step_avg:57.22ms
step:85/2330 train_time:4862ms step_avg:57.21ms
step:86/2330 train_time:4922ms step_avg:57.23ms
step:87/2330 train_time:4977ms step_avg:57.21ms
step:88/2330 train_time:5036ms step_avg:57.23ms
step:89/2330 train_time:5092ms step_avg:57.22ms
step:90/2330 train_time:5151ms step_avg:57.23ms
step:91/2330 train_time:5207ms step_avg:57.22ms
step:92/2330 train_time:5267ms step_avg:57.25ms
step:93/2330 train_time:5323ms step_avg:57.24ms
step:94/2330 train_time:5382ms step_avg:57.26ms
step:95/2330 train_time:5438ms step_avg:57.24ms
step:96/2330 train_time:5497ms step_avg:57.26ms
step:97/2330 train_time:5553ms step_avg:57.25ms
step:98/2330 train_time:5612ms step_avg:57.26ms
step:99/2330 train_time:5668ms step_avg:57.25ms
step:100/2330 train_time:5727ms step_avg:57.27ms
step:101/2330 train_time:5783ms step_avg:57.26ms
step:102/2330 train_time:5842ms step_avg:57.27ms
step:103/2330 train_time:5898ms step_avg:57.26ms
step:104/2330 train_time:5956ms step_avg:57.27ms
step:105/2330 train_time:6012ms step_avg:57.26ms
step:106/2330 train_time:6071ms step_avg:57.27ms
step:107/2330 train_time:6127ms step_avg:57.26ms
step:108/2330 train_time:6186ms step_avg:57.28ms
step:109/2330 train_time:6242ms step_avg:57.27ms
step:110/2330 train_time:6301ms step_avg:57.28ms
step:111/2330 train_time:6357ms step_avg:57.27ms
step:112/2330 train_time:6416ms step_avg:57.29ms
step:113/2330 train_time:6471ms step_avg:57.27ms
step:114/2330 train_time:6530ms step_avg:57.28ms
step:115/2330 train_time:6586ms step_avg:57.27ms
step:116/2330 train_time:6645ms step_avg:57.29ms
step:117/2330 train_time:6701ms step_avg:57.27ms
step:118/2330 train_time:6760ms step_avg:57.29ms
step:119/2330 train_time:6816ms step_avg:57.27ms
step:120/2330 train_time:6875ms step_avg:57.29ms
step:121/2330 train_time:6930ms step_avg:57.27ms
step:122/2330 train_time:6989ms step_avg:57.29ms
step:123/2330 train_time:7046ms step_avg:57.28ms
step:124/2330 train_time:7105ms step_avg:57.30ms
step:125/2330 train_time:7161ms step_avg:57.28ms
step:126/2330 train_time:7219ms step_avg:57.29ms
step:127/2330 train_time:7275ms step_avg:57.28ms
step:128/2330 train_time:7335ms step_avg:57.30ms
step:129/2330 train_time:7390ms step_avg:57.29ms
step:130/2330 train_time:7449ms step_avg:57.30ms
step:131/2330 train_time:7505ms step_avg:57.29ms
step:132/2330 train_time:7564ms step_avg:57.30ms
step:133/2330 train_time:7619ms step_avg:57.29ms
step:134/2330 train_time:7680ms step_avg:57.31ms
step:135/2330 train_time:7736ms step_avg:57.30ms
step:136/2330 train_time:7794ms step_avg:57.31ms
step:137/2330 train_time:7850ms step_avg:57.30ms
step:138/2330 train_time:7909ms step_avg:57.31ms
step:139/2330 train_time:7964ms step_avg:57.30ms
step:140/2330 train_time:8024ms step_avg:57.32ms
step:141/2330 train_time:8080ms step_avg:57.31ms
step:142/2330 train_time:8140ms step_avg:57.32ms
step:143/2330 train_time:8196ms step_avg:57.32ms
step:144/2330 train_time:8256ms step_avg:57.33ms
step:145/2330 train_time:8312ms step_avg:57.32ms
step:146/2330 train_time:8370ms step_avg:57.33ms
step:147/2330 train_time:8426ms step_avg:57.32ms
step:148/2330 train_time:8485ms step_avg:57.33ms
step:149/2330 train_time:8540ms step_avg:57.32ms
step:150/2330 train_time:8600ms step_avg:57.34ms
step:151/2330 train_time:8657ms step_avg:57.33ms
step:152/2330 train_time:8716ms step_avg:57.34ms
step:153/2330 train_time:8773ms step_avg:57.34ms
step:154/2330 train_time:8831ms step_avg:57.34ms
step:155/2330 train_time:8886ms step_avg:57.33ms
step:156/2330 train_time:8945ms step_avg:57.34ms
step:157/2330 train_time:9001ms step_avg:57.33ms
step:158/2330 train_time:9060ms step_avg:57.34ms
step:159/2330 train_time:9116ms step_avg:57.34ms
step:160/2330 train_time:9175ms step_avg:57.34ms
step:161/2330 train_time:9231ms step_avg:57.33ms
step:162/2330 train_time:9289ms step_avg:57.34ms
step:163/2330 train_time:9345ms step_avg:57.33ms
step:164/2330 train_time:9404ms step_avg:57.34ms
step:165/2330 train_time:9460ms step_avg:57.33ms
step:166/2330 train_time:9519ms step_avg:57.34ms
step:167/2330 train_time:9574ms step_avg:57.33ms
step:168/2330 train_time:9634ms step_avg:57.34ms
step:169/2330 train_time:9690ms step_avg:57.34ms
step:170/2330 train_time:9749ms step_avg:57.35ms
step:171/2330 train_time:9804ms step_avg:57.34ms
step:172/2330 train_time:9863ms step_avg:57.34ms
step:173/2330 train_time:9919ms step_avg:57.34ms
step:174/2330 train_time:9978ms step_avg:57.35ms
step:175/2330 train_time:10034ms step_avg:57.34ms
step:176/2330 train_time:10093ms step_avg:57.35ms
step:177/2330 train_time:10149ms step_avg:57.34ms
step:178/2330 train_time:10207ms step_avg:57.34ms
step:179/2330 train_time:10263ms step_avg:57.34ms
step:180/2330 train_time:10322ms step_avg:57.34ms
step:181/2330 train_time:10378ms step_avg:57.34ms
step:182/2330 train_time:10438ms step_avg:57.35ms
step:183/2330 train_time:10495ms step_avg:57.35ms
step:184/2330 train_time:10553ms step_avg:57.36ms
step:185/2330 train_time:10609ms step_avg:57.35ms
step:186/2330 train_time:10668ms step_avg:57.35ms
step:187/2330 train_time:10724ms step_avg:57.35ms
step:188/2330 train_time:10783ms step_avg:57.36ms
step:189/2330 train_time:10839ms step_avg:57.35ms
step:190/2330 train_time:10898ms step_avg:57.36ms
step:191/2330 train_time:10953ms step_avg:57.35ms
step:192/2330 train_time:11012ms step_avg:57.36ms
step:193/2330 train_time:11068ms step_avg:57.35ms
step:194/2330 train_time:11127ms step_avg:57.36ms
step:195/2330 train_time:11183ms step_avg:57.35ms
step:196/2330 train_time:11242ms step_avg:57.36ms
step:197/2330 train_time:11298ms step_avg:57.35ms
step:198/2330 train_time:11357ms step_avg:57.36ms
step:199/2330 train_time:11412ms step_avg:57.35ms
step:200/2330 train_time:11471ms step_avg:57.36ms
step:201/2330 train_time:11527ms step_avg:57.35ms
step:202/2330 train_time:11587ms step_avg:57.36ms
step:203/2330 train_time:11642ms step_avg:57.35ms
step:204/2330 train_time:11702ms step_avg:57.36ms
step:205/2330 train_time:11758ms step_avg:57.36ms
step:206/2330 train_time:11816ms step_avg:57.36ms
step:207/2330 train_time:11872ms step_avg:57.35ms
step:208/2330 train_time:11931ms step_avg:57.36ms
step:209/2330 train_time:11987ms step_avg:57.36ms
step:210/2330 train_time:12046ms step_avg:57.36ms
step:211/2330 train_time:12102ms step_avg:57.36ms
step:212/2330 train_time:12161ms step_avg:57.36ms
step:213/2330 train_time:12217ms step_avg:57.36ms
step:214/2330 train_time:12277ms step_avg:57.37ms
step:215/2330 train_time:12332ms step_avg:57.36ms
step:216/2330 train_time:12392ms step_avg:57.37ms
step:217/2330 train_time:12448ms step_avg:57.36ms
step:218/2330 train_time:12507ms step_avg:57.37ms
step:219/2330 train_time:12563ms step_avg:57.37ms
step:220/2330 train_time:12623ms step_avg:57.38ms
step:221/2330 train_time:12679ms step_avg:57.37ms
step:222/2330 train_time:12737ms step_avg:57.37ms
step:223/2330 train_time:12794ms step_avg:57.37ms
step:224/2330 train_time:12852ms step_avg:57.38ms
step:225/2330 train_time:12908ms step_avg:57.37ms
step:226/2330 train_time:12967ms step_avg:57.38ms
step:227/2330 train_time:13023ms step_avg:57.37ms
step:228/2330 train_time:13082ms step_avg:57.38ms
step:229/2330 train_time:13138ms step_avg:57.37ms
step:230/2330 train_time:13197ms step_avg:57.38ms
step:231/2330 train_time:13253ms step_avg:57.37ms
step:232/2330 train_time:13312ms step_avg:57.38ms
step:233/2330 train_time:13368ms step_avg:57.37ms
step:234/2330 train_time:13426ms step_avg:57.38ms
step:235/2330 train_time:13483ms step_avg:57.38ms
step:236/2330 train_time:13542ms step_avg:57.38ms
step:237/2330 train_time:13597ms step_avg:57.37ms
step:238/2330 train_time:13657ms step_avg:57.38ms
step:239/2330 train_time:13713ms step_avg:57.38ms
step:240/2330 train_time:13771ms step_avg:57.38ms
step:241/2330 train_time:13827ms step_avg:57.38ms
step:242/2330 train_time:13886ms step_avg:57.38ms
step:243/2330 train_time:13942ms step_avg:57.37ms
step:244/2330 train_time:14001ms step_avg:57.38ms
step:245/2330 train_time:14058ms step_avg:57.38ms
step:246/2330 train_time:14116ms step_avg:57.38ms
step:247/2330 train_time:14172ms step_avg:57.38ms
step:248/2330 train_time:14231ms step_avg:57.38ms
step:249/2330 train_time:14286ms step_avg:57.38ms
step:250/2330 train_time:14346ms step_avg:57.38ms
step:250/2330 val_loss:4.9037 train_time:14425ms step_avg:57.70ms
step:251/2330 train_time:14443ms step_avg:57.54ms
step:252/2330 train_time:14463ms step_avg:57.39ms
step:253/2330 train_time:14518ms step_avg:57.38ms
step:254/2330 train_time:14581ms step_avg:57.41ms
step:255/2330 train_time:14637ms step_avg:57.40ms
step:256/2330 train_time:14702ms step_avg:57.43ms
step:257/2330 train_time:14757ms step_avg:57.42ms
step:258/2330 train_time:14816ms step_avg:57.43ms
step:259/2330 train_time:14871ms step_avg:57.42ms
step:260/2330 train_time:14931ms step_avg:57.43ms
step:261/2330 train_time:14986ms step_avg:57.42ms
step:262/2330 train_time:15044ms step_avg:57.42ms
step:263/2330 train_time:15099ms step_avg:57.41ms
step:264/2330 train_time:15157ms step_avg:57.41ms
step:265/2330 train_time:15213ms step_avg:57.41ms
step:266/2330 train_time:15271ms step_avg:57.41ms
step:267/2330 train_time:15327ms step_avg:57.40ms
step:268/2330 train_time:15387ms step_avg:57.41ms
step:269/2330 train_time:15444ms step_avg:57.41ms
step:270/2330 train_time:15504ms step_avg:57.42ms
step:271/2330 train_time:15562ms step_avg:57.42ms
step:272/2330 train_time:15621ms step_avg:57.43ms
step:273/2330 train_time:15676ms step_avg:57.42ms
step:274/2330 train_time:15737ms step_avg:57.44ms
step:275/2330 train_time:15793ms step_avg:57.43ms
step:276/2330 train_time:15853ms step_avg:57.44ms
step:277/2330 train_time:15908ms step_avg:57.43ms
step:278/2330 train_time:15968ms step_avg:57.44ms
step:279/2330 train_time:16024ms step_avg:57.43ms
step:280/2330 train_time:16082ms step_avg:57.44ms
step:281/2330 train_time:16137ms step_avg:57.43ms
step:282/2330 train_time:16196ms step_avg:57.43ms
step:283/2330 train_time:16251ms step_avg:57.43ms
step:284/2330 train_time:16310ms step_avg:57.43ms
step:285/2330 train_time:16366ms step_avg:57.43ms
step:286/2330 train_time:16425ms step_avg:57.43ms
step:287/2330 train_time:16481ms step_avg:57.43ms
step:288/2330 train_time:16540ms step_avg:57.43ms
step:289/2330 train_time:16597ms step_avg:57.43ms
step:290/2330 train_time:16657ms step_avg:57.44ms
step:291/2330 train_time:16713ms step_avg:57.43ms
step:292/2330 train_time:16772ms step_avg:57.44ms
step:293/2330 train_time:16828ms step_avg:57.43ms
step:294/2330 train_time:16886ms step_avg:57.44ms
step:295/2330 train_time:16943ms step_avg:57.43ms
step:296/2330 train_time:17001ms step_avg:57.44ms
step:297/2330 train_time:17056ms step_avg:57.43ms
step:298/2330 train_time:17116ms step_avg:57.44ms
step:299/2330 train_time:17171ms step_avg:57.43ms
step:300/2330 train_time:17230ms step_avg:57.43ms
step:301/2330 train_time:17286ms step_avg:57.43ms
step:302/2330 train_time:17345ms step_avg:57.43ms
step:303/2330 train_time:17401ms step_avg:57.43ms
step:304/2330 train_time:17460ms step_avg:57.44ms
step:305/2330 train_time:17516ms step_avg:57.43ms
step:306/2330 train_time:17576ms step_avg:57.44ms
step:307/2330 train_time:17632ms step_avg:57.43ms
step:308/2330 train_time:17692ms step_avg:57.44ms
step:309/2330 train_time:17748ms step_avg:57.44ms
step:310/2330 train_time:17808ms step_avg:57.44ms
step:311/2330 train_time:17864ms step_avg:57.44ms
step:312/2330 train_time:17923ms step_avg:57.44ms
step:313/2330 train_time:17978ms step_avg:57.44ms
step:314/2330 train_time:18037ms step_avg:57.44ms
step:315/2330 train_time:18092ms step_avg:57.44ms
step:316/2330 train_time:18152ms step_avg:57.44ms
step:317/2330 train_time:18208ms step_avg:57.44ms
step:318/2330 train_time:18267ms step_avg:57.44ms
step:319/2330 train_time:18323ms step_avg:57.44ms
step:320/2330 train_time:18382ms step_avg:57.44ms
step:321/2330 train_time:18438ms step_avg:57.44ms
step:322/2330 train_time:18497ms step_avg:57.44ms
step:323/2330 train_time:18553ms step_avg:57.44ms
step:324/2330 train_time:18613ms step_avg:57.45ms
step:325/2330 train_time:18670ms step_avg:57.45ms
step:326/2330 train_time:18729ms step_avg:57.45ms
step:327/2330 train_time:18786ms step_avg:57.45ms
step:328/2330 train_time:18844ms step_avg:57.45ms
step:329/2330 train_time:18901ms step_avg:57.45ms
step:330/2330 train_time:18959ms step_avg:57.45ms
step:331/2330 train_time:19015ms step_avg:57.45ms
step:332/2330 train_time:19074ms step_avg:57.45ms
step:333/2330 train_time:19130ms step_avg:57.45ms
step:334/2330 train_time:19190ms step_avg:57.46ms
step:335/2330 train_time:19246ms step_avg:57.45ms
step:336/2330 train_time:19307ms step_avg:57.46ms
step:337/2330 train_time:19364ms step_avg:57.46ms
step:338/2330 train_time:19423ms step_avg:57.46ms
step:339/2330 train_time:19478ms step_avg:57.46ms
step:340/2330 train_time:19537ms step_avg:57.46ms
step:341/2330 train_time:19594ms step_avg:57.46ms
step:342/2330 train_time:19652ms step_avg:57.46ms
step:343/2330 train_time:19708ms step_avg:57.46ms
step:344/2330 train_time:19767ms step_avg:57.46ms
step:345/2330 train_time:19824ms step_avg:57.46ms
step:346/2330 train_time:19883ms step_avg:57.46ms
step:347/2330 train_time:19939ms step_avg:57.46ms
step:348/2330 train_time:19998ms step_avg:57.47ms
step:349/2330 train_time:20054ms step_avg:57.46ms
step:350/2330 train_time:20113ms step_avg:57.47ms
step:351/2330 train_time:20168ms step_avg:57.46ms
step:352/2330 train_time:20228ms step_avg:57.47ms
step:353/2330 train_time:20284ms step_avg:57.46ms
step:354/2330 train_time:20343ms step_avg:57.47ms
step:355/2330 train_time:20399ms step_avg:57.46ms
step:356/2330 train_time:20458ms step_avg:57.47ms
step:357/2330 train_time:20514ms step_avg:57.46ms
step:358/2330 train_time:20573ms step_avg:57.47ms
step:359/2330 train_time:20629ms step_avg:57.46ms
step:360/2330 train_time:20688ms step_avg:57.47ms
step:361/2330 train_time:20744ms step_avg:57.46ms
step:362/2330 train_time:20803ms step_avg:57.47ms
step:363/2330 train_time:20859ms step_avg:57.46ms
step:364/2330 train_time:20918ms step_avg:57.47ms
step:365/2330 train_time:20974ms step_avg:57.46ms
step:366/2330 train_time:21034ms step_avg:57.47ms
step:367/2330 train_time:21089ms step_avg:57.46ms
step:368/2330 train_time:21148ms step_avg:57.47ms
step:369/2330 train_time:21204ms step_avg:57.46ms
step:370/2330 train_time:21264ms step_avg:57.47ms
step:371/2330 train_time:21319ms step_avg:57.46ms
step:372/2330 train_time:21378ms step_avg:57.47ms
step:373/2330 train_time:21434ms step_avg:57.46ms
step:374/2330 train_time:21494ms step_avg:57.47ms
step:375/2330 train_time:21550ms step_avg:57.47ms
step:376/2330 train_time:21609ms step_avg:57.47ms
step:377/2330 train_time:21665ms step_avg:57.47ms
step:378/2330 train_time:21723ms step_avg:57.47ms
step:379/2330 train_time:21780ms step_avg:57.47ms
step:380/2330 train_time:21839ms step_avg:57.47ms
step:381/2330 train_time:21895ms step_avg:57.47ms
step:382/2330 train_time:21954ms step_avg:57.47ms
step:383/2330 train_time:22010ms step_avg:57.47ms
step:384/2330 train_time:22069ms step_avg:57.47ms
step:385/2330 train_time:22125ms step_avg:57.47ms
step:386/2330 train_time:22184ms step_avg:57.47ms
step:387/2330 train_time:22240ms step_avg:57.47ms
step:388/2330 train_time:22299ms step_avg:57.47ms
step:389/2330 train_time:22354ms step_avg:57.47ms
step:390/2330 train_time:22415ms step_avg:57.47ms
step:391/2330 train_time:22470ms step_avg:57.47ms
step:392/2330 train_time:22530ms step_avg:57.48ms
step:393/2330 train_time:22586ms step_avg:57.47ms
step:394/2330 train_time:22645ms step_avg:57.47ms
step:395/2330 train_time:22701ms step_avg:57.47ms
step:396/2330 train_time:22761ms step_avg:57.48ms
step:397/2330 train_time:22817ms step_avg:57.47ms
step:398/2330 train_time:22875ms step_avg:57.48ms
step:399/2330 train_time:22931ms step_avg:57.47ms
step:400/2330 train_time:22991ms step_avg:57.48ms
step:401/2330 train_time:23048ms step_avg:57.48ms
step:402/2330 train_time:23107ms step_avg:57.48ms
step:403/2330 train_time:23163ms step_avg:57.48ms
step:404/2330 train_time:23221ms step_avg:57.48ms
step:405/2330 train_time:23277ms step_avg:57.47ms
step:406/2330 train_time:23336ms step_avg:57.48ms
step:407/2330 train_time:23391ms step_avg:57.47ms
step:408/2330 train_time:23451ms step_avg:57.48ms
step:409/2330 train_time:23507ms step_avg:57.48ms
step:410/2330 train_time:23566ms step_avg:57.48ms
step:411/2330 train_time:23622ms step_avg:57.48ms
step:412/2330 train_time:23681ms step_avg:57.48ms
step:413/2330 train_time:23737ms step_avg:57.47ms
step:414/2330 train_time:23797ms step_avg:57.48ms
step:415/2330 train_time:23853ms step_avg:57.48ms
step:416/2330 train_time:23913ms step_avg:57.48ms
step:417/2330 train_time:23969ms step_avg:57.48ms
step:418/2330 train_time:24028ms step_avg:57.48ms
step:419/2330 train_time:24084ms step_avg:57.48ms
step:420/2330 train_time:24144ms step_avg:57.48ms
step:421/2330 train_time:24200ms step_avg:57.48ms
step:422/2330 train_time:24258ms step_avg:57.48ms
step:423/2330 train_time:24314ms step_avg:57.48ms
step:424/2330 train_time:24373ms step_avg:57.48ms
step:425/2330 train_time:24429ms step_avg:57.48ms
step:426/2330 train_time:24488ms step_avg:57.48ms
step:427/2330 train_time:24546ms step_avg:57.48ms
step:428/2330 train_time:24605ms step_avg:57.49ms
step:429/2330 train_time:24662ms step_avg:57.49ms
step:430/2330 train_time:24720ms step_avg:57.49ms
step:431/2330 train_time:24776ms step_avg:57.48ms
step:432/2330 train_time:24834ms step_avg:57.49ms
step:433/2330 train_time:24890ms step_avg:57.48ms
step:434/2330 train_time:24950ms step_avg:57.49ms
step:435/2330 train_time:25006ms step_avg:57.48ms
step:436/2330 train_time:25065ms step_avg:57.49ms
step:437/2330 train_time:25121ms step_avg:57.48ms
step:438/2330 train_time:25180ms step_avg:57.49ms
step:439/2330 train_time:25236ms step_avg:57.49ms
step:440/2330 train_time:25295ms step_avg:57.49ms
step:441/2330 train_time:25351ms step_avg:57.49ms
step:442/2330 train_time:25411ms step_avg:57.49ms
step:443/2330 train_time:25467ms step_avg:57.49ms
step:444/2330 train_time:25526ms step_avg:57.49ms
step:445/2330 train_time:25582ms step_avg:57.49ms
step:446/2330 train_time:25641ms step_avg:57.49ms
step:447/2330 train_time:25697ms step_avg:57.49ms
step:448/2330 train_time:25756ms step_avg:57.49ms
step:449/2330 train_time:25811ms step_avg:57.49ms
step:450/2330 train_time:25871ms step_avg:57.49ms
step:451/2330 train_time:25927ms step_avg:57.49ms
step:452/2330 train_time:25986ms step_avg:57.49ms
step:453/2330 train_time:26042ms step_avg:57.49ms
step:454/2330 train_time:26101ms step_avg:57.49ms
step:455/2330 train_time:26157ms step_avg:57.49ms
step:456/2330 train_time:26216ms step_avg:57.49ms
step:457/2330 train_time:26271ms step_avg:57.49ms
step:458/2330 train_time:26331ms step_avg:57.49ms
step:459/2330 train_time:26387ms step_avg:57.49ms
step:460/2330 train_time:26447ms step_avg:57.49ms
step:461/2330 train_time:26503ms step_avg:57.49ms
step:462/2330 train_time:26562ms step_avg:57.49ms
step:463/2330 train_time:26618ms step_avg:57.49ms
step:464/2330 train_time:26677ms step_avg:57.49ms
step:465/2330 train_time:26733ms step_avg:57.49ms
step:466/2330 train_time:26791ms step_avg:57.49ms
step:467/2330 train_time:26847ms step_avg:57.49ms
step:468/2330 train_time:26907ms step_avg:57.49ms
step:469/2330 train_time:26963ms step_avg:57.49ms
step:470/2330 train_time:27023ms step_avg:57.50ms
step:471/2330 train_time:27079ms step_avg:57.49ms
step:472/2330 train_time:27137ms step_avg:57.49ms
step:473/2330 train_time:27193ms step_avg:57.49ms
step:474/2330 train_time:27254ms step_avg:57.50ms
step:475/2330 train_time:27309ms step_avg:57.49ms
step:476/2330 train_time:27368ms step_avg:57.50ms
step:477/2330 train_time:27425ms step_avg:57.49ms
step:478/2330 train_time:27484ms step_avg:57.50ms
step:479/2330 train_time:27539ms step_avg:57.49ms
step:480/2330 train_time:27599ms step_avg:57.50ms
step:481/2330 train_time:27655ms step_avg:57.49ms
step:482/2330 train_time:27714ms step_avg:57.50ms
step:483/2330 train_time:27770ms step_avg:57.49ms
step:484/2330 train_time:27830ms step_avg:57.50ms
step:485/2330 train_time:27885ms step_avg:57.50ms
step:486/2330 train_time:27946ms step_avg:57.50ms
step:487/2330 train_time:28002ms step_avg:57.50ms
step:488/2330 train_time:28062ms step_avg:57.50ms
step:489/2330 train_time:28118ms step_avg:57.50ms
step:490/2330 train_time:28176ms step_avg:57.50ms
step:491/2330 train_time:28232ms step_avg:57.50ms
step:492/2330 train_time:28292ms step_avg:57.50ms
step:493/2330 train_time:28348ms step_avg:57.50ms
step:494/2330 train_time:28407ms step_avg:57.50ms
step:495/2330 train_time:28463ms step_avg:57.50ms
step:496/2330 train_time:28523ms step_avg:57.51ms
step:497/2330 train_time:28579ms step_avg:57.50ms
step:498/2330 train_time:28639ms step_avg:57.51ms
step:499/2330 train_time:28694ms step_avg:57.50ms
step:500/2330 train_time:28755ms step_avg:57.51ms
step:500/2330 val_loss:4.4077 train_time:28834ms step_avg:57.67ms
step:501/2330 train_time:28852ms step_avg:57.59ms
step:502/2330 train_time:28873ms step_avg:57.52ms
step:503/2330 train_time:28929ms step_avg:57.51ms
step:504/2330 train_time:28992ms step_avg:57.52ms
step:505/2330 train_time:29048ms step_avg:57.52ms
step:506/2330 train_time:29109ms step_avg:57.53ms
step:507/2330 train_time:29164ms step_avg:57.52ms
step:508/2330 train_time:29223ms step_avg:57.53ms
step:509/2330 train_time:29279ms step_avg:57.52ms
step:510/2330 train_time:29339ms step_avg:57.53ms
step:511/2330 train_time:29394ms step_avg:57.52ms
step:512/2330 train_time:29453ms step_avg:57.52ms
step:513/2330 train_time:29509ms step_avg:57.52ms
step:514/2330 train_time:29566ms step_avg:57.52ms
step:515/2330 train_time:29621ms step_avg:57.52ms
step:516/2330 train_time:29680ms step_avg:57.52ms
step:517/2330 train_time:29736ms step_avg:57.52ms
step:518/2330 train_time:29796ms step_avg:57.52ms
step:519/2330 train_time:29853ms step_avg:57.52ms
step:520/2330 train_time:29913ms step_avg:57.52ms
step:521/2330 train_time:29970ms step_avg:57.52ms
step:522/2330 train_time:30030ms step_avg:57.53ms
step:523/2330 train_time:30086ms step_avg:57.53ms
step:524/2330 train_time:30146ms step_avg:57.53ms
step:525/2330 train_time:30203ms step_avg:57.53ms
step:526/2330 train_time:30262ms step_avg:57.53ms
step:527/2330 train_time:30318ms step_avg:57.53ms
step:528/2330 train_time:30378ms step_avg:57.53ms
step:529/2330 train_time:30433ms step_avg:57.53ms
step:530/2330 train_time:30492ms step_avg:57.53ms
step:531/2330 train_time:30547ms step_avg:57.53ms
step:532/2330 train_time:30605ms step_avg:57.53ms
step:533/2330 train_time:30661ms step_avg:57.53ms
step:534/2330 train_time:30720ms step_avg:57.53ms
step:535/2330 train_time:30776ms step_avg:57.53ms
step:536/2330 train_time:30835ms step_avg:57.53ms
step:537/2330 train_time:30893ms step_avg:57.53ms
step:538/2330 train_time:30952ms step_avg:57.53ms
step:539/2330 train_time:31009ms step_avg:57.53ms
step:540/2330 train_time:31068ms step_avg:57.53ms
step:541/2330 train_time:31125ms step_avg:57.53ms
step:542/2330 train_time:31184ms step_avg:57.54ms
step:543/2330 train_time:31240ms step_avg:57.53ms
step:544/2330 train_time:31301ms step_avg:57.54ms
step:545/2330 train_time:31357ms step_avg:57.54ms
step:546/2330 train_time:31416ms step_avg:57.54ms
step:547/2330 train_time:31472ms step_avg:57.54ms
step:548/2330 train_time:31530ms step_avg:57.54ms
step:549/2330 train_time:31586ms step_avg:57.53ms
step:550/2330 train_time:31645ms step_avg:57.54ms
step:551/2330 train_time:31701ms step_avg:57.53ms
step:552/2330 train_time:31760ms step_avg:57.54ms
step:553/2330 train_time:31816ms step_avg:57.53ms
step:554/2330 train_time:31875ms step_avg:57.54ms
step:555/2330 train_time:31932ms step_avg:57.53ms
step:556/2330 train_time:31993ms step_avg:57.54ms
step:557/2330 train_time:32049ms step_avg:57.54ms
step:558/2330 train_time:32109ms step_avg:57.54ms
step:559/2330 train_time:32165ms step_avg:57.54ms
step:560/2330 train_time:32224ms step_avg:57.54ms
step:561/2330 train_time:32280ms step_avg:57.54ms
step:562/2330 train_time:32341ms step_avg:57.55ms
step:563/2330 train_time:32396ms step_avg:57.54ms
step:564/2330 train_time:32455ms step_avg:57.54ms
step:565/2330 train_time:32510ms step_avg:57.54ms
step:566/2330 train_time:32569ms step_avg:57.54ms
step:567/2330 train_time:32624ms step_avg:57.54ms
step:568/2330 train_time:32683ms step_avg:57.54ms
step:569/2330 train_time:32739ms step_avg:57.54ms
step:570/2330 train_time:32799ms step_avg:57.54ms
step:571/2330 train_time:32854ms step_avg:57.54ms
step:572/2330 train_time:32914ms step_avg:57.54ms
step:573/2330 train_time:32972ms step_avg:57.54ms
step:574/2330 train_time:33031ms step_avg:57.55ms
step:575/2330 train_time:33088ms step_avg:57.54ms
step:576/2330 train_time:33146ms step_avg:57.55ms
step:577/2330 train_time:33203ms step_avg:57.54ms
step:578/2330 train_time:33262ms step_avg:57.55ms
step:579/2330 train_time:33317ms step_avg:57.54ms
step:580/2330 train_time:33378ms step_avg:57.55ms
step:581/2330 train_time:33434ms step_avg:57.55ms
step:582/2330 train_time:33493ms step_avg:57.55ms
step:583/2330 train_time:33549ms step_avg:57.54ms
step:584/2330 train_time:33608ms step_avg:57.55ms
step:585/2330 train_time:33664ms step_avg:57.54ms
step:586/2330 train_time:33722ms step_avg:57.55ms
step:587/2330 train_time:33778ms step_avg:57.54ms
step:588/2330 train_time:33838ms step_avg:57.55ms
step:589/2330 train_time:33894ms step_avg:57.55ms
step:590/2330 train_time:33954ms step_avg:57.55ms
step:591/2330 train_time:34010ms step_avg:57.55ms
step:592/2330 train_time:34069ms step_avg:57.55ms
step:593/2330 train_time:34125ms step_avg:57.55ms
step:594/2330 train_time:34185ms step_avg:57.55ms
step:595/2330 train_time:34241ms step_avg:57.55ms
step:596/2330 train_time:34301ms step_avg:57.55ms
step:597/2330 train_time:34356ms step_avg:57.55ms
step:598/2330 train_time:34416ms step_avg:57.55ms
step:599/2330 train_time:34472ms step_avg:57.55ms
step:600/2330 train_time:34531ms step_avg:57.55ms
step:601/2330 train_time:34588ms step_avg:57.55ms
step:602/2330 train_time:34646ms step_avg:57.55ms
step:603/2330 train_time:34702ms step_avg:57.55ms
step:604/2330 train_time:34761ms step_avg:57.55ms
step:605/2330 train_time:34817ms step_avg:57.55ms
step:606/2330 train_time:34876ms step_avg:57.55ms
step:607/2330 train_time:34932ms step_avg:57.55ms
step:608/2330 train_time:34993ms step_avg:57.55ms
step:609/2330 train_time:35049ms step_avg:57.55ms
step:610/2330 train_time:35109ms step_avg:57.56ms
step:611/2330 train_time:35165ms step_avg:57.55ms
step:612/2330 train_time:35224ms step_avg:57.55ms
step:613/2330 train_time:35279ms step_avg:57.55ms
step:614/2330 train_time:35340ms step_avg:57.56ms
step:615/2330 train_time:35396ms step_avg:57.55ms
step:616/2330 train_time:35455ms step_avg:57.56ms
step:617/2330 train_time:35512ms step_avg:57.56ms
step:618/2330 train_time:35571ms step_avg:57.56ms
step:619/2330 train_time:35628ms step_avg:57.56ms
step:620/2330 train_time:35686ms step_avg:57.56ms
step:621/2330 train_time:35742ms step_avg:57.56ms
step:622/2330 train_time:35801ms step_avg:57.56ms
step:623/2330 train_time:35857ms step_avg:57.56ms
step:624/2330 train_time:35916ms step_avg:57.56ms
step:625/2330 train_time:35972ms step_avg:57.56ms
step:626/2330 train_time:36033ms step_avg:57.56ms
step:627/2330 train_time:36089ms step_avg:57.56ms
step:628/2330 train_time:36147ms step_avg:57.56ms
step:629/2330 train_time:36204ms step_avg:57.56ms
step:630/2330 train_time:36262ms step_avg:57.56ms
step:631/2330 train_time:36318ms step_avg:57.56ms
step:632/2330 train_time:36378ms step_avg:57.56ms
step:633/2330 train_time:36433ms step_avg:57.56ms
step:634/2330 train_time:36495ms step_avg:57.56ms
step:635/2330 train_time:36551ms step_avg:57.56ms
step:636/2330 train_time:36610ms step_avg:57.56ms
step:637/2330 train_time:36666ms step_avg:57.56ms
step:638/2330 train_time:36724ms step_avg:57.56ms
step:639/2330 train_time:36780ms step_avg:57.56ms
step:640/2330 train_time:36840ms step_avg:57.56ms
step:641/2330 train_time:36896ms step_avg:57.56ms
step:642/2330 train_time:36955ms step_avg:57.56ms
step:643/2330 train_time:37011ms step_avg:57.56ms
step:644/2330 train_time:37072ms step_avg:57.56ms
step:645/2330 train_time:37127ms step_avg:57.56ms
step:646/2330 train_time:37187ms step_avg:57.56ms
step:647/2330 train_time:37243ms step_avg:57.56ms
step:648/2330 train_time:37303ms step_avg:57.57ms
step:649/2330 train_time:37358ms step_avg:57.56ms
step:650/2330 train_time:37418ms step_avg:57.57ms
step:651/2330 train_time:37474ms step_avg:57.56ms
step:652/2330 train_time:37535ms step_avg:57.57ms
step:653/2330 train_time:37590ms step_avg:57.57ms
step:654/2330 train_time:37650ms step_avg:57.57ms
step:655/2330 train_time:37706ms step_avg:57.57ms
step:656/2330 train_time:37765ms step_avg:57.57ms
step:657/2330 train_time:37820ms step_avg:57.57ms
step:658/2330 train_time:37881ms step_avg:57.57ms
step:659/2330 train_time:37937ms step_avg:57.57ms
step:660/2330 train_time:37996ms step_avg:57.57ms
step:661/2330 train_time:38053ms step_avg:57.57ms
step:662/2330 train_time:38112ms step_avg:57.57ms
step:663/2330 train_time:38168ms step_avg:57.57ms
step:664/2330 train_time:38227ms step_avg:57.57ms
step:665/2330 train_time:38284ms step_avg:57.57ms
step:666/2330 train_time:38343ms step_avg:57.57ms
step:667/2330 train_time:38399ms step_avg:57.57ms
step:668/2330 train_time:38459ms step_avg:57.57ms
step:669/2330 train_time:38515ms step_avg:57.57ms
step:670/2330 train_time:38575ms step_avg:57.57ms
step:671/2330 train_time:38631ms step_avg:57.57ms
step:672/2330 train_time:38690ms step_avg:57.57ms
step:673/2330 train_time:38746ms step_avg:57.57ms
step:674/2330 train_time:38805ms step_avg:57.57ms
step:675/2330 train_time:38861ms step_avg:57.57ms
step:676/2330 train_time:38920ms step_avg:57.57ms
step:677/2330 train_time:38976ms step_avg:57.57ms
step:678/2330 train_time:39036ms step_avg:57.57ms
step:679/2330 train_time:39093ms step_avg:57.57ms
step:680/2330 train_time:39152ms step_avg:57.58ms
step:681/2330 train_time:39208ms step_avg:57.57ms
step:682/2330 train_time:39267ms step_avg:57.58ms
step:683/2330 train_time:39322ms step_avg:57.57ms
step:684/2330 train_time:39382ms step_avg:57.58ms
step:685/2330 train_time:39437ms step_avg:57.57ms
step:686/2330 train_time:39497ms step_avg:57.58ms
step:687/2330 train_time:39553ms step_avg:57.57ms
step:688/2330 train_time:39613ms step_avg:57.58ms
step:689/2330 train_time:39668ms step_avg:57.57ms
step:690/2330 train_time:39728ms step_avg:57.58ms
step:691/2330 train_time:39783ms step_avg:57.57ms
step:692/2330 train_time:39842ms step_avg:57.58ms
step:693/2330 train_time:39898ms step_avg:57.57ms
step:694/2330 train_time:39958ms step_avg:57.58ms
step:695/2330 train_time:40015ms step_avg:57.58ms
step:696/2330 train_time:40074ms step_avg:57.58ms
step:697/2330 train_time:40132ms step_avg:57.58ms
step:698/2330 train_time:40191ms step_avg:57.58ms
step:699/2330 train_time:40248ms step_avg:57.58ms
step:700/2330 train_time:40307ms step_avg:57.58ms
step:701/2330 train_time:40363ms step_avg:57.58ms
step:702/2330 train_time:40422ms step_avg:57.58ms
step:703/2330 train_time:40478ms step_avg:57.58ms
step:704/2330 train_time:40539ms step_avg:57.58ms
step:705/2330 train_time:40594ms step_avg:57.58ms
step:706/2330 train_time:40655ms step_avg:57.58ms
step:707/2330 train_time:40710ms step_avg:57.58ms
step:708/2330 train_time:40770ms step_avg:57.58ms
step:709/2330 train_time:40826ms step_avg:57.58ms
step:710/2330 train_time:40884ms step_avg:57.58ms
step:711/2330 train_time:40941ms step_avg:57.58ms
step:712/2330 train_time:41001ms step_avg:57.59ms
step:713/2330 train_time:41057ms step_avg:57.58ms
step:714/2330 train_time:41116ms step_avg:57.59ms
step:715/2330 train_time:41173ms step_avg:57.59ms
step:716/2330 train_time:41232ms step_avg:57.59ms
step:717/2330 train_time:41288ms step_avg:57.58ms
step:718/2330 train_time:41347ms step_avg:57.59ms
step:719/2330 train_time:41402ms step_avg:57.58ms
step:720/2330 train_time:41462ms step_avg:57.59ms
step:721/2330 train_time:41517ms step_avg:57.58ms
step:722/2330 train_time:41578ms step_avg:57.59ms
step:723/2330 train_time:41634ms step_avg:57.58ms
step:724/2330 train_time:41694ms step_avg:57.59ms
step:725/2330 train_time:41750ms step_avg:57.59ms
step:726/2330 train_time:41809ms step_avg:57.59ms
step:727/2330 train_time:41865ms step_avg:57.59ms
step:728/2330 train_time:41924ms step_avg:57.59ms
step:729/2330 train_time:41979ms step_avg:57.58ms
step:730/2330 train_time:42040ms step_avg:57.59ms
step:731/2330 train_time:42096ms step_avg:57.59ms
step:732/2330 train_time:42156ms step_avg:57.59ms
step:733/2330 train_time:42212ms step_avg:57.59ms
step:734/2330 train_time:42272ms step_avg:57.59ms
step:735/2330 train_time:42328ms step_avg:57.59ms
step:736/2330 train_time:42387ms step_avg:57.59ms
step:737/2330 train_time:42443ms step_avg:57.59ms
step:738/2330 train_time:42502ms step_avg:57.59ms
step:739/2330 train_time:42557ms step_avg:57.59ms
step:740/2330 train_time:42618ms step_avg:57.59ms
step:741/2330 train_time:42674ms step_avg:57.59ms
step:742/2330 train_time:42733ms step_avg:57.59ms
step:743/2330 train_time:42790ms step_avg:57.59ms
step:744/2330 train_time:42849ms step_avg:57.59ms
step:745/2330 train_time:42906ms step_avg:57.59ms
step:746/2330 train_time:42964ms step_avg:57.59ms
step:747/2330 train_time:43018ms step_avg:57.59ms
step:748/2330 train_time:43080ms step_avg:57.59ms
step:749/2330 train_time:43136ms step_avg:57.59ms
step:750/2330 train_time:43196ms step_avg:57.59ms
step:750/2330 val_loss:4.2071 train_time:43275ms step_avg:57.70ms
step:751/2330 train_time:43294ms step_avg:57.65ms
step:752/2330 train_time:43313ms step_avg:57.60ms
step:753/2330 train_time:43370ms step_avg:57.60ms
step:754/2330 train_time:43435ms step_avg:57.61ms
step:755/2330 train_time:43492ms step_avg:57.61ms
step:756/2330 train_time:43551ms step_avg:57.61ms
step:757/2330 train_time:43608ms step_avg:57.61ms
step:758/2330 train_time:43666ms step_avg:57.61ms
step:759/2330 train_time:43722ms step_avg:57.61ms
step:760/2330 train_time:43782ms step_avg:57.61ms
step:761/2330 train_time:43837ms step_avg:57.60ms
step:762/2330 train_time:43896ms step_avg:57.61ms
step:763/2330 train_time:43952ms step_avg:57.60ms
step:764/2330 train_time:44010ms step_avg:57.60ms
step:765/2330 train_time:44067ms step_avg:57.60ms
step:766/2330 train_time:44125ms step_avg:57.60ms
step:767/2330 train_time:44181ms step_avg:57.60ms
step:768/2330 train_time:44242ms step_avg:57.61ms
step:769/2330 train_time:44299ms step_avg:57.61ms
step:770/2330 train_time:44362ms step_avg:57.61ms
step:771/2330 train_time:44419ms step_avg:57.61ms
step:772/2330 train_time:44480ms step_avg:57.62ms
step:773/2330 train_time:44538ms step_avg:57.62ms
step:774/2330 train_time:44600ms step_avg:57.62ms
step:775/2330 train_time:44658ms step_avg:57.62ms
step:776/2330 train_time:44718ms step_avg:57.63ms
step:777/2330 train_time:44775ms step_avg:57.62ms
step:778/2330 train_time:44834ms step_avg:57.63ms
step:779/2330 train_time:44891ms step_avg:57.63ms
step:780/2330 train_time:44951ms step_avg:57.63ms
step:781/2330 train_time:45008ms step_avg:57.63ms
step:782/2330 train_time:45067ms step_avg:57.63ms
step:783/2330 train_time:45124ms step_avg:57.63ms
step:784/2330 train_time:45183ms step_avg:57.63ms
step:785/2330 train_time:45240ms step_avg:57.63ms
step:786/2330 train_time:45301ms step_avg:57.63ms
step:787/2330 train_time:45358ms step_avg:57.63ms
step:788/2330 train_time:45420ms step_avg:57.64ms
step:789/2330 train_time:45477ms step_avg:57.64ms
step:790/2330 train_time:45538ms step_avg:57.64ms
step:791/2330 train_time:45595ms step_avg:57.64ms
step:792/2330 train_time:45655ms step_avg:57.65ms
step:793/2330 train_time:45712ms step_avg:57.65ms
step:794/2330 train_time:45772ms step_avg:57.65ms
step:795/2330 train_time:45829ms step_avg:57.65ms
step:796/2330 train_time:45888ms step_avg:57.65ms
step:797/2330 train_time:45944ms step_avg:57.65ms
step:798/2330 train_time:46004ms step_avg:57.65ms
step:799/2330 train_time:46060ms step_avg:57.65ms
step:800/2330 train_time:46120ms step_avg:57.65ms
step:801/2330 train_time:46177ms step_avg:57.65ms
step:802/2330 train_time:46237ms step_avg:57.65ms
step:803/2330 train_time:46294ms step_avg:57.65ms
step:804/2330 train_time:46354ms step_avg:57.65ms
step:805/2330 train_time:46411ms step_avg:57.65ms
step:806/2330 train_time:46473ms step_avg:57.66ms
step:807/2330 train_time:46531ms step_avg:57.66ms
step:808/2330 train_time:46591ms step_avg:57.66ms
step:809/2330 train_time:46648ms step_avg:57.66ms
step:810/2330 train_time:46708ms step_avg:57.66ms
step:811/2330 train_time:46765ms step_avg:57.66ms
step:812/2330 train_time:46825ms step_avg:57.67ms
step:813/2330 train_time:46882ms step_avg:57.67ms
step:814/2330 train_time:46941ms step_avg:57.67ms
step:815/2330 train_time:46998ms step_avg:57.67ms
step:816/2330 train_time:47058ms step_avg:57.67ms
step:817/2330 train_time:47115ms step_avg:57.67ms
step:818/2330 train_time:47174ms step_avg:57.67ms
step:819/2330 train_time:47232ms step_avg:57.67ms
step:820/2330 train_time:47292ms step_avg:57.67ms
step:821/2330 train_time:47349ms step_avg:57.67ms
step:822/2330 train_time:47408ms step_avg:57.67ms
step:823/2330 train_time:47465ms step_avg:57.67ms
step:824/2330 train_time:47527ms step_avg:57.68ms
step:825/2330 train_time:47584ms step_avg:57.68ms
step:826/2330 train_time:47645ms step_avg:57.68ms
step:827/2330 train_time:47701ms step_avg:57.68ms
step:828/2330 train_time:47761ms step_avg:57.68ms
step:829/2330 train_time:47819ms step_avg:57.68ms
step:830/2330 train_time:47879ms step_avg:57.69ms
step:831/2330 train_time:47936ms step_avg:57.68ms
step:832/2330 train_time:47995ms step_avg:57.69ms
step:833/2330 train_time:48052ms step_avg:57.69ms
step:834/2330 train_time:48112ms step_avg:57.69ms
step:835/2330 train_time:48168ms step_avg:57.69ms
step:836/2330 train_time:48228ms step_avg:57.69ms
step:837/2330 train_time:48286ms step_avg:57.69ms
step:838/2330 train_time:48346ms step_avg:57.69ms
step:839/2330 train_time:48402ms step_avg:57.69ms
step:840/2330 train_time:48463ms step_avg:57.69ms
step:841/2330 train_time:48520ms step_avg:57.69ms
step:842/2330 train_time:48580ms step_avg:57.70ms
step:843/2330 train_time:48637ms step_avg:57.70ms
step:844/2330 train_time:48698ms step_avg:57.70ms
step:845/2330 train_time:48755ms step_avg:57.70ms
step:846/2330 train_time:48815ms step_avg:57.70ms
step:847/2330 train_time:48872ms step_avg:57.70ms
step:848/2330 train_time:48932ms step_avg:57.70ms
step:849/2330 train_time:48989ms step_avg:57.70ms
step:850/2330 train_time:49048ms step_avg:57.70ms
step:851/2330 train_time:49105ms step_avg:57.70ms
step:852/2330 train_time:49165ms step_avg:57.71ms
step:853/2330 train_time:49222ms step_avg:57.70ms
step:854/2330 train_time:49282ms step_avg:57.71ms
step:855/2330 train_time:49339ms step_avg:57.71ms
step:856/2330 train_time:49399ms step_avg:57.71ms
step:857/2330 train_time:49456ms step_avg:57.71ms
step:858/2330 train_time:49517ms step_avg:57.71ms
step:859/2330 train_time:49574ms step_avg:57.71ms
step:860/2330 train_time:49636ms step_avg:57.72ms
step:861/2330 train_time:49693ms step_avg:57.72ms
step:862/2330 train_time:49754ms step_avg:57.72ms
step:863/2330 train_time:49811ms step_avg:57.72ms
step:864/2330 train_time:49871ms step_avg:57.72ms
step:865/2330 train_time:49929ms step_avg:57.72ms
step:866/2330 train_time:49988ms step_avg:57.72ms
step:867/2330 train_time:50045ms step_avg:57.72ms
step:868/2330 train_time:50105ms step_avg:57.72ms
step:869/2330 train_time:50162ms step_avg:57.72ms
step:870/2330 train_time:50222ms step_avg:57.73ms
step:871/2330 train_time:50279ms step_avg:57.73ms
step:872/2330 train_time:50340ms step_avg:57.73ms
step:873/2330 train_time:50397ms step_avg:57.73ms
step:874/2330 train_time:50457ms step_avg:57.73ms
step:875/2330 train_time:50514ms step_avg:57.73ms
step:876/2330 train_time:50575ms step_avg:57.73ms
step:877/2330 train_time:50632ms step_avg:57.73ms
step:878/2330 train_time:50692ms step_avg:57.74ms
step:879/2330 train_time:50749ms step_avg:57.73ms
step:880/2330 train_time:50810ms step_avg:57.74ms
step:881/2330 train_time:50867ms step_avg:57.74ms
step:882/2330 train_time:50926ms step_avg:57.74ms
step:883/2330 train_time:50984ms step_avg:57.74ms
step:884/2330 train_time:51043ms step_avg:57.74ms
step:885/2330 train_time:51100ms step_avg:57.74ms
step:886/2330 train_time:51160ms step_avg:57.74ms
step:887/2330 train_time:51217ms step_avg:57.74ms
step:888/2330 train_time:51277ms step_avg:57.74ms
step:889/2330 train_time:51333ms step_avg:57.74ms
step:890/2330 train_time:51394ms step_avg:57.75ms
step:891/2330 train_time:51451ms step_avg:57.74ms
step:892/2330 train_time:51511ms step_avg:57.75ms
step:893/2330 train_time:51568ms step_avg:57.75ms
step:894/2330 train_time:51629ms step_avg:57.75ms
step:895/2330 train_time:51686ms step_avg:57.75ms
step:896/2330 train_time:51746ms step_avg:57.75ms
step:897/2330 train_time:51803ms step_avg:57.75ms
step:898/2330 train_time:51862ms step_avg:57.75ms
step:899/2330 train_time:51918ms step_avg:57.75ms
step:900/2330 train_time:51979ms step_avg:57.75ms
step:901/2330 train_time:52037ms step_avg:57.75ms
step:902/2330 train_time:52096ms step_avg:57.76ms
step:903/2330 train_time:52154ms step_avg:57.76ms
step:904/2330 train_time:52213ms step_avg:57.76ms
step:905/2330 train_time:52270ms step_avg:57.76ms
step:906/2330 train_time:52330ms step_avg:57.76ms
step:907/2330 train_time:52387ms step_avg:57.76ms
step:908/2330 train_time:52447ms step_avg:57.76ms
step:909/2330 train_time:52504ms step_avg:57.76ms
step:910/2330 train_time:52563ms step_avg:57.76ms
step:911/2330 train_time:52621ms step_avg:57.76ms
step:912/2330 train_time:52682ms step_avg:57.76ms
step:913/2330 train_time:52739ms step_avg:57.76ms
step:914/2330 train_time:52799ms step_avg:57.77ms
step:915/2330 train_time:52856ms step_avg:57.77ms
step:916/2330 train_time:52916ms step_avg:57.77ms
step:917/2330 train_time:52973ms step_avg:57.77ms
step:918/2330 train_time:53034ms step_avg:57.77ms
step:919/2330 train_time:53091ms step_avg:57.77ms
step:920/2330 train_time:53151ms step_avg:57.77ms
step:921/2330 train_time:53208ms step_avg:57.77ms
step:922/2330 train_time:53267ms step_avg:57.77ms
step:923/2330 train_time:53324ms step_avg:57.77ms
step:924/2330 train_time:53384ms step_avg:57.78ms
step:925/2330 train_time:53441ms step_avg:57.77ms
step:926/2330 train_time:53502ms step_avg:57.78ms
step:927/2330 train_time:53558ms step_avg:57.78ms
step:928/2330 train_time:53620ms step_avg:57.78ms
step:929/2330 train_time:53676ms step_avg:57.78ms
step:930/2330 train_time:53737ms step_avg:57.78ms
step:931/2330 train_time:53793ms step_avg:57.78ms
step:932/2330 train_time:53854ms step_avg:57.78ms
step:933/2330 train_time:53911ms step_avg:57.78ms
step:934/2330 train_time:53971ms step_avg:57.78ms
step:935/2330 train_time:54027ms step_avg:57.78ms
step:936/2330 train_time:54089ms step_avg:57.79ms
step:937/2330 train_time:54146ms step_avg:57.79ms
step:938/2330 train_time:54206ms step_avg:57.79ms
step:939/2330 train_time:54262ms step_avg:57.79ms
step:940/2330 train_time:54323ms step_avg:57.79ms
step:941/2330 train_time:54380ms step_avg:57.79ms
step:942/2330 train_time:54440ms step_avg:57.79ms
step:943/2330 train_time:54496ms step_avg:57.79ms
step:944/2330 train_time:54557ms step_avg:57.79ms
step:945/2330 train_time:54614ms step_avg:57.79ms
step:946/2330 train_time:54674ms step_avg:57.79ms
step:947/2330 train_time:54731ms step_avg:57.79ms
step:948/2330 train_time:54791ms step_avg:57.80ms
step:949/2330 train_time:54847ms step_avg:57.79ms
step:950/2330 train_time:54909ms step_avg:57.80ms
step:951/2330 train_time:54965ms step_avg:57.80ms
step:952/2330 train_time:55025ms step_avg:57.80ms
step:953/2330 train_time:55082ms step_avg:57.80ms
step:954/2330 train_time:55143ms step_avg:57.80ms
step:955/2330 train_time:55200ms step_avg:57.80ms
step:956/2330 train_time:55260ms step_avg:57.80ms
step:957/2330 train_time:55316ms step_avg:57.80ms
step:958/2330 train_time:55377ms step_avg:57.80ms
step:959/2330 train_time:55434ms step_avg:57.80ms
step:960/2330 train_time:55494ms step_avg:57.81ms
step:961/2330 train_time:55552ms step_avg:57.81ms
step:962/2330 train_time:55612ms step_avg:57.81ms
step:963/2330 train_time:55669ms step_avg:57.81ms
step:964/2330 train_time:55729ms step_avg:57.81ms
step:965/2330 train_time:55786ms step_avg:57.81ms
step:966/2330 train_time:55846ms step_avg:57.81ms
step:967/2330 train_time:55902ms step_avg:57.81ms
step:968/2330 train_time:55962ms step_avg:57.81ms
step:969/2330 train_time:56019ms step_avg:57.81ms
step:970/2330 train_time:56081ms step_avg:57.81ms
step:971/2330 train_time:56138ms step_avg:57.81ms
step:972/2330 train_time:56199ms step_avg:57.82ms
step:973/2330 train_time:56256ms step_avg:57.82ms
step:974/2330 train_time:56317ms step_avg:57.82ms
step:975/2330 train_time:56374ms step_avg:57.82ms
step:976/2330 train_time:56434ms step_avg:57.82ms
step:977/2330 train_time:56491ms step_avg:57.82ms
step:978/2330 train_time:56551ms step_avg:57.82ms
step:979/2330 train_time:56608ms step_avg:57.82ms
step:980/2330 train_time:56668ms step_avg:57.82ms
step:981/2330 train_time:56725ms step_avg:57.82ms
step:982/2330 train_time:56785ms step_avg:57.83ms
step:983/2330 train_time:56842ms step_avg:57.83ms
step:984/2330 train_time:56902ms step_avg:57.83ms
step:985/2330 train_time:56959ms step_avg:57.83ms
step:986/2330 train_time:57019ms step_avg:57.83ms
step:987/2330 train_time:57076ms step_avg:57.83ms
step:988/2330 train_time:57137ms step_avg:57.83ms
step:989/2330 train_time:57195ms step_avg:57.83ms
step:990/2330 train_time:57255ms step_avg:57.83ms
step:991/2330 train_time:57312ms step_avg:57.83ms
step:992/2330 train_time:57371ms step_avg:57.83ms
step:993/2330 train_time:57429ms step_avg:57.83ms
step:994/2330 train_time:57488ms step_avg:57.84ms
step:995/2330 train_time:57545ms step_avg:57.83ms
step:996/2330 train_time:57605ms step_avg:57.84ms
step:997/2330 train_time:57662ms step_avg:57.84ms
step:998/2330 train_time:57723ms step_avg:57.84ms
step:999/2330 train_time:57780ms step_avg:57.84ms
step:1000/2330 train_time:57841ms step_avg:57.84ms
step:1000/2330 val_loss:4.0651 train_time:57921ms step_avg:57.92ms
step:1001/2330 train_time:57939ms step_avg:57.88ms
step:1002/2330 train_time:57958ms step_avg:57.84ms
step:1003/2330 train_time:58013ms step_avg:57.84ms
step:1004/2330 train_time:58081ms step_avg:57.85ms
step:1005/2330 train_time:58137ms step_avg:57.85ms
step:1006/2330 train_time:58201ms step_avg:57.85ms
step:1007/2330 train_time:58257ms step_avg:57.85ms
step:1008/2330 train_time:58317ms step_avg:57.85ms
step:1009/2330 train_time:58374ms step_avg:57.85ms
step:1010/2330 train_time:58434ms step_avg:57.85ms
step:1011/2330 train_time:58489ms step_avg:57.85ms
step:1012/2330 train_time:58549ms step_avg:57.86ms
step:1013/2330 train_time:58605ms step_avg:57.85ms
step:1014/2330 train_time:58665ms step_avg:57.86ms
step:1015/2330 train_time:58721ms step_avg:57.85ms
step:1016/2330 train_time:58780ms step_avg:57.85ms
step:1017/2330 train_time:58840ms step_avg:57.86ms
step:1018/2330 train_time:58902ms step_avg:57.86ms
step:1019/2330 train_time:58961ms step_avg:57.86ms
step:1020/2330 train_time:59022ms step_avg:57.86ms
step:1021/2330 train_time:59080ms step_avg:57.86ms
step:1022/2330 train_time:59139ms step_avg:57.87ms
step:1023/2330 train_time:59196ms step_avg:57.87ms
step:1024/2330 train_time:59256ms step_avg:57.87ms
step:1025/2330 train_time:59312ms step_avg:57.87ms
step:1026/2330 train_time:59373ms step_avg:57.87ms
step:1027/2330 train_time:59429ms step_avg:57.87ms
step:1028/2330 train_time:59488ms step_avg:57.87ms
step:1029/2330 train_time:59545ms step_avg:57.87ms
step:1030/2330 train_time:59604ms step_avg:57.87ms
step:1031/2330 train_time:59661ms step_avg:57.87ms
step:1032/2330 train_time:59721ms step_avg:57.87ms
step:1033/2330 train_time:59779ms step_avg:57.87ms
step:1034/2330 train_time:59839ms step_avg:57.87ms
step:1035/2330 train_time:59898ms step_avg:57.87ms
step:1036/2330 train_time:59959ms step_avg:57.88ms
step:1037/2330 train_time:60017ms step_avg:57.88ms
step:1038/2330 train_time:60077ms step_avg:57.88ms
step:1039/2330 train_time:60134ms step_avg:57.88ms
step:1040/2330 train_time:60194ms step_avg:57.88ms
step:1041/2330 train_time:60251ms step_avg:57.88ms
step:1042/2330 train_time:60312ms step_avg:57.88ms
step:1043/2330 train_time:60368ms step_avg:57.88ms
step:1044/2330 train_time:60428ms step_avg:57.88ms
step:1045/2330 train_time:60484ms step_avg:57.88ms
step:1046/2330 train_time:60546ms step_avg:57.88ms
step:1047/2330 train_time:60602ms step_avg:57.88ms
step:1048/2330 train_time:60662ms step_avg:57.88ms
step:1049/2330 train_time:60719ms step_avg:57.88ms
step:1050/2330 train_time:60779ms step_avg:57.88ms
step:1051/2330 train_time:60836ms step_avg:57.88ms
step:1052/2330 train_time:60895ms step_avg:57.89ms
step:1053/2330 train_time:60952ms step_avg:57.88ms
step:1054/2330 train_time:61012ms step_avg:57.89ms
step:1055/2330 train_time:61069ms step_avg:57.89ms
step:1056/2330 train_time:61130ms step_avg:57.89ms
step:1057/2330 train_time:61186ms step_avg:57.89ms
step:1058/2330 train_time:61249ms step_avg:57.89ms
step:1059/2330 train_time:61305ms step_avg:57.89ms
step:1060/2330 train_time:61365ms step_avg:57.89ms
step:1061/2330 train_time:61422ms step_avg:57.89ms
step:1062/2330 train_time:61482ms step_avg:57.89ms
step:1063/2330 train_time:61539ms step_avg:57.89ms
step:1064/2330 train_time:61598ms step_avg:57.89ms
step:1065/2330 train_time:61654ms step_avg:57.89ms
step:1066/2330 train_time:61714ms step_avg:57.89ms
step:1067/2330 train_time:61771ms step_avg:57.89ms
step:1068/2330 train_time:61831ms step_avg:57.89ms
step:1069/2330 train_time:61888ms step_avg:57.89ms
step:1070/2330 train_time:61949ms step_avg:57.90ms
step:1071/2330 train_time:62005ms step_avg:57.89ms
step:1072/2330 train_time:62067ms step_avg:57.90ms
step:1073/2330 train_time:62123ms step_avg:57.90ms
step:1074/2330 train_time:62185ms step_avg:57.90ms
step:1075/2330 train_time:62243ms step_avg:57.90ms
step:1076/2330 train_time:62303ms step_avg:57.90ms
step:1077/2330 train_time:62360ms step_avg:57.90ms
step:1078/2330 train_time:62420ms step_avg:57.90ms
step:1079/2330 train_time:62477ms step_avg:57.90ms
step:1080/2330 train_time:62537ms step_avg:57.90ms
step:1081/2330 train_time:62593ms step_avg:57.90ms
step:1082/2330 train_time:62654ms step_avg:57.91ms
step:1083/2330 train_time:62710ms step_avg:57.90ms
step:1084/2330 train_time:62771ms step_avg:57.91ms
step:1085/2330 train_time:62827ms step_avg:57.90ms
step:1086/2330 train_time:62887ms step_avg:57.91ms
step:1087/2330 train_time:62944ms step_avg:57.91ms
step:1088/2330 train_time:63005ms step_avg:57.91ms
step:1089/2330 train_time:63062ms step_avg:57.91ms
step:1090/2330 train_time:63123ms step_avg:57.91ms
step:1091/2330 train_time:63179ms step_avg:57.91ms
step:1092/2330 train_time:63241ms step_avg:57.91ms
step:1093/2330 train_time:63298ms step_avg:57.91ms
step:1094/2330 train_time:63357ms step_avg:57.91ms
step:1095/2330 train_time:63414ms step_avg:57.91ms
step:1096/2330 train_time:63474ms step_avg:57.91ms
step:1097/2330 train_time:63531ms step_avg:57.91ms
step:1098/2330 train_time:63590ms step_avg:57.91ms
step:1099/2330 train_time:63647ms step_avg:57.91ms
step:1100/2330 train_time:63708ms step_avg:57.92ms
step:1101/2330 train_time:63764ms step_avg:57.92ms
step:1102/2330 train_time:63825ms step_avg:57.92ms
step:1103/2330 train_time:63882ms step_avg:57.92ms
step:1104/2330 train_time:63941ms step_avg:57.92ms
step:1105/2330 train_time:63999ms step_avg:57.92ms
step:1106/2330 train_time:64060ms step_avg:57.92ms
step:1107/2330 train_time:64117ms step_avg:57.92ms
step:1108/2330 train_time:64177ms step_avg:57.92ms
step:1109/2330 train_time:64234ms step_avg:57.92ms
step:1110/2330 train_time:64294ms step_avg:57.92ms
step:1111/2330 train_time:64350ms step_avg:57.92ms
step:1112/2330 train_time:64412ms step_avg:57.92ms
step:1113/2330 train_time:64469ms step_avg:57.92ms
step:1114/2330 train_time:64529ms step_avg:57.93ms
step:1115/2330 train_time:64585ms step_avg:57.92ms
step:1116/2330 train_time:64647ms step_avg:57.93ms
step:1117/2330 train_time:64704ms step_avg:57.93ms
step:1118/2330 train_time:64764ms step_avg:57.93ms
step:1119/2330 train_time:64821ms step_avg:57.93ms
step:1120/2330 train_time:64881ms step_avg:57.93ms
step:1121/2330 train_time:64938ms step_avg:57.93ms
step:1122/2330 train_time:64997ms step_avg:57.93ms
step:1123/2330 train_time:65054ms step_avg:57.93ms
step:1124/2330 train_time:65115ms step_avg:57.93ms
step:1125/2330 train_time:65172ms step_avg:57.93ms
step:1126/2330 train_time:65232ms step_avg:57.93ms
step:1127/2330 train_time:65289ms step_avg:57.93ms
step:1128/2330 train_time:65350ms step_avg:57.93ms
step:1129/2330 train_time:65407ms step_avg:57.93ms
step:1130/2330 train_time:65467ms step_avg:57.94ms
step:1131/2330 train_time:65524ms step_avg:57.93ms
step:1132/2330 train_time:65585ms step_avg:57.94ms
step:1133/2330 train_time:65642ms step_avg:57.94ms
step:1134/2330 train_time:65702ms step_avg:57.94ms
step:1135/2330 train_time:65760ms step_avg:57.94ms
step:1136/2330 train_time:65819ms step_avg:57.94ms
step:1137/2330 train_time:65876ms step_avg:57.94ms
step:1138/2330 train_time:65935ms step_avg:57.94ms
step:1139/2330 train_time:65992ms step_avg:57.94ms
step:1140/2330 train_time:66053ms step_avg:57.94ms
step:1141/2330 train_time:66110ms step_avg:57.94ms
step:1142/2330 train_time:66170ms step_avg:57.94ms
step:1143/2330 train_time:66227ms step_avg:57.94ms
step:1144/2330 train_time:66287ms step_avg:57.94ms
step:1145/2330 train_time:66344ms step_avg:57.94ms
step:1146/2330 train_time:66403ms step_avg:57.94ms
step:1147/2330 train_time:66460ms step_avg:57.94ms
step:1148/2330 train_time:66521ms step_avg:57.95ms
step:1149/2330 train_time:66578ms step_avg:57.94ms
step:1150/2330 train_time:66638ms step_avg:57.95ms
step:1151/2330 train_time:66695ms step_avg:57.95ms
step:1152/2330 train_time:66755ms step_avg:57.95ms
step:1153/2330 train_time:66812ms step_avg:57.95ms
step:1154/2330 train_time:66872ms step_avg:57.95ms
step:1155/2330 train_time:66930ms step_avg:57.95ms
step:1156/2330 train_time:66989ms step_avg:57.95ms
step:1157/2330 train_time:67046ms step_avg:57.95ms
step:1158/2330 train_time:67106ms step_avg:57.95ms
step:1159/2330 train_time:67164ms step_avg:57.95ms
step:1160/2330 train_time:67224ms step_avg:57.95ms
step:1161/2330 train_time:67281ms step_avg:57.95ms
step:1162/2330 train_time:67341ms step_avg:57.95ms
step:1163/2330 train_time:67398ms step_avg:57.95ms
step:1164/2330 train_time:67458ms step_avg:57.95ms
step:1165/2330 train_time:67515ms step_avg:57.95ms
step:1166/2330 train_time:67576ms step_avg:57.96ms
step:1167/2330 train_time:67632ms step_avg:57.95ms
step:1168/2330 train_time:67692ms step_avg:57.96ms
step:1169/2330 train_time:67750ms step_avg:57.96ms
step:1170/2330 train_time:67810ms step_avg:57.96ms
step:1171/2330 train_time:67867ms step_avg:57.96ms
step:1172/2330 train_time:67927ms step_avg:57.96ms
step:1173/2330 train_time:67984ms step_avg:57.96ms
step:1174/2330 train_time:68044ms step_avg:57.96ms
step:1175/2330 train_time:68101ms step_avg:57.96ms
step:1176/2330 train_time:68162ms step_avg:57.96ms
step:1177/2330 train_time:68219ms step_avg:57.96ms
step:1178/2330 train_time:68279ms step_avg:57.96ms
step:1179/2330 train_time:68336ms step_avg:57.96ms
step:1180/2330 train_time:68396ms step_avg:57.96ms
step:1181/2330 train_time:68453ms step_avg:57.96ms
step:1182/2330 train_time:68513ms step_avg:57.96ms
step:1183/2330 train_time:68571ms step_avg:57.96ms
step:1184/2330 train_time:68631ms step_avg:57.96ms
step:1185/2330 train_time:68687ms step_avg:57.96ms
step:1186/2330 train_time:68747ms step_avg:57.97ms
step:1187/2330 train_time:68804ms step_avg:57.96ms
step:1188/2330 train_time:68865ms step_avg:57.97ms
step:1189/2330 train_time:68922ms step_avg:57.97ms
step:1190/2330 train_time:68982ms step_avg:57.97ms
step:1191/2330 train_time:69039ms step_avg:57.97ms
step:1192/2330 train_time:69099ms step_avg:57.97ms
step:1193/2330 train_time:69156ms step_avg:57.97ms
step:1194/2330 train_time:69215ms step_avg:57.97ms
step:1195/2330 train_time:69273ms step_avg:57.97ms
step:1196/2330 train_time:69333ms step_avg:57.97ms
step:1197/2330 train_time:69389ms step_avg:57.97ms
step:1198/2330 train_time:69450ms step_avg:57.97ms
step:1199/2330 train_time:69507ms step_avg:57.97ms
step:1200/2330 train_time:69567ms step_avg:57.97ms
step:1201/2330 train_time:69623ms step_avg:57.97ms
step:1202/2330 train_time:69683ms step_avg:57.97ms
step:1203/2330 train_time:69740ms step_avg:57.97ms
step:1204/2330 train_time:69800ms step_avg:57.97ms
step:1205/2330 train_time:69858ms step_avg:57.97ms
step:1206/2330 train_time:69917ms step_avg:57.97ms
step:1207/2330 train_time:69973ms step_avg:57.97ms
step:1208/2330 train_time:70034ms step_avg:57.98ms
step:1209/2330 train_time:70091ms step_avg:57.97ms
step:1210/2330 train_time:70152ms step_avg:57.98ms
step:1211/2330 train_time:70208ms step_avg:57.98ms
step:1212/2330 train_time:70269ms step_avg:57.98ms
step:1213/2330 train_time:70326ms step_avg:57.98ms
step:1214/2330 train_time:70387ms step_avg:57.98ms
step:1215/2330 train_time:70444ms step_avg:57.98ms
step:1216/2330 train_time:70505ms step_avg:57.98ms
step:1217/2330 train_time:70562ms step_avg:57.98ms
step:1218/2330 train_time:70623ms step_avg:57.98ms
step:1219/2330 train_time:70679ms step_avg:57.98ms
step:1220/2330 train_time:70740ms step_avg:57.98ms
step:1221/2330 train_time:70797ms step_avg:57.98ms
step:1222/2330 train_time:70857ms step_avg:57.98ms
step:1223/2330 train_time:70913ms step_avg:57.98ms
step:1224/2330 train_time:70974ms step_avg:57.99ms
step:1225/2330 train_time:71031ms step_avg:57.98ms
step:1226/2330 train_time:71091ms step_avg:57.99ms
step:1227/2330 train_time:71147ms step_avg:57.98ms
step:1228/2330 train_time:71208ms step_avg:57.99ms
step:1229/2330 train_time:71264ms step_avg:57.99ms
step:1230/2330 train_time:71325ms step_avg:57.99ms
step:1231/2330 train_time:71382ms step_avg:57.99ms
step:1232/2330 train_time:71443ms step_avg:57.99ms
step:1233/2330 train_time:71500ms step_avg:57.99ms
step:1234/2330 train_time:71561ms step_avg:57.99ms
step:1235/2330 train_time:71618ms step_avg:57.99ms
step:1236/2330 train_time:71677ms step_avg:57.99ms
step:1237/2330 train_time:71734ms step_avg:57.99ms
step:1238/2330 train_time:71794ms step_avg:57.99ms
step:1239/2330 train_time:71851ms step_avg:57.99ms
step:1240/2330 train_time:71912ms step_avg:57.99ms
step:1241/2330 train_time:71969ms step_avg:57.99ms
step:1242/2330 train_time:72029ms step_avg:57.99ms
step:1243/2330 train_time:72085ms step_avg:57.99ms
step:1244/2330 train_time:72146ms step_avg:58.00ms
step:1245/2330 train_time:72203ms step_avg:57.99ms
step:1246/2330 train_time:72263ms step_avg:58.00ms
step:1247/2330 train_time:72320ms step_avg:58.00ms
step:1248/2330 train_time:72380ms step_avg:58.00ms
step:1249/2330 train_time:72436ms step_avg:57.99ms
step:1250/2330 train_time:72497ms step_avg:58.00ms
step:1250/2330 val_loss:3.9850 train_time:72580ms step_avg:58.06ms
step:1251/2330 train_time:72598ms step_avg:58.03ms
step:1252/2330 train_time:72618ms step_avg:58.00ms
step:1253/2330 train_time:72676ms step_avg:58.00ms
step:1254/2330 train_time:72743ms step_avg:58.01ms
step:1255/2330 train_time:72799ms step_avg:58.01ms
step:1256/2330 train_time:72861ms step_avg:58.01ms
step:1257/2330 train_time:72917ms step_avg:58.01ms
step:1258/2330 train_time:72978ms step_avg:58.01ms
step:1259/2330 train_time:73033ms step_avg:58.01ms
step:1260/2330 train_time:73095ms step_avg:58.01ms
step:1261/2330 train_time:73150ms step_avg:58.01ms
step:1262/2330 train_time:73210ms step_avg:58.01ms
step:1263/2330 train_time:73266ms step_avg:58.01ms
step:1264/2330 train_time:73325ms step_avg:58.01ms
step:1265/2330 train_time:73381ms step_avg:58.01ms
step:1266/2330 train_time:73441ms step_avg:58.01ms
step:1267/2330 train_time:73498ms step_avg:58.01ms
step:1268/2330 train_time:73557ms step_avg:58.01ms
step:1269/2330 train_time:73615ms step_avg:58.01ms
step:1270/2330 train_time:73679ms step_avg:58.01ms
step:1271/2330 train_time:73737ms step_avg:58.01ms
step:1272/2330 train_time:73799ms step_avg:58.02ms
step:1273/2330 train_time:73856ms step_avg:58.02ms
step:1274/2330 train_time:73917ms step_avg:58.02ms
step:1275/2330 train_time:73973ms step_avg:58.02ms
step:1276/2330 train_time:74036ms step_avg:58.02ms
step:1277/2330 train_time:74092ms step_avg:58.02ms
step:1278/2330 train_time:74152ms step_avg:58.02ms
step:1279/2330 train_time:74207ms step_avg:58.02ms
step:1280/2330 train_time:74268ms step_avg:58.02ms
step:1281/2330 train_time:74324ms step_avg:58.02ms
step:1282/2330 train_time:74385ms step_avg:58.02ms
step:1283/2330 train_time:74442ms step_avg:58.02ms
step:1284/2330 train_time:74502ms step_avg:58.02ms
step:1285/2330 train_time:74558ms step_avg:58.02ms
step:1286/2330 train_time:74619ms step_avg:58.02ms
step:1287/2330 train_time:74676ms step_avg:58.02ms
step:1288/2330 train_time:74739ms step_avg:58.03ms
step:1289/2330 train_time:74795ms step_avg:58.03ms
step:1290/2330 train_time:74857ms step_avg:58.03ms
step:1291/2330 train_time:74913ms step_avg:58.03ms
step:1292/2330 train_time:74975ms step_avg:58.03ms
step:1293/2330 train_time:75032ms step_avg:58.03ms
step:1294/2330 train_time:75092ms step_avg:58.03ms
step:1295/2330 train_time:75148ms step_avg:58.03ms
step:1296/2330 train_time:75208ms step_avg:58.03ms
step:1297/2330 train_time:75265ms step_avg:58.03ms
step:1298/2330 train_time:75324ms step_avg:58.03ms
step:1299/2330 train_time:75381ms step_avg:58.03ms
step:1300/2330 train_time:75441ms step_avg:58.03ms
step:1301/2330 train_time:75498ms step_avg:58.03ms
step:1302/2330 train_time:75557ms step_avg:58.03ms
step:1303/2330 train_time:75614ms step_avg:58.03ms
step:1304/2330 train_time:75675ms step_avg:58.03ms
step:1305/2330 train_time:75732ms step_avg:58.03ms
step:1306/2330 train_time:75795ms step_avg:58.04ms
step:1307/2330 train_time:75853ms step_avg:58.04ms
step:1308/2330 train_time:75913ms step_avg:58.04ms
step:1309/2330 train_time:75970ms step_avg:58.04ms
step:1310/2330 train_time:76029ms step_avg:58.04ms
step:1311/2330 train_time:76086ms step_avg:58.04ms
step:1312/2330 train_time:76146ms step_avg:58.04ms
step:1313/2330 train_time:76203ms step_avg:58.04ms
step:1314/2330 train_time:76262ms step_avg:58.04ms
step:1315/2330 train_time:76318ms step_avg:58.04ms
step:1316/2330 train_time:76378ms step_avg:58.04ms
step:1317/2330 train_time:76435ms step_avg:58.04ms
step:1318/2330 train_time:76495ms step_avg:58.04ms
step:1319/2330 train_time:76552ms step_avg:58.04ms
step:1320/2330 train_time:76612ms step_avg:58.04ms
step:1321/2330 train_time:76670ms step_avg:58.04ms
step:1322/2330 train_time:76730ms step_avg:58.04ms
step:1323/2330 train_time:76788ms step_avg:58.04ms
step:1324/2330 train_time:76849ms step_avg:58.04ms
step:1325/2330 train_time:76906ms step_avg:58.04ms
step:1326/2330 train_time:76967ms step_avg:58.04ms
step:1327/2330 train_time:77024ms step_avg:58.04ms
step:1328/2330 train_time:77085ms step_avg:58.05ms
step:1329/2330 train_time:77141ms step_avg:58.04ms
step:1330/2330 train_time:77201ms step_avg:58.05ms
step:1331/2330 train_time:77257ms step_avg:58.04ms
step:1332/2330 train_time:77317ms step_avg:58.05ms
step:1333/2330 train_time:77374ms step_avg:58.05ms
step:1334/2330 train_time:77435ms step_avg:58.05ms
step:1335/2330 train_time:77492ms step_avg:58.05ms
step:1336/2330 train_time:77551ms step_avg:58.05ms
step:1337/2330 train_time:77609ms step_avg:58.05ms
step:1338/2330 train_time:77669ms step_avg:58.05ms
step:1339/2330 train_time:77726ms step_avg:58.05ms
step:1340/2330 train_time:77786ms step_avg:58.05ms
step:1341/2330 train_time:77843ms step_avg:58.05ms
step:1342/2330 train_time:77903ms step_avg:58.05ms
step:1343/2330 train_time:77961ms step_avg:58.05ms
step:1344/2330 train_time:78021ms step_avg:58.05ms
step:1345/2330 train_time:78077ms step_avg:58.05ms
step:1346/2330 train_time:78138ms step_avg:58.05ms
step:1347/2330 train_time:78194ms step_avg:58.05ms
step:1348/2330 train_time:78255ms step_avg:58.05ms
step:1349/2330 train_time:78312ms step_avg:58.05ms
step:1350/2330 train_time:78372ms step_avg:58.05ms
step:1351/2330 train_time:78428ms step_avg:58.05ms
step:1352/2330 train_time:78489ms step_avg:58.05ms
step:1353/2330 train_time:78547ms step_avg:58.05ms
step:1354/2330 train_time:78607ms step_avg:58.06ms
step:1355/2330 train_time:78664ms step_avg:58.05ms
step:1356/2330 train_time:78724ms step_avg:58.06ms
step:1357/2330 train_time:78782ms step_avg:58.06ms
step:1358/2330 train_time:78841ms step_avg:58.06ms
step:1359/2330 train_time:78899ms step_avg:58.06ms
step:1360/2330 train_time:78958ms step_avg:58.06ms
step:1361/2330 train_time:79015ms step_avg:58.06ms
step:1362/2330 train_time:79076ms step_avg:58.06ms
step:1363/2330 train_time:79133ms step_avg:58.06ms
step:1364/2330 train_time:79194ms step_avg:58.06ms
step:1365/2330 train_time:79251ms step_avg:58.06ms
step:1366/2330 train_time:79311ms step_avg:58.06ms
step:1367/2330 train_time:79368ms step_avg:58.06ms
step:1368/2330 train_time:79427ms step_avg:58.06ms
step:1369/2330 train_time:79484ms step_avg:58.06ms
step:1370/2330 train_time:79544ms step_avg:58.06ms
step:1371/2330 train_time:79601ms step_avg:58.06ms
step:1372/2330 train_time:79661ms step_avg:58.06ms
step:1373/2330 train_time:79717ms step_avg:58.06ms
step:1374/2330 train_time:79778ms step_avg:58.06ms
step:1375/2330 train_time:79835ms step_avg:58.06ms
step:1376/2330 train_time:79896ms step_avg:58.06ms
step:1377/2330 train_time:79953ms step_avg:58.06ms
step:1378/2330 train_time:80014ms step_avg:58.07ms
step:1379/2330 train_time:80071ms step_avg:58.06ms
step:1380/2330 train_time:80130ms step_avg:58.07ms
step:1381/2330 train_time:80188ms step_avg:58.07ms
step:1382/2330 train_time:80247ms step_avg:58.07ms
step:1383/2330 train_time:80304ms step_avg:58.07ms
step:1384/2330 train_time:80364ms step_avg:58.07ms
step:1385/2330 train_time:80421ms step_avg:58.07ms
step:1386/2330 train_time:80482ms step_avg:58.07ms
step:1387/2330 train_time:80539ms step_avg:58.07ms
step:1388/2330 train_time:80598ms step_avg:58.07ms
step:1389/2330 train_time:80655ms step_avg:58.07ms
step:1390/2330 train_time:80715ms step_avg:58.07ms
step:1391/2330 train_time:80772ms step_avg:58.07ms
step:1392/2330 train_time:80833ms step_avg:58.07ms
step:1393/2330 train_time:80890ms step_avg:58.07ms
step:1394/2330 train_time:80951ms step_avg:58.07ms
step:1395/2330 train_time:81007ms step_avg:58.07ms
step:1396/2330 train_time:81067ms step_avg:58.07ms
step:1397/2330 train_time:81124ms step_avg:58.07ms
step:1398/2330 train_time:81184ms step_avg:58.07ms
step:1399/2330 train_time:81242ms step_avg:58.07ms
step:1400/2330 train_time:81301ms step_avg:58.07ms
step:1401/2330 train_time:81357ms step_avg:58.07ms
step:1402/2330 train_time:81418ms step_avg:58.07ms
step:1403/2330 train_time:81474ms step_avg:58.07ms
step:1404/2330 train_time:81535ms step_avg:58.07ms
step:1405/2330 train_time:81592ms step_avg:58.07ms
step:1406/2330 train_time:81652ms step_avg:58.07ms
step:1407/2330 train_time:81710ms step_avg:58.07ms
step:1408/2330 train_time:81770ms step_avg:58.08ms
step:1409/2330 train_time:81827ms step_avg:58.07ms
step:1410/2330 train_time:81888ms step_avg:58.08ms
step:1411/2330 train_time:81945ms step_avg:58.08ms
step:1412/2330 train_time:82006ms step_avg:58.08ms
step:1413/2330 train_time:82063ms step_avg:58.08ms
step:1414/2330 train_time:82123ms step_avg:58.08ms
step:1415/2330 train_time:82180ms step_avg:58.08ms
step:1416/2330 train_time:82241ms step_avg:58.08ms
step:1417/2330 train_time:82298ms step_avg:58.08ms
step:1418/2330 train_time:82357ms step_avg:58.08ms
step:1419/2330 train_time:82414ms step_avg:58.08ms
step:1420/2330 train_time:82475ms step_avg:58.08ms
step:1421/2330 train_time:82532ms step_avg:58.08ms
step:1422/2330 train_time:82593ms step_avg:58.08ms
step:1423/2330 train_time:82651ms step_avg:58.08ms
step:1424/2330 train_time:82710ms step_avg:58.08ms
step:1425/2330 train_time:82767ms step_avg:58.08ms
step:1426/2330 train_time:82828ms step_avg:58.08ms
step:1427/2330 train_time:82884ms step_avg:58.08ms
step:1428/2330 train_time:82945ms step_avg:58.09ms
step:1429/2330 train_time:83003ms step_avg:58.08ms
step:1430/2330 train_time:83062ms step_avg:58.09ms
step:1431/2330 train_time:83119ms step_avg:58.08ms
step:1432/2330 train_time:83180ms step_avg:58.09ms
step:1433/2330 train_time:83237ms step_avg:58.09ms
step:1434/2330 train_time:83297ms step_avg:58.09ms
step:1435/2330 train_time:83354ms step_avg:58.09ms
step:1436/2330 train_time:83414ms step_avg:58.09ms
step:1437/2330 train_time:83472ms step_avg:58.09ms
step:1438/2330 train_time:83532ms step_avg:58.09ms
step:1439/2330 train_time:83589ms step_avg:58.09ms
step:1440/2330 train_time:83649ms step_avg:58.09ms
step:1441/2330 train_time:83707ms step_avg:58.09ms
step:1442/2330 train_time:83767ms step_avg:58.09ms
step:1443/2330 train_time:83824ms step_avg:58.09ms
step:1444/2330 train_time:83884ms step_avg:58.09ms
step:1445/2330 train_time:83941ms step_avg:58.09ms
step:1446/2330 train_time:84001ms step_avg:58.09ms
step:1447/2330 train_time:84059ms step_avg:58.09ms
step:1448/2330 train_time:84118ms step_avg:58.09ms
step:1449/2330 train_time:84175ms step_avg:58.09ms
step:1450/2330 train_time:84235ms step_avg:58.09ms
step:1451/2330 train_time:84292ms step_avg:58.09ms
step:1452/2330 train_time:84352ms step_avg:58.09ms
step:1453/2330 train_time:84410ms step_avg:58.09ms
step:1454/2330 train_time:84470ms step_avg:58.09ms
step:1455/2330 train_time:84527ms step_avg:58.09ms
step:1456/2330 train_time:84587ms step_avg:58.10ms
step:1457/2330 train_time:84644ms step_avg:58.09ms
step:1458/2330 train_time:84704ms step_avg:58.10ms
step:1459/2330 train_time:84761ms step_avg:58.10ms
step:1460/2330 train_time:84820ms step_avg:58.10ms
step:1461/2330 train_time:84877ms step_avg:58.10ms
step:1462/2330 train_time:84938ms step_avg:58.10ms
step:1463/2330 train_time:84995ms step_avg:58.10ms
step:1464/2330 train_time:85056ms step_avg:58.10ms
step:1465/2330 train_time:85113ms step_avg:58.10ms
step:1466/2330 train_time:85173ms step_avg:58.10ms
step:1467/2330 train_time:85230ms step_avg:58.10ms
step:1468/2330 train_time:85290ms step_avg:58.10ms
step:1469/2330 train_time:85347ms step_avg:58.10ms
step:1470/2330 train_time:85407ms step_avg:58.10ms
step:1471/2330 train_time:85464ms step_avg:58.10ms
step:1472/2330 train_time:85524ms step_avg:58.10ms
step:1473/2330 train_time:85581ms step_avg:58.10ms
step:1474/2330 train_time:85641ms step_avg:58.10ms
step:1475/2330 train_time:85697ms step_avg:58.10ms
step:1476/2330 train_time:85757ms step_avg:58.10ms
step:1477/2330 train_time:85814ms step_avg:58.10ms
step:1478/2330 train_time:85875ms step_avg:58.10ms
step:1479/2330 train_time:85933ms step_avg:58.10ms
step:1480/2330 train_time:85995ms step_avg:58.10ms
step:1481/2330 train_time:86051ms step_avg:58.10ms
step:1482/2330 train_time:86112ms step_avg:58.11ms
step:1483/2330 train_time:86169ms step_avg:58.10ms
step:1484/2330 train_time:86229ms step_avg:58.11ms
step:1485/2330 train_time:86286ms step_avg:58.10ms
step:1486/2330 train_time:86346ms step_avg:58.11ms
step:1487/2330 train_time:86403ms step_avg:58.11ms
step:1488/2330 train_time:86462ms step_avg:58.11ms
step:1489/2330 train_time:86519ms step_avg:58.11ms
step:1490/2330 train_time:86580ms step_avg:58.11ms
step:1491/2330 train_time:86637ms step_avg:58.11ms
step:1492/2330 train_time:86697ms step_avg:58.11ms
step:1493/2330 train_time:86754ms step_avg:58.11ms
step:1494/2330 train_time:86814ms step_avg:58.11ms
step:1495/2330 train_time:86872ms step_avg:58.11ms
step:1496/2330 train_time:86932ms step_avg:58.11ms
step:1497/2330 train_time:86990ms step_avg:58.11ms
step:1498/2330 train_time:87050ms step_avg:58.11ms
step:1499/2330 train_time:87107ms step_avg:58.11ms
step:1500/2330 train_time:87168ms step_avg:58.11ms
step:1500/2330 val_loss:3.9041 train_time:87248ms step_avg:58.17ms
step:1501/2330 train_time:87266ms step_avg:58.14ms
step:1502/2330 train_time:87286ms step_avg:58.11ms
step:1503/2330 train_time:87344ms step_avg:58.11ms
step:1504/2330 train_time:87411ms step_avg:58.12ms
step:1505/2330 train_time:87468ms step_avg:58.12ms
step:1506/2330 train_time:87529ms step_avg:58.12ms
step:1507/2330 train_time:87586ms step_avg:58.12ms
step:1508/2330 train_time:87645ms step_avg:58.12ms
step:1509/2330 train_time:87702ms step_avg:58.12ms
step:1510/2330 train_time:87762ms step_avg:58.12ms
step:1511/2330 train_time:87818ms step_avg:58.12ms
step:1512/2330 train_time:87877ms step_avg:58.12ms
step:1513/2330 train_time:87933ms step_avg:58.12ms
step:1514/2330 train_time:87992ms step_avg:58.12ms
step:1515/2330 train_time:88049ms step_avg:58.12ms
step:1516/2330 train_time:88108ms step_avg:58.12ms
step:1517/2330 train_time:88165ms step_avg:58.12ms
step:1518/2330 train_time:88226ms step_avg:58.12ms
step:1519/2330 train_time:88284ms step_avg:58.12ms
step:1520/2330 train_time:88345ms step_avg:58.12ms
step:1521/2330 train_time:88403ms step_avg:58.12ms
step:1522/2330 train_time:88467ms step_avg:58.13ms
step:1523/2330 train_time:88523ms step_avg:58.12ms
step:1524/2330 train_time:88584ms step_avg:58.13ms
step:1525/2330 train_time:88641ms step_avg:58.12ms
step:1526/2330 train_time:88701ms step_avg:58.13ms
step:1527/2330 train_time:88757ms step_avg:58.13ms
step:1528/2330 train_time:88819ms step_avg:58.13ms
step:1529/2330 train_time:88877ms step_avg:58.13ms
step:1530/2330 train_time:88935ms step_avg:58.13ms
step:1531/2330 train_time:88991ms step_avg:58.13ms
step:1532/2330 train_time:89051ms step_avg:58.13ms
step:1533/2330 train_time:89108ms step_avg:58.13ms
step:1534/2330 train_time:89168ms step_avg:58.13ms
step:1535/2330 train_time:89226ms step_avg:58.13ms
step:1536/2330 train_time:89287ms step_avg:58.13ms
step:1537/2330 train_time:89345ms step_avg:58.13ms
step:1538/2330 train_time:89407ms step_avg:58.13ms
step:1539/2330 train_time:89464ms step_avg:58.13ms
step:1540/2330 train_time:89527ms step_avg:58.13ms
step:1541/2330 train_time:89584ms step_avg:58.13ms
step:1542/2330 train_time:89644ms step_avg:58.14ms
step:1543/2330 train_time:89701ms step_avg:58.13ms
step:1544/2330 train_time:89764ms step_avg:58.14ms
step:1545/2330 train_time:89821ms step_avg:58.14ms
step:1546/2330 train_time:89883ms step_avg:58.14ms
step:1547/2330 train_time:89940ms step_avg:58.14ms
step:1548/2330 train_time:90001ms step_avg:58.14ms
step:1549/2330 train_time:90058ms step_avg:58.14ms
step:1550/2330 train_time:90120ms step_avg:58.14ms
step:1551/2330 train_time:90177ms step_avg:58.14ms
step:1552/2330 train_time:90238ms step_avg:58.14ms
step:1553/2330 train_time:90297ms step_avg:58.14ms
step:1554/2330 train_time:90358ms step_avg:58.15ms
step:1555/2330 train_time:90417ms step_avg:58.15ms
step:1556/2330 train_time:90478ms step_avg:58.15ms
step:1557/2330 train_time:90537ms step_avg:58.15ms
step:1558/2330 train_time:90597ms step_avg:58.15ms
step:1559/2330 train_time:90656ms step_avg:58.15ms
step:1560/2330 train_time:90716ms step_avg:58.15ms
step:1561/2330 train_time:90774ms step_avg:58.15ms
step:1562/2330 train_time:90834ms step_avg:58.15ms
step:1563/2330 train_time:90892ms step_avg:58.15ms
step:1564/2330 train_time:90952ms step_avg:58.15ms
step:1565/2330 train_time:91009ms step_avg:58.15ms
step:1566/2330 train_time:91069ms step_avg:58.15ms
step:1567/2330 train_time:91126ms step_avg:58.15ms
step:1568/2330 train_time:91187ms step_avg:58.15ms
step:1569/2330 train_time:91244ms step_avg:58.15ms
step:1570/2330 train_time:91307ms step_avg:58.16ms
step:1571/2330 train_time:91364ms step_avg:58.16ms
step:1572/2330 train_time:91427ms step_avg:58.16ms
step:1573/2330 train_time:91485ms step_avg:58.16ms
step:1574/2330 train_time:91545ms step_avg:58.16ms
step:1575/2330 train_time:91603ms step_avg:58.16ms
step:1576/2330 train_time:91664ms step_avg:58.16ms
step:1577/2330 train_time:91721ms step_avg:58.16ms
step:1578/2330 train_time:91783ms step_avg:58.16ms
step:1579/2330 train_time:91839ms step_avg:58.16ms
step:1580/2330 train_time:91900ms step_avg:58.16ms
step:1581/2330 train_time:91956ms step_avg:58.16ms
step:1582/2330 train_time:92019ms step_avg:58.17ms
step:1583/2330 train_time:92076ms step_avg:58.17ms
step:1584/2330 train_time:92137ms step_avg:58.17ms
step:1585/2330 train_time:92195ms step_avg:58.17ms
step:1586/2330 train_time:92255ms step_avg:58.17ms
step:1587/2330 train_time:92313ms step_avg:58.17ms
step:1588/2330 train_time:92374ms step_avg:58.17ms
step:1589/2330 train_time:92433ms step_avg:58.17ms
step:1590/2330 train_time:92494ms step_avg:58.17ms
step:1591/2330 train_time:92552ms step_avg:58.17ms
step:1592/2330 train_time:92613ms step_avg:58.17ms
step:1593/2330 train_time:92671ms step_avg:58.17ms
step:1594/2330 train_time:92732ms step_avg:58.18ms
step:1595/2330 train_time:92790ms step_avg:58.18ms
step:1596/2330 train_time:92850ms step_avg:58.18ms
step:1597/2330 train_time:92907ms step_avg:58.18ms
step:1598/2330 train_time:92968ms step_avg:58.18ms
step:1599/2330 train_time:93025ms step_avg:58.18ms
step:1600/2330 train_time:93086ms step_avg:58.18ms
step:1601/2330 train_time:93143ms step_avg:58.18ms
step:1602/2330 train_time:93204ms step_avg:58.18ms
step:1603/2330 train_time:93261ms step_avg:58.18ms
step:1604/2330 train_time:93323ms step_avg:58.18ms
step:1605/2330 train_time:93380ms step_avg:58.18ms
step:1606/2330 train_time:93442ms step_avg:58.18ms
step:1607/2330 train_time:93499ms step_avg:58.18ms
step:1608/2330 train_time:93561ms step_avg:58.18ms
step:1609/2330 train_time:93618ms step_avg:58.18ms
step:1610/2330 train_time:93680ms step_avg:58.19ms
step:1611/2330 train_time:93737ms step_avg:58.19ms
step:1612/2330 train_time:93798ms step_avg:58.19ms
step:1613/2330 train_time:93856ms step_avg:58.19ms
step:1614/2330 train_time:93917ms step_avg:58.19ms
step:1615/2330 train_time:93975ms step_avg:58.19ms
step:1616/2330 train_time:94035ms step_avg:58.19ms
step:1617/2330 train_time:94093ms step_avg:58.19ms
step:1618/2330 train_time:94153ms step_avg:58.19ms
step:1619/2330 train_time:94210ms step_avg:58.19ms
step:1620/2330 train_time:94270ms step_avg:58.19ms
step:1621/2330 train_time:94329ms step_avg:58.19ms
step:1622/2330 train_time:94388ms step_avg:58.19ms
step:1623/2330 train_time:94445ms step_avg:58.19ms
step:1624/2330 train_time:94506ms step_avg:58.19ms
step:1625/2330 train_time:94563ms step_avg:58.19ms
step:1626/2330 train_time:94624ms step_avg:58.19ms
step:1627/2330 train_time:94682ms step_avg:58.19ms
step:1628/2330 train_time:94743ms step_avg:58.20ms
step:1629/2330 train_time:94800ms step_avg:58.20ms
step:1630/2330 train_time:94862ms step_avg:58.20ms
step:1631/2330 train_time:94918ms step_avg:58.20ms
step:1632/2330 train_time:94982ms step_avg:58.20ms
step:1633/2330 train_time:95038ms step_avg:58.20ms
step:1634/2330 train_time:95101ms step_avg:58.20ms
step:1635/2330 train_time:95158ms step_avg:58.20ms
step:1636/2330 train_time:95220ms step_avg:58.20ms
step:1637/2330 train_time:95278ms step_avg:58.20ms
step:1638/2330 train_time:95339ms step_avg:58.20ms
step:1639/2330 train_time:95397ms step_avg:58.20ms
step:1640/2330 train_time:95457ms step_avg:58.21ms
step:1641/2330 train_time:95515ms step_avg:58.21ms
step:1642/2330 train_time:95576ms step_avg:58.21ms
step:1643/2330 train_time:95634ms step_avg:58.21ms
step:1644/2330 train_time:95694ms step_avg:58.21ms
step:1645/2330 train_time:95752ms step_avg:58.21ms
step:1646/2330 train_time:95814ms step_avg:58.21ms
step:1647/2330 train_time:95873ms step_avg:58.21ms
step:1648/2330 train_time:95933ms step_avg:58.21ms
step:1649/2330 train_time:95991ms step_avg:58.21ms
step:1650/2330 train_time:96051ms step_avg:58.21ms
step:1651/2330 train_time:96108ms step_avg:58.21ms
step:1652/2330 train_time:96168ms step_avg:58.21ms
step:1653/2330 train_time:96226ms step_avg:58.21ms
step:1654/2330 train_time:96287ms step_avg:58.21ms
step:1655/2330 train_time:96344ms step_avg:58.21ms
step:1656/2330 train_time:96406ms step_avg:58.22ms
step:1657/2330 train_time:96463ms step_avg:58.22ms
step:1658/2330 train_time:96526ms step_avg:58.22ms
step:1659/2330 train_time:96583ms step_avg:58.22ms
step:1660/2330 train_time:96644ms step_avg:58.22ms
step:1661/2330 train_time:96700ms step_avg:58.22ms
step:1662/2330 train_time:96764ms step_avg:58.22ms
step:1663/2330 train_time:96821ms step_avg:58.22ms
step:1664/2330 train_time:96883ms step_avg:58.22ms
step:1665/2330 train_time:96940ms step_avg:58.22ms
step:1666/2330 train_time:97001ms step_avg:58.22ms
step:1667/2330 train_time:97058ms step_avg:58.22ms
step:1668/2330 train_time:97119ms step_avg:58.22ms
step:1669/2330 train_time:97177ms step_avg:58.22ms
step:1670/2330 train_time:97237ms step_avg:58.23ms
step:1671/2330 train_time:97295ms step_avg:58.23ms
step:1672/2330 train_time:97356ms step_avg:58.23ms
step:1673/2330 train_time:97415ms step_avg:58.23ms
step:1674/2330 train_time:97475ms step_avg:58.23ms
step:1675/2330 train_time:97534ms step_avg:58.23ms
step:1676/2330 train_time:97594ms step_avg:58.23ms
step:1677/2330 train_time:97651ms step_avg:58.23ms
step:1678/2330 train_time:97711ms step_avg:58.23ms
step:1679/2330 train_time:97769ms step_avg:58.23ms
step:1680/2330 train_time:97828ms step_avg:58.23ms
step:1681/2330 train_time:97886ms step_avg:58.23ms
step:1682/2330 train_time:97945ms step_avg:58.23ms
step:1683/2330 train_time:98002ms step_avg:58.23ms
step:1684/2330 train_time:98064ms step_avg:58.23ms
step:1685/2330 train_time:98122ms step_avg:58.23ms
step:1686/2330 train_time:98181ms step_avg:58.23ms
step:1687/2330 train_time:98238ms step_avg:58.23ms
step:1688/2330 train_time:98301ms step_avg:58.24ms
step:1689/2330 train_time:98358ms step_avg:58.23ms
step:1690/2330 train_time:98420ms step_avg:58.24ms
step:1691/2330 train_time:98478ms step_avg:58.24ms
step:1692/2330 train_time:98539ms step_avg:58.24ms
step:1693/2330 train_time:98597ms step_avg:58.24ms
step:1694/2330 train_time:98658ms step_avg:58.24ms
step:1695/2330 train_time:98715ms step_avg:58.24ms
step:1696/2330 train_time:98777ms step_avg:58.24ms
step:1697/2330 train_time:98835ms step_avg:58.24ms
step:1698/2330 train_time:98896ms step_avg:58.24ms
step:1699/2330 train_time:98953ms step_avg:58.24ms
step:1700/2330 train_time:99014ms step_avg:58.24ms
step:1701/2330 train_time:99071ms step_avg:58.24ms
step:1702/2330 train_time:99131ms step_avg:58.24ms
step:1703/2330 train_time:99189ms step_avg:58.24ms
step:1704/2330 train_time:99248ms step_avg:58.24ms
step:1705/2330 train_time:99304ms step_avg:58.24ms
step:1706/2330 train_time:99367ms step_avg:58.25ms
step:1707/2330 train_time:99425ms step_avg:58.25ms
step:1708/2330 train_time:99487ms step_avg:58.25ms
step:1709/2330 train_time:99544ms step_avg:58.25ms
step:1710/2330 train_time:99606ms step_avg:58.25ms
step:1711/2330 train_time:99663ms step_avg:58.25ms
step:1712/2330 train_time:99725ms step_avg:58.25ms
step:1713/2330 train_time:99781ms step_avg:58.25ms
step:1714/2330 train_time:99843ms step_avg:58.25ms
step:1715/2330 train_time:99900ms step_avg:58.25ms
step:1716/2330 train_time:99962ms step_avg:58.25ms
step:1717/2330 train_time:100019ms step_avg:58.25ms
step:1718/2330 train_time:100081ms step_avg:58.25ms
step:1719/2330 train_time:100138ms step_avg:58.25ms
step:1720/2330 train_time:100199ms step_avg:58.26ms
step:1721/2330 train_time:100256ms step_avg:58.25ms
step:1722/2330 train_time:100318ms step_avg:58.26ms
step:1723/2330 train_time:100376ms step_avg:58.26ms
step:1724/2330 train_time:100438ms step_avg:58.26ms
step:1725/2330 train_time:100496ms step_avg:58.26ms
step:1726/2330 train_time:100557ms step_avg:58.26ms
step:1727/2330 train_time:100615ms step_avg:58.26ms
step:1728/2330 train_time:100675ms step_avg:58.26ms
step:1729/2330 train_time:100733ms step_avg:58.26ms
step:1730/2330 train_time:100793ms step_avg:58.26ms
step:1731/2330 train_time:100851ms step_avg:58.26ms
step:1732/2330 train_time:100911ms step_avg:58.26ms
step:1733/2330 train_time:100968ms step_avg:58.26ms
step:1734/2330 train_time:101028ms step_avg:58.26ms
step:1735/2330 train_time:101086ms step_avg:58.26ms
step:1736/2330 train_time:101146ms step_avg:58.26ms
step:1737/2330 train_time:101202ms step_avg:58.26ms
step:1738/2330 train_time:101264ms step_avg:58.26ms
step:1739/2330 train_time:101322ms step_avg:58.26ms
step:1740/2330 train_time:101384ms step_avg:58.27ms
step:1741/2330 train_time:101440ms step_avg:58.27ms
step:1742/2330 train_time:101503ms step_avg:58.27ms
step:1743/2330 train_time:101560ms step_avg:58.27ms
step:1744/2330 train_time:101622ms step_avg:58.27ms
step:1745/2330 train_time:101679ms step_avg:58.27ms
step:1746/2330 train_time:101740ms step_avg:58.27ms
step:1747/2330 train_time:101797ms step_avg:58.27ms
step:1748/2330 train_time:101858ms step_avg:58.27ms
step:1749/2330 train_time:101916ms step_avg:58.27ms
step:1750/2330 train_time:101977ms step_avg:58.27ms
step:1750/2330 val_loss:3.8200 train_time:102059ms step_avg:58.32ms
step:1751/2330 train_time:102077ms step_avg:58.30ms
step:1752/2330 train_time:102097ms step_avg:58.27ms
step:1753/2330 train_time:102152ms step_avg:58.27ms
step:1754/2330 train_time:102223ms step_avg:58.28ms
step:1755/2330 train_time:102280ms step_avg:58.28ms
step:1756/2330 train_time:102343ms step_avg:58.28ms
step:1757/2330 train_time:102398ms step_avg:58.28ms
step:1758/2330 train_time:102459ms step_avg:58.28ms
step:1759/2330 train_time:102515ms step_avg:58.28ms
step:1760/2330 train_time:102575ms step_avg:58.28ms
step:1761/2330 train_time:102631ms step_avg:58.28ms
step:1762/2330 train_time:102692ms step_avg:58.28ms
step:1763/2330 train_time:102749ms step_avg:58.28ms
step:1764/2330 train_time:102808ms step_avg:58.28ms
step:1765/2330 train_time:102864ms step_avg:58.28ms
step:1766/2330 train_time:102925ms step_avg:58.28ms
step:1767/2330 train_time:102986ms step_avg:58.28ms
step:1768/2330 train_time:103048ms step_avg:58.28ms
step:1769/2330 train_time:103107ms step_avg:58.29ms
step:1770/2330 train_time:103168ms step_avg:58.29ms
step:1771/2330 train_time:103227ms step_avg:58.29ms
step:1772/2330 train_time:103287ms step_avg:58.29ms
step:1773/2330 train_time:103344ms step_avg:58.29ms
step:1774/2330 train_time:103406ms step_avg:58.29ms
step:1775/2330 train_time:103463ms step_avg:58.29ms
step:1776/2330 train_time:103523ms step_avg:58.29ms
step:1777/2330 train_time:103579ms step_avg:58.29ms
step:1778/2330 train_time:103641ms step_avg:58.29ms
step:1779/2330 train_time:103697ms step_avg:58.29ms
step:1780/2330 train_time:103757ms step_avg:58.29ms
step:1781/2330 train_time:103814ms step_avg:58.29ms
step:1782/2330 train_time:103874ms step_avg:58.29ms
step:1783/2330 train_time:103931ms step_avg:58.29ms
step:1784/2330 train_time:103994ms step_avg:58.29ms
step:1785/2330 train_time:104051ms step_avg:58.29ms
step:1786/2330 train_time:104115ms step_avg:58.30ms
step:1787/2330 train_time:104172ms step_avg:58.29ms
step:1788/2330 train_time:104236ms step_avg:58.30ms
step:1789/2330 train_time:104293ms step_avg:58.30ms
step:1790/2330 train_time:104354ms step_avg:58.30ms
step:1791/2330 train_time:104411ms step_avg:58.30ms
step:1792/2330 train_time:104472ms step_avg:58.30ms
step:1793/2330 train_time:104529ms step_avg:58.30ms
step:1794/2330 train_time:104590ms step_avg:58.30ms
step:1795/2330 train_time:104648ms step_avg:58.30ms
step:1796/2330 train_time:104707ms step_avg:58.30ms
step:1797/2330 train_time:104764ms step_avg:58.30ms
step:1798/2330 train_time:104824ms step_avg:58.30ms
step:1799/2330 train_time:104881ms step_avg:58.30ms
step:1800/2330 train_time:104942ms step_avg:58.30ms
step:1801/2330 train_time:104999ms step_avg:58.30ms
step:1802/2330 train_time:105060ms step_avg:58.30ms
step:1803/2330 train_time:105119ms step_avg:58.30ms
step:1804/2330 train_time:105179ms step_avg:58.30ms
step:1805/2330 train_time:105237ms step_avg:58.30ms
step:1806/2330 train_time:105299ms step_avg:58.31ms
step:1807/2330 train_time:105356ms step_avg:58.30ms
step:1808/2330 train_time:105417ms step_avg:58.31ms
step:1809/2330 train_time:105474ms step_avg:58.30ms
step:1810/2330 train_time:105536ms step_avg:58.31ms
step:1811/2330 train_time:105593ms step_avg:58.31ms
step:1812/2330 train_time:105654ms step_avg:58.31ms
step:1813/2330 train_time:105710ms step_avg:58.31ms
step:1814/2330 train_time:105771ms step_avg:58.31ms
step:1815/2330 train_time:105829ms step_avg:58.31ms
step:1816/2330 train_time:105890ms step_avg:58.31ms
step:1817/2330 train_time:105947ms step_avg:58.31ms
step:1818/2330 train_time:106008ms step_avg:58.31ms
step:1819/2330 train_time:106066ms step_avg:58.31ms
step:1820/2330 train_time:106127ms step_avg:58.31ms
step:1821/2330 train_time:106187ms step_avg:58.31ms
step:1822/2330 train_time:106248ms step_avg:58.31ms
step:1823/2330 train_time:106307ms step_avg:58.31ms
step:1824/2330 train_time:106367ms step_avg:58.32ms
step:1825/2330 train_time:106425ms step_avg:58.31ms
step:1826/2330 train_time:106485ms step_avg:58.32ms
step:1827/2330 train_time:106542ms step_avg:58.32ms
step:1828/2330 train_time:106602ms step_avg:58.32ms
step:1829/2330 train_time:106658ms step_avg:58.32ms
step:1830/2330 train_time:106719ms step_avg:58.32ms
step:1831/2330 train_time:106776ms step_avg:58.32ms
step:1832/2330 train_time:106838ms step_avg:58.32ms
step:1833/2330 train_time:106894ms step_avg:58.32ms
step:1834/2330 train_time:106956ms step_avg:58.32ms
step:1835/2330 train_time:107012ms step_avg:58.32ms
step:1836/2330 train_time:107074ms step_avg:58.32ms
step:1837/2330 train_time:107132ms step_avg:58.32ms
step:1838/2330 train_time:107193ms step_avg:58.32ms
step:1839/2330 train_time:107250ms step_avg:58.32ms
step:1840/2330 train_time:107313ms step_avg:58.32ms
step:1841/2330 train_time:107370ms step_avg:58.32ms
step:1842/2330 train_time:107431ms step_avg:58.32ms
step:1843/2330 train_time:107489ms step_avg:58.32ms
step:1844/2330 train_time:107550ms step_avg:58.32ms
step:1845/2330 train_time:107607ms step_avg:58.32ms
step:1846/2330 train_time:107668ms step_avg:58.32ms
step:1847/2330 train_time:107726ms step_avg:58.33ms
step:1848/2330 train_time:107787ms step_avg:58.33ms
step:1849/2330 train_time:107844ms step_avg:58.33ms
step:1850/2330 train_time:107905ms step_avg:58.33ms
step:1851/2330 train_time:107963ms step_avg:58.33ms
step:1852/2330 train_time:108023ms step_avg:58.33ms
step:1853/2330 train_time:108080ms step_avg:58.33ms
step:1854/2330 train_time:108141ms step_avg:58.33ms
step:1855/2330 train_time:108198ms step_avg:58.33ms
step:1856/2330 train_time:108259ms step_avg:58.33ms
step:1857/2330 train_time:108317ms step_avg:58.33ms
step:1858/2330 train_time:108379ms step_avg:58.33ms
step:1859/2330 train_time:108435ms step_avg:58.33ms
step:1860/2330 train_time:108497ms step_avg:58.33ms
step:1861/2330 train_time:108554ms step_avg:58.33ms
step:1862/2330 train_time:108615ms step_avg:58.33ms
step:1863/2330 train_time:108672ms step_avg:58.33ms
step:1864/2330 train_time:108734ms step_avg:58.33ms
step:1865/2330 train_time:108790ms step_avg:58.33ms
step:1866/2330 train_time:108852ms step_avg:58.33ms
step:1867/2330 train_time:108908ms step_avg:58.33ms
step:1868/2330 train_time:108971ms step_avg:58.34ms
step:1869/2330 train_time:109029ms step_avg:58.34ms
step:1870/2330 train_time:109090ms step_avg:58.34ms
step:1871/2330 train_time:109147ms step_avg:58.34ms
step:1872/2330 train_time:109209ms step_avg:58.34ms
step:1873/2330 train_time:109268ms step_avg:58.34ms
step:1874/2330 train_time:109328ms step_avg:58.34ms
step:1875/2330 train_time:109387ms step_avg:58.34ms
step:1876/2330 train_time:109447ms step_avg:58.34ms
step:1877/2330 train_time:109505ms step_avg:58.34ms
step:1878/2330 train_time:109565ms step_avg:58.34ms
step:1879/2330 train_time:109623ms step_avg:58.34ms
step:1880/2330 train_time:109683ms step_avg:58.34ms
step:1881/2330 train_time:109741ms step_avg:58.34ms
step:1882/2330 train_time:109801ms step_avg:58.34ms
step:1883/2330 train_time:109857ms step_avg:58.34ms
step:1884/2330 train_time:109920ms step_avg:58.34ms
step:1885/2330 train_time:109977ms step_avg:58.34ms
step:1886/2330 train_time:110037ms step_avg:58.34ms
step:1887/2330 train_time:110094ms step_avg:58.34ms
step:1888/2330 train_time:110156ms step_avg:58.35ms
step:1889/2330 train_time:110213ms step_avg:58.34ms
step:1890/2330 train_time:110277ms step_avg:58.35ms
step:1891/2330 train_time:110334ms step_avg:58.35ms
step:1892/2330 train_time:110396ms step_avg:58.35ms
step:1893/2330 train_time:110452ms step_avg:58.35ms
step:1894/2330 train_time:110514ms step_avg:58.35ms
step:1895/2330 train_time:110571ms step_avg:58.35ms
step:1896/2330 train_time:110633ms step_avg:58.35ms
step:1897/2330 train_time:110690ms step_avg:58.35ms
step:1898/2330 train_time:110752ms step_avg:58.35ms
step:1899/2330 train_time:110810ms step_avg:58.35ms
step:1900/2330 train_time:110870ms step_avg:58.35ms
step:1901/2330 train_time:110929ms step_avg:58.35ms
step:1902/2330 train_time:110989ms step_avg:58.35ms
step:1903/2330 train_time:111046ms step_avg:58.35ms
step:1904/2330 train_time:111108ms step_avg:58.35ms
step:1905/2330 train_time:111165ms step_avg:58.35ms
step:1906/2330 train_time:111226ms step_avg:58.36ms
step:1907/2330 train_time:111285ms step_avg:58.36ms
step:1908/2330 train_time:111345ms step_avg:58.36ms
step:1909/2330 train_time:111403ms step_avg:58.36ms
step:1910/2330 train_time:111463ms step_avg:58.36ms
step:1911/2330 train_time:111520ms step_avg:58.36ms
step:1912/2330 train_time:111580ms step_avg:58.36ms
step:1913/2330 train_time:111637ms step_avg:58.36ms
step:1914/2330 train_time:111699ms step_avg:58.36ms
step:1915/2330 train_time:111756ms step_avg:58.36ms
step:1916/2330 train_time:111818ms step_avg:58.36ms
step:1917/2330 train_time:111875ms step_avg:58.36ms
step:1918/2330 train_time:111937ms step_avg:58.36ms
step:1919/2330 train_time:111994ms step_avg:58.36ms
step:1920/2330 train_time:112055ms step_avg:58.36ms
step:1921/2330 train_time:112112ms step_avg:58.36ms
step:1922/2330 train_time:112175ms step_avg:58.36ms
step:1923/2330 train_time:112231ms step_avg:58.36ms
step:1924/2330 train_time:112294ms step_avg:58.36ms
step:1925/2330 train_time:112351ms step_avg:58.36ms
step:1926/2330 train_time:112412ms step_avg:58.37ms
step:1927/2330 train_time:112470ms step_avg:58.37ms
step:1928/2330 train_time:112532ms step_avg:58.37ms
step:1929/2330 train_time:112590ms step_avg:58.37ms
step:1930/2330 train_time:112652ms step_avg:58.37ms
step:1931/2330 train_time:112709ms step_avg:58.37ms
step:1932/2330 train_time:112770ms step_avg:58.37ms
step:1933/2330 train_time:112829ms step_avg:58.37ms
step:1934/2330 train_time:112889ms step_avg:58.37ms
step:1935/2330 train_time:112946ms step_avg:58.37ms
step:1936/2330 train_time:113007ms step_avg:58.37ms
step:1937/2330 train_time:113064ms step_avg:58.37ms
step:1938/2330 train_time:113125ms step_avg:58.37ms
step:1939/2330 train_time:113183ms step_avg:58.37ms
step:1940/2330 train_time:113243ms step_avg:58.37ms
step:1941/2330 train_time:113301ms step_avg:58.37ms
step:1942/2330 train_time:113361ms step_avg:58.37ms
step:1943/2330 train_time:113418ms step_avg:58.37ms
step:1944/2330 train_time:113481ms step_avg:58.37ms
step:1945/2330 train_time:113538ms step_avg:58.37ms
step:1946/2330 train_time:113600ms step_avg:58.38ms
step:1947/2330 train_time:113656ms step_avg:58.37ms
step:1948/2330 train_time:113719ms step_avg:58.38ms
step:1949/2330 train_time:113776ms step_avg:58.38ms
step:1950/2330 train_time:113838ms step_avg:58.38ms
step:1951/2330 train_time:113895ms step_avg:58.38ms
step:1952/2330 train_time:113956ms step_avg:58.38ms
step:1953/2330 train_time:114013ms step_avg:58.38ms
step:1954/2330 train_time:114074ms step_avg:58.38ms
step:1955/2330 train_time:114131ms step_avg:58.38ms
step:1956/2330 train_time:114194ms step_avg:58.38ms
step:1957/2330 train_time:114250ms step_avg:58.38ms
step:1958/2330 train_time:114312ms step_avg:58.38ms
step:1959/2330 train_time:114370ms step_avg:58.38ms
step:1960/2330 train_time:114432ms step_avg:58.38ms
step:1961/2330 train_time:114490ms step_avg:58.38ms
step:1962/2330 train_time:114551ms step_avg:58.38ms
step:1963/2330 train_time:114609ms step_avg:58.38ms
step:1964/2330 train_time:114669ms step_avg:58.39ms
step:1965/2330 train_time:114727ms step_avg:58.39ms
step:1966/2330 train_time:114788ms step_avg:58.39ms
step:1967/2330 train_time:114846ms step_avg:58.39ms
step:1968/2330 train_time:114906ms step_avg:58.39ms
step:1969/2330 train_time:114963ms step_avg:58.39ms
step:1970/2330 train_time:115023ms step_avg:58.39ms
step:1971/2330 train_time:115081ms step_avg:58.39ms
step:1972/2330 train_time:115141ms step_avg:58.39ms
step:1973/2330 train_time:115198ms step_avg:58.39ms
step:1974/2330 train_time:115259ms step_avg:58.39ms
step:1975/2330 train_time:115317ms step_avg:58.39ms
step:1976/2330 train_time:115379ms step_avg:58.39ms
step:1977/2330 train_time:115436ms step_avg:58.39ms
step:1978/2330 train_time:115498ms step_avg:58.39ms
step:1979/2330 train_time:115555ms step_avg:58.39ms
step:1980/2330 train_time:115617ms step_avg:58.39ms
step:1981/2330 train_time:115674ms step_avg:58.39ms
step:1982/2330 train_time:115736ms step_avg:58.39ms
step:1983/2330 train_time:115794ms step_avg:58.39ms
step:1984/2330 train_time:115855ms step_avg:58.39ms
step:1985/2330 train_time:115911ms step_avg:58.39ms
step:1986/2330 train_time:115973ms step_avg:58.40ms
step:1987/2330 train_time:116030ms step_avg:58.39ms
step:1988/2330 train_time:116091ms step_avg:58.40ms
step:1989/2330 train_time:116149ms step_avg:58.40ms
step:1990/2330 train_time:116209ms step_avg:58.40ms
step:1991/2330 train_time:116266ms step_avg:58.40ms
step:1992/2330 train_time:116328ms step_avg:58.40ms
step:1993/2330 train_time:116387ms step_avg:58.40ms
step:1994/2330 train_time:116447ms step_avg:58.40ms
step:1995/2330 train_time:116506ms step_avg:58.40ms
step:1996/2330 train_time:116566ms step_avg:58.40ms
step:1997/2330 train_time:116622ms step_avg:58.40ms
step:1998/2330 train_time:116683ms step_avg:58.40ms
step:1999/2330 train_time:116741ms step_avg:58.40ms
step:2000/2330 train_time:116802ms step_avg:58.40ms
step:2000/2330 val_loss:3.7587 train_time:116883ms step_avg:58.44ms
step:2001/2330 train_time:116902ms step_avg:58.42ms
step:2002/2330 train_time:116923ms step_avg:58.40ms
step:2003/2330 train_time:116981ms step_avg:58.40ms
step:2004/2330 train_time:117045ms step_avg:58.41ms
step:2005/2330 train_time:117103ms step_avg:58.41ms
step:2006/2330 train_time:117165ms step_avg:58.41ms
step:2007/2330 train_time:117222ms step_avg:58.41ms
step:2008/2330 train_time:117282ms step_avg:58.41ms
step:2009/2330 train_time:117339ms step_avg:58.41ms
step:2010/2330 train_time:117399ms step_avg:58.41ms
step:2011/2330 train_time:117455ms step_avg:58.41ms
step:2012/2330 train_time:117515ms step_avg:58.41ms
step:2013/2330 train_time:117571ms step_avg:58.41ms
step:2014/2330 train_time:117632ms step_avg:58.41ms
step:2015/2330 train_time:117689ms step_avg:58.41ms
step:2016/2330 train_time:117750ms step_avg:58.41ms
step:2017/2330 train_time:117807ms step_avg:58.41ms
step:2018/2330 train_time:117869ms step_avg:58.41ms
step:2019/2330 train_time:117928ms step_avg:58.41ms
step:2020/2330 train_time:117990ms step_avg:58.41ms
step:2021/2330 train_time:118048ms step_avg:58.41ms
step:2022/2330 train_time:118111ms step_avg:58.41ms
step:2023/2330 train_time:118168ms step_avg:58.41ms
step:2024/2330 train_time:118231ms step_avg:58.41ms
step:2025/2330 train_time:118289ms step_avg:58.41ms
step:2026/2330 train_time:118350ms step_avg:58.42ms
step:2027/2330 train_time:118408ms step_avg:58.42ms
step:2028/2330 train_time:118468ms step_avg:58.42ms
step:2029/2330 train_time:118525ms step_avg:58.42ms
step:2030/2330 train_time:118585ms step_avg:58.42ms
step:2031/2330 train_time:118642ms step_avg:58.42ms
step:2032/2330 train_time:118702ms step_avg:58.42ms
step:2033/2330 train_time:118758ms step_avg:58.42ms
step:2034/2330 train_time:118819ms step_avg:58.42ms
step:2035/2330 train_time:118876ms step_avg:58.42ms
step:2036/2330 train_time:118938ms step_avg:58.42ms
step:2037/2330 train_time:118995ms step_avg:58.42ms
step:2038/2330 train_time:119058ms step_avg:58.42ms
step:2039/2330 train_time:119115ms step_avg:58.42ms
step:2040/2330 train_time:119177ms step_avg:58.42ms
step:2041/2330 train_time:119234ms step_avg:58.42ms
step:2042/2330 train_time:119296ms step_avg:58.42ms
step:2043/2330 train_time:119354ms step_avg:58.42ms
step:2044/2330 train_time:119415ms step_avg:58.42ms
step:2045/2330 train_time:119472ms step_avg:58.42ms
step:2046/2330 train_time:119535ms step_avg:58.42ms
step:2047/2330 train_time:119591ms step_avg:58.42ms
step:2048/2330 train_time:119652ms step_avg:58.42ms
step:2049/2330 train_time:119709ms step_avg:58.42ms
step:2050/2330 train_time:119769ms step_avg:58.42ms
step:2051/2330 train_time:119827ms step_avg:58.42ms
step:2052/2330 train_time:119888ms step_avg:58.42ms
step:2053/2330 train_time:119947ms step_avg:58.43ms
step:2054/2330 train_time:120007ms step_avg:58.43ms
step:2055/2330 train_time:120065ms step_avg:58.43ms
step:2056/2330 train_time:120126ms step_avg:58.43ms
step:2057/2330 train_time:120184ms step_avg:58.43ms
step:2058/2330 train_time:120246ms step_avg:58.43ms
step:2059/2330 train_time:120305ms step_avg:58.43ms
step:2060/2330 train_time:120365ms step_avg:58.43ms
step:2061/2330 train_time:120423ms step_avg:58.43ms
step:2062/2330 train_time:120483ms step_avg:58.43ms
step:2063/2330 train_time:120541ms step_avg:58.43ms
step:2064/2330 train_time:120600ms step_avg:58.43ms
step:2065/2330 train_time:120657ms step_avg:58.43ms
step:2066/2330 train_time:120717ms step_avg:58.43ms
step:2067/2330 train_time:120773ms step_avg:58.43ms
step:2068/2330 train_time:120835ms step_avg:58.43ms
step:2069/2330 train_time:120892ms step_avg:58.43ms
step:2070/2330 train_time:120954ms step_avg:58.43ms
step:2071/2330 train_time:121011ms step_avg:58.43ms
step:2072/2330 train_time:121073ms step_avg:58.43ms
step:2073/2330 train_time:121130ms step_avg:58.43ms
step:2074/2330 train_time:121192ms step_avg:58.43ms
step:2075/2330 train_time:121250ms step_avg:58.43ms
step:2076/2330 train_time:121311ms step_avg:58.43ms
step:2077/2330 train_time:121368ms step_avg:58.43ms
step:2078/2330 train_time:121430ms step_avg:58.44ms
step:2079/2330 train_time:121487ms step_avg:58.44ms
step:2080/2330 train_time:121548ms step_avg:58.44ms
step:2081/2330 train_time:121606ms step_avg:58.44ms
step:2082/2330 train_time:121666ms step_avg:58.44ms
step:2083/2330 train_time:121723ms step_avg:58.44ms
step:2084/2330 train_time:121783ms step_avg:58.44ms
step:2085/2330 train_time:121840ms step_avg:58.44ms
step:2086/2330 train_time:121901ms step_avg:58.44ms
step:2087/2330 train_time:121958ms step_avg:58.44ms
step:2088/2330 train_time:122019ms step_avg:58.44ms
step:2089/2330 train_time:122076ms step_avg:58.44ms
step:2090/2330 train_time:122137ms step_avg:58.44ms
step:2091/2330 train_time:122195ms step_avg:58.44ms
step:2092/2330 train_time:122257ms step_avg:58.44ms
step:2093/2330 train_time:122314ms step_avg:58.44ms
step:2094/2330 train_time:122377ms step_avg:58.44ms
step:2095/2330 train_time:122433ms step_avg:58.44ms
step:2096/2330 train_time:122495ms step_avg:58.44ms
step:2097/2330 train_time:122552ms step_avg:58.44ms
step:2098/2330 train_time:122615ms step_avg:58.44ms
step:2099/2330 train_time:122671ms step_avg:58.44ms
step:2100/2330 train_time:122733ms step_avg:58.44ms
step:2101/2330 train_time:122790ms step_avg:58.44ms
step:2102/2330 train_time:122852ms step_avg:58.45ms
step:2103/2330 train_time:122909ms step_avg:58.44ms
step:2104/2330 train_time:122971ms step_avg:58.45ms
step:2105/2330 train_time:123028ms step_avg:58.45ms
step:2106/2330 train_time:123089ms step_avg:58.45ms
step:2107/2330 train_time:123147ms step_avg:58.45ms
step:2108/2330 train_time:123207ms step_avg:58.45ms
step:2109/2330 train_time:123265ms step_avg:58.45ms
step:2110/2330 train_time:123326ms step_avg:58.45ms
step:2111/2330 train_time:123385ms step_avg:58.45ms
step:2112/2330 train_time:123445ms step_avg:58.45ms
step:2113/2330 train_time:123502ms step_avg:58.45ms
step:2114/2330 train_time:123563ms step_avg:58.45ms
step:2115/2330 train_time:123622ms step_avg:58.45ms
step:2116/2330 train_time:123682ms step_avg:58.45ms
step:2117/2330 train_time:123739ms step_avg:58.45ms
step:2118/2330 train_time:123799ms step_avg:58.45ms
step:2119/2330 train_time:123856ms step_avg:58.45ms
step:2120/2330 train_time:123917ms step_avg:58.45ms
step:2121/2330 train_time:123973ms step_avg:58.45ms
step:2122/2330 train_time:124035ms step_avg:58.45ms
step:2123/2330 train_time:124092ms step_avg:58.45ms
step:2124/2330 train_time:124154ms step_avg:58.45ms
step:2125/2330 train_time:124211ms step_avg:58.45ms
step:2126/2330 train_time:124273ms step_avg:58.45ms
step:2127/2330 train_time:124330ms step_avg:58.45ms
step:2128/2330 train_time:124394ms step_avg:58.46ms
step:2129/2330 train_time:124451ms step_avg:58.46ms
step:2130/2330 train_time:124513ms step_avg:58.46ms
step:2131/2330 train_time:124570ms step_avg:58.46ms
step:2132/2330 train_time:124631ms step_avg:58.46ms
step:2133/2330 train_time:124688ms step_avg:58.46ms
step:2134/2330 train_time:124750ms step_avg:58.46ms
step:2135/2330 train_time:124808ms step_avg:58.46ms
step:2136/2330 train_time:124868ms step_avg:58.46ms
step:2137/2330 train_time:124927ms step_avg:58.46ms
step:2138/2330 train_time:124987ms step_avg:58.46ms
step:2139/2330 train_time:125045ms step_avg:58.46ms
step:2140/2330 train_time:125105ms step_avg:58.46ms
step:2141/2330 train_time:125163ms step_avg:58.46ms
step:2142/2330 train_time:125223ms step_avg:58.46ms
step:2143/2330 train_time:125280ms step_avg:58.46ms
step:2144/2330 train_time:125341ms step_avg:58.46ms
step:2145/2330 train_time:125398ms step_avg:58.46ms
step:2146/2330 train_time:125459ms step_avg:58.46ms
step:2147/2330 train_time:125516ms step_avg:58.46ms
step:2148/2330 train_time:125577ms step_avg:58.46ms
step:2149/2330 train_time:125634ms step_avg:58.46ms
step:2150/2330 train_time:125696ms step_avg:58.46ms
step:2151/2330 train_time:125753ms step_avg:58.46ms
step:2152/2330 train_time:125815ms step_avg:58.46ms
step:2153/2330 train_time:125872ms step_avg:58.46ms
step:2154/2330 train_time:125933ms step_avg:58.46ms
step:2155/2330 train_time:125990ms step_avg:58.46ms
step:2156/2330 train_time:126053ms step_avg:58.47ms
step:2157/2330 train_time:126110ms step_avg:58.47ms
step:2158/2330 train_time:126172ms step_avg:58.47ms
step:2159/2330 train_time:126229ms step_avg:58.47ms
step:2160/2330 train_time:126291ms step_avg:58.47ms
step:2161/2330 train_time:126348ms step_avg:58.47ms
step:2162/2330 train_time:126409ms step_avg:58.47ms
step:2163/2330 train_time:126467ms step_avg:58.47ms
step:2164/2330 train_time:126527ms step_avg:58.47ms
step:2165/2330 train_time:126585ms step_avg:58.47ms
step:2166/2330 train_time:126646ms step_avg:58.47ms
step:2167/2330 train_time:126704ms step_avg:58.47ms
step:2168/2330 train_time:126764ms step_avg:58.47ms
step:2169/2330 train_time:126822ms step_avg:58.47ms
step:2170/2330 train_time:126882ms step_avg:58.47ms
step:2171/2330 train_time:126940ms step_avg:58.47ms
step:2172/2330 train_time:127000ms step_avg:58.47ms
step:2173/2330 train_time:127057ms step_avg:58.47ms
step:2174/2330 train_time:127118ms step_avg:58.47ms
step:2175/2330 train_time:127174ms step_avg:58.47ms
step:2176/2330 train_time:127237ms step_avg:58.47ms
step:2177/2330 train_time:127294ms step_avg:58.47ms
step:2178/2330 train_time:127357ms step_avg:58.47ms
step:2179/2330 train_time:127414ms step_avg:58.47ms
step:2180/2330 train_time:127476ms step_avg:58.48ms
step:2181/2330 train_time:127533ms step_avg:58.47ms
step:2182/2330 train_time:127595ms step_avg:58.48ms
step:2183/2330 train_time:127653ms step_avg:58.48ms
step:2184/2330 train_time:127715ms step_avg:58.48ms
step:2185/2330 train_time:127772ms step_avg:58.48ms
step:2186/2330 train_time:127833ms step_avg:58.48ms
step:2187/2330 train_time:127890ms step_avg:58.48ms
step:2188/2330 train_time:127953ms step_avg:58.48ms
step:2189/2330 train_time:128009ms step_avg:58.48ms
step:2190/2330 train_time:128071ms step_avg:58.48ms
step:2191/2330 train_time:128129ms step_avg:58.48ms
step:2192/2330 train_time:128189ms step_avg:58.48ms
step:2193/2330 train_time:128248ms step_avg:58.48ms
step:2194/2330 train_time:128308ms step_avg:58.48ms
step:2195/2330 train_time:128366ms step_avg:58.48ms
step:2196/2330 train_time:128427ms step_avg:58.48ms
step:2197/2330 train_time:128485ms step_avg:58.48ms
step:2198/2330 train_time:128545ms step_avg:58.48ms
step:2199/2330 train_time:128604ms step_avg:58.48ms
step:2200/2330 train_time:128665ms step_avg:58.48ms
step:2201/2330 train_time:128722ms step_avg:58.48ms
step:2202/2330 train_time:128781ms step_avg:58.48ms
step:2203/2330 train_time:128839ms step_avg:58.48ms
step:2204/2330 train_time:128899ms step_avg:58.48ms
step:2205/2330 train_time:128956ms step_avg:58.48ms
step:2206/2330 train_time:129017ms step_avg:58.48ms
step:2207/2330 train_time:129074ms step_avg:58.48ms
step:2208/2330 train_time:129136ms step_avg:58.49ms
step:2209/2330 train_time:129193ms step_avg:58.48ms
step:2210/2330 train_time:129255ms step_avg:58.49ms
step:2211/2330 train_time:129312ms step_avg:58.49ms
step:2212/2330 train_time:129374ms step_avg:58.49ms
step:2213/2330 train_time:129430ms step_avg:58.49ms
step:2214/2330 train_time:129493ms step_avg:58.49ms
step:2215/2330 train_time:129550ms step_avg:58.49ms
step:2216/2330 train_time:129611ms step_avg:58.49ms
step:2217/2330 train_time:129669ms step_avg:58.49ms
step:2218/2330 train_time:129730ms step_avg:58.49ms
step:2219/2330 train_time:129787ms step_avg:58.49ms
step:2220/2330 train_time:129848ms step_avg:58.49ms
step:2221/2330 train_time:129906ms step_avg:58.49ms
step:2222/2330 train_time:129966ms step_avg:58.49ms
step:2223/2330 train_time:130024ms step_avg:58.49ms
step:2224/2330 train_time:130084ms step_avg:58.49ms
step:2225/2330 train_time:130143ms step_avg:58.49ms
step:2226/2330 train_time:130204ms step_avg:58.49ms
step:2227/2330 train_time:130263ms step_avg:58.49ms
step:2228/2330 train_time:130323ms step_avg:58.49ms
step:2229/2330 train_time:130381ms step_avg:58.49ms
step:2230/2330 train_time:130441ms step_avg:58.49ms
step:2231/2330 train_time:130499ms step_avg:58.49ms
step:2232/2330 train_time:130559ms step_avg:58.49ms
step:2233/2330 train_time:130616ms step_avg:58.49ms
step:2234/2330 train_time:130679ms step_avg:58.50ms
step:2235/2330 train_time:130735ms step_avg:58.49ms
step:2236/2330 train_time:130798ms step_avg:58.50ms
step:2237/2330 train_time:130854ms step_avg:58.50ms
step:2238/2330 train_time:130916ms step_avg:58.50ms
step:2239/2330 train_time:130973ms step_avg:58.50ms
step:2240/2330 train_time:131035ms step_avg:58.50ms
step:2241/2330 train_time:131092ms step_avg:58.50ms
step:2242/2330 train_time:131153ms step_avg:58.50ms
step:2243/2330 train_time:131210ms step_avg:58.50ms
step:2244/2330 train_time:131272ms step_avg:58.50ms
step:2245/2330 train_time:131330ms step_avg:58.50ms
step:2246/2330 train_time:131390ms step_avg:58.50ms
step:2247/2330 train_time:131448ms step_avg:58.50ms
step:2248/2330 train_time:131509ms step_avg:58.50ms
step:2249/2330 train_time:131568ms step_avg:58.50ms
step:2250/2330 train_time:131628ms step_avg:58.50ms
step:2250/2330 val_loss:3.7110 train_time:131710ms step_avg:58.54ms
step:2251/2330 train_time:131728ms step_avg:58.52ms
step:2252/2330 train_time:131750ms step_avg:58.50ms
step:2253/2330 train_time:131812ms step_avg:58.51ms
step:2254/2330 train_time:131875ms step_avg:58.51ms
step:2255/2330 train_time:131934ms step_avg:58.51ms
step:2256/2330 train_time:131996ms step_avg:58.51ms
step:2257/2330 train_time:132053ms step_avg:58.51ms
step:2258/2330 train_time:132114ms step_avg:58.51ms
step:2259/2330 train_time:132170ms step_avg:58.51ms
step:2260/2330 train_time:132230ms step_avg:58.51ms
step:2261/2330 train_time:132287ms step_avg:58.51ms
step:2262/2330 train_time:132348ms step_avg:58.51ms
step:2263/2330 train_time:132404ms step_avg:58.51ms
step:2264/2330 train_time:132465ms step_avg:58.51ms
step:2265/2330 train_time:132521ms step_avg:58.51ms
step:2266/2330 train_time:132581ms step_avg:58.51ms
step:2267/2330 train_time:132638ms step_avg:58.51ms
step:2268/2330 train_time:132699ms step_avg:58.51ms
step:2269/2330 train_time:132758ms step_avg:58.51ms
step:2270/2330 train_time:132820ms step_avg:58.51ms
step:2271/2330 train_time:132880ms step_avg:58.51ms
step:2272/2330 train_time:132943ms step_avg:58.51ms
step:2273/2330 train_time:133001ms step_avg:58.51ms
step:2274/2330 train_time:133063ms step_avg:58.51ms
step:2275/2330 train_time:133121ms step_avg:58.51ms
step:2276/2330 train_time:133181ms step_avg:58.52ms
step:2277/2330 train_time:133238ms step_avg:58.51ms
step:2278/2330 train_time:133299ms step_avg:58.52ms
step:2279/2330 train_time:133355ms step_avg:58.51ms
step:2280/2330 train_time:133414ms step_avg:58.51ms
step:2281/2330 train_time:133471ms step_avg:58.51ms
step:2282/2330 train_time:133531ms step_avg:58.51ms
step:2283/2330 train_time:133587ms step_avg:58.51ms
step:2284/2330 train_time:133648ms step_avg:58.51ms
step:2285/2330 train_time:133705ms step_avg:58.51ms
step:2286/2330 train_time:133767ms step_avg:58.52ms
step:2287/2330 train_time:133826ms step_avg:58.52ms
step:2288/2330 train_time:133887ms step_avg:58.52ms
step:2289/2330 train_time:133945ms step_avg:58.52ms
step:2290/2330 train_time:134007ms step_avg:58.52ms
step:2291/2330 train_time:134064ms step_avg:58.52ms
step:2292/2330 train_time:134126ms step_avg:58.52ms
step:2293/2330 train_time:134183ms step_avg:58.52ms
step:2294/2330 train_time:134245ms step_avg:58.52ms
step:2295/2330 train_time:134302ms step_avg:58.52ms
step:2296/2330 train_time:134362ms step_avg:58.52ms
step:2297/2330 train_time:134420ms step_avg:58.52ms
step:2298/2330 train_time:134479ms step_avg:58.52ms
step:2299/2330 train_time:134538ms step_avg:58.52ms
step:2300/2330 train_time:134598ms step_avg:58.52ms
step:2301/2330 train_time:134655ms step_avg:58.52ms
step:2302/2330 train_time:134715ms step_avg:58.52ms
step:2303/2330 train_time:134773ms step_avg:58.52ms
step:2304/2330 train_time:134834ms step_avg:58.52ms
step:2305/2330 train_time:134891ms step_avg:58.52ms
step:2306/2330 train_time:134954ms step_avg:58.52ms
step:2307/2330 train_time:135011ms step_avg:58.52ms
step:2308/2330 train_time:135074ms step_avg:58.52ms
step:2309/2330 train_time:135131ms step_avg:58.52ms
step:2310/2330 train_time:135193ms step_avg:58.53ms
step:2311/2330 train_time:135250ms step_avg:58.52ms
step:2312/2330 train_time:135312ms step_avg:58.53ms
step:2313/2330 train_time:135368ms step_avg:58.52ms
step:2314/2330 train_time:135430ms step_avg:58.53ms
step:2315/2330 train_time:135486ms step_avg:58.53ms
step:2316/2330 train_time:135547ms step_avg:58.53ms
step:2317/2330 train_time:135604ms step_avg:58.53ms
step:2318/2330 train_time:135666ms step_avg:58.53ms
step:2319/2330 train_time:135723ms step_avg:58.53ms
step:2320/2330 train_time:135785ms step_avg:58.53ms
step:2321/2330 train_time:135842ms step_avg:58.53ms
step:2322/2330 train_time:135903ms step_avg:58.53ms
step:2323/2330 train_time:135962ms step_avg:58.53ms
step:2324/2330 train_time:136023ms step_avg:58.53ms
step:2325/2330 train_time:136081ms step_avg:58.53ms
step:2326/2330 train_time:136142ms step_avg:58.53ms
step:2327/2330 train_time:136199ms step_avg:58.53ms
step:2328/2330 train_time:136260ms step_avg:58.53ms
step:2329/2330 train_time:136319ms step_avg:58.53ms
step:2330/2330 train_time:136379ms step_avg:58.53ms
step:2330/2330 val_loss:3.6957 train_time:136460ms step_avg:58.57ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
