import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--adamw_lr', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_lr_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_lr_tuning/{args2.adamw_lr}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=float(args2.adamw_lr),  betas=(0.85, 0.85), eps=1e-10, weight_decay=0.0, fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_lr_tuning/{args2.adamw_lr}", exist_ok=True)
            torch.save(log, f"logs_adamw_small_lr_tuning/{args2.adamw_lr}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 01:33:13 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   28C    P0             114W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   27C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   26C    P0             115W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   25C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   27C    P0             113W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   25C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   26C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:86ms step_avg:86.08ms
step:2/2330 train_time:181ms step_avg:90.36ms
step:3/2330 train_time:200ms step_avg:66.80ms
step:4/2330 train_time:220ms step_avg:55.02ms
step:5/2330 train_time:272ms step_avg:54.40ms
step:6/2330 train_time:330ms step_avg:54.95ms
step:7/2330 train_time:385ms step_avg:54.96ms
step:8/2330 train_time:443ms step_avg:55.35ms
step:9/2330 train_time:498ms step_avg:55.37ms
step:10/2330 train_time:556ms step_avg:55.62ms
step:11/2330 train_time:612ms step_avg:55.60ms
step:12/2330 train_time:670ms step_avg:55.79ms
step:13/2330 train_time:725ms step_avg:55.77ms
step:14/2330 train_time:783ms step_avg:55.92ms
step:15/2330 train_time:838ms step_avg:55.84ms
step:16/2330 train_time:896ms step_avg:56.01ms
step:17/2330 train_time:951ms step_avg:55.95ms
step:18/2330 train_time:1010ms step_avg:56.11ms
step:19/2330 train_time:1067ms step_avg:56.15ms
step:20/2330 train_time:1130ms step_avg:56.51ms
step:21/2330 train_time:1187ms step_avg:56.51ms
step:22/2330 train_time:1246ms step_avg:56.64ms
step:23/2330 train_time:1302ms step_avg:56.60ms
step:24/2330 train_time:1361ms step_avg:56.71ms
step:25/2330 train_time:1416ms step_avg:56.65ms
step:26/2330 train_time:1475ms step_avg:56.72ms
step:27/2330 train_time:1530ms step_avg:56.66ms
step:28/2330 train_time:1589ms step_avg:56.74ms
step:29/2330 train_time:1644ms step_avg:56.70ms
step:30/2330 train_time:1702ms step_avg:56.75ms
step:31/2330 train_time:1757ms step_avg:56.69ms
step:32/2330 train_time:1816ms step_avg:56.75ms
step:33/2330 train_time:1871ms step_avg:56.71ms
step:34/2330 train_time:1929ms step_avg:56.74ms
step:35/2330 train_time:1984ms step_avg:56.70ms
step:36/2330 train_time:2045ms step_avg:56.79ms
step:37/2330 train_time:2101ms step_avg:56.79ms
step:38/2330 train_time:2161ms step_avg:56.87ms
step:39/2330 train_time:2217ms step_avg:56.86ms
step:40/2330 train_time:2278ms step_avg:56.94ms
step:41/2330 train_time:2334ms step_avg:56.92ms
step:42/2330 train_time:2392ms step_avg:56.96ms
step:43/2330 train_time:2448ms step_avg:56.93ms
step:44/2330 train_time:2507ms step_avg:56.97ms
step:45/2330 train_time:2562ms step_avg:56.93ms
step:46/2330 train_time:2621ms step_avg:56.99ms
step:47/2330 train_time:2677ms step_avg:56.96ms
step:48/2330 train_time:2735ms step_avg:56.98ms
step:49/2330 train_time:2790ms step_avg:56.95ms
step:50/2330 train_time:2849ms step_avg:56.97ms
step:51/2330 train_time:2904ms step_avg:56.94ms
step:52/2330 train_time:2963ms step_avg:56.97ms
step:53/2330 train_time:3019ms step_avg:56.96ms
step:54/2330 train_time:3078ms step_avg:57.01ms
step:55/2330 train_time:3135ms step_avg:57.00ms
step:56/2330 train_time:3194ms step_avg:57.03ms
step:57/2330 train_time:3250ms step_avg:57.01ms
step:58/2330 train_time:3310ms step_avg:57.07ms
step:59/2330 train_time:3366ms step_avg:57.04ms
step:60/2330 train_time:3425ms step_avg:57.08ms
step:61/2330 train_time:3481ms step_avg:57.06ms
step:62/2330 train_time:3539ms step_avg:57.09ms
step:63/2330 train_time:3595ms step_avg:57.06ms
step:64/2330 train_time:3654ms step_avg:57.09ms
step:65/2330 train_time:3709ms step_avg:57.06ms
step:66/2330 train_time:3768ms step_avg:57.09ms
step:67/2330 train_time:3824ms step_avg:57.08ms
step:68/2330 train_time:3882ms step_avg:57.09ms
step:69/2330 train_time:3938ms step_avg:57.07ms
step:70/2330 train_time:3997ms step_avg:57.10ms
step:71/2330 train_time:4053ms step_avg:57.08ms
step:72/2330 train_time:4112ms step_avg:57.11ms
step:73/2330 train_time:4168ms step_avg:57.09ms
step:74/2330 train_time:4228ms step_avg:57.14ms
step:75/2330 train_time:4284ms step_avg:57.11ms
step:76/2330 train_time:4342ms step_avg:57.13ms
step:77/2330 train_time:4398ms step_avg:57.12ms
step:78/2330 train_time:4457ms step_avg:57.14ms
step:79/2330 train_time:4513ms step_avg:57.13ms
step:80/2330 train_time:4572ms step_avg:57.15ms
step:81/2330 train_time:4628ms step_avg:57.13ms
step:82/2330 train_time:4686ms step_avg:57.14ms
step:83/2330 train_time:4742ms step_avg:57.13ms
step:84/2330 train_time:4800ms step_avg:57.15ms
step:85/2330 train_time:4856ms step_avg:57.13ms
step:86/2330 train_time:4915ms step_avg:57.15ms
step:87/2330 train_time:4971ms step_avg:57.14ms
step:88/2330 train_time:5030ms step_avg:57.15ms
step:89/2330 train_time:5085ms step_avg:57.13ms
step:90/2330 train_time:5144ms step_avg:57.16ms
step:91/2330 train_time:5200ms step_avg:57.14ms
step:92/2330 train_time:5260ms step_avg:57.17ms
step:93/2330 train_time:5315ms step_avg:57.15ms
step:94/2330 train_time:5375ms step_avg:57.18ms
step:95/2330 train_time:5431ms step_avg:57.17ms
step:96/2330 train_time:5490ms step_avg:57.19ms
step:97/2330 train_time:5546ms step_avg:57.17ms
step:98/2330 train_time:5604ms step_avg:57.19ms
step:99/2330 train_time:5660ms step_avg:57.17ms
step:100/2330 train_time:5719ms step_avg:57.19ms
step:101/2330 train_time:5775ms step_avg:57.18ms
step:102/2330 train_time:5833ms step_avg:57.19ms
step:103/2330 train_time:5889ms step_avg:57.17ms
step:104/2330 train_time:5948ms step_avg:57.19ms
step:105/2330 train_time:6004ms step_avg:57.18ms
step:106/2330 train_time:6062ms step_avg:57.19ms
step:107/2330 train_time:6118ms step_avg:57.18ms
step:108/2330 train_time:6177ms step_avg:57.19ms
step:109/2330 train_time:6233ms step_avg:57.18ms
step:110/2330 train_time:6292ms step_avg:57.20ms
step:111/2330 train_time:6347ms step_avg:57.18ms
step:112/2330 train_time:6407ms step_avg:57.21ms
step:113/2330 train_time:6462ms step_avg:57.19ms
step:114/2330 train_time:6521ms step_avg:57.20ms
step:115/2330 train_time:6577ms step_avg:57.19ms
step:116/2330 train_time:6635ms step_avg:57.20ms
step:117/2330 train_time:6691ms step_avg:57.19ms
step:118/2330 train_time:6750ms step_avg:57.20ms
step:119/2330 train_time:6806ms step_avg:57.19ms
step:120/2330 train_time:6865ms step_avg:57.21ms
step:121/2330 train_time:6921ms step_avg:57.19ms
step:122/2330 train_time:6980ms step_avg:57.21ms
step:123/2330 train_time:7035ms step_avg:57.20ms
step:124/2330 train_time:7095ms step_avg:57.22ms
step:125/2330 train_time:7150ms step_avg:57.20ms
step:126/2330 train_time:7209ms step_avg:57.21ms
step:127/2330 train_time:7265ms step_avg:57.20ms
step:128/2330 train_time:7324ms step_avg:57.22ms
step:129/2330 train_time:7379ms step_avg:57.21ms
step:130/2330 train_time:7438ms step_avg:57.22ms
step:131/2330 train_time:7495ms step_avg:57.22ms
step:132/2330 train_time:7554ms step_avg:57.23ms
step:133/2330 train_time:7610ms step_avg:57.22ms
step:134/2330 train_time:7668ms step_avg:57.23ms
step:135/2330 train_time:7724ms step_avg:57.21ms
step:136/2330 train_time:7783ms step_avg:57.22ms
step:137/2330 train_time:7838ms step_avg:57.21ms
step:138/2330 train_time:7897ms step_avg:57.23ms
step:139/2330 train_time:7953ms step_avg:57.21ms
step:140/2330 train_time:8013ms step_avg:57.23ms
step:141/2330 train_time:8068ms step_avg:57.22ms
step:142/2330 train_time:8127ms step_avg:57.23ms
step:143/2330 train_time:8184ms step_avg:57.23ms
step:144/2330 train_time:8242ms step_avg:57.24ms
step:145/2330 train_time:8298ms step_avg:57.23ms
step:146/2330 train_time:8358ms step_avg:57.25ms
step:147/2330 train_time:8414ms step_avg:57.24ms
step:148/2330 train_time:8473ms step_avg:57.25ms
step:149/2330 train_time:8529ms step_avg:57.24ms
step:150/2330 train_time:8588ms step_avg:57.25ms
step:151/2330 train_time:8643ms step_avg:57.24ms
step:152/2330 train_time:8702ms step_avg:57.25ms
step:153/2330 train_time:8757ms step_avg:57.24ms
step:154/2330 train_time:8817ms step_avg:57.25ms
step:155/2330 train_time:8873ms step_avg:57.25ms
step:156/2330 train_time:8932ms step_avg:57.26ms
step:157/2330 train_time:8987ms step_avg:57.24ms
step:158/2330 train_time:9047ms step_avg:57.26ms
step:159/2330 train_time:9103ms step_avg:57.25ms
step:160/2330 train_time:9161ms step_avg:57.26ms
step:161/2330 train_time:9217ms step_avg:57.25ms
step:162/2330 train_time:9276ms step_avg:57.26ms
step:163/2330 train_time:9332ms step_avg:57.25ms
step:164/2330 train_time:9391ms step_avg:57.26ms
step:165/2330 train_time:9446ms step_avg:57.25ms
step:166/2330 train_time:9506ms step_avg:57.27ms
step:167/2330 train_time:9562ms step_avg:57.26ms
step:168/2330 train_time:9621ms step_avg:57.27ms
step:169/2330 train_time:9676ms step_avg:57.26ms
step:170/2330 train_time:9735ms step_avg:57.27ms
step:171/2330 train_time:9791ms step_avg:57.26ms
step:172/2330 train_time:9850ms step_avg:57.27ms
step:173/2330 train_time:9906ms step_avg:57.26ms
step:174/2330 train_time:9964ms step_avg:57.27ms
step:175/2330 train_time:10020ms step_avg:57.26ms
step:176/2330 train_time:10079ms step_avg:57.27ms
step:177/2330 train_time:10136ms step_avg:57.26ms
step:178/2330 train_time:10194ms step_avg:57.27ms
step:179/2330 train_time:10249ms step_avg:57.26ms
step:180/2330 train_time:10309ms step_avg:57.27ms
step:181/2330 train_time:10364ms step_avg:57.26ms
step:182/2330 train_time:10424ms step_avg:57.27ms
step:183/2330 train_time:10479ms step_avg:57.26ms
step:184/2330 train_time:10539ms step_avg:57.28ms
step:185/2330 train_time:10595ms step_avg:57.27ms
step:186/2330 train_time:10654ms step_avg:57.28ms
step:187/2330 train_time:10710ms step_avg:57.27ms
step:188/2330 train_time:10769ms step_avg:57.28ms
step:189/2330 train_time:10825ms step_avg:57.28ms
step:190/2330 train_time:10883ms step_avg:57.28ms
step:191/2330 train_time:10938ms step_avg:57.27ms
step:192/2330 train_time:10997ms step_avg:57.28ms
step:193/2330 train_time:11053ms step_avg:57.27ms
step:194/2330 train_time:11112ms step_avg:57.28ms
step:195/2330 train_time:11168ms step_avg:57.27ms
step:196/2330 train_time:11227ms step_avg:57.28ms
step:197/2330 train_time:11283ms step_avg:57.27ms
step:198/2330 train_time:11342ms step_avg:57.28ms
step:199/2330 train_time:11398ms step_avg:57.28ms
step:200/2330 train_time:11456ms step_avg:57.28ms
step:201/2330 train_time:11512ms step_avg:57.28ms
step:202/2330 train_time:11570ms step_avg:57.28ms
step:203/2330 train_time:11626ms step_avg:57.27ms
step:204/2330 train_time:11685ms step_avg:57.28ms
step:205/2330 train_time:11741ms step_avg:57.27ms
step:206/2330 train_time:11800ms step_avg:57.28ms
step:207/2330 train_time:11856ms step_avg:57.28ms
step:208/2330 train_time:11915ms step_avg:57.28ms
step:209/2330 train_time:11970ms step_avg:57.27ms
step:210/2330 train_time:12029ms step_avg:57.28ms
step:211/2330 train_time:12085ms step_avg:57.27ms
step:212/2330 train_time:12144ms step_avg:57.28ms
step:213/2330 train_time:12200ms step_avg:57.28ms
step:214/2330 train_time:12260ms step_avg:57.29ms
step:215/2330 train_time:12316ms step_avg:57.28ms
step:216/2330 train_time:12375ms step_avg:57.29ms
step:217/2330 train_time:12431ms step_avg:57.28ms
step:218/2330 train_time:12491ms step_avg:57.30ms
step:219/2330 train_time:12546ms step_avg:57.29ms
step:220/2330 train_time:12605ms step_avg:57.29ms
step:221/2330 train_time:12661ms step_avg:57.29ms
step:222/2330 train_time:12720ms step_avg:57.30ms
step:223/2330 train_time:12775ms step_avg:57.29ms
step:224/2330 train_time:12834ms step_avg:57.30ms
step:225/2330 train_time:12890ms step_avg:57.29ms
step:226/2330 train_time:12948ms step_avg:57.29ms
step:227/2330 train_time:13004ms step_avg:57.28ms
step:228/2330 train_time:13063ms step_avg:57.29ms
step:229/2330 train_time:13118ms step_avg:57.29ms
step:230/2330 train_time:13178ms step_avg:57.29ms
step:231/2330 train_time:13234ms step_avg:57.29ms
step:232/2330 train_time:13293ms step_avg:57.30ms
step:233/2330 train_time:13348ms step_avg:57.29ms
step:234/2330 train_time:13408ms step_avg:57.30ms
step:235/2330 train_time:13464ms step_avg:57.29ms
step:236/2330 train_time:13523ms step_avg:57.30ms
step:237/2330 train_time:13578ms step_avg:57.29ms
step:238/2330 train_time:13637ms step_avg:57.30ms
step:239/2330 train_time:13694ms step_avg:57.30ms
step:240/2330 train_time:13752ms step_avg:57.30ms
step:241/2330 train_time:13808ms step_avg:57.29ms
step:242/2330 train_time:13867ms step_avg:57.30ms
step:243/2330 train_time:13923ms step_avg:57.30ms
step:244/2330 train_time:13982ms step_avg:57.30ms
step:245/2330 train_time:14037ms step_avg:57.29ms
step:246/2330 train_time:14097ms step_avg:57.30ms
step:247/2330 train_time:14152ms step_avg:57.30ms
step:248/2330 train_time:14212ms step_avg:57.30ms
step:249/2330 train_time:14267ms step_avg:57.30ms
step:250/2330 train_time:14326ms step_avg:57.31ms
step:250/2330 val_loss:4.8743 train_time:14405ms step_avg:57.62ms
step:251/2330 train_time:14424ms step_avg:57.46ms
step:252/2330 train_time:14443ms step_avg:57.31ms
step:253/2330 train_time:14499ms step_avg:57.31ms
step:254/2330 train_time:14564ms step_avg:57.34ms
step:255/2330 train_time:14620ms step_avg:57.33ms
step:256/2330 train_time:14686ms step_avg:57.37ms
step:257/2330 train_time:14741ms step_avg:57.36ms
step:258/2330 train_time:14802ms step_avg:57.37ms
step:259/2330 train_time:14857ms step_avg:57.36ms
step:260/2330 train_time:14915ms step_avg:57.37ms
step:261/2330 train_time:14971ms step_avg:57.36ms
step:262/2330 train_time:15029ms step_avg:57.36ms
step:263/2330 train_time:15085ms step_avg:57.36ms
step:264/2330 train_time:15143ms step_avg:57.36ms
step:265/2330 train_time:15198ms step_avg:57.35ms
step:266/2330 train_time:15256ms step_avg:57.35ms
step:267/2330 train_time:15311ms step_avg:57.35ms
step:268/2330 train_time:15370ms step_avg:57.35ms
step:269/2330 train_time:15427ms step_avg:57.35ms
step:270/2330 train_time:15486ms step_avg:57.36ms
step:271/2330 train_time:15543ms step_avg:57.36ms
step:272/2330 train_time:15605ms step_avg:57.37ms
step:273/2330 train_time:15662ms step_avg:57.37ms
step:274/2330 train_time:15721ms step_avg:57.38ms
step:275/2330 train_time:15777ms step_avg:57.37ms
step:276/2330 train_time:15836ms step_avg:57.38ms
step:277/2330 train_time:15892ms step_avg:57.37ms
step:278/2330 train_time:15950ms step_avg:57.38ms
step:279/2330 train_time:16006ms step_avg:57.37ms
step:280/2330 train_time:16064ms step_avg:57.37ms
step:281/2330 train_time:16120ms step_avg:57.37ms
step:282/2330 train_time:16178ms step_avg:57.37ms
step:283/2330 train_time:16233ms step_avg:57.36ms
step:284/2330 train_time:16291ms step_avg:57.36ms
step:285/2330 train_time:16347ms step_avg:57.36ms
step:286/2330 train_time:16406ms step_avg:57.36ms
step:287/2330 train_time:16462ms step_avg:57.36ms
step:288/2330 train_time:16522ms step_avg:57.37ms
step:289/2330 train_time:16579ms step_avg:57.37ms
step:290/2330 train_time:16638ms step_avg:57.37ms
step:291/2330 train_time:16695ms step_avg:57.37ms
step:292/2330 train_time:16755ms step_avg:57.38ms
step:293/2330 train_time:16810ms step_avg:57.37ms
step:294/2330 train_time:16871ms step_avg:57.38ms
step:295/2330 train_time:16926ms step_avg:57.38ms
step:296/2330 train_time:16986ms step_avg:57.38ms
step:297/2330 train_time:17042ms step_avg:57.38ms
step:298/2330 train_time:17100ms step_avg:57.38ms
step:299/2330 train_time:17156ms step_avg:57.38ms
step:300/2330 train_time:17214ms step_avg:57.38ms
step:301/2330 train_time:17269ms step_avg:57.37ms
step:302/2330 train_time:17328ms step_avg:57.38ms
step:303/2330 train_time:17385ms step_avg:57.38ms
step:304/2330 train_time:17443ms step_avg:57.38ms
step:305/2330 train_time:17500ms step_avg:57.38ms
step:306/2330 train_time:17559ms step_avg:57.38ms
step:307/2330 train_time:17616ms step_avg:57.38ms
step:308/2330 train_time:17675ms step_avg:57.39ms
step:309/2330 train_time:17732ms step_avg:57.38ms
step:310/2330 train_time:17791ms step_avg:57.39ms
step:311/2330 train_time:17846ms step_avg:57.38ms
step:312/2330 train_time:17906ms step_avg:57.39ms
step:313/2330 train_time:17962ms step_avg:57.39ms
step:314/2330 train_time:18021ms step_avg:57.39ms
step:315/2330 train_time:18076ms step_avg:57.39ms
step:316/2330 train_time:18135ms step_avg:57.39ms
step:317/2330 train_time:18191ms step_avg:57.38ms
step:318/2330 train_time:18250ms step_avg:57.39ms
step:319/2330 train_time:18305ms step_avg:57.38ms
step:320/2330 train_time:18364ms step_avg:57.39ms
step:321/2330 train_time:18419ms step_avg:57.38ms
step:322/2330 train_time:18479ms step_avg:57.39ms
step:323/2330 train_time:18536ms step_avg:57.39ms
step:324/2330 train_time:18595ms step_avg:57.39ms
step:325/2330 train_time:18650ms step_avg:57.39ms
step:326/2330 train_time:18710ms step_avg:57.39ms
step:327/2330 train_time:18766ms step_avg:57.39ms
step:328/2330 train_time:18825ms step_avg:57.39ms
step:329/2330 train_time:18881ms step_avg:57.39ms
step:330/2330 train_time:18941ms step_avg:57.40ms
step:331/2330 train_time:18996ms step_avg:57.39ms
step:332/2330 train_time:19055ms step_avg:57.40ms
step:333/2330 train_time:19111ms step_avg:57.39ms
step:334/2330 train_time:19170ms step_avg:57.40ms
step:335/2330 train_time:19225ms step_avg:57.39ms
step:336/2330 train_time:19285ms step_avg:57.40ms
step:337/2330 train_time:19341ms step_avg:57.39ms
step:338/2330 train_time:19400ms step_avg:57.40ms
step:339/2330 train_time:19456ms step_avg:57.39ms
step:340/2330 train_time:19514ms step_avg:57.39ms
step:341/2330 train_time:19570ms step_avg:57.39ms
step:342/2330 train_time:19630ms step_avg:57.40ms
step:343/2330 train_time:19686ms step_avg:57.39ms
step:344/2330 train_time:19745ms step_avg:57.40ms
step:345/2330 train_time:19801ms step_avg:57.39ms
step:346/2330 train_time:19860ms step_avg:57.40ms
step:347/2330 train_time:19916ms step_avg:57.40ms
step:348/2330 train_time:19975ms step_avg:57.40ms
step:349/2330 train_time:20031ms step_avg:57.40ms
step:350/2330 train_time:20091ms step_avg:57.40ms
step:351/2330 train_time:20146ms step_avg:57.40ms
step:352/2330 train_time:20205ms step_avg:57.40ms
step:353/2330 train_time:20261ms step_avg:57.40ms
step:354/2330 train_time:20320ms step_avg:57.40ms
step:355/2330 train_time:20376ms step_avg:57.40ms
step:356/2330 train_time:20435ms step_avg:57.40ms
step:357/2330 train_time:20491ms step_avg:57.40ms
step:358/2330 train_time:20550ms step_avg:57.40ms
step:359/2330 train_time:20605ms step_avg:57.40ms
step:360/2330 train_time:20665ms step_avg:57.40ms
step:361/2330 train_time:20721ms step_avg:57.40ms
step:362/2330 train_time:20780ms step_avg:57.40ms
step:363/2330 train_time:20835ms step_avg:57.40ms
step:364/2330 train_time:20896ms step_avg:57.41ms
step:365/2330 train_time:20951ms step_avg:57.40ms
step:366/2330 train_time:21010ms step_avg:57.40ms
step:367/2330 train_time:21066ms step_avg:57.40ms
step:368/2330 train_time:21124ms step_avg:57.40ms
step:369/2330 train_time:21180ms step_avg:57.40ms
step:370/2330 train_time:21239ms step_avg:57.40ms
step:371/2330 train_time:21295ms step_avg:57.40ms
step:372/2330 train_time:21353ms step_avg:57.40ms
step:373/2330 train_time:21409ms step_avg:57.40ms
step:374/2330 train_time:21469ms step_avg:57.40ms
step:375/2330 train_time:21525ms step_avg:57.40ms
step:376/2330 train_time:21584ms step_avg:57.41ms
step:377/2330 train_time:21640ms step_avg:57.40ms
step:378/2330 train_time:21699ms step_avg:57.41ms
step:379/2330 train_time:21755ms step_avg:57.40ms
step:380/2330 train_time:21814ms step_avg:57.41ms
step:381/2330 train_time:21870ms step_avg:57.40ms
step:382/2330 train_time:21929ms step_avg:57.41ms
step:383/2330 train_time:21985ms step_avg:57.40ms
step:384/2330 train_time:22045ms step_avg:57.41ms
step:385/2330 train_time:22101ms step_avg:57.40ms
step:386/2330 train_time:22159ms step_avg:57.41ms
step:387/2330 train_time:22215ms step_avg:57.40ms
step:388/2330 train_time:22274ms step_avg:57.41ms
step:389/2330 train_time:22329ms step_avg:57.40ms
step:390/2330 train_time:22389ms step_avg:57.41ms
step:391/2330 train_time:22445ms step_avg:57.40ms
step:392/2330 train_time:22503ms step_avg:57.41ms
step:393/2330 train_time:22559ms step_avg:57.40ms
step:394/2330 train_time:22618ms step_avg:57.41ms
step:395/2330 train_time:22674ms step_avg:57.40ms
step:396/2330 train_time:22734ms step_avg:57.41ms
step:397/2330 train_time:22789ms step_avg:57.40ms
step:398/2330 train_time:22850ms step_avg:57.41ms
step:399/2330 train_time:22906ms step_avg:57.41ms
step:400/2330 train_time:22965ms step_avg:57.41ms
step:401/2330 train_time:23021ms step_avg:57.41ms
step:402/2330 train_time:23080ms step_avg:57.41ms
step:403/2330 train_time:23136ms step_avg:57.41ms
step:404/2330 train_time:23196ms step_avg:57.42ms
step:405/2330 train_time:23252ms step_avg:57.41ms
step:406/2330 train_time:23310ms step_avg:57.41ms
step:407/2330 train_time:23366ms step_avg:57.41ms
step:408/2330 train_time:23425ms step_avg:57.41ms
step:409/2330 train_time:23481ms step_avg:57.41ms
step:410/2330 train_time:23540ms step_avg:57.41ms
step:411/2330 train_time:23596ms step_avg:57.41ms
step:412/2330 train_time:23654ms step_avg:57.41ms
step:413/2330 train_time:23710ms step_avg:57.41ms
step:414/2330 train_time:23770ms step_avg:57.42ms
step:415/2330 train_time:23826ms step_avg:57.41ms
step:416/2330 train_time:23886ms step_avg:57.42ms
step:417/2330 train_time:23942ms step_avg:57.42ms
step:418/2330 train_time:24002ms step_avg:57.42ms
step:419/2330 train_time:24058ms step_avg:57.42ms
step:420/2330 train_time:24116ms step_avg:57.42ms
step:421/2330 train_time:24172ms step_avg:57.41ms
step:422/2330 train_time:24231ms step_avg:57.42ms
step:423/2330 train_time:24287ms step_avg:57.42ms
step:424/2330 train_time:24347ms step_avg:57.42ms
step:425/2330 train_time:24402ms step_avg:57.42ms
step:426/2330 train_time:24462ms step_avg:57.42ms
step:427/2330 train_time:24518ms step_avg:57.42ms
step:428/2330 train_time:24577ms step_avg:57.42ms
step:429/2330 train_time:24632ms step_avg:57.42ms
step:430/2330 train_time:24691ms step_avg:57.42ms
step:431/2330 train_time:24747ms step_avg:57.42ms
step:432/2330 train_time:24807ms step_avg:57.42ms
step:433/2330 train_time:24864ms step_avg:57.42ms
step:434/2330 train_time:24923ms step_avg:57.43ms
step:435/2330 train_time:24979ms step_avg:57.42ms
step:436/2330 train_time:25038ms step_avg:57.43ms
step:437/2330 train_time:25094ms step_avg:57.42ms
step:438/2330 train_time:25152ms step_avg:57.43ms
step:439/2330 train_time:25208ms step_avg:57.42ms
step:440/2330 train_time:25267ms step_avg:57.43ms
step:441/2330 train_time:25324ms step_avg:57.42ms
step:442/2330 train_time:25383ms step_avg:57.43ms
step:443/2330 train_time:25439ms step_avg:57.42ms
step:444/2330 train_time:25498ms step_avg:57.43ms
step:445/2330 train_time:25553ms step_avg:57.42ms
step:446/2330 train_time:25612ms step_avg:57.43ms
step:447/2330 train_time:25668ms step_avg:57.42ms
step:448/2330 train_time:25727ms step_avg:57.43ms
step:449/2330 train_time:25783ms step_avg:57.42ms
step:450/2330 train_time:25844ms step_avg:57.43ms
step:451/2330 train_time:25900ms step_avg:57.43ms
step:452/2330 train_time:25960ms step_avg:57.43ms
step:453/2330 train_time:26015ms step_avg:57.43ms
step:454/2330 train_time:26074ms step_avg:57.43ms
step:455/2330 train_time:26129ms step_avg:57.43ms
step:456/2330 train_time:26189ms step_avg:57.43ms
step:457/2330 train_time:26246ms step_avg:57.43ms
step:458/2330 train_time:26304ms step_avg:57.43ms
step:459/2330 train_time:26361ms step_avg:57.43ms
step:460/2330 train_time:26420ms step_avg:57.43ms
step:461/2330 train_time:26476ms step_avg:57.43ms
step:462/2330 train_time:26535ms step_avg:57.43ms
step:463/2330 train_time:26590ms step_avg:57.43ms
step:464/2330 train_time:26650ms step_avg:57.43ms
step:465/2330 train_time:26706ms step_avg:57.43ms
step:466/2330 train_time:26764ms step_avg:57.43ms
step:467/2330 train_time:26820ms step_avg:57.43ms
step:468/2330 train_time:26881ms step_avg:57.44ms
step:469/2330 train_time:26937ms step_avg:57.44ms
step:470/2330 train_time:26997ms step_avg:57.44ms
step:471/2330 train_time:27053ms step_avg:57.44ms
step:472/2330 train_time:27112ms step_avg:57.44ms
step:473/2330 train_time:27168ms step_avg:57.44ms
step:474/2330 train_time:27228ms step_avg:57.44ms
step:475/2330 train_time:27283ms step_avg:57.44ms
step:476/2330 train_time:27343ms step_avg:57.44ms
step:477/2330 train_time:27399ms step_avg:57.44ms
step:478/2330 train_time:27458ms step_avg:57.44ms
step:479/2330 train_time:27514ms step_avg:57.44ms
step:480/2330 train_time:27573ms step_avg:57.44ms
step:481/2330 train_time:27628ms step_avg:57.44ms
step:482/2330 train_time:27688ms step_avg:57.44ms
step:483/2330 train_time:27744ms step_avg:57.44ms
step:484/2330 train_time:27803ms step_avg:57.44ms
step:485/2330 train_time:27859ms step_avg:57.44ms
step:486/2330 train_time:27919ms step_avg:57.45ms
step:487/2330 train_time:27975ms step_avg:57.44ms
step:488/2330 train_time:28034ms step_avg:57.45ms
step:489/2330 train_time:28089ms step_avg:57.44ms
step:490/2330 train_time:28150ms step_avg:57.45ms
step:491/2330 train_time:28205ms step_avg:57.44ms
step:492/2330 train_time:28264ms step_avg:57.45ms
step:493/2330 train_time:28320ms step_avg:57.44ms
step:494/2330 train_time:28379ms step_avg:57.45ms
step:495/2330 train_time:28435ms step_avg:57.44ms
step:496/2330 train_time:28494ms step_avg:57.45ms
step:497/2330 train_time:28550ms step_avg:57.44ms
step:498/2330 train_time:28608ms step_avg:57.45ms
step:499/2330 train_time:28664ms step_avg:57.44ms
step:500/2330 train_time:28723ms step_avg:57.45ms
step:500/2330 val_loss:4.3866 train_time:28801ms step_avg:57.60ms
step:501/2330 train_time:28821ms step_avg:57.53ms
step:502/2330 train_time:28840ms step_avg:57.45ms
step:503/2330 train_time:28898ms step_avg:57.45ms
step:504/2330 train_time:28959ms step_avg:57.46ms
step:505/2330 train_time:29016ms step_avg:57.46ms
step:506/2330 train_time:29077ms step_avg:57.46ms
step:507/2330 train_time:29133ms step_avg:57.46ms
step:508/2330 train_time:29191ms step_avg:57.46ms
step:509/2330 train_time:29247ms step_avg:57.46ms
step:510/2330 train_time:29306ms step_avg:57.46ms
step:511/2330 train_time:29362ms step_avg:57.46ms
step:512/2330 train_time:29420ms step_avg:57.46ms
step:513/2330 train_time:29475ms step_avg:57.46ms
step:514/2330 train_time:29533ms step_avg:57.46ms
step:515/2330 train_time:29589ms step_avg:57.45ms
step:516/2330 train_time:29647ms step_avg:57.46ms
step:517/2330 train_time:29703ms step_avg:57.45ms
step:518/2330 train_time:29762ms step_avg:57.45ms
step:519/2330 train_time:29818ms step_avg:57.45ms
step:520/2330 train_time:29878ms step_avg:57.46ms
step:521/2330 train_time:29935ms step_avg:57.46ms
step:522/2330 train_time:29996ms step_avg:57.46ms
step:523/2330 train_time:30053ms step_avg:57.46ms
step:524/2330 train_time:30112ms step_avg:57.47ms
step:525/2330 train_time:30168ms step_avg:57.46ms
step:526/2330 train_time:30228ms step_avg:57.47ms
step:527/2330 train_time:30284ms step_avg:57.46ms
step:528/2330 train_time:30343ms step_avg:57.47ms
step:529/2330 train_time:30398ms step_avg:57.46ms
step:530/2330 train_time:30457ms step_avg:57.47ms
step:531/2330 train_time:30512ms step_avg:57.46ms
step:532/2330 train_time:30570ms step_avg:57.46ms
step:533/2330 train_time:30626ms step_avg:57.46ms
step:534/2330 train_time:30684ms step_avg:57.46ms
step:535/2330 train_time:30740ms step_avg:57.46ms
step:536/2330 train_time:30799ms step_avg:57.46ms
step:537/2330 train_time:30855ms step_avg:57.46ms
step:538/2330 train_time:30917ms step_avg:57.47ms
step:539/2330 train_time:30973ms step_avg:57.46ms
step:540/2330 train_time:31033ms step_avg:57.47ms
step:541/2330 train_time:31088ms step_avg:57.46ms
step:542/2330 train_time:31150ms step_avg:57.47ms
step:543/2330 train_time:31206ms step_avg:57.47ms
step:544/2330 train_time:31264ms step_avg:57.47ms
step:545/2330 train_time:31320ms step_avg:57.47ms
step:546/2330 train_time:31379ms step_avg:57.47ms
step:547/2330 train_time:31436ms step_avg:57.47ms
step:548/2330 train_time:31494ms step_avg:57.47ms
step:549/2330 train_time:31549ms step_avg:57.47ms
step:550/2330 train_time:31608ms step_avg:57.47ms
step:551/2330 train_time:31664ms step_avg:57.47ms
step:552/2330 train_time:31723ms step_avg:57.47ms
step:553/2330 train_time:31779ms step_avg:57.47ms
step:554/2330 train_time:31839ms step_avg:57.47ms
step:555/2330 train_time:31895ms step_avg:57.47ms
step:556/2330 train_time:31954ms step_avg:57.47ms
step:557/2330 train_time:32010ms step_avg:57.47ms
step:558/2330 train_time:32071ms step_avg:57.47ms
step:559/2330 train_time:32127ms step_avg:57.47ms
step:560/2330 train_time:32187ms step_avg:57.48ms
step:561/2330 train_time:32243ms step_avg:57.47ms
step:562/2330 train_time:32302ms step_avg:57.48ms
step:563/2330 train_time:32358ms step_avg:57.47ms
step:564/2330 train_time:32417ms step_avg:57.48ms
step:565/2330 train_time:32472ms step_avg:57.47ms
step:566/2330 train_time:32532ms step_avg:57.48ms
step:567/2330 train_time:32587ms step_avg:57.47ms
step:568/2330 train_time:32645ms step_avg:57.47ms
step:569/2330 train_time:32702ms step_avg:57.47ms
step:570/2330 train_time:32761ms step_avg:57.47ms
step:571/2330 train_time:32817ms step_avg:57.47ms
step:572/2330 train_time:32876ms step_avg:57.48ms
step:573/2330 train_time:32932ms step_avg:57.47ms
step:574/2330 train_time:32992ms step_avg:57.48ms
step:575/2330 train_time:33048ms step_avg:57.48ms
step:576/2330 train_time:33108ms step_avg:57.48ms
step:577/2330 train_time:33164ms step_avg:57.48ms
step:578/2330 train_time:33224ms step_avg:57.48ms
step:579/2330 train_time:33280ms step_avg:57.48ms
step:580/2330 train_time:33340ms step_avg:57.48ms
step:581/2330 train_time:33396ms step_avg:57.48ms
step:582/2330 train_time:33454ms step_avg:57.48ms
step:583/2330 train_time:33510ms step_avg:57.48ms
step:584/2330 train_time:33569ms step_avg:57.48ms
step:585/2330 train_time:33625ms step_avg:57.48ms
step:586/2330 train_time:33685ms step_avg:57.48ms
step:587/2330 train_time:33742ms step_avg:57.48ms
step:588/2330 train_time:33800ms step_avg:57.48ms
step:589/2330 train_time:33856ms step_avg:57.48ms
step:590/2330 train_time:33915ms step_avg:57.48ms
step:591/2330 train_time:33971ms step_avg:57.48ms
step:592/2330 train_time:34031ms step_avg:57.49ms
step:593/2330 train_time:34088ms step_avg:57.48ms
step:594/2330 train_time:34147ms step_avg:57.49ms
step:595/2330 train_time:34202ms step_avg:57.48ms
step:596/2330 train_time:34262ms step_avg:57.49ms
step:597/2330 train_time:34318ms step_avg:57.48ms
step:598/2330 train_time:34377ms step_avg:57.49ms
step:599/2330 train_time:34433ms step_avg:57.48ms
step:600/2330 train_time:34492ms step_avg:57.49ms
step:601/2330 train_time:34547ms step_avg:57.48ms
step:602/2330 train_time:34607ms step_avg:57.49ms
step:603/2330 train_time:34663ms step_avg:57.48ms
step:604/2330 train_time:34722ms step_avg:57.49ms
step:605/2330 train_time:34779ms step_avg:57.49ms
step:606/2330 train_time:34839ms step_avg:57.49ms
step:607/2330 train_time:34894ms step_avg:57.49ms
step:608/2330 train_time:34954ms step_avg:57.49ms
step:609/2330 train_time:35010ms step_avg:57.49ms
step:610/2330 train_time:35069ms step_avg:57.49ms
step:611/2330 train_time:35126ms step_avg:57.49ms
step:612/2330 train_time:35185ms step_avg:57.49ms
step:613/2330 train_time:35241ms step_avg:57.49ms
step:614/2330 train_time:35301ms step_avg:57.49ms
step:615/2330 train_time:35357ms step_avg:57.49ms
step:616/2330 train_time:35416ms step_avg:57.49ms
step:617/2330 train_time:35472ms step_avg:57.49ms
step:618/2330 train_time:35532ms step_avg:57.49ms
step:619/2330 train_time:35587ms step_avg:57.49ms
step:620/2330 train_time:35647ms step_avg:57.50ms
step:621/2330 train_time:35703ms step_avg:57.49ms
step:622/2330 train_time:35762ms step_avg:57.50ms
step:623/2330 train_time:35818ms step_avg:57.49ms
step:624/2330 train_time:35878ms step_avg:57.50ms
step:625/2330 train_time:35934ms step_avg:57.49ms
step:626/2330 train_time:35993ms step_avg:57.50ms
step:627/2330 train_time:36048ms step_avg:57.49ms
step:628/2330 train_time:36108ms step_avg:57.50ms
step:629/2330 train_time:36165ms step_avg:57.50ms
step:630/2330 train_time:36224ms step_avg:57.50ms
step:631/2330 train_time:36280ms step_avg:57.50ms
step:632/2330 train_time:36339ms step_avg:57.50ms
step:633/2330 train_time:36395ms step_avg:57.50ms
step:634/2330 train_time:36455ms step_avg:57.50ms
step:635/2330 train_time:36510ms step_avg:57.50ms
step:636/2330 train_time:36570ms step_avg:57.50ms
step:637/2330 train_time:36626ms step_avg:57.50ms
step:638/2330 train_time:36686ms step_avg:57.50ms
step:639/2330 train_time:36742ms step_avg:57.50ms
step:640/2330 train_time:36801ms step_avg:57.50ms
step:641/2330 train_time:36858ms step_avg:57.50ms
step:642/2330 train_time:36917ms step_avg:57.50ms
step:643/2330 train_time:36973ms step_avg:57.50ms
step:644/2330 train_time:37031ms step_avg:57.50ms
step:645/2330 train_time:37087ms step_avg:57.50ms
step:646/2330 train_time:37147ms step_avg:57.50ms
step:647/2330 train_time:37203ms step_avg:57.50ms
step:648/2330 train_time:37263ms step_avg:57.51ms
step:649/2330 train_time:37320ms step_avg:57.50ms
step:650/2330 train_time:37379ms step_avg:57.51ms
step:651/2330 train_time:37435ms step_avg:57.50ms
step:652/2330 train_time:37494ms step_avg:57.51ms
step:653/2330 train_time:37550ms step_avg:57.50ms
step:654/2330 train_time:37610ms step_avg:57.51ms
step:655/2330 train_time:37666ms step_avg:57.51ms
step:656/2330 train_time:37725ms step_avg:57.51ms
step:657/2330 train_time:37782ms step_avg:57.51ms
step:658/2330 train_time:37842ms step_avg:57.51ms
step:659/2330 train_time:37899ms step_avg:57.51ms
step:660/2330 train_time:37957ms step_avg:57.51ms
step:661/2330 train_time:38013ms step_avg:57.51ms
step:662/2330 train_time:38073ms step_avg:57.51ms
step:663/2330 train_time:38129ms step_avg:57.51ms
step:664/2330 train_time:38189ms step_avg:57.51ms
step:665/2330 train_time:38245ms step_avg:57.51ms
step:666/2330 train_time:38306ms step_avg:57.52ms
step:667/2330 train_time:38362ms step_avg:57.51ms
step:668/2330 train_time:38421ms step_avg:57.52ms
step:669/2330 train_time:38476ms step_avg:57.51ms
step:670/2330 train_time:38536ms step_avg:57.52ms
step:671/2330 train_time:38592ms step_avg:57.51ms
step:672/2330 train_time:38651ms step_avg:57.52ms
step:673/2330 train_time:38707ms step_avg:57.51ms
step:674/2330 train_time:38767ms step_avg:57.52ms
step:675/2330 train_time:38824ms step_avg:57.52ms
step:676/2330 train_time:38884ms step_avg:57.52ms
step:677/2330 train_time:38941ms step_avg:57.52ms
step:678/2330 train_time:39000ms step_avg:57.52ms
step:679/2330 train_time:39055ms step_avg:57.52ms
step:680/2330 train_time:39115ms step_avg:57.52ms
step:681/2330 train_time:39171ms step_avg:57.52ms
step:682/2330 train_time:39232ms step_avg:57.52ms
step:683/2330 train_time:39287ms step_avg:57.52ms
step:684/2330 train_time:39346ms step_avg:57.52ms
step:685/2330 train_time:39403ms step_avg:57.52ms
step:686/2330 train_time:39463ms step_avg:57.53ms
step:687/2330 train_time:39519ms step_avg:57.52ms
step:688/2330 train_time:39578ms step_avg:57.53ms
step:689/2330 train_time:39633ms step_avg:57.52ms
step:690/2330 train_time:39693ms step_avg:57.53ms
step:691/2330 train_time:39748ms step_avg:57.52ms
step:692/2330 train_time:39809ms step_avg:57.53ms
step:693/2330 train_time:39865ms step_avg:57.53ms
step:694/2330 train_time:39925ms step_avg:57.53ms
step:695/2330 train_time:39982ms step_avg:57.53ms
step:696/2330 train_time:40042ms step_avg:57.53ms
step:697/2330 train_time:40097ms step_avg:57.53ms
step:698/2330 train_time:40156ms step_avg:57.53ms
step:699/2330 train_time:40213ms step_avg:57.53ms
step:700/2330 train_time:40273ms step_avg:57.53ms
step:701/2330 train_time:40328ms step_avg:57.53ms
step:702/2330 train_time:40388ms step_avg:57.53ms
step:703/2330 train_time:40445ms step_avg:57.53ms
step:704/2330 train_time:40504ms step_avg:57.53ms
step:705/2330 train_time:40559ms step_avg:57.53ms
step:706/2330 train_time:40619ms step_avg:57.53ms
step:707/2330 train_time:40675ms step_avg:57.53ms
step:708/2330 train_time:40735ms step_avg:57.53ms
step:709/2330 train_time:40790ms step_avg:57.53ms
step:710/2330 train_time:40850ms step_avg:57.54ms
step:711/2330 train_time:40906ms step_avg:57.53ms
step:712/2330 train_time:40965ms step_avg:57.54ms
step:713/2330 train_time:41021ms step_avg:57.53ms
step:714/2330 train_time:41081ms step_avg:57.54ms
step:715/2330 train_time:41136ms step_avg:57.53ms
step:716/2330 train_time:41196ms step_avg:57.54ms
step:717/2330 train_time:41252ms step_avg:57.53ms
step:718/2330 train_time:41311ms step_avg:57.54ms
step:719/2330 train_time:41367ms step_avg:57.53ms
step:720/2330 train_time:41428ms step_avg:57.54ms
step:721/2330 train_time:41484ms step_avg:57.54ms
step:722/2330 train_time:41544ms step_avg:57.54ms
step:723/2330 train_time:41601ms step_avg:57.54ms
step:724/2330 train_time:41660ms step_avg:57.54ms
step:725/2330 train_time:41716ms step_avg:57.54ms
step:726/2330 train_time:41775ms step_avg:57.54ms
step:727/2330 train_time:41831ms step_avg:57.54ms
step:728/2330 train_time:41891ms step_avg:57.54ms
step:729/2330 train_time:41946ms step_avg:57.54ms
step:730/2330 train_time:42006ms step_avg:57.54ms
step:731/2330 train_time:42063ms step_avg:57.54ms
step:732/2330 train_time:42123ms step_avg:57.54ms
step:733/2330 train_time:42179ms step_avg:57.54ms
step:734/2330 train_time:42238ms step_avg:57.54ms
step:735/2330 train_time:42294ms step_avg:57.54ms
step:736/2330 train_time:42353ms step_avg:57.54ms
step:737/2330 train_time:42409ms step_avg:57.54ms
step:738/2330 train_time:42469ms step_avg:57.55ms
step:739/2330 train_time:42524ms step_avg:57.54ms
step:740/2330 train_time:42585ms step_avg:57.55ms
step:741/2330 train_time:42642ms step_avg:57.55ms
step:742/2330 train_time:42701ms step_avg:57.55ms
step:743/2330 train_time:42757ms step_avg:57.55ms
step:744/2330 train_time:42816ms step_avg:57.55ms
step:745/2330 train_time:42872ms step_avg:57.55ms
step:746/2330 train_time:42931ms step_avg:57.55ms
step:747/2330 train_time:42987ms step_avg:57.55ms
step:748/2330 train_time:43047ms step_avg:57.55ms
step:749/2330 train_time:43103ms step_avg:57.55ms
step:750/2330 train_time:43163ms step_avg:57.55ms
step:750/2330 val_loss:4.2019 train_time:43242ms step_avg:57.66ms
step:751/2330 train_time:43261ms step_avg:57.60ms
step:752/2330 train_time:43281ms step_avg:57.55ms
step:753/2330 train_time:43337ms step_avg:57.55ms
step:754/2330 train_time:43401ms step_avg:57.56ms
step:755/2330 train_time:43459ms step_avg:57.56ms
step:756/2330 train_time:43520ms step_avg:57.57ms
step:757/2330 train_time:43577ms step_avg:57.56ms
step:758/2330 train_time:43636ms step_avg:57.57ms
step:759/2330 train_time:43691ms step_avg:57.56ms
step:760/2330 train_time:43751ms step_avg:57.57ms
step:761/2330 train_time:43807ms step_avg:57.56ms
step:762/2330 train_time:43866ms step_avg:57.57ms
step:763/2330 train_time:43921ms step_avg:57.56ms
step:764/2330 train_time:43980ms step_avg:57.56ms
step:765/2330 train_time:44036ms step_avg:57.56ms
step:766/2330 train_time:44094ms step_avg:57.56ms
step:767/2330 train_time:44150ms step_avg:57.56ms
step:768/2330 train_time:44209ms step_avg:57.56ms
step:769/2330 train_time:44267ms step_avg:57.56ms
step:770/2330 train_time:44329ms step_avg:57.57ms
step:771/2330 train_time:44387ms step_avg:57.57ms
step:772/2330 train_time:44448ms step_avg:57.58ms
step:773/2330 train_time:44507ms step_avg:57.58ms
step:774/2330 train_time:44567ms step_avg:57.58ms
step:775/2330 train_time:44625ms step_avg:57.58ms
step:776/2330 train_time:44685ms step_avg:57.58ms
step:777/2330 train_time:44742ms step_avg:57.58ms
step:778/2330 train_time:44802ms step_avg:57.59ms
step:779/2330 train_time:44859ms step_avg:57.59ms
step:780/2330 train_time:44918ms step_avg:57.59ms
step:781/2330 train_time:44974ms step_avg:57.59ms
step:782/2330 train_time:45033ms step_avg:57.59ms
step:783/2330 train_time:45090ms step_avg:57.59ms
step:784/2330 train_time:45149ms step_avg:57.59ms
step:785/2330 train_time:45206ms step_avg:57.59ms
step:786/2330 train_time:45266ms step_avg:57.59ms
step:787/2330 train_time:45323ms step_avg:57.59ms
step:788/2330 train_time:45384ms step_avg:57.59ms
step:789/2330 train_time:45441ms step_avg:57.59ms
step:790/2330 train_time:45502ms step_avg:57.60ms
step:791/2330 train_time:45559ms step_avg:57.60ms
step:792/2330 train_time:45619ms step_avg:57.60ms
step:793/2330 train_time:45676ms step_avg:57.60ms
step:794/2330 train_time:45736ms step_avg:57.60ms
step:795/2330 train_time:45792ms step_avg:57.60ms
step:796/2330 train_time:45851ms step_avg:57.60ms
step:797/2330 train_time:45908ms step_avg:57.60ms
step:798/2330 train_time:45968ms step_avg:57.60ms
step:799/2330 train_time:46024ms step_avg:57.60ms
step:800/2330 train_time:46084ms step_avg:57.61ms
step:801/2330 train_time:46141ms step_avg:57.60ms
step:802/2330 train_time:46201ms step_avg:57.61ms
step:803/2330 train_time:46258ms step_avg:57.61ms
step:804/2330 train_time:46317ms step_avg:57.61ms
step:805/2330 train_time:46375ms step_avg:57.61ms
step:806/2330 train_time:46435ms step_avg:57.61ms
step:807/2330 train_time:46491ms step_avg:57.61ms
step:808/2330 train_time:46553ms step_avg:57.62ms
step:809/2330 train_time:46610ms step_avg:57.61ms
step:810/2330 train_time:46672ms step_avg:57.62ms
step:811/2330 train_time:46728ms step_avg:57.62ms
step:812/2330 train_time:46788ms step_avg:57.62ms
step:813/2330 train_time:46845ms step_avg:57.62ms
step:814/2330 train_time:46905ms step_avg:57.62ms
step:815/2330 train_time:46962ms step_avg:57.62ms
step:816/2330 train_time:47022ms step_avg:57.62ms
step:817/2330 train_time:47078ms step_avg:57.62ms
step:818/2330 train_time:47137ms step_avg:57.63ms
step:819/2330 train_time:47195ms step_avg:57.62ms
step:820/2330 train_time:47254ms step_avg:57.63ms
step:821/2330 train_time:47311ms step_avg:57.63ms
step:822/2330 train_time:47372ms step_avg:57.63ms
step:823/2330 train_time:47430ms step_avg:57.63ms
step:824/2330 train_time:47489ms step_avg:57.63ms
step:825/2330 train_time:47547ms step_avg:57.63ms
step:826/2330 train_time:47607ms step_avg:57.64ms
step:827/2330 train_time:47664ms step_avg:57.63ms
step:828/2330 train_time:47724ms step_avg:57.64ms
step:829/2330 train_time:47781ms step_avg:57.64ms
step:830/2330 train_time:47841ms step_avg:57.64ms
step:831/2330 train_time:47898ms step_avg:57.64ms
step:832/2330 train_time:47958ms step_avg:57.64ms
step:833/2330 train_time:48014ms step_avg:57.64ms
step:834/2330 train_time:48073ms step_avg:57.64ms
step:835/2330 train_time:48130ms step_avg:57.64ms
step:836/2330 train_time:48190ms step_avg:57.64ms
step:837/2330 train_time:48247ms step_avg:57.64ms
step:838/2330 train_time:48307ms step_avg:57.65ms
step:839/2330 train_time:48366ms step_avg:57.65ms
step:840/2330 train_time:48426ms step_avg:57.65ms
step:841/2330 train_time:48483ms step_avg:57.65ms
step:842/2330 train_time:48543ms step_avg:57.65ms
step:843/2330 train_time:48600ms step_avg:57.65ms
step:844/2330 train_time:48660ms step_avg:57.65ms
step:845/2330 train_time:48717ms step_avg:57.65ms
step:846/2330 train_time:48777ms step_avg:57.66ms
step:847/2330 train_time:48834ms step_avg:57.66ms
step:848/2330 train_time:48894ms step_avg:57.66ms
step:849/2330 train_time:48951ms step_avg:57.66ms
step:850/2330 train_time:49011ms step_avg:57.66ms
step:851/2330 train_time:49067ms step_avg:57.66ms
step:852/2330 train_time:49127ms step_avg:57.66ms
step:853/2330 train_time:49184ms step_avg:57.66ms
step:854/2330 train_time:49244ms step_avg:57.66ms
step:855/2330 train_time:49301ms step_avg:57.66ms
step:856/2330 train_time:49360ms step_avg:57.66ms
step:857/2330 train_time:49416ms step_avg:57.66ms
step:858/2330 train_time:49477ms step_avg:57.67ms
step:859/2330 train_time:49533ms step_avg:57.66ms
step:860/2330 train_time:49594ms step_avg:57.67ms
step:861/2330 train_time:49652ms step_avg:57.67ms
step:862/2330 train_time:49711ms step_avg:57.67ms
step:863/2330 train_time:49768ms step_avg:57.67ms
step:864/2330 train_time:49828ms step_avg:57.67ms
step:865/2330 train_time:49886ms step_avg:57.67ms
step:866/2330 train_time:49945ms step_avg:57.67ms
step:867/2330 train_time:50002ms step_avg:57.67ms
step:868/2330 train_time:50062ms step_avg:57.68ms
step:869/2330 train_time:50119ms step_avg:57.67ms
step:870/2330 train_time:50179ms step_avg:57.68ms
step:871/2330 train_time:50235ms step_avg:57.68ms
step:872/2330 train_time:50295ms step_avg:57.68ms
step:873/2330 train_time:50353ms step_avg:57.68ms
step:874/2330 train_time:50413ms step_avg:57.68ms
step:875/2330 train_time:50470ms step_avg:57.68ms
step:876/2330 train_time:50530ms step_avg:57.68ms
step:877/2330 train_time:50587ms step_avg:57.68ms
step:878/2330 train_time:50647ms step_avg:57.68ms
step:879/2330 train_time:50704ms step_avg:57.68ms
step:880/2330 train_time:50764ms step_avg:57.69ms
step:881/2330 train_time:50823ms step_avg:57.69ms
step:882/2330 train_time:50882ms step_avg:57.69ms
step:883/2330 train_time:50939ms step_avg:57.69ms
step:884/2330 train_time:50998ms step_avg:57.69ms
step:885/2330 train_time:51055ms step_avg:57.69ms
step:886/2330 train_time:51115ms step_avg:57.69ms
step:887/2330 train_time:51172ms step_avg:57.69ms
step:888/2330 train_time:51232ms step_avg:57.69ms
step:889/2330 train_time:51288ms step_avg:57.69ms
step:890/2330 train_time:51348ms step_avg:57.69ms
step:891/2330 train_time:51405ms step_avg:57.69ms
step:892/2330 train_time:51466ms step_avg:57.70ms
step:893/2330 train_time:51524ms step_avg:57.70ms
step:894/2330 train_time:51583ms step_avg:57.70ms
step:895/2330 train_time:51640ms step_avg:57.70ms
step:896/2330 train_time:51700ms step_avg:57.70ms
step:897/2330 train_time:51756ms step_avg:57.70ms
step:898/2330 train_time:51816ms step_avg:57.70ms
step:899/2330 train_time:51872ms step_avg:57.70ms
step:900/2330 train_time:51932ms step_avg:57.70ms
step:901/2330 train_time:51988ms step_avg:57.70ms
step:902/2330 train_time:52050ms step_avg:57.70ms
step:903/2330 train_time:52107ms step_avg:57.70ms
step:904/2330 train_time:52167ms step_avg:57.71ms
step:905/2330 train_time:52224ms step_avg:57.71ms
step:906/2330 train_time:52284ms step_avg:57.71ms
step:907/2330 train_time:52341ms step_avg:57.71ms
step:908/2330 train_time:52401ms step_avg:57.71ms
step:909/2330 train_time:52458ms step_avg:57.71ms
step:910/2330 train_time:52517ms step_avg:57.71ms
step:911/2330 train_time:52575ms step_avg:57.71ms
step:912/2330 train_time:52635ms step_avg:57.71ms
step:913/2330 train_time:52691ms step_avg:57.71ms
step:914/2330 train_time:52751ms step_avg:57.71ms
step:915/2330 train_time:52808ms step_avg:57.71ms
step:916/2330 train_time:52869ms step_avg:57.72ms
step:917/2330 train_time:52926ms step_avg:57.72ms
step:918/2330 train_time:52986ms step_avg:57.72ms
step:919/2330 train_time:53042ms step_avg:57.72ms
step:920/2330 train_time:53103ms step_avg:57.72ms
step:921/2330 train_time:53160ms step_avg:57.72ms
step:922/2330 train_time:53220ms step_avg:57.72ms
step:923/2330 train_time:53276ms step_avg:57.72ms
step:924/2330 train_time:53336ms step_avg:57.72ms
step:925/2330 train_time:53393ms step_avg:57.72ms
step:926/2330 train_time:53454ms step_avg:57.73ms
step:927/2330 train_time:53511ms step_avg:57.73ms
step:928/2330 train_time:53571ms step_avg:57.73ms
step:929/2330 train_time:53628ms step_avg:57.73ms
step:930/2330 train_time:53687ms step_avg:57.73ms
step:931/2330 train_time:53745ms step_avg:57.73ms
step:932/2330 train_time:53805ms step_avg:57.73ms
step:933/2330 train_time:53862ms step_avg:57.73ms
step:934/2330 train_time:53922ms step_avg:57.73ms
step:935/2330 train_time:53979ms step_avg:57.73ms
step:936/2330 train_time:54039ms step_avg:57.73ms
step:937/2330 train_time:54096ms step_avg:57.73ms
step:938/2330 train_time:54155ms step_avg:57.73ms
step:939/2330 train_time:54213ms step_avg:57.73ms
step:940/2330 train_time:54272ms step_avg:57.74ms
step:941/2330 train_time:54329ms step_avg:57.74ms
step:942/2330 train_time:54389ms step_avg:57.74ms
step:943/2330 train_time:54446ms step_avg:57.74ms
step:944/2330 train_time:54506ms step_avg:57.74ms
step:945/2330 train_time:54563ms step_avg:57.74ms
step:946/2330 train_time:54622ms step_avg:57.74ms
step:947/2330 train_time:54680ms step_avg:57.74ms
step:948/2330 train_time:54740ms step_avg:57.74ms
step:949/2330 train_time:54797ms step_avg:57.74ms
step:950/2330 train_time:54856ms step_avg:57.74ms
step:951/2330 train_time:54912ms step_avg:57.74ms
step:952/2330 train_time:54972ms step_avg:57.74ms
step:953/2330 train_time:55029ms step_avg:57.74ms
step:954/2330 train_time:55089ms step_avg:57.75ms
step:955/2330 train_time:55146ms step_avg:57.74ms
step:956/2330 train_time:55207ms step_avg:57.75ms
step:957/2330 train_time:55264ms step_avg:57.75ms
step:958/2330 train_time:55324ms step_avg:57.75ms
step:959/2330 train_time:55381ms step_avg:57.75ms
step:960/2330 train_time:55441ms step_avg:57.75ms
step:961/2330 train_time:55499ms step_avg:57.75ms
step:962/2330 train_time:55558ms step_avg:57.75ms
step:963/2330 train_time:55614ms step_avg:57.75ms
step:964/2330 train_time:55674ms step_avg:57.75ms
step:965/2330 train_time:55731ms step_avg:57.75ms
step:966/2330 train_time:55791ms step_avg:57.75ms
step:967/2330 train_time:55848ms step_avg:57.75ms
step:968/2330 train_time:55909ms step_avg:57.76ms
step:969/2330 train_time:55966ms step_avg:57.76ms
step:970/2330 train_time:56025ms step_avg:57.76ms
step:971/2330 train_time:56082ms step_avg:57.76ms
step:972/2330 train_time:56142ms step_avg:57.76ms
step:973/2330 train_time:56199ms step_avg:57.76ms
step:974/2330 train_time:56260ms step_avg:57.76ms
step:975/2330 train_time:56316ms step_avg:57.76ms
step:976/2330 train_time:56376ms step_avg:57.76ms
step:977/2330 train_time:56433ms step_avg:57.76ms
step:978/2330 train_time:56493ms step_avg:57.76ms
step:979/2330 train_time:56549ms step_avg:57.76ms
step:980/2330 train_time:56610ms step_avg:57.77ms
step:981/2330 train_time:56668ms step_avg:57.77ms
step:982/2330 train_time:56727ms step_avg:57.77ms
step:983/2330 train_time:56785ms step_avg:57.77ms
step:984/2330 train_time:56844ms step_avg:57.77ms
step:985/2330 train_time:56902ms step_avg:57.77ms
step:986/2330 train_time:56961ms step_avg:57.77ms
step:987/2330 train_time:57018ms step_avg:57.77ms
step:988/2330 train_time:57078ms step_avg:57.77ms
step:989/2330 train_time:57135ms step_avg:57.77ms
step:990/2330 train_time:57194ms step_avg:57.77ms
step:991/2330 train_time:57251ms step_avg:57.77ms
step:992/2330 train_time:57311ms step_avg:57.77ms
step:993/2330 train_time:57368ms step_avg:57.77ms
step:994/2330 train_time:57428ms step_avg:57.77ms
step:995/2330 train_time:57485ms step_avg:57.77ms
step:996/2330 train_time:57545ms step_avg:57.78ms
step:997/2330 train_time:57602ms step_avg:57.78ms
step:998/2330 train_time:57662ms step_avg:57.78ms
step:999/2330 train_time:57720ms step_avg:57.78ms
step:1000/2330 train_time:57780ms step_avg:57.78ms
step:1000/2330 val_loss:4.0620 train_time:57860ms step_avg:57.86ms
step:1001/2330 train_time:57880ms step_avg:57.82ms
step:1002/2330 train_time:57900ms step_avg:57.78ms
step:1003/2330 train_time:57953ms step_avg:57.78ms
step:1004/2330 train_time:58019ms step_avg:57.79ms
step:1005/2330 train_time:58075ms step_avg:57.79ms
step:1006/2330 train_time:58139ms step_avg:57.79ms
step:1007/2330 train_time:58195ms step_avg:57.79ms
step:1008/2330 train_time:58254ms step_avg:57.79ms
step:1009/2330 train_time:58310ms step_avg:57.79ms
step:1010/2330 train_time:58370ms step_avg:57.79ms
step:1011/2330 train_time:58427ms step_avg:57.79ms
step:1012/2330 train_time:58486ms step_avg:57.79ms
step:1013/2330 train_time:58542ms step_avg:57.79ms
step:1014/2330 train_time:58601ms step_avg:57.79ms
step:1015/2330 train_time:58657ms step_avg:57.79ms
step:1016/2330 train_time:58716ms step_avg:57.79ms
step:1017/2330 train_time:58774ms step_avg:57.79ms
step:1018/2330 train_time:58840ms step_avg:57.80ms
step:1019/2330 train_time:58898ms step_avg:57.80ms
step:1020/2330 train_time:58959ms step_avg:57.80ms
step:1021/2330 train_time:59016ms step_avg:57.80ms
step:1022/2330 train_time:59077ms step_avg:57.81ms
step:1023/2330 train_time:59134ms step_avg:57.80ms
step:1024/2330 train_time:59194ms step_avg:57.81ms
step:1025/2330 train_time:59251ms step_avg:57.81ms
step:1026/2330 train_time:59310ms step_avg:57.81ms
step:1027/2330 train_time:59366ms step_avg:57.81ms
step:1028/2330 train_time:59426ms step_avg:57.81ms
step:1029/2330 train_time:59483ms step_avg:57.81ms
step:1030/2330 train_time:59542ms step_avg:57.81ms
step:1031/2330 train_time:59598ms step_avg:57.81ms
step:1032/2330 train_time:59657ms step_avg:57.81ms
step:1033/2330 train_time:59715ms step_avg:57.81ms
step:1034/2330 train_time:59775ms step_avg:57.81ms
step:1035/2330 train_time:59833ms step_avg:57.81ms
step:1036/2330 train_time:59895ms step_avg:57.81ms
step:1037/2330 train_time:59952ms step_avg:57.81ms
step:1038/2330 train_time:60014ms step_avg:57.82ms
step:1039/2330 train_time:60072ms step_avg:57.82ms
step:1040/2330 train_time:60132ms step_avg:57.82ms
step:1041/2330 train_time:60189ms step_avg:57.82ms
step:1042/2330 train_time:60249ms step_avg:57.82ms
step:1043/2330 train_time:60306ms step_avg:57.82ms
step:1044/2330 train_time:60365ms step_avg:57.82ms
step:1045/2330 train_time:60422ms step_avg:57.82ms
step:1046/2330 train_time:60481ms step_avg:57.82ms
step:1047/2330 train_time:60537ms step_avg:57.82ms
step:1048/2330 train_time:60597ms step_avg:57.82ms
step:1049/2330 train_time:60654ms step_avg:57.82ms
step:1050/2330 train_time:60715ms step_avg:57.82ms
step:1051/2330 train_time:60772ms step_avg:57.82ms
step:1052/2330 train_time:60832ms step_avg:57.82ms
step:1053/2330 train_time:60889ms step_avg:57.82ms
step:1054/2330 train_time:60952ms step_avg:57.83ms
step:1055/2330 train_time:61008ms step_avg:57.83ms
step:1056/2330 train_time:61070ms step_avg:57.83ms
step:1057/2330 train_time:61126ms step_avg:57.83ms
step:1058/2330 train_time:61188ms step_avg:57.83ms
step:1059/2330 train_time:61245ms step_avg:57.83ms
step:1060/2330 train_time:61306ms step_avg:57.84ms
step:1061/2330 train_time:61363ms step_avg:57.83ms
step:1062/2330 train_time:61421ms step_avg:57.84ms
step:1063/2330 train_time:61478ms step_avg:57.83ms
step:1064/2330 train_time:61538ms step_avg:57.84ms
step:1065/2330 train_time:61594ms step_avg:57.83ms
step:1066/2330 train_time:61654ms step_avg:57.84ms
step:1067/2330 train_time:61711ms step_avg:57.84ms
step:1068/2330 train_time:61771ms step_avg:57.84ms
step:1069/2330 train_time:61829ms step_avg:57.84ms
step:1070/2330 train_time:61890ms step_avg:57.84ms
step:1071/2330 train_time:61948ms step_avg:57.84ms
step:1072/2330 train_time:62008ms step_avg:57.84ms
step:1073/2330 train_time:62064ms step_avg:57.84ms
step:1074/2330 train_time:62126ms step_avg:57.85ms
step:1075/2330 train_time:62183ms step_avg:57.84ms
step:1076/2330 train_time:62242ms step_avg:57.85ms
step:1077/2330 train_time:62300ms step_avg:57.85ms
step:1078/2330 train_time:62360ms step_avg:57.85ms
step:1079/2330 train_time:62417ms step_avg:57.85ms
step:1080/2330 train_time:62475ms step_avg:57.85ms
step:1081/2330 train_time:62532ms step_avg:57.85ms
step:1082/2330 train_time:62592ms step_avg:57.85ms
step:1083/2330 train_time:62649ms step_avg:57.85ms
step:1084/2330 train_time:62709ms step_avg:57.85ms
step:1085/2330 train_time:62767ms step_avg:57.85ms
step:1086/2330 train_time:62826ms step_avg:57.85ms
step:1087/2330 train_time:62883ms step_avg:57.85ms
step:1088/2330 train_time:62943ms step_avg:57.85ms
step:1089/2330 train_time:62999ms step_avg:57.85ms
step:1090/2330 train_time:63060ms step_avg:57.85ms
step:1091/2330 train_time:63117ms step_avg:57.85ms
step:1092/2330 train_time:63176ms step_avg:57.85ms
step:1093/2330 train_time:63233ms step_avg:57.85ms
step:1094/2330 train_time:63294ms step_avg:57.86ms
step:1095/2330 train_time:63351ms step_avg:57.85ms
step:1096/2330 train_time:63411ms step_avg:57.86ms
step:1097/2330 train_time:63469ms step_avg:57.86ms
step:1098/2330 train_time:63529ms step_avg:57.86ms
step:1099/2330 train_time:63586ms step_avg:57.86ms
step:1100/2330 train_time:63646ms step_avg:57.86ms
step:1101/2330 train_time:63703ms step_avg:57.86ms
step:1102/2330 train_time:63762ms step_avg:57.86ms
step:1103/2330 train_time:63819ms step_avg:57.86ms
step:1104/2330 train_time:63879ms step_avg:57.86ms
step:1105/2330 train_time:63936ms step_avg:57.86ms
step:1106/2330 train_time:63996ms step_avg:57.86ms
step:1107/2330 train_time:64054ms step_avg:57.86ms
step:1108/2330 train_time:64114ms step_avg:57.86ms
step:1109/2330 train_time:64170ms step_avg:57.86ms
step:1110/2330 train_time:64231ms step_avg:57.87ms
step:1111/2330 train_time:64288ms step_avg:57.86ms
step:1112/2330 train_time:64348ms step_avg:57.87ms
step:1113/2330 train_time:64404ms step_avg:57.87ms
step:1114/2330 train_time:64465ms step_avg:57.87ms
step:1115/2330 train_time:64521ms step_avg:57.87ms
step:1116/2330 train_time:64582ms step_avg:57.87ms
step:1117/2330 train_time:64639ms step_avg:57.87ms
step:1118/2330 train_time:64698ms step_avg:57.87ms
step:1119/2330 train_time:64755ms step_avg:57.87ms
step:1120/2330 train_time:64815ms step_avg:57.87ms
step:1121/2330 train_time:64871ms step_avg:57.87ms
step:1122/2330 train_time:64933ms step_avg:57.87ms
step:1123/2330 train_time:64989ms step_avg:57.87ms
step:1124/2330 train_time:65051ms step_avg:57.87ms
step:1125/2330 train_time:65108ms step_avg:57.87ms
step:1126/2330 train_time:65169ms step_avg:57.88ms
step:1127/2330 train_time:65226ms step_avg:57.88ms
step:1128/2330 train_time:65286ms step_avg:57.88ms
step:1129/2330 train_time:65344ms step_avg:57.88ms
step:1130/2330 train_time:65403ms step_avg:57.88ms
step:1131/2330 train_time:65461ms step_avg:57.88ms
step:1132/2330 train_time:65520ms step_avg:57.88ms
step:1133/2330 train_time:65577ms step_avg:57.88ms
step:1134/2330 train_time:65636ms step_avg:57.88ms
step:1135/2330 train_time:65693ms step_avg:57.88ms
step:1136/2330 train_time:65753ms step_avg:57.88ms
step:1137/2330 train_time:65810ms step_avg:57.88ms
step:1138/2330 train_time:65870ms step_avg:57.88ms
step:1139/2330 train_time:65927ms step_avg:57.88ms
step:1140/2330 train_time:65988ms step_avg:57.88ms
step:1141/2330 train_time:66046ms step_avg:57.88ms
step:1142/2330 train_time:66106ms step_avg:57.89ms
step:1143/2330 train_time:66164ms step_avg:57.89ms
step:1144/2330 train_time:66223ms step_avg:57.89ms
step:1145/2330 train_time:66279ms step_avg:57.89ms
step:1146/2330 train_time:66339ms step_avg:57.89ms
step:1147/2330 train_time:66397ms step_avg:57.89ms
step:1148/2330 train_time:66457ms step_avg:57.89ms
step:1149/2330 train_time:66514ms step_avg:57.89ms
step:1150/2330 train_time:66574ms step_avg:57.89ms
step:1151/2330 train_time:66630ms step_avg:57.89ms
step:1152/2330 train_time:66691ms step_avg:57.89ms
step:1153/2330 train_time:66748ms step_avg:57.89ms
step:1154/2330 train_time:66808ms step_avg:57.89ms
step:1155/2330 train_time:66865ms step_avg:57.89ms
step:1156/2330 train_time:66925ms step_avg:57.89ms
step:1157/2330 train_time:66982ms step_avg:57.89ms
step:1158/2330 train_time:67042ms step_avg:57.90ms
step:1159/2330 train_time:67099ms step_avg:57.89ms
step:1160/2330 train_time:67159ms step_avg:57.90ms
step:1161/2330 train_time:67215ms step_avg:57.89ms
step:1162/2330 train_time:67276ms step_avg:57.90ms
step:1163/2330 train_time:67333ms step_avg:57.90ms
step:1164/2330 train_time:67394ms step_avg:57.90ms
step:1165/2330 train_time:67451ms step_avg:57.90ms
step:1166/2330 train_time:67511ms step_avg:57.90ms
step:1167/2330 train_time:67568ms step_avg:57.90ms
step:1168/2330 train_time:67629ms step_avg:57.90ms
step:1169/2330 train_time:67686ms step_avg:57.90ms
step:1170/2330 train_time:67746ms step_avg:57.90ms
step:1171/2330 train_time:67803ms step_avg:57.90ms
step:1172/2330 train_time:67863ms step_avg:57.90ms
step:1173/2330 train_time:67920ms step_avg:57.90ms
step:1174/2330 train_time:67979ms step_avg:57.90ms
step:1175/2330 train_time:68036ms step_avg:57.90ms
step:1176/2330 train_time:68096ms step_avg:57.90ms
step:1177/2330 train_time:68153ms step_avg:57.90ms
step:1178/2330 train_time:68213ms step_avg:57.91ms
step:1179/2330 train_time:68270ms step_avg:57.91ms
step:1180/2330 train_time:68330ms step_avg:57.91ms
step:1181/2330 train_time:68387ms step_avg:57.91ms
step:1182/2330 train_time:68448ms step_avg:57.91ms
step:1183/2330 train_time:68505ms step_avg:57.91ms
step:1184/2330 train_time:68565ms step_avg:57.91ms
step:1185/2330 train_time:68622ms step_avg:57.91ms
step:1186/2330 train_time:68682ms step_avg:57.91ms
step:1187/2330 train_time:68739ms step_avg:57.91ms
step:1188/2330 train_time:68798ms step_avg:57.91ms
step:1189/2330 train_time:68855ms step_avg:57.91ms
step:1190/2330 train_time:68915ms step_avg:57.91ms
step:1191/2330 train_time:68972ms step_avg:57.91ms
step:1192/2330 train_time:69032ms step_avg:57.91ms
step:1193/2330 train_time:69089ms step_avg:57.91ms
step:1194/2330 train_time:69149ms step_avg:57.91ms
step:1195/2330 train_time:69206ms step_avg:57.91ms
step:1196/2330 train_time:69266ms step_avg:57.91ms
step:1197/2330 train_time:69323ms step_avg:57.91ms
step:1198/2330 train_time:69383ms step_avg:57.92ms
step:1199/2330 train_time:69439ms step_avg:57.91ms
step:1200/2330 train_time:69501ms step_avg:57.92ms
step:1201/2330 train_time:69557ms step_avg:57.92ms
step:1202/2330 train_time:69617ms step_avg:57.92ms
step:1203/2330 train_time:69673ms step_avg:57.92ms
step:1204/2330 train_time:69734ms step_avg:57.92ms
step:1205/2330 train_time:69791ms step_avg:57.92ms
step:1206/2330 train_time:69852ms step_avg:57.92ms
step:1207/2330 train_time:69909ms step_avg:57.92ms
step:1208/2330 train_time:69970ms step_avg:57.92ms
step:1209/2330 train_time:70027ms step_avg:57.92ms
step:1210/2330 train_time:70086ms step_avg:57.92ms
step:1211/2330 train_time:70144ms step_avg:57.92ms
step:1212/2330 train_time:70203ms step_avg:57.92ms
step:1213/2330 train_time:70260ms step_avg:57.92ms
step:1214/2330 train_time:70321ms step_avg:57.92ms
step:1215/2330 train_time:70378ms step_avg:57.92ms
step:1216/2330 train_time:70438ms step_avg:57.93ms
step:1217/2330 train_time:70495ms step_avg:57.93ms
step:1218/2330 train_time:70556ms step_avg:57.93ms
step:1219/2330 train_time:70613ms step_avg:57.93ms
step:1220/2330 train_time:70673ms step_avg:57.93ms
step:1221/2330 train_time:70730ms step_avg:57.93ms
step:1222/2330 train_time:70789ms step_avg:57.93ms
step:1223/2330 train_time:70846ms step_avg:57.93ms
step:1224/2330 train_time:70907ms step_avg:57.93ms
step:1225/2330 train_time:70964ms step_avg:57.93ms
step:1226/2330 train_time:71024ms step_avg:57.93ms
step:1227/2330 train_time:71081ms step_avg:57.93ms
step:1228/2330 train_time:71140ms step_avg:57.93ms
step:1229/2330 train_time:71197ms step_avg:57.93ms
step:1230/2330 train_time:71259ms step_avg:57.93ms
step:1231/2330 train_time:71316ms step_avg:57.93ms
step:1232/2330 train_time:71376ms step_avg:57.93ms
step:1233/2330 train_time:71433ms step_avg:57.93ms
step:1234/2330 train_time:71493ms step_avg:57.94ms
step:1235/2330 train_time:71551ms step_avg:57.94ms
step:1236/2330 train_time:71612ms step_avg:57.94ms
step:1237/2330 train_time:71668ms step_avg:57.94ms
step:1238/2330 train_time:71728ms step_avg:57.94ms
step:1239/2330 train_time:71785ms step_avg:57.94ms
step:1240/2330 train_time:71845ms step_avg:57.94ms
step:1241/2330 train_time:71903ms step_avg:57.94ms
step:1242/2330 train_time:71962ms step_avg:57.94ms
step:1243/2330 train_time:72019ms step_avg:57.94ms
step:1244/2330 train_time:72079ms step_avg:57.94ms
step:1245/2330 train_time:72136ms step_avg:57.94ms
step:1246/2330 train_time:72195ms step_avg:57.94ms
step:1247/2330 train_time:72253ms step_avg:57.94ms
step:1248/2330 train_time:72313ms step_avg:57.94ms
step:1249/2330 train_time:72370ms step_avg:57.94ms
step:1250/2330 train_time:72430ms step_avg:57.94ms
step:1250/2330 val_loss:3.9845 train_time:72511ms step_avg:58.01ms
step:1251/2330 train_time:72531ms step_avg:57.98ms
step:1252/2330 train_time:72552ms step_avg:57.95ms
step:1253/2330 train_time:72609ms step_avg:57.95ms
step:1254/2330 train_time:72674ms step_avg:57.95ms
step:1255/2330 train_time:72732ms step_avg:57.95ms
step:1256/2330 train_time:72794ms step_avg:57.96ms
step:1257/2330 train_time:72852ms step_avg:57.96ms
step:1258/2330 train_time:72912ms step_avg:57.96ms
step:1259/2330 train_time:72968ms step_avg:57.96ms
step:1260/2330 train_time:73027ms step_avg:57.96ms
step:1261/2330 train_time:73084ms step_avg:57.96ms
step:1262/2330 train_time:73143ms step_avg:57.96ms
step:1263/2330 train_time:73199ms step_avg:57.96ms
step:1264/2330 train_time:73259ms step_avg:57.96ms
step:1265/2330 train_time:73315ms step_avg:57.96ms
step:1266/2330 train_time:73374ms step_avg:57.96ms
step:1267/2330 train_time:73431ms step_avg:57.96ms
step:1268/2330 train_time:73492ms step_avg:57.96ms
step:1269/2330 train_time:73551ms step_avg:57.96ms
step:1270/2330 train_time:73613ms step_avg:57.96ms
step:1271/2330 train_time:73670ms step_avg:57.96ms
step:1272/2330 train_time:73731ms step_avg:57.96ms
step:1273/2330 train_time:73788ms step_avg:57.96ms
step:1274/2330 train_time:73849ms step_avg:57.97ms
step:1275/2330 train_time:73905ms step_avg:57.96ms
step:1276/2330 train_time:73965ms step_avg:57.97ms
step:1277/2330 train_time:74022ms step_avg:57.97ms
step:1278/2330 train_time:74082ms step_avg:57.97ms
step:1279/2330 train_time:74138ms step_avg:57.97ms
step:1280/2330 train_time:74198ms step_avg:57.97ms
step:1281/2330 train_time:74254ms step_avg:57.97ms
step:1282/2330 train_time:74314ms step_avg:57.97ms
step:1283/2330 train_time:74371ms step_avg:57.97ms
step:1284/2330 train_time:74430ms step_avg:57.97ms
step:1285/2330 train_time:74487ms step_avg:57.97ms
step:1286/2330 train_time:74547ms step_avg:57.97ms
step:1287/2330 train_time:74604ms step_avg:57.97ms
step:1288/2330 train_time:74666ms step_avg:57.97ms
step:1289/2330 train_time:74723ms step_avg:57.97ms
step:1290/2330 train_time:74784ms step_avg:57.97ms
step:1291/2330 train_time:74842ms step_avg:57.97ms
step:1292/2330 train_time:74903ms step_avg:57.97ms
step:1293/2330 train_time:74959ms step_avg:57.97ms
step:1294/2330 train_time:75019ms step_avg:57.97ms
step:1295/2330 train_time:75077ms step_avg:57.97ms
step:1296/2330 train_time:75136ms step_avg:57.98ms
step:1297/2330 train_time:75193ms step_avg:57.97ms
step:1298/2330 train_time:75252ms step_avg:57.98ms
step:1299/2330 train_time:75308ms step_avg:57.97ms
step:1300/2330 train_time:75368ms step_avg:57.98ms
step:1301/2330 train_time:75425ms step_avg:57.97ms
step:1302/2330 train_time:75485ms step_avg:57.98ms
step:1303/2330 train_time:75542ms step_avg:57.98ms
step:1304/2330 train_time:75602ms step_avg:57.98ms
step:1305/2330 train_time:75660ms step_avg:57.98ms
step:1306/2330 train_time:75721ms step_avg:57.98ms
step:1307/2330 train_time:75778ms step_avg:57.98ms
step:1308/2330 train_time:75839ms step_avg:57.98ms
step:1309/2330 train_time:75897ms step_avg:57.98ms
step:1310/2330 train_time:75957ms step_avg:57.98ms
step:1311/2330 train_time:76014ms step_avg:57.98ms
step:1312/2330 train_time:76073ms step_avg:57.98ms
step:1313/2330 train_time:76130ms step_avg:57.98ms
step:1314/2330 train_time:76190ms step_avg:57.98ms
step:1315/2330 train_time:76246ms step_avg:57.98ms
step:1316/2330 train_time:76305ms step_avg:57.98ms
step:1317/2330 train_time:76362ms step_avg:57.98ms
step:1318/2330 train_time:76422ms step_avg:57.98ms
step:1319/2330 train_time:76479ms step_avg:57.98ms
step:1320/2330 train_time:76538ms step_avg:57.98ms
step:1321/2330 train_time:76596ms step_avg:57.98ms
step:1322/2330 train_time:76657ms step_avg:57.99ms
step:1323/2330 train_time:76715ms step_avg:57.99ms
step:1324/2330 train_time:76776ms step_avg:57.99ms
step:1325/2330 train_time:76834ms step_avg:57.99ms
step:1326/2330 train_time:76895ms step_avg:57.99ms
step:1327/2330 train_time:76952ms step_avg:57.99ms
step:1328/2330 train_time:77012ms step_avg:57.99ms
step:1329/2330 train_time:77069ms step_avg:57.99ms
step:1330/2330 train_time:77129ms step_avg:57.99ms
step:1331/2330 train_time:77185ms step_avg:57.99ms
step:1332/2330 train_time:77244ms step_avg:57.99ms
step:1333/2330 train_time:77301ms step_avg:57.99ms
step:1334/2330 train_time:77361ms step_avg:57.99ms
step:1335/2330 train_time:77418ms step_avg:57.99ms
step:1336/2330 train_time:77478ms step_avg:57.99ms
step:1337/2330 train_time:77537ms step_avg:57.99ms
step:1338/2330 train_time:77597ms step_avg:57.99ms
step:1339/2330 train_time:77655ms step_avg:57.99ms
step:1340/2330 train_time:77714ms step_avg:58.00ms
step:1341/2330 train_time:77772ms step_avg:58.00ms
step:1342/2330 train_time:77832ms step_avg:58.00ms
step:1343/2330 train_time:77889ms step_avg:58.00ms
step:1344/2330 train_time:77949ms step_avg:58.00ms
step:1345/2330 train_time:78006ms step_avg:58.00ms
step:1346/2330 train_time:78067ms step_avg:58.00ms
step:1347/2330 train_time:78123ms step_avg:58.00ms
step:1348/2330 train_time:78182ms step_avg:58.00ms
step:1349/2330 train_time:78238ms step_avg:58.00ms
step:1350/2330 train_time:78299ms step_avg:58.00ms
step:1351/2330 train_time:78356ms step_avg:58.00ms
step:1352/2330 train_time:78416ms step_avg:58.00ms
step:1353/2330 train_time:78474ms step_avg:58.00ms
step:1354/2330 train_time:78534ms step_avg:58.00ms
step:1355/2330 train_time:78591ms step_avg:58.00ms
step:1356/2330 train_time:78650ms step_avg:58.00ms
step:1357/2330 train_time:78708ms step_avg:58.00ms
step:1358/2330 train_time:78767ms step_avg:58.00ms
step:1359/2330 train_time:78824ms step_avg:58.00ms
step:1360/2330 train_time:78885ms step_avg:58.00ms
step:1361/2330 train_time:78942ms step_avg:58.00ms
step:1362/2330 train_time:79002ms step_avg:58.00ms
step:1363/2330 train_time:79058ms step_avg:58.00ms
step:1364/2330 train_time:79119ms step_avg:58.01ms
step:1365/2330 train_time:79175ms step_avg:58.00ms
step:1366/2330 train_time:79237ms step_avg:58.01ms
step:1367/2330 train_time:79294ms step_avg:58.01ms
step:1368/2330 train_time:79354ms step_avg:58.01ms
step:1369/2330 train_time:79411ms step_avg:58.01ms
step:1370/2330 train_time:79472ms step_avg:58.01ms
step:1371/2330 train_time:79529ms step_avg:58.01ms
step:1372/2330 train_time:79589ms step_avg:58.01ms
step:1373/2330 train_time:79646ms step_avg:58.01ms
step:1374/2330 train_time:79705ms step_avg:58.01ms
step:1375/2330 train_time:79762ms step_avg:58.01ms
step:1376/2330 train_time:79823ms step_avg:58.01ms
step:1377/2330 train_time:79880ms step_avg:58.01ms
step:1378/2330 train_time:79941ms step_avg:58.01ms
step:1379/2330 train_time:79997ms step_avg:58.01ms
step:1380/2330 train_time:80059ms step_avg:58.01ms
step:1381/2330 train_time:80115ms step_avg:58.01ms
step:1382/2330 train_time:80176ms step_avg:58.01ms
step:1383/2330 train_time:80233ms step_avg:58.01ms
step:1384/2330 train_time:80293ms step_avg:58.02ms
step:1385/2330 train_time:80350ms step_avg:58.01ms
step:1386/2330 train_time:80410ms step_avg:58.02ms
step:1387/2330 train_time:80467ms step_avg:58.02ms
step:1388/2330 train_time:80527ms step_avg:58.02ms
step:1389/2330 train_time:80584ms step_avg:58.02ms
step:1390/2330 train_time:80643ms step_avg:58.02ms
step:1391/2330 train_time:80700ms step_avg:58.02ms
step:1392/2330 train_time:80761ms step_avg:58.02ms
step:1393/2330 train_time:80818ms step_avg:58.02ms
step:1394/2330 train_time:80879ms step_avg:58.02ms
step:1395/2330 train_time:80937ms step_avg:58.02ms
step:1396/2330 train_time:80996ms step_avg:58.02ms
step:1397/2330 train_time:81054ms step_avg:58.02ms
step:1398/2330 train_time:81113ms step_avg:58.02ms
step:1399/2330 train_time:81170ms step_avg:58.02ms
step:1400/2330 train_time:81230ms step_avg:58.02ms
step:1401/2330 train_time:81286ms step_avg:58.02ms
step:1402/2330 train_time:81347ms step_avg:58.02ms
step:1403/2330 train_time:81404ms step_avg:58.02ms
step:1404/2330 train_time:81463ms step_avg:58.02ms
step:1405/2330 train_time:81520ms step_avg:58.02ms
step:1406/2330 train_time:81581ms step_avg:58.02ms
step:1407/2330 train_time:81638ms step_avg:58.02ms
step:1408/2330 train_time:81698ms step_avg:58.02ms
step:1409/2330 train_time:81755ms step_avg:58.02ms
step:1410/2330 train_time:81816ms step_avg:58.03ms
step:1411/2330 train_time:81874ms step_avg:58.03ms
step:1412/2330 train_time:81934ms step_avg:58.03ms
step:1413/2330 train_time:81991ms step_avg:58.03ms
step:1414/2330 train_time:82050ms step_avg:58.03ms
step:1415/2330 train_time:82107ms step_avg:58.03ms
step:1416/2330 train_time:82168ms step_avg:58.03ms
step:1417/2330 train_time:82224ms step_avg:58.03ms
step:1418/2330 train_time:82285ms step_avg:58.03ms
step:1419/2330 train_time:82342ms step_avg:58.03ms
step:1420/2330 train_time:82402ms step_avg:58.03ms
step:1421/2330 train_time:82459ms step_avg:58.03ms
step:1422/2330 train_time:82519ms step_avg:58.03ms
step:1423/2330 train_time:82576ms step_avg:58.03ms
step:1424/2330 train_time:82637ms step_avg:58.03ms
step:1425/2330 train_time:82694ms step_avg:58.03ms
step:1426/2330 train_time:82755ms step_avg:58.03ms
step:1427/2330 train_time:82812ms step_avg:58.03ms
step:1428/2330 train_time:82873ms step_avg:58.03ms
step:1429/2330 train_time:82930ms step_avg:58.03ms
step:1430/2330 train_time:82990ms step_avg:58.04ms
step:1431/2330 train_time:83048ms step_avg:58.04ms
step:1432/2330 train_time:83107ms step_avg:58.04ms
step:1433/2330 train_time:83165ms step_avg:58.04ms
step:1434/2330 train_time:83224ms step_avg:58.04ms
step:1435/2330 train_time:83281ms step_avg:58.04ms
step:1436/2330 train_time:83342ms step_avg:58.04ms
step:1437/2330 train_time:83398ms step_avg:58.04ms
step:1438/2330 train_time:83459ms step_avg:58.04ms
step:1439/2330 train_time:83516ms step_avg:58.04ms
step:1440/2330 train_time:83576ms step_avg:58.04ms
step:1441/2330 train_time:83632ms step_avg:58.04ms
step:1442/2330 train_time:83694ms step_avg:58.04ms
step:1443/2330 train_time:83750ms step_avg:58.04ms
step:1444/2330 train_time:83811ms step_avg:58.04ms
step:1445/2330 train_time:83868ms step_avg:58.04ms
step:1446/2330 train_time:83928ms step_avg:58.04ms
step:1447/2330 train_time:83985ms step_avg:58.04ms
step:1448/2330 train_time:84045ms step_avg:58.04ms
step:1449/2330 train_time:84102ms step_avg:58.04ms
step:1450/2330 train_time:84162ms step_avg:58.04ms
step:1451/2330 train_time:84220ms step_avg:58.04ms
step:1452/2330 train_time:84280ms step_avg:58.04ms
step:1453/2330 train_time:84338ms step_avg:58.04ms
step:1454/2330 train_time:84397ms step_avg:58.04ms
step:1455/2330 train_time:84455ms step_avg:58.04ms
step:1456/2330 train_time:84514ms step_avg:58.05ms
step:1457/2330 train_time:84571ms step_avg:58.04ms
step:1458/2330 train_time:84631ms step_avg:58.05ms
step:1459/2330 train_time:84689ms step_avg:58.05ms
step:1460/2330 train_time:84748ms step_avg:58.05ms
step:1461/2330 train_time:84805ms step_avg:58.05ms
step:1462/2330 train_time:84866ms step_avg:58.05ms
step:1463/2330 train_time:84923ms step_avg:58.05ms
step:1464/2330 train_time:84983ms step_avg:58.05ms
step:1465/2330 train_time:85040ms step_avg:58.05ms
step:1466/2330 train_time:85101ms step_avg:58.05ms
step:1467/2330 train_time:85158ms step_avg:58.05ms
step:1468/2330 train_time:85219ms step_avg:58.05ms
step:1469/2330 train_time:85275ms step_avg:58.05ms
step:1470/2330 train_time:85336ms step_avg:58.05ms
step:1471/2330 train_time:85393ms step_avg:58.05ms
step:1472/2330 train_time:85453ms step_avg:58.05ms
step:1473/2330 train_time:85510ms step_avg:58.05ms
step:1474/2330 train_time:85570ms step_avg:58.05ms
step:1475/2330 train_time:85627ms step_avg:58.05ms
step:1476/2330 train_time:85687ms step_avg:58.05ms
step:1477/2330 train_time:85744ms step_avg:58.05ms
step:1478/2330 train_time:85804ms step_avg:58.05ms
step:1479/2330 train_time:85861ms step_avg:58.05ms
step:1480/2330 train_time:85923ms step_avg:58.06ms
step:1481/2330 train_time:85980ms step_avg:58.06ms
step:1482/2330 train_time:86039ms step_avg:58.06ms
step:1483/2330 train_time:86096ms step_avg:58.06ms
step:1484/2330 train_time:86158ms step_avg:58.06ms
step:1485/2330 train_time:86214ms step_avg:58.06ms
step:1486/2330 train_time:86275ms step_avg:58.06ms
step:1487/2330 train_time:86332ms step_avg:58.06ms
step:1488/2330 train_time:86392ms step_avg:58.06ms
step:1489/2330 train_time:86449ms step_avg:58.06ms
step:1490/2330 train_time:86508ms step_avg:58.06ms
step:1491/2330 train_time:86566ms step_avg:58.06ms
step:1492/2330 train_time:86625ms step_avg:58.06ms
step:1493/2330 train_time:86682ms step_avg:58.06ms
step:1494/2330 train_time:86742ms step_avg:58.06ms
step:1495/2330 train_time:86799ms step_avg:58.06ms
step:1496/2330 train_time:86860ms step_avg:58.06ms
step:1497/2330 train_time:86918ms step_avg:58.06ms
step:1498/2330 train_time:86978ms step_avg:58.06ms
step:1499/2330 train_time:87035ms step_avg:58.06ms
step:1500/2330 train_time:87095ms step_avg:58.06ms
step:1500/2330 val_loss:3.9041 train_time:87176ms step_avg:58.12ms
step:1501/2330 train_time:87195ms step_avg:58.09ms
step:1502/2330 train_time:87215ms step_avg:58.07ms
step:1503/2330 train_time:87273ms step_avg:58.07ms
step:1504/2330 train_time:87339ms step_avg:58.07ms
step:1505/2330 train_time:87396ms step_avg:58.07ms
step:1506/2330 train_time:87458ms step_avg:58.07ms
step:1507/2330 train_time:87514ms step_avg:58.07ms
step:1508/2330 train_time:87575ms step_avg:58.07ms
step:1509/2330 train_time:87631ms step_avg:58.07ms
step:1510/2330 train_time:87692ms step_avg:58.07ms
step:1511/2330 train_time:87748ms step_avg:58.07ms
step:1512/2330 train_time:87807ms step_avg:58.07ms
step:1513/2330 train_time:87864ms step_avg:58.07ms
step:1514/2330 train_time:87923ms step_avg:58.07ms
step:1515/2330 train_time:87979ms step_avg:58.07ms
step:1516/2330 train_time:88038ms step_avg:58.07ms
step:1517/2330 train_time:88095ms step_avg:58.07ms
step:1518/2330 train_time:88155ms step_avg:58.07ms
step:1519/2330 train_time:88212ms step_avg:58.07ms
step:1520/2330 train_time:88274ms step_avg:58.08ms
step:1521/2330 train_time:88332ms step_avg:58.07ms
step:1522/2330 train_time:88393ms step_avg:58.08ms
step:1523/2330 train_time:88450ms step_avg:58.08ms
step:1524/2330 train_time:88511ms step_avg:58.08ms
step:1525/2330 train_time:88568ms step_avg:58.08ms
step:1526/2330 train_time:88629ms step_avg:58.08ms
step:1527/2330 train_time:88685ms step_avg:58.08ms
step:1528/2330 train_time:88745ms step_avg:58.08ms
step:1529/2330 train_time:88804ms step_avg:58.08ms
step:1530/2330 train_time:88862ms step_avg:58.08ms
step:1531/2330 train_time:88919ms step_avg:58.08ms
step:1532/2330 train_time:88979ms step_avg:58.08ms
step:1533/2330 train_time:89036ms step_avg:58.08ms
step:1534/2330 train_time:89095ms step_avg:58.08ms
step:1535/2330 train_time:89154ms step_avg:58.08ms
step:1536/2330 train_time:89214ms step_avg:58.08ms
step:1537/2330 train_time:89271ms step_avg:58.08ms
step:1538/2330 train_time:89334ms step_avg:58.08ms
step:1539/2330 train_time:89392ms step_avg:58.08ms
step:1540/2330 train_time:89453ms step_avg:58.09ms
step:1541/2330 train_time:89511ms step_avg:58.09ms
step:1542/2330 train_time:89571ms step_avg:58.09ms
step:1543/2330 train_time:89627ms step_avg:58.09ms
step:1544/2330 train_time:89689ms step_avg:58.09ms
step:1545/2330 train_time:89746ms step_avg:58.09ms
step:1546/2330 train_time:89808ms step_avg:58.09ms
step:1547/2330 train_time:89865ms step_avg:58.09ms
step:1548/2330 train_time:89926ms step_avg:58.09ms
step:1549/2330 train_time:89982ms step_avg:58.09ms
step:1550/2330 train_time:90043ms step_avg:58.09ms
step:1551/2330 train_time:90101ms step_avg:58.09ms
step:1552/2330 train_time:90162ms step_avg:58.09ms
step:1553/2330 train_time:90220ms step_avg:58.09ms
step:1554/2330 train_time:90281ms step_avg:58.10ms
step:1555/2330 train_time:90339ms step_avg:58.10ms
step:1556/2330 train_time:90400ms step_avg:58.10ms
step:1557/2330 train_time:90458ms step_avg:58.10ms
step:1558/2330 train_time:90519ms step_avg:58.10ms
step:1559/2330 train_time:90577ms step_avg:58.10ms
step:1560/2330 train_time:90638ms step_avg:58.10ms
step:1561/2330 train_time:90695ms step_avg:58.10ms
step:1562/2330 train_time:90755ms step_avg:58.10ms
step:1563/2330 train_time:90812ms step_avg:58.10ms
step:1564/2330 train_time:90873ms step_avg:58.10ms
step:1565/2330 train_time:90930ms step_avg:58.10ms
step:1566/2330 train_time:90991ms step_avg:58.10ms
step:1567/2330 train_time:91048ms step_avg:58.10ms
step:1568/2330 train_time:91109ms step_avg:58.10ms
step:1569/2330 train_time:91166ms step_avg:58.10ms
step:1570/2330 train_time:91226ms step_avg:58.11ms
step:1571/2330 train_time:91284ms step_avg:58.11ms
step:1572/2330 train_time:91345ms step_avg:58.11ms
step:1573/2330 train_time:91403ms step_avg:58.11ms
step:1574/2330 train_time:91464ms step_avg:58.11ms
step:1575/2330 train_time:91522ms step_avg:58.11ms
step:1576/2330 train_time:91584ms step_avg:58.11ms
step:1577/2330 train_time:91642ms step_avg:58.11ms
step:1578/2330 train_time:91703ms step_avg:58.11ms
step:1579/2330 train_time:91762ms step_avg:58.11ms
step:1580/2330 train_time:91822ms step_avg:58.12ms
step:1581/2330 train_time:91881ms step_avg:58.12ms
step:1582/2330 train_time:91941ms step_avg:58.12ms
step:1583/2330 train_time:91998ms step_avg:58.12ms
step:1584/2330 train_time:92059ms step_avg:58.12ms
step:1585/2330 train_time:92116ms step_avg:58.12ms
step:1586/2330 train_time:92175ms step_avg:58.12ms
step:1587/2330 train_time:92232ms step_avg:58.12ms
step:1588/2330 train_time:92294ms step_avg:58.12ms
step:1589/2330 train_time:92351ms step_avg:58.12ms
step:1590/2330 train_time:92413ms step_avg:58.12ms
step:1591/2330 train_time:92470ms step_avg:58.12ms
step:1592/2330 train_time:92531ms step_avg:58.12ms
step:1593/2330 train_time:92588ms step_avg:58.12ms
step:1594/2330 train_time:92651ms step_avg:58.12ms
step:1595/2330 train_time:92709ms step_avg:58.12ms
step:1596/2330 train_time:92769ms step_avg:58.13ms
step:1597/2330 train_time:92826ms step_avg:58.13ms
step:1598/2330 train_time:92888ms step_avg:58.13ms
step:1599/2330 train_time:92945ms step_avg:58.13ms
step:1600/2330 train_time:93006ms step_avg:58.13ms
step:1601/2330 train_time:93063ms step_avg:58.13ms
step:1602/2330 train_time:93123ms step_avg:58.13ms
step:1603/2330 train_time:93181ms step_avg:58.13ms
step:1604/2330 train_time:93241ms step_avg:58.13ms
step:1605/2330 train_time:93299ms step_avg:58.13ms
step:1606/2330 train_time:93360ms step_avg:58.13ms
step:1607/2330 train_time:93419ms step_avg:58.13ms
step:1608/2330 train_time:93479ms step_avg:58.13ms
step:1609/2330 train_time:93536ms step_avg:58.13ms
step:1610/2330 train_time:93597ms step_avg:58.13ms
step:1611/2330 train_time:93654ms step_avg:58.13ms
step:1612/2330 train_time:93715ms step_avg:58.14ms
step:1613/2330 train_time:93772ms step_avg:58.14ms
step:1614/2330 train_time:93833ms step_avg:58.14ms
step:1615/2330 train_time:93891ms step_avg:58.14ms
step:1616/2330 train_time:93952ms step_avg:58.14ms
step:1617/2330 train_time:94009ms step_avg:58.14ms
step:1618/2330 train_time:94071ms step_avg:58.14ms
step:1619/2330 train_time:94128ms step_avg:58.14ms
step:1620/2330 train_time:94190ms step_avg:58.14ms
step:1621/2330 train_time:94247ms step_avg:58.14ms
step:1622/2330 train_time:94308ms step_avg:58.14ms
step:1623/2330 train_time:94365ms step_avg:58.14ms
step:1624/2330 train_time:94426ms step_avg:58.14ms
step:1625/2330 train_time:94483ms step_avg:58.14ms
step:1626/2330 train_time:94544ms step_avg:58.15ms
step:1627/2330 train_time:94601ms step_avg:58.14ms
step:1628/2330 train_time:94663ms step_avg:58.15ms
step:1629/2330 train_time:94721ms step_avg:58.15ms
step:1630/2330 train_time:94782ms step_avg:58.15ms
step:1631/2330 train_time:94839ms step_avg:58.15ms
step:1632/2330 train_time:94899ms step_avg:58.15ms
step:1633/2330 train_time:94957ms step_avg:58.15ms
step:1634/2330 train_time:95018ms step_avg:58.15ms
step:1635/2330 train_time:95075ms step_avg:58.15ms
step:1636/2330 train_time:95135ms step_avg:58.15ms
step:1637/2330 train_time:95192ms step_avg:58.15ms
step:1638/2330 train_time:95253ms step_avg:58.15ms
step:1639/2330 train_time:95310ms step_avg:58.15ms
step:1640/2330 train_time:95371ms step_avg:58.15ms
step:1641/2330 train_time:95429ms step_avg:58.15ms
step:1642/2330 train_time:95490ms step_avg:58.15ms
step:1643/2330 train_time:95547ms step_avg:58.15ms
step:1644/2330 train_time:95609ms step_avg:58.16ms
step:1645/2330 train_time:95666ms step_avg:58.16ms
step:1646/2330 train_time:95727ms step_avg:58.16ms
step:1647/2330 train_time:95785ms step_avg:58.16ms
step:1648/2330 train_time:95846ms step_avg:58.16ms
step:1649/2330 train_time:95903ms step_avg:58.16ms
step:1650/2330 train_time:95964ms step_avg:58.16ms
step:1651/2330 train_time:96023ms step_avg:58.16ms
step:1652/2330 train_time:96083ms step_avg:58.16ms
step:1653/2330 train_time:96141ms step_avg:58.16ms
step:1654/2330 train_time:96202ms step_avg:58.16ms
step:1655/2330 train_time:96260ms step_avg:58.16ms
step:1656/2330 train_time:96320ms step_avg:58.16ms
step:1657/2330 train_time:96378ms step_avg:58.16ms
step:1658/2330 train_time:96438ms step_avg:58.17ms
step:1659/2330 train_time:96496ms step_avg:58.17ms
step:1660/2330 train_time:96556ms step_avg:58.17ms
step:1661/2330 train_time:96613ms step_avg:58.17ms
step:1662/2330 train_time:96674ms step_avg:58.17ms
step:1663/2330 train_time:96731ms step_avg:58.17ms
step:1664/2330 train_time:96795ms step_avg:58.17ms
step:1665/2330 train_time:96851ms step_avg:58.17ms
step:1666/2330 train_time:96914ms step_avg:58.17ms
step:1667/2330 train_time:96971ms step_avg:58.17ms
step:1668/2330 train_time:97031ms step_avg:58.17ms
step:1669/2330 train_time:97089ms step_avg:58.17ms
step:1670/2330 train_time:97150ms step_avg:58.17ms
step:1671/2330 train_time:97207ms step_avg:58.17ms
step:1672/2330 train_time:97268ms step_avg:58.17ms
step:1673/2330 train_time:97326ms step_avg:58.17ms
step:1674/2330 train_time:97387ms step_avg:58.18ms
step:1675/2330 train_time:97445ms step_avg:58.18ms
step:1676/2330 train_time:97505ms step_avg:58.18ms
step:1677/2330 train_time:97562ms step_avg:58.18ms
step:1678/2330 train_time:97623ms step_avg:58.18ms
step:1679/2330 train_time:97682ms step_avg:58.18ms
step:1680/2330 train_time:97742ms step_avg:58.18ms
step:1681/2330 train_time:97801ms step_avg:58.18ms
step:1682/2330 train_time:97861ms step_avg:58.18ms
step:1683/2330 train_time:97919ms step_avg:58.18ms
step:1684/2330 train_time:97980ms step_avg:58.18ms
step:1685/2330 train_time:98037ms step_avg:58.18ms
step:1686/2330 train_time:98097ms step_avg:58.18ms
step:1687/2330 train_time:98154ms step_avg:58.18ms
step:1688/2330 train_time:98215ms step_avg:58.18ms
step:1689/2330 train_time:98272ms step_avg:58.18ms
step:1690/2330 train_time:98332ms step_avg:58.18ms
step:1691/2330 train_time:98389ms step_avg:58.18ms
step:1692/2330 train_time:98452ms step_avg:58.19ms
step:1693/2330 train_time:98509ms step_avg:58.19ms
step:1694/2330 train_time:98569ms step_avg:58.19ms
step:1695/2330 train_time:98626ms step_avg:58.19ms
step:1696/2330 train_time:98689ms step_avg:58.19ms
step:1697/2330 train_time:98746ms step_avg:58.19ms
step:1698/2330 train_time:98808ms step_avg:58.19ms
step:1699/2330 train_time:98866ms step_avg:58.19ms
step:1700/2330 train_time:98926ms step_avg:58.19ms
step:1701/2330 train_time:98984ms step_avg:58.19ms
step:1702/2330 train_time:99044ms step_avg:58.19ms
step:1703/2330 train_time:99102ms step_avg:58.19ms
step:1704/2330 train_time:99163ms step_avg:58.19ms
step:1705/2330 train_time:99221ms step_avg:58.19ms
step:1706/2330 train_time:99281ms step_avg:58.20ms
step:1707/2330 train_time:99339ms step_avg:58.20ms
step:1708/2330 train_time:99399ms step_avg:58.20ms
step:1709/2330 train_time:99456ms step_avg:58.20ms
step:1710/2330 train_time:99517ms step_avg:58.20ms
step:1711/2330 train_time:99574ms step_avg:58.20ms
step:1712/2330 train_time:99634ms step_avg:58.20ms
step:1713/2330 train_time:99692ms step_avg:58.20ms
step:1714/2330 train_time:99754ms step_avg:58.20ms
step:1715/2330 train_time:99811ms step_avg:58.20ms
step:1716/2330 train_time:99872ms step_avg:58.20ms
step:1717/2330 train_time:99929ms step_avg:58.20ms
step:1718/2330 train_time:99990ms step_avg:58.20ms
step:1719/2330 train_time:100047ms step_avg:58.20ms
step:1720/2330 train_time:100110ms step_avg:58.20ms
step:1721/2330 train_time:100166ms step_avg:58.20ms
step:1722/2330 train_time:100228ms step_avg:58.20ms
step:1723/2330 train_time:100285ms step_avg:58.20ms
step:1724/2330 train_time:100346ms step_avg:58.21ms
step:1725/2330 train_time:100404ms step_avg:58.21ms
step:1726/2330 train_time:100465ms step_avg:58.21ms
step:1727/2330 train_time:100523ms step_avg:58.21ms
step:1728/2330 train_time:100583ms step_avg:58.21ms
step:1729/2330 train_time:100641ms step_avg:58.21ms
step:1730/2330 train_time:100702ms step_avg:58.21ms
step:1731/2330 train_time:100761ms step_avg:58.21ms
step:1732/2330 train_time:100821ms step_avg:58.21ms
step:1733/2330 train_time:100879ms step_avg:58.21ms
step:1734/2330 train_time:100939ms step_avg:58.21ms
step:1735/2330 train_time:100997ms step_avg:58.21ms
step:1736/2330 train_time:101057ms step_avg:58.21ms
step:1737/2330 train_time:101114ms step_avg:58.21ms
step:1738/2330 train_time:101175ms step_avg:58.21ms
step:1739/2330 train_time:101231ms step_avg:58.21ms
step:1740/2330 train_time:101292ms step_avg:58.21ms
step:1741/2330 train_time:101349ms step_avg:58.21ms
step:1742/2330 train_time:101411ms step_avg:58.22ms
step:1743/2330 train_time:101468ms step_avg:58.21ms
step:1744/2330 train_time:101530ms step_avg:58.22ms
step:1745/2330 train_time:101588ms step_avg:58.22ms
step:1746/2330 train_time:101649ms step_avg:58.22ms
step:1747/2330 train_time:101707ms step_avg:58.22ms
step:1748/2330 train_time:101767ms step_avg:58.22ms
step:1749/2330 train_time:101825ms step_avg:58.22ms
step:1750/2330 train_time:101885ms step_avg:58.22ms
step:1750/2330 val_loss:3.8218 train_time:101967ms step_avg:58.27ms
step:1751/2330 train_time:101987ms step_avg:58.25ms
step:1752/2330 train_time:102007ms step_avg:58.22ms
step:1753/2330 train_time:102061ms step_avg:58.22ms
step:1754/2330 train_time:102125ms step_avg:58.22ms
step:1755/2330 train_time:102181ms step_avg:58.22ms
step:1756/2330 train_time:102244ms step_avg:58.23ms
step:1757/2330 train_time:102300ms step_avg:58.22ms
step:1758/2330 train_time:102360ms step_avg:58.23ms
step:1759/2330 train_time:102417ms step_avg:58.22ms
step:1760/2330 train_time:102476ms step_avg:58.23ms
step:1761/2330 train_time:102533ms step_avg:58.22ms
step:1762/2330 train_time:102593ms step_avg:58.23ms
step:1763/2330 train_time:102649ms step_avg:58.22ms
step:1764/2330 train_time:102709ms step_avg:58.23ms
step:1765/2330 train_time:102765ms step_avg:58.22ms
step:1766/2330 train_time:102825ms step_avg:58.23ms
step:1767/2330 train_time:102888ms step_avg:58.23ms
step:1768/2330 train_time:102953ms step_avg:58.23ms
step:1769/2330 train_time:103011ms step_avg:58.23ms
step:1770/2330 train_time:103073ms step_avg:58.23ms
step:1771/2330 train_time:103131ms step_avg:58.23ms
step:1772/2330 train_time:103191ms step_avg:58.23ms
step:1773/2330 train_time:103247ms step_avg:58.23ms
step:1774/2330 train_time:103308ms step_avg:58.23ms
step:1775/2330 train_time:103365ms step_avg:58.23ms
step:1776/2330 train_time:103426ms step_avg:58.24ms
step:1777/2330 train_time:103483ms step_avg:58.23ms
step:1778/2330 train_time:103543ms step_avg:58.24ms
step:1779/2330 train_time:103601ms step_avg:58.24ms
step:1780/2330 train_time:103661ms step_avg:58.24ms
step:1781/2330 train_time:103718ms step_avg:58.24ms
step:1782/2330 train_time:103777ms step_avg:58.24ms
step:1783/2330 train_time:103835ms step_avg:58.24ms
step:1784/2330 train_time:103897ms step_avg:58.24ms
step:1785/2330 train_time:103955ms step_avg:58.24ms
step:1786/2330 train_time:104015ms step_avg:58.24ms
step:1787/2330 train_time:104073ms step_avg:58.24ms
step:1788/2330 train_time:104135ms step_avg:58.24ms
step:1789/2330 train_time:104192ms step_avg:58.24ms
step:1790/2330 train_time:104253ms step_avg:58.24ms
step:1791/2330 train_time:104310ms step_avg:58.24ms
step:1792/2330 train_time:104371ms step_avg:58.24ms
step:1793/2330 train_time:104428ms step_avg:58.24ms
step:1794/2330 train_time:104490ms step_avg:58.24ms
step:1795/2330 train_time:104546ms step_avg:58.24ms
step:1796/2330 train_time:104607ms step_avg:58.24ms
step:1797/2330 train_time:104663ms step_avg:58.24ms
step:1798/2330 train_time:104724ms step_avg:58.24ms
step:1799/2330 train_time:104782ms step_avg:58.24ms
step:1800/2330 train_time:104845ms step_avg:58.25ms
step:1801/2330 train_time:104904ms step_avg:58.25ms
step:1802/2330 train_time:104966ms step_avg:58.25ms
step:1803/2330 train_time:105025ms step_avg:58.25ms
step:1804/2330 train_time:105086ms step_avg:58.25ms
step:1805/2330 train_time:105144ms step_avg:58.25ms
step:1806/2330 train_time:105205ms step_avg:58.25ms
step:1807/2330 train_time:105262ms step_avg:58.25ms
step:1808/2330 train_time:105323ms step_avg:58.25ms
step:1809/2330 train_time:105379ms step_avg:58.25ms
step:1810/2330 train_time:105440ms step_avg:58.25ms
step:1811/2330 train_time:105497ms step_avg:58.25ms
step:1812/2330 train_time:105557ms step_avg:58.25ms
step:1813/2330 train_time:105614ms step_avg:58.25ms
step:1814/2330 train_time:105675ms step_avg:58.26ms
step:1815/2330 train_time:105732ms step_avg:58.25ms
step:1816/2330 train_time:105792ms step_avg:58.26ms
step:1817/2330 train_time:105849ms step_avg:58.25ms
step:1818/2330 train_time:105913ms step_avg:58.26ms
step:1819/2330 train_time:105970ms step_avg:58.26ms
step:1820/2330 train_time:106032ms step_avg:58.26ms
step:1821/2330 train_time:106089ms step_avg:58.26ms
step:1822/2330 train_time:106151ms step_avg:58.26ms
step:1823/2330 train_time:106209ms step_avg:58.26ms
step:1824/2330 train_time:106269ms step_avg:58.26ms
step:1825/2330 train_time:106327ms step_avg:58.26ms
step:1826/2330 train_time:106389ms step_avg:58.26ms
step:1827/2330 train_time:106446ms step_avg:58.26ms
step:1828/2330 train_time:106507ms step_avg:58.26ms
step:1829/2330 train_time:106564ms step_avg:58.26ms
step:1830/2330 train_time:106625ms step_avg:58.26ms
step:1831/2330 train_time:106683ms step_avg:58.26ms
step:1832/2330 train_time:106743ms step_avg:58.27ms
step:1833/2330 train_time:106801ms step_avg:58.27ms
step:1834/2330 train_time:106862ms step_avg:58.27ms
step:1835/2330 train_time:106920ms step_avg:58.27ms
step:1836/2330 train_time:106981ms step_avg:58.27ms
step:1837/2330 train_time:107038ms step_avg:58.27ms
step:1838/2330 train_time:107098ms step_avg:58.27ms
step:1839/2330 train_time:107156ms step_avg:58.27ms
step:1840/2330 train_time:107217ms step_avg:58.27ms
step:1841/2330 train_time:107274ms step_avg:58.27ms
step:1842/2330 train_time:107335ms step_avg:58.27ms
step:1843/2330 train_time:107392ms step_avg:58.27ms
step:1844/2330 train_time:107453ms step_avg:58.27ms
step:1845/2330 train_time:107510ms step_avg:58.27ms
step:1846/2330 train_time:107572ms step_avg:58.27ms
step:1847/2330 train_time:107629ms step_avg:58.27ms
step:1848/2330 train_time:107691ms step_avg:58.27ms
step:1849/2330 train_time:107748ms step_avg:58.27ms
step:1850/2330 train_time:107809ms step_avg:58.28ms
step:1851/2330 train_time:107866ms step_avg:58.27ms
step:1852/2330 train_time:107930ms step_avg:58.28ms
step:1853/2330 train_time:107987ms step_avg:58.28ms
step:1854/2330 train_time:108048ms step_avg:58.28ms
step:1855/2330 train_time:108107ms step_avg:58.28ms
step:1856/2330 train_time:108167ms step_avg:58.28ms
step:1857/2330 train_time:108226ms step_avg:58.28ms
step:1858/2330 train_time:108287ms step_avg:58.28ms
step:1859/2330 train_time:108345ms step_avg:58.28ms
step:1860/2330 train_time:108406ms step_avg:58.28ms
step:1861/2330 train_time:108463ms step_avg:58.28ms
step:1862/2330 train_time:108524ms step_avg:58.28ms
step:1863/2330 train_time:108582ms step_avg:58.28ms
step:1864/2330 train_time:108642ms step_avg:58.28ms
step:1865/2330 train_time:108700ms step_avg:58.28ms
step:1866/2330 train_time:108760ms step_avg:58.29ms
step:1867/2330 train_time:108817ms step_avg:58.28ms
step:1868/2330 train_time:108878ms step_avg:58.29ms
step:1869/2330 train_time:108935ms step_avg:58.29ms
step:1870/2330 train_time:108996ms step_avg:58.29ms
step:1871/2330 train_time:109052ms step_avg:58.29ms
step:1872/2330 train_time:109113ms step_avg:58.29ms
step:1873/2330 train_time:109171ms step_avg:58.29ms
step:1874/2330 train_time:109232ms step_avg:58.29ms
step:1875/2330 train_time:109290ms step_avg:58.29ms
step:1876/2330 train_time:109350ms step_avg:58.29ms
step:1877/2330 train_time:109407ms step_avg:58.29ms
step:1878/2330 train_time:109468ms step_avg:58.29ms
step:1879/2330 train_time:109525ms step_avg:58.29ms
step:1880/2330 train_time:109585ms step_avg:58.29ms
step:1881/2330 train_time:109642ms step_avg:58.29ms
step:1882/2330 train_time:109704ms step_avg:58.29ms
step:1883/2330 train_time:109762ms step_avg:58.29ms
step:1884/2330 train_time:109824ms step_avg:58.29ms
step:1885/2330 train_time:109883ms step_avg:58.29ms
step:1886/2330 train_time:109943ms step_avg:58.29ms
step:1887/2330 train_time:110001ms step_avg:58.29ms
step:1888/2330 train_time:110061ms step_avg:58.30ms
step:1889/2330 train_time:110118ms step_avg:58.29ms
step:1890/2330 train_time:110179ms step_avg:58.30ms
step:1891/2330 train_time:110237ms step_avg:58.30ms
step:1892/2330 train_time:110297ms step_avg:58.30ms
step:1893/2330 train_time:110355ms step_avg:58.30ms
step:1894/2330 train_time:110415ms step_avg:58.30ms
step:1895/2330 train_time:110472ms step_avg:58.30ms
step:1896/2330 train_time:110532ms step_avg:58.30ms
step:1897/2330 train_time:110589ms step_avg:58.30ms
step:1898/2330 train_time:110650ms step_avg:58.30ms
step:1899/2330 train_time:110706ms step_avg:58.30ms
step:1900/2330 train_time:110768ms step_avg:58.30ms
step:1901/2330 train_time:110827ms step_avg:58.30ms
step:1902/2330 train_time:110888ms step_avg:58.30ms
step:1903/2330 train_time:110945ms step_avg:58.30ms
step:1904/2330 train_time:111006ms step_avg:58.30ms
step:1905/2330 train_time:111064ms step_avg:58.30ms
step:1906/2330 train_time:111125ms step_avg:58.30ms
step:1907/2330 train_time:111184ms step_avg:58.30ms
step:1908/2330 train_time:111244ms step_avg:58.30ms
step:1909/2330 train_time:111303ms step_avg:58.30ms
step:1910/2330 train_time:111363ms step_avg:58.31ms
step:1911/2330 train_time:111421ms step_avg:58.31ms
step:1912/2330 train_time:111482ms step_avg:58.31ms
step:1913/2330 train_time:111540ms step_avg:58.31ms
step:1914/2330 train_time:111600ms step_avg:58.31ms
step:1915/2330 train_time:111657ms step_avg:58.31ms
step:1916/2330 train_time:111717ms step_avg:58.31ms
step:1917/2330 train_time:111774ms step_avg:58.31ms
step:1918/2330 train_time:111836ms step_avg:58.31ms
step:1919/2330 train_time:111893ms step_avg:58.31ms
step:1920/2330 train_time:111954ms step_avg:58.31ms
step:1921/2330 train_time:112011ms step_avg:58.31ms
step:1922/2330 train_time:112074ms step_avg:58.31ms
step:1923/2330 train_time:112130ms step_avg:58.31ms
step:1924/2330 train_time:112192ms step_avg:58.31ms
step:1925/2330 train_time:112249ms step_avg:58.31ms
step:1926/2330 train_time:112311ms step_avg:58.31ms
step:1927/2330 train_time:112369ms step_avg:58.31ms
step:1928/2330 train_time:112431ms step_avg:58.31ms
step:1929/2330 train_time:112488ms step_avg:58.31ms
step:1930/2330 train_time:112549ms step_avg:58.32ms
step:1931/2330 train_time:112607ms step_avg:58.32ms
step:1932/2330 train_time:112667ms step_avg:58.32ms
step:1933/2330 train_time:112724ms step_avg:58.32ms
step:1934/2330 train_time:112785ms step_avg:58.32ms
step:1935/2330 train_time:112843ms step_avg:58.32ms
step:1936/2330 train_time:112904ms step_avg:58.32ms
step:1937/2330 train_time:112963ms step_avg:58.32ms
step:1938/2330 train_time:113023ms step_avg:58.32ms
step:1939/2330 train_time:113081ms step_avg:58.32ms
step:1940/2330 train_time:113142ms step_avg:58.32ms
step:1941/2330 train_time:113199ms step_avg:58.32ms
step:1942/2330 train_time:113259ms step_avg:58.32ms
step:1943/2330 train_time:113317ms step_avg:58.32ms
step:1944/2330 train_time:113377ms step_avg:58.32ms
step:1945/2330 train_time:113434ms step_avg:58.32ms
step:1946/2330 train_time:113495ms step_avg:58.32ms
step:1947/2330 train_time:113552ms step_avg:58.32ms
step:1948/2330 train_time:113614ms step_avg:58.32ms
step:1949/2330 train_time:113672ms step_avg:58.32ms
step:1950/2330 train_time:113732ms step_avg:58.32ms
step:1951/2330 train_time:113789ms step_avg:58.32ms
step:1952/2330 train_time:113850ms step_avg:58.32ms
step:1953/2330 train_time:113908ms step_avg:58.32ms
step:1954/2330 train_time:113967ms step_avg:58.33ms
step:1955/2330 train_time:114025ms step_avg:58.32ms
step:1956/2330 train_time:114088ms step_avg:58.33ms
step:1957/2330 train_time:114146ms step_avg:58.33ms
step:1958/2330 train_time:114206ms step_avg:58.33ms
step:1959/2330 train_time:114264ms step_avg:58.33ms
step:1960/2330 train_time:114326ms step_avg:58.33ms
step:1961/2330 train_time:114384ms step_avg:58.33ms
step:1962/2330 train_time:114445ms step_avg:58.33ms
step:1963/2330 train_time:114503ms step_avg:58.33ms
step:1964/2330 train_time:114563ms step_avg:58.33ms
step:1965/2330 train_time:114621ms step_avg:58.33ms
step:1966/2330 train_time:114680ms step_avg:58.33ms
step:1967/2330 train_time:114739ms step_avg:58.33ms
step:1968/2330 train_time:114799ms step_avg:58.33ms
step:1969/2330 train_time:114856ms step_avg:58.33ms
step:1970/2330 train_time:114916ms step_avg:58.33ms
step:1971/2330 train_time:114973ms step_avg:58.33ms
step:1972/2330 train_time:115034ms step_avg:58.33ms
step:1973/2330 train_time:115091ms step_avg:58.33ms
step:1974/2330 train_time:115153ms step_avg:58.33ms
step:1975/2330 train_time:115210ms step_avg:58.33ms
step:1976/2330 train_time:115272ms step_avg:58.34ms
step:1977/2330 train_time:115330ms step_avg:58.34ms
step:1978/2330 train_time:115390ms step_avg:58.34ms
step:1979/2330 train_time:115448ms step_avg:58.34ms
step:1980/2330 train_time:115509ms step_avg:58.34ms
step:1981/2330 train_time:115566ms step_avg:58.34ms
step:1982/2330 train_time:115627ms step_avg:58.34ms
step:1983/2330 train_time:115686ms step_avg:58.34ms
step:1984/2330 train_time:115746ms step_avg:58.34ms
step:1985/2330 train_time:115804ms step_avg:58.34ms
step:1986/2330 train_time:115865ms step_avg:58.34ms
step:1987/2330 train_time:115923ms step_avg:58.34ms
step:1988/2330 train_time:115983ms step_avg:58.34ms
step:1989/2330 train_time:116041ms step_avg:58.34ms
step:1990/2330 train_time:116101ms step_avg:58.34ms
step:1991/2330 train_time:116158ms step_avg:58.34ms
step:1992/2330 train_time:116219ms step_avg:58.34ms
step:1993/2330 train_time:116276ms step_avg:58.34ms
step:1994/2330 train_time:116336ms step_avg:58.34ms
step:1995/2330 train_time:116393ms step_avg:58.34ms
step:1996/2330 train_time:116453ms step_avg:58.34ms
step:1997/2330 train_time:116510ms step_avg:58.34ms
step:1998/2330 train_time:116571ms step_avg:58.34ms
step:1999/2330 train_time:116628ms step_avg:58.34ms
step:2000/2330 train_time:116691ms step_avg:58.35ms
step:2000/2330 val_loss:3.7612 train_time:116773ms step_avg:58.39ms
step:2001/2330 train_time:116793ms step_avg:58.37ms
step:2002/2330 train_time:116813ms step_avg:58.35ms
step:2003/2330 train_time:116874ms step_avg:58.35ms
step:2004/2330 train_time:116938ms step_avg:58.35ms
step:2005/2330 train_time:116996ms step_avg:58.35ms
step:2006/2330 train_time:117059ms step_avg:58.35ms
step:2007/2330 train_time:117115ms step_avg:58.35ms
step:2008/2330 train_time:117176ms step_avg:58.35ms
step:2009/2330 train_time:117233ms step_avg:58.35ms
step:2010/2330 train_time:117293ms step_avg:58.35ms
step:2011/2330 train_time:117350ms step_avg:58.35ms
step:2012/2330 train_time:117410ms step_avg:58.35ms
step:2013/2330 train_time:117467ms step_avg:58.35ms
step:2014/2330 train_time:117526ms step_avg:58.35ms
step:2015/2330 train_time:117583ms step_avg:58.35ms
step:2016/2330 train_time:117643ms step_avg:58.35ms
step:2017/2330 train_time:117699ms step_avg:58.35ms
step:2018/2330 train_time:117760ms step_avg:58.35ms
step:2019/2330 train_time:117818ms step_avg:58.35ms
step:2020/2330 train_time:117881ms step_avg:58.36ms
step:2021/2330 train_time:117940ms step_avg:58.36ms
step:2022/2330 train_time:118002ms step_avg:58.36ms
step:2023/2330 train_time:118060ms step_avg:58.36ms
step:2024/2330 train_time:118122ms step_avg:58.36ms
step:2025/2330 train_time:118179ms step_avg:58.36ms
step:2026/2330 train_time:118238ms step_avg:58.36ms
step:2027/2330 train_time:118296ms step_avg:58.36ms
step:2028/2330 train_time:118355ms step_avg:58.36ms
step:2029/2330 train_time:118412ms step_avg:58.36ms
step:2030/2330 train_time:118471ms step_avg:58.36ms
step:2031/2330 train_time:118528ms step_avg:58.36ms
step:2032/2330 train_time:118588ms step_avg:58.36ms
step:2033/2330 train_time:118645ms step_avg:58.36ms
step:2034/2330 train_time:118706ms step_avg:58.36ms
step:2035/2330 train_time:118764ms step_avg:58.36ms
step:2036/2330 train_time:118825ms step_avg:58.36ms
step:2037/2330 train_time:118883ms step_avg:58.36ms
step:2038/2330 train_time:118947ms step_avg:58.36ms
step:2039/2330 train_time:119006ms step_avg:58.36ms
step:2040/2330 train_time:119067ms step_avg:58.37ms
step:2041/2330 train_time:119126ms step_avg:58.37ms
step:2042/2330 train_time:119186ms step_avg:58.37ms
step:2043/2330 train_time:119245ms step_avg:58.37ms
step:2044/2330 train_time:119305ms step_avg:58.37ms
step:2045/2330 train_time:119363ms step_avg:58.37ms
step:2046/2330 train_time:119423ms step_avg:58.37ms
step:2047/2330 train_time:119481ms step_avg:58.37ms
step:2048/2330 train_time:119540ms step_avg:58.37ms
step:2049/2330 train_time:119597ms step_avg:58.37ms
step:2050/2330 train_time:119657ms step_avg:58.37ms
step:2051/2330 train_time:119714ms step_avg:58.37ms
step:2052/2330 train_time:119774ms step_avg:58.37ms
step:2053/2330 train_time:119831ms step_avg:58.37ms
step:2054/2330 train_time:119893ms step_avg:58.37ms
step:2055/2330 train_time:119950ms step_avg:58.37ms
step:2056/2330 train_time:120012ms step_avg:58.37ms
step:2057/2330 train_time:120070ms step_avg:58.37ms
step:2058/2330 train_time:120132ms step_avg:58.37ms
step:2059/2330 train_time:120190ms step_avg:58.37ms
step:2060/2330 train_time:120251ms step_avg:58.37ms
step:2061/2330 train_time:120309ms step_avg:58.37ms
step:2062/2330 train_time:120369ms step_avg:58.38ms
step:2063/2330 train_time:120427ms step_avg:58.37ms
step:2064/2330 train_time:120487ms step_avg:58.38ms
step:2065/2330 train_time:120544ms step_avg:58.37ms
step:2066/2330 train_time:120605ms step_avg:58.38ms
step:2067/2330 train_time:120663ms step_avg:58.38ms
step:2068/2330 train_time:120724ms step_avg:58.38ms
step:2069/2330 train_time:120782ms step_avg:58.38ms
step:2070/2330 train_time:120842ms step_avg:58.38ms
step:2071/2330 train_time:120900ms step_avg:58.38ms
step:2072/2330 train_time:120960ms step_avg:58.38ms
step:2073/2330 train_time:121018ms step_avg:58.38ms
step:2074/2330 train_time:121078ms step_avg:58.38ms
step:2075/2330 train_time:121136ms step_avg:58.38ms
step:2076/2330 train_time:121197ms step_avg:58.38ms
step:2077/2330 train_time:121254ms step_avg:58.38ms
step:2078/2330 train_time:121316ms step_avg:58.38ms
step:2079/2330 train_time:121373ms step_avg:58.38ms
step:2080/2330 train_time:121435ms step_avg:58.38ms
step:2081/2330 train_time:121491ms step_avg:58.38ms
step:2082/2330 train_time:121554ms step_avg:58.38ms
step:2083/2330 train_time:121610ms step_avg:58.38ms
step:2084/2330 train_time:121672ms step_avg:58.38ms
step:2085/2330 train_time:121729ms step_avg:58.38ms
step:2086/2330 train_time:121789ms step_avg:58.38ms
step:2087/2330 train_time:121847ms step_avg:58.38ms
step:2088/2330 train_time:121908ms step_avg:58.39ms
step:2089/2330 train_time:121967ms step_avg:58.39ms
step:2090/2330 train_time:122027ms step_avg:58.39ms
step:2091/2330 train_time:122086ms step_avg:58.39ms
step:2092/2330 train_time:122147ms step_avg:58.39ms
step:2093/2330 train_time:122205ms step_avg:58.39ms
step:2094/2330 train_time:122266ms step_avg:58.39ms
step:2095/2330 train_time:122323ms step_avg:58.39ms
step:2096/2330 train_time:122383ms step_avg:58.39ms
step:2097/2330 train_time:122440ms step_avg:58.39ms
step:2098/2330 train_time:122501ms step_avg:58.39ms
step:2099/2330 train_time:122558ms step_avg:58.39ms
step:2100/2330 train_time:122618ms step_avg:58.39ms
step:2101/2330 train_time:122675ms step_avg:58.39ms
step:2102/2330 train_time:122736ms step_avg:58.39ms
step:2103/2330 train_time:122793ms step_avg:58.39ms
step:2104/2330 train_time:122854ms step_avg:58.39ms
step:2105/2330 train_time:122911ms step_avg:58.39ms
step:2106/2330 train_time:122972ms step_avg:58.39ms
step:2107/2330 train_time:123031ms step_avg:58.39ms
step:2108/2330 train_time:123092ms step_avg:58.39ms
step:2109/2330 train_time:123151ms step_avg:58.39ms
step:2110/2330 train_time:123211ms step_avg:58.39ms
step:2111/2330 train_time:123269ms step_avg:58.39ms
step:2112/2330 train_time:123330ms step_avg:58.39ms
step:2113/2330 train_time:123388ms step_avg:58.39ms
step:2114/2330 train_time:123449ms step_avg:58.40ms
step:2115/2330 train_time:123507ms step_avg:58.40ms
step:2116/2330 train_time:123567ms step_avg:58.40ms
step:2117/2330 train_time:123624ms step_avg:58.40ms
step:2118/2330 train_time:123685ms step_avg:58.40ms
step:2119/2330 train_time:123743ms step_avg:58.40ms
step:2120/2330 train_time:123804ms step_avg:58.40ms
step:2121/2330 train_time:123861ms step_avg:58.40ms
step:2122/2330 train_time:123922ms step_avg:58.40ms
step:2123/2330 train_time:123980ms step_avg:58.40ms
step:2124/2330 train_time:124040ms step_avg:58.40ms
step:2125/2330 train_time:124098ms step_avg:58.40ms
step:2126/2330 train_time:124159ms step_avg:58.40ms
step:2127/2330 train_time:124215ms step_avg:58.40ms
step:2128/2330 train_time:124276ms step_avg:58.40ms
step:2129/2330 train_time:124333ms step_avg:58.40ms
step:2130/2330 train_time:124396ms step_avg:58.40ms
step:2131/2330 train_time:124453ms step_avg:58.40ms
step:2132/2330 train_time:124515ms step_avg:58.40ms
step:2133/2330 train_time:124572ms step_avg:58.40ms
step:2134/2330 train_time:124633ms step_avg:58.40ms
step:2135/2330 train_time:124689ms step_avg:58.40ms
step:2136/2330 train_time:124752ms step_avg:58.40ms
step:2137/2330 train_time:124809ms step_avg:58.40ms
step:2138/2330 train_time:124871ms step_avg:58.41ms
step:2139/2330 train_time:124929ms step_avg:58.41ms
step:2140/2330 train_time:124990ms step_avg:58.41ms
step:2141/2330 train_time:125048ms step_avg:58.41ms
step:2142/2330 train_time:125109ms step_avg:58.41ms
step:2143/2330 train_time:125167ms step_avg:58.41ms
step:2144/2330 train_time:125228ms step_avg:58.41ms
step:2145/2330 train_time:125286ms step_avg:58.41ms
step:2146/2330 train_time:125347ms step_avg:58.41ms
step:2147/2330 train_time:125406ms step_avg:58.41ms
step:2148/2330 train_time:125466ms step_avg:58.41ms
step:2149/2330 train_time:125524ms step_avg:58.41ms
step:2150/2330 train_time:125584ms step_avg:58.41ms
step:2151/2330 train_time:125641ms step_avg:58.41ms
step:2152/2330 train_time:125702ms step_avg:58.41ms
step:2153/2330 train_time:125760ms step_avg:58.41ms
step:2154/2330 train_time:125820ms step_avg:58.41ms
step:2155/2330 train_time:125877ms step_avg:58.41ms
step:2156/2330 train_time:125938ms step_avg:58.41ms
step:2157/2330 train_time:125995ms step_avg:58.41ms
step:2158/2330 train_time:126056ms step_avg:58.41ms
step:2159/2330 train_time:126113ms step_avg:58.41ms
step:2160/2330 train_time:126174ms step_avg:58.41ms
step:2161/2330 train_time:126231ms step_avg:58.41ms
step:2162/2330 train_time:126292ms step_avg:58.41ms
step:2163/2330 train_time:126351ms step_avg:58.41ms
step:2164/2330 train_time:126411ms step_avg:58.42ms
step:2165/2330 train_time:126470ms step_avg:58.42ms
step:2166/2330 train_time:126530ms step_avg:58.42ms
step:2167/2330 train_time:126587ms step_avg:58.42ms
step:2168/2330 train_time:126648ms step_avg:58.42ms
step:2169/2330 train_time:126705ms step_avg:58.42ms
step:2170/2330 train_time:126767ms step_avg:58.42ms
step:2171/2330 train_time:126825ms step_avg:58.42ms
step:2172/2330 train_time:126886ms step_avg:58.42ms
step:2173/2330 train_time:126944ms step_avg:58.42ms
step:2174/2330 train_time:127005ms step_avg:58.42ms
step:2175/2330 train_time:127064ms step_avg:58.42ms
step:2176/2330 train_time:127124ms step_avg:58.42ms
step:2177/2330 train_time:127181ms step_avg:58.42ms
step:2178/2330 train_time:127241ms step_avg:58.42ms
step:2179/2330 train_time:127297ms step_avg:58.42ms
step:2180/2330 train_time:127358ms step_avg:58.42ms
step:2181/2330 train_time:127415ms step_avg:58.42ms
step:2182/2330 train_time:127476ms step_avg:58.42ms
step:2183/2330 train_time:127533ms step_avg:58.42ms
step:2184/2330 train_time:127596ms step_avg:58.42ms
step:2185/2330 train_time:127653ms step_avg:58.42ms
step:2186/2330 train_time:127714ms step_avg:58.42ms
step:2187/2330 train_time:127770ms step_avg:58.42ms
step:2188/2330 train_time:127833ms step_avg:58.42ms
step:2189/2330 train_time:127890ms step_avg:58.42ms
step:2190/2330 train_time:127952ms step_avg:58.43ms
step:2191/2330 train_time:128010ms step_avg:58.43ms
step:2192/2330 train_time:128071ms step_avg:58.43ms
step:2193/2330 train_time:128129ms step_avg:58.43ms
step:2194/2330 train_time:128190ms step_avg:58.43ms
step:2195/2330 train_time:128248ms step_avg:58.43ms
step:2196/2330 train_time:128309ms step_avg:58.43ms
step:2197/2330 train_time:128367ms step_avg:58.43ms
step:2198/2330 train_time:128427ms step_avg:58.43ms
step:2199/2330 train_time:128485ms step_avg:58.43ms
step:2200/2330 train_time:128546ms step_avg:58.43ms
step:2201/2330 train_time:128604ms step_avg:58.43ms
step:2202/2330 train_time:128666ms step_avg:58.43ms
step:2203/2330 train_time:128723ms step_avg:58.43ms
step:2204/2330 train_time:128784ms step_avg:58.43ms
step:2205/2330 train_time:128842ms step_avg:58.43ms
step:2206/2330 train_time:128902ms step_avg:58.43ms
step:2207/2330 train_time:128960ms step_avg:58.43ms
step:2208/2330 train_time:129020ms step_avg:58.43ms
step:2209/2330 train_time:129077ms step_avg:58.43ms
step:2210/2330 train_time:129138ms step_avg:58.43ms
step:2211/2330 train_time:129196ms step_avg:58.43ms
step:2212/2330 train_time:129257ms step_avg:58.43ms
step:2213/2330 train_time:129314ms step_avg:58.43ms
step:2214/2330 train_time:129374ms step_avg:58.43ms
step:2215/2330 train_time:129432ms step_avg:58.43ms
step:2216/2330 train_time:129492ms step_avg:58.44ms
step:2217/2330 train_time:129549ms step_avg:58.43ms
step:2218/2330 train_time:129611ms step_avg:58.44ms
step:2219/2330 train_time:129668ms step_avg:58.44ms
step:2220/2330 train_time:129729ms step_avg:58.44ms
step:2221/2330 train_time:129786ms step_avg:58.44ms
step:2222/2330 train_time:129848ms step_avg:58.44ms
step:2223/2330 train_time:129907ms step_avg:58.44ms
step:2224/2330 train_time:129967ms step_avg:58.44ms
step:2225/2330 train_time:130025ms step_avg:58.44ms
step:2226/2330 train_time:130086ms step_avg:58.44ms
step:2227/2330 train_time:130145ms step_avg:58.44ms
step:2228/2330 train_time:130206ms step_avg:58.44ms
step:2229/2330 train_time:130264ms step_avg:58.44ms
step:2230/2330 train_time:130325ms step_avg:58.44ms
step:2231/2330 train_time:130382ms step_avg:58.44ms
step:2232/2330 train_time:130442ms step_avg:58.44ms
step:2233/2330 train_time:130499ms step_avg:58.44ms
step:2234/2330 train_time:130560ms step_avg:58.44ms
step:2235/2330 train_time:130617ms step_avg:58.44ms
step:2236/2330 train_time:130678ms step_avg:58.44ms
step:2237/2330 train_time:130734ms step_avg:58.44ms
step:2238/2330 train_time:130796ms step_avg:58.44ms
step:2239/2330 train_time:130853ms step_avg:58.44ms
step:2240/2330 train_time:130914ms step_avg:58.44ms
step:2241/2330 train_time:130971ms step_avg:58.44ms
step:2242/2330 train_time:131031ms step_avg:58.44ms
step:2243/2330 train_time:131088ms step_avg:58.44ms
step:2244/2330 train_time:131151ms step_avg:58.45ms
step:2245/2330 train_time:131209ms step_avg:58.44ms
step:2246/2330 train_time:131269ms step_avg:58.45ms
step:2247/2330 train_time:131327ms step_avg:58.45ms
step:2248/2330 train_time:131387ms step_avg:58.45ms
step:2249/2330 train_time:131446ms step_avg:58.45ms
step:2250/2330 train_time:131506ms step_avg:58.45ms
step:2250/2330 val_loss:3.7139 train_time:131589ms step_avg:58.48ms
step:2251/2330 train_time:131609ms step_avg:58.47ms
step:2252/2330 train_time:131629ms step_avg:58.45ms
step:2253/2330 train_time:131685ms step_avg:58.45ms
step:2254/2330 train_time:131754ms step_avg:58.45ms
step:2255/2330 train_time:131811ms step_avg:58.45ms
step:2256/2330 train_time:131873ms step_avg:58.45ms
step:2257/2330 train_time:131929ms step_avg:58.45ms
step:2258/2330 train_time:131992ms step_avg:58.46ms
step:2259/2330 train_time:132049ms step_avg:58.45ms
step:2260/2330 train_time:132109ms step_avg:58.46ms
step:2261/2330 train_time:132166ms step_avg:58.45ms
step:2262/2330 train_time:132227ms step_avg:58.46ms
step:2263/2330 train_time:132284ms step_avg:58.46ms
step:2264/2330 train_time:132344ms step_avg:58.46ms
step:2265/2330 train_time:132401ms step_avg:58.46ms
step:2266/2330 train_time:132461ms step_avg:58.46ms
step:2267/2330 train_time:132518ms step_avg:58.46ms
step:2268/2330 train_time:132579ms step_avg:58.46ms
step:2269/2330 train_time:132637ms step_avg:58.46ms
step:2270/2330 train_time:132700ms step_avg:58.46ms
step:2271/2330 train_time:132758ms step_avg:58.46ms
step:2272/2330 train_time:132820ms step_avg:58.46ms
step:2273/2330 train_time:132877ms step_avg:58.46ms
step:2274/2330 train_time:132937ms step_avg:58.46ms
step:2275/2330 train_time:132994ms step_avg:58.46ms
step:2276/2330 train_time:133056ms step_avg:58.46ms
step:2277/2330 train_time:133114ms step_avg:58.46ms
step:2278/2330 train_time:133174ms step_avg:58.46ms
step:2279/2330 train_time:133231ms step_avg:58.46ms
step:2280/2330 train_time:133291ms step_avg:58.46ms
step:2281/2330 train_time:133348ms step_avg:58.46ms
step:2282/2330 train_time:133408ms step_avg:58.46ms
step:2283/2330 train_time:133464ms step_avg:58.46ms
step:2284/2330 train_time:133526ms step_avg:58.46ms
step:2285/2330 train_time:133584ms step_avg:58.46ms
step:2286/2330 train_time:133648ms step_avg:58.46ms
step:2287/2330 train_time:133707ms step_avg:58.46ms
step:2288/2330 train_time:133768ms step_avg:58.47ms
step:2289/2330 train_time:133827ms step_avg:58.47ms
step:2290/2330 train_time:133887ms step_avg:58.47ms
step:2291/2330 train_time:133946ms step_avg:58.47ms
step:2292/2330 train_time:134006ms step_avg:58.47ms
step:2293/2330 train_time:134064ms step_avg:58.47ms
step:2294/2330 train_time:134126ms step_avg:58.47ms
step:2295/2330 train_time:134183ms step_avg:58.47ms
step:2296/2330 train_time:134243ms step_avg:58.47ms
step:2297/2330 train_time:134300ms step_avg:58.47ms
step:2298/2330 train_time:134360ms step_avg:58.47ms
step:2299/2330 train_time:134417ms step_avg:58.47ms
step:2300/2330 train_time:134477ms step_avg:58.47ms
step:2301/2330 train_time:134535ms step_avg:58.47ms
step:2302/2330 train_time:134595ms step_avg:58.47ms
step:2303/2330 train_time:134652ms step_avg:58.47ms
step:2304/2330 train_time:134715ms step_avg:58.47ms
step:2305/2330 train_time:134773ms step_avg:58.47ms
step:2306/2330 train_time:134835ms step_avg:58.47ms
step:2307/2330 train_time:134892ms step_avg:58.47ms
step:2308/2330 train_time:134954ms step_avg:58.47ms
step:2309/2330 train_time:135011ms step_avg:58.47ms
step:2310/2330 train_time:135073ms step_avg:58.47ms
step:2311/2330 train_time:135130ms step_avg:58.47ms
step:2312/2330 train_time:135191ms step_avg:58.47ms
step:2313/2330 train_time:135248ms step_avg:58.47ms
step:2314/2330 train_time:135310ms step_avg:58.47ms
step:2315/2330 train_time:135367ms step_avg:58.47ms
step:2316/2330 train_time:135428ms step_avg:58.48ms
step:2317/2330 train_time:135486ms step_avg:58.47ms
step:2318/2330 train_time:135547ms step_avg:58.48ms
step:2319/2330 train_time:135605ms step_avg:58.48ms
step:2320/2330 train_time:135667ms step_avg:58.48ms
step:2321/2330 train_time:135726ms step_avg:58.48ms
step:2322/2330 train_time:135787ms step_avg:58.48ms
step:2323/2330 train_time:135845ms step_avg:58.48ms
step:2324/2330 train_time:135906ms step_avg:58.48ms
step:2325/2330 train_time:135965ms step_avg:58.48ms
step:2326/2330 train_time:136026ms step_avg:58.48ms
step:2327/2330 train_time:136083ms step_avg:58.48ms
step:2328/2330 train_time:136144ms step_avg:58.48ms
step:2329/2330 train_time:136202ms step_avg:58.48ms
step:2330/2330 train_time:136262ms step_avg:58.48ms
step:2330/2330 val_loss:3.6988 train_time:136344ms step_avg:58.52ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
