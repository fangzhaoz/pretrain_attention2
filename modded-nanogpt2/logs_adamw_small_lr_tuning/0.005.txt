import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--adamw_lr', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_lr_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_lr_tuning/{args2.adamw_lr}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=float(args2.adamw_lr),  betas=(0.85, 0.85), eps=1e-10, weight_decay=0.0, fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_lr_tuning/{args2.adamw_lr}", exist_ok=True)
            torch.save(log, f"logs_adamw_small_lr_tuning/{args2.adamw_lr}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 01:55:11 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   36C    P0             117W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   35C    P0             117W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             117W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   28C    P0             115W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   34C    P0             118W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             114W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:78ms step_avg:77.97ms
step:2/2330 train_time:194ms step_avg:96.83ms
step:3/2330 train_time:212ms step_avg:70.58ms
step:4/2330 train_time:231ms step_avg:57.75ms
step:5/2330 train_time:285ms step_avg:56.95ms
step:6/2330 train_time:342ms step_avg:57.08ms
step:7/2330 train_time:397ms step_avg:56.77ms
step:8/2330 train_time:455ms step_avg:56.85ms
step:9/2330 train_time:509ms step_avg:56.61ms
step:10/2330 train_time:566ms step_avg:56.63ms
step:11/2330 train_time:621ms step_avg:56.44ms
step:12/2330 train_time:678ms step_avg:56.50ms
step:13/2330 train_time:733ms step_avg:56.36ms
step:14/2330 train_time:790ms step_avg:56.41ms
step:15/2330 train_time:844ms step_avg:56.26ms
step:16/2330 train_time:902ms step_avg:56.35ms
step:17/2330 train_time:956ms step_avg:56.25ms
step:18/2330 train_time:1014ms step_avg:56.32ms
step:19/2330 train_time:1069ms step_avg:56.25ms
step:20/2330 train_time:1129ms step_avg:56.46ms
step:21/2330 train_time:1186ms step_avg:56.50ms
step:22/2330 train_time:1245ms step_avg:56.59ms
step:23/2330 train_time:1301ms step_avg:56.54ms
step:24/2330 train_time:1359ms step_avg:56.61ms
step:25/2330 train_time:1414ms step_avg:56.55ms
step:26/2330 train_time:1472ms step_avg:56.61ms
step:27/2330 train_time:1526ms step_avg:56.54ms
step:28/2330 train_time:1585ms step_avg:56.59ms
step:29/2330 train_time:1639ms step_avg:56.52ms
step:30/2330 train_time:1697ms step_avg:56.57ms
step:31/2330 train_time:1752ms step_avg:56.52ms
step:32/2330 train_time:1809ms step_avg:56.54ms
step:33/2330 train_time:1864ms step_avg:56.49ms
step:34/2330 train_time:1922ms step_avg:56.54ms
step:35/2330 train_time:1977ms step_avg:56.49ms
step:36/2330 train_time:2036ms step_avg:56.55ms
step:37/2330 train_time:2091ms step_avg:56.52ms
step:38/2330 train_time:2149ms step_avg:56.56ms
step:39/2330 train_time:2205ms step_avg:56.54ms
step:40/2330 train_time:2265ms step_avg:56.62ms
step:41/2330 train_time:2320ms step_avg:56.60ms
step:42/2330 train_time:2379ms step_avg:56.65ms
step:43/2330 train_time:2435ms step_avg:56.63ms
step:44/2330 train_time:2493ms step_avg:56.67ms
step:45/2330 train_time:2549ms step_avg:56.64ms
step:46/2330 train_time:2606ms step_avg:56.66ms
step:47/2330 train_time:2661ms step_avg:56.62ms
step:48/2330 train_time:2720ms step_avg:56.67ms
step:49/2330 train_time:2776ms step_avg:56.64ms
step:50/2330 train_time:2833ms step_avg:56.66ms
step:51/2330 train_time:2889ms step_avg:56.64ms
step:52/2330 train_time:2946ms step_avg:56.65ms
step:53/2330 train_time:3001ms step_avg:56.62ms
step:54/2330 train_time:3059ms step_avg:56.65ms
step:55/2330 train_time:3116ms step_avg:56.65ms
step:56/2330 train_time:3174ms step_avg:56.68ms
step:57/2330 train_time:3230ms step_avg:56.67ms
step:58/2330 train_time:3288ms step_avg:56.69ms
step:59/2330 train_time:3344ms step_avg:56.68ms
step:60/2330 train_time:3404ms step_avg:56.74ms
step:61/2330 train_time:3460ms step_avg:56.72ms
step:62/2330 train_time:3519ms step_avg:56.75ms
step:63/2330 train_time:3574ms step_avg:56.74ms
step:64/2330 train_time:3632ms step_avg:56.76ms
step:65/2330 train_time:3688ms step_avg:56.73ms
step:66/2330 train_time:3745ms step_avg:56.75ms
step:67/2330 train_time:3801ms step_avg:56.73ms
step:68/2330 train_time:3859ms step_avg:56.75ms
step:69/2330 train_time:3915ms step_avg:56.74ms
step:70/2330 train_time:3973ms step_avg:56.75ms
step:71/2330 train_time:4028ms step_avg:56.73ms
step:72/2330 train_time:4086ms step_avg:56.75ms
step:73/2330 train_time:4141ms step_avg:56.73ms
step:74/2330 train_time:4200ms step_avg:56.76ms
step:75/2330 train_time:4256ms step_avg:56.75ms
step:76/2330 train_time:4315ms step_avg:56.78ms
step:77/2330 train_time:4371ms step_avg:56.77ms
step:78/2330 train_time:4429ms step_avg:56.79ms
step:79/2330 train_time:4485ms step_avg:56.77ms
step:80/2330 train_time:4543ms step_avg:56.79ms
step:81/2330 train_time:4599ms step_avg:56.77ms
step:82/2330 train_time:4657ms step_avg:56.80ms
step:83/2330 train_time:4713ms step_avg:56.79ms
step:84/2330 train_time:4771ms step_avg:56.80ms
step:85/2330 train_time:4827ms step_avg:56.79ms
step:86/2330 train_time:4885ms step_avg:56.80ms
step:87/2330 train_time:4940ms step_avg:56.79ms
step:88/2330 train_time:4999ms step_avg:56.80ms
step:89/2330 train_time:5055ms step_avg:56.80ms
step:90/2330 train_time:5113ms step_avg:56.81ms
step:91/2330 train_time:5168ms step_avg:56.79ms
step:92/2330 train_time:5226ms step_avg:56.81ms
step:93/2330 train_time:5282ms step_avg:56.80ms
step:94/2330 train_time:5341ms step_avg:56.82ms
step:95/2330 train_time:5397ms step_avg:56.81ms
step:96/2330 train_time:5455ms step_avg:56.83ms
step:97/2330 train_time:5511ms step_avg:56.82ms
step:98/2330 train_time:5570ms step_avg:56.83ms
step:99/2330 train_time:5625ms step_avg:56.82ms
step:100/2330 train_time:5684ms step_avg:56.84ms
step:101/2330 train_time:5740ms step_avg:56.83ms
step:102/2330 train_time:5798ms step_avg:56.85ms
step:103/2330 train_time:5855ms step_avg:56.84ms
step:104/2330 train_time:5913ms step_avg:56.85ms
step:105/2330 train_time:5968ms step_avg:56.84ms
step:106/2330 train_time:6026ms step_avg:56.85ms
step:107/2330 train_time:6082ms step_avg:56.84ms
step:108/2330 train_time:6140ms step_avg:56.86ms
step:109/2330 train_time:6196ms step_avg:56.85ms
step:110/2330 train_time:6256ms step_avg:56.87ms
step:111/2330 train_time:6311ms step_avg:56.86ms
step:112/2330 train_time:6369ms step_avg:56.87ms
step:113/2330 train_time:6425ms step_avg:56.86ms
step:114/2330 train_time:6484ms step_avg:56.88ms
step:115/2330 train_time:6541ms step_avg:56.87ms
step:116/2330 train_time:6599ms step_avg:56.89ms
step:117/2330 train_time:6654ms step_avg:56.88ms
step:118/2330 train_time:6713ms step_avg:56.89ms
step:119/2330 train_time:6769ms step_avg:56.88ms
step:120/2330 train_time:6827ms step_avg:56.89ms
step:121/2330 train_time:6882ms step_avg:56.88ms
step:122/2330 train_time:6942ms step_avg:56.90ms
step:123/2330 train_time:6998ms step_avg:56.89ms
step:124/2330 train_time:7056ms step_avg:56.91ms
step:125/2330 train_time:7112ms step_avg:56.90ms
step:126/2330 train_time:7171ms step_avg:56.91ms
step:127/2330 train_time:7226ms step_avg:56.90ms
step:128/2330 train_time:7285ms step_avg:56.91ms
step:129/2330 train_time:7341ms step_avg:56.91ms
step:130/2330 train_time:7400ms step_avg:56.92ms
step:131/2330 train_time:7457ms step_avg:56.92ms
step:132/2330 train_time:7515ms step_avg:56.93ms
step:133/2330 train_time:7571ms step_avg:56.92ms
step:134/2330 train_time:7629ms step_avg:56.93ms
step:135/2330 train_time:7685ms step_avg:56.92ms
step:136/2330 train_time:7744ms step_avg:56.94ms
step:137/2330 train_time:7800ms step_avg:56.93ms
step:138/2330 train_time:7858ms step_avg:56.94ms
step:139/2330 train_time:7914ms step_avg:56.94ms
step:140/2330 train_time:7973ms step_avg:56.95ms
step:141/2330 train_time:8029ms step_avg:56.95ms
step:142/2330 train_time:8087ms step_avg:56.95ms
step:143/2330 train_time:8143ms step_avg:56.94ms
step:144/2330 train_time:8201ms step_avg:56.95ms
step:145/2330 train_time:8257ms step_avg:56.94ms
step:146/2330 train_time:8315ms step_avg:56.95ms
step:147/2330 train_time:8371ms step_avg:56.95ms
step:148/2330 train_time:8429ms step_avg:56.95ms
step:149/2330 train_time:8485ms step_avg:56.94ms
step:150/2330 train_time:8543ms step_avg:56.96ms
step:151/2330 train_time:8599ms step_avg:56.95ms
step:152/2330 train_time:8658ms step_avg:56.96ms
step:153/2330 train_time:8714ms step_avg:56.95ms
step:154/2330 train_time:8772ms step_avg:56.96ms
step:155/2330 train_time:8828ms step_avg:56.96ms
step:156/2330 train_time:8887ms step_avg:56.97ms
step:157/2330 train_time:8942ms step_avg:56.95ms
step:158/2330 train_time:9001ms step_avg:56.97ms
step:159/2330 train_time:9057ms step_avg:56.96ms
step:160/2330 train_time:9116ms step_avg:56.97ms
step:161/2330 train_time:9171ms step_avg:56.96ms
step:162/2330 train_time:9229ms step_avg:56.97ms
step:163/2330 train_time:9285ms step_avg:56.96ms
step:164/2330 train_time:9344ms step_avg:56.97ms
step:165/2330 train_time:9399ms step_avg:56.97ms
step:166/2330 train_time:9458ms step_avg:56.98ms
step:167/2330 train_time:9513ms step_avg:56.97ms
step:168/2330 train_time:9572ms step_avg:56.98ms
step:169/2330 train_time:9628ms step_avg:56.97ms
step:170/2330 train_time:9686ms step_avg:56.98ms
step:171/2330 train_time:9742ms step_avg:56.97ms
step:172/2330 train_time:9801ms step_avg:56.98ms
step:173/2330 train_time:9857ms step_avg:56.98ms
step:174/2330 train_time:9915ms step_avg:56.98ms
step:175/2330 train_time:9971ms step_avg:56.98ms
step:176/2330 train_time:10030ms step_avg:56.99ms
step:177/2330 train_time:10086ms step_avg:56.98ms
step:178/2330 train_time:10144ms step_avg:56.99ms
step:179/2330 train_time:10200ms step_avg:56.98ms
step:180/2330 train_time:10259ms step_avg:57.00ms
step:181/2330 train_time:10315ms step_avg:56.99ms
step:182/2330 train_time:10374ms step_avg:57.00ms
step:183/2330 train_time:10430ms step_avg:56.99ms
step:184/2330 train_time:10488ms step_avg:57.00ms
step:185/2330 train_time:10544ms step_avg:56.99ms
step:186/2330 train_time:10602ms step_avg:57.00ms
step:187/2330 train_time:10659ms step_avg:57.00ms
step:188/2330 train_time:10718ms step_avg:57.01ms
step:189/2330 train_time:10773ms step_avg:57.00ms
step:190/2330 train_time:10832ms step_avg:57.01ms
step:191/2330 train_time:10887ms step_avg:57.00ms
step:192/2330 train_time:10946ms step_avg:57.01ms
step:193/2330 train_time:11002ms step_avg:57.00ms
step:194/2330 train_time:11061ms step_avg:57.01ms
step:195/2330 train_time:11117ms step_avg:57.01ms
step:196/2330 train_time:11176ms step_avg:57.02ms
step:197/2330 train_time:11232ms step_avg:57.01ms
step:198/2330 train_time:11290ms step_avg:57.02ms
step:199/2330 train_time:11346ms step_avg:57.01ms
step:200/2330 train_time:11405ms step_avg:57.02ms
step:201/2330 train_time:11461ms step_avg:57.02ms
step:202/2330 train_time:11519ms step_avg:57.03ms
step:203/2330 train_time:11575ms step_avg:57.02ms
step:204/2330 train_time:11633ms step_avg:57.03ms
step:205/2330 train_time:11689ms step_avg:57.02ms
step:206/2330 train_time:11747ms step_avg:57.03ms
step:207/2330 train_time:11803ms step_avg:57.02ms
step:208/2330 train_time:11862ms step_avg:57.03ms
step:209/2330 train_time:11919ms step_avg:57.03ms
step:210/2330 train_time:11977ms step_avg:57.04ms
step:211/2330 train_time:12033ms step_avg:57.03ms
step:212/2330 train_time:12091ms step_avg:57.03ms
step:213/2330 train_time:12147ms step_avg:57.03ms
step:214/2330 train_time:12206ms step_avg:57.04ms
step:215/2330 train_time:12262ms step_avg:57.03ms
step:216/2330 train_time:12321ms step_avg:57.04ms
step:217/2330 train_time:12378ms step_avg:57.04ms
step:218/2330 train_time:12437ms step_avg:57.05ms
step:219/2330 train_time:12492ms step_avg:57.04ms
step:220/2330 train_time:12551ms step_avg:57.05ms
step:221/2330 train_time:12606ms step_avg:57.04ms
step:222/2330 train_time:12665ms step_avg:57.05ms
step:223/2330 train_time:12720ms step_avg:57.04ms
step:224/2330 train_time:12778ms step_avg:57.05ms
step:225/2330 train_time:12834ms step_avg:57.04ms
step:226/2330 train_time:12893ms step_avg:57.05ms
step:227/2330 train_time:12948ms step_avg:57.04ms
step:228/2330 train_time:13007ms step_avg:57.05ms
step:229/2330 train_time:13063ms step_avg:57.04ms
step:230/2330 train_time:13122ms step_avg:57.05ms
step:231/2330 train_time:13179ms step_avg:57.05ms
step:232/2330 train_time:13237ms step_avg:57.06ms
step:233/2330 train_time:13292ms step_avg:57.05ms
step:234/2330 train_time:13351ms step_avg:57.05ms
step:235/2330 train_time:13407ms step_avg:57.05ms
step:236/2330 train_time:13466ms step_avg:57.06ms
step:237/2330 train_time:13522ms step_avg:57.05ms
step:238/2330 train_time:13581ms step_avg:57.06ms
step:239/2330 train_time:13637ms step_avg:57.06ms
step:240/2330 train_time:13695ms step_avg:57.06ms
step:241/2330 train_time:13751ms step_avg:57.06ms
step:242/2330 train_time:13809ms step_avg:57.06ms
step:243/2330 train_time:13865ms step_avg:57.06ms
step:244/2330 train_time:13923ms step_avg:57.06ms
step:245/2330 train_time:13979ms step_avg:57.06ms
step:246/2330 train_time:14038ms step_avg:57.06ms
step:247/2330 train_time:14093ms step_avg:57.06ms
step:248/2330 train_time:14152ms step_avg:57.06ms
step:249/2330 train_time:14207ms step_avg:57.06ms
step:250/2330 train_time:14266ms step_avg:57.06ms
step:250/2330 val_loss:5.5346 train_time:14344ms step_avg:57.38ms
step:251/2330 train_time:14362ms step_avg:57.22ms
step:252/2330 train_time:14382ms step_avg:57.07ms
step:253/2330 train_time:14437ms step_avg:57.06ms
step:254/2330 train_time:14501ms step_avg:57.09ms
step:255/2330 train_time:14555ms step_avg:57.08ms
step:256/2330 train_time:14622ms step_avg:57.12ms
step:257/2330 train_time:14677ms step_avg:57.11ms
step:258/2330 train_time:14738ms step_avg:57.12ms
step:259/2330 train_time:14793ms step_avg:57.11ms
step:260/2330 train_time:14851ms step_avg:57.12ms
step:261/2330 train_time:14906ms step_avg:57.11ms
step:262/2330 train_time:14964ms step_avg:57.12ms
step:263/2330 train_time:15020ms step_avg:57.11ms
step:264/2330 train_time:15078ms step_avg:57.11ms
step:265/2330 train_time:15133ms step_avg:57.11ms
step:266/2330 train_time:15190ms step_avg:57.11ms
step:267/2330 train_time:15246ms step_avg:57.10ms
step:268/2330 train_time:15305ms step_avg:57.11ms
step:269/2330 train_time:15362ms step_avg:57.11ms
step:270/2330 train_time:15421ms step_avg:57.11ms
step:271/2330 train_time:15477ms step_avg:57.11ms
step:272/2330 train_time:15536ms step_avg:57.12ms
step:273/2330 train_time:15591ms step_avg:57.11ms
step:274/2330 train_time:15651ms step_avg:57.12ms
step:275/2330 train_time:15707ms step_avg:57.12ms
step:276/2330 train_time:15766ms step_avg:57.12ms
step:277/2330 train_time:15822ms step_avg:57.12ms
step:278/2330 train_time:15881ms step_avg:57.13ms
step:279/2330 train_time:15937ms step_avg:57.12ms
step:280/2330 train_time:15995ms step_avg:57.13ms
step:281/2330 train_time:16051ms step_avg:57.12ms
step:282/2330 train_time:16109ms step_avg:57.12ms
step:283/2330 train_time:16164ms step_avg:57.12ms
step:284/2330 train_time:16223ms step_avg:57.12ms
step:285/2330 train_time:16280ms step_avg:57.12ms
step:286/2330 train_time:16339ms step_avg:57.13ms
step:287/2330 train_time:16395ms step_avg:57.13ms
step:288/2330 train_time:16454ms step_avg:57.13ms
step:289/2330 train_time:16510ms step_avg:57.13ms
step:290/2330 train_time:16570ms step_avg:57.14ms
step:291/2330 train_time:16626ms step_avg:57.13ms
step:292/2330 train_time:16685ms step_avg:57.14ms
step:293/2330 train_time:16741ms step_avg:57.14ms
step:294/2330 train_time:16799ms step_avg:57.14ms
step:295/2330 train_time:16855ms step_avg:57.14ms
step:296/2330 train_time:16914ms step_avg:57.14ms
step:297/2330 train_time:16969ms step_avg:57.14ms
step:298/2330 train_time:17028ms step_avg:57.14ms
step:299/2330 train_time:17084ms step_avg:57.14ms
step:300/2330 train_time:17142ms step_avg:57.14ms
step:301/2330 train_time:17197ms step_avg:57.13ms
step:302/2330 train_time:17256ms step_avg:57.14ms
step:303/2330 train_time:17311ms step_avg:57.13ms
step:304/2330 train_time:17370ms step_avg:57.14ms
step:305/2330 train_time:17427ms step_avg:57.14ms
step:306/2330 train_time:17485ms step_avg:57.14ms
step:307/2330 train_time:17542ms step_avg:57.14ms
step:308/2330 train_time:17601ms step_avg:57.14ms
step:309/2330 train_time:17656ms step_avg:57.14ms
step:310/2330 train_time:17715ms step_avg:57.14ms
step:311/2330 train_time:17770ms step_avg:57.14ms
step:312/2330 train_time:17830ms step_avg:57.15ms
step:313/2330 train_time:17885ms step_avg:57.14ms
step:314/2330 train_time:17945ms step_avg:57.15ms
step:315/2330 train_time:18001ms step_avg:57.15ms
step:316/2330 train_time:18060ms step_avg:57.15ms
step:317/2330 train_time:18116ms step_avg:57.15ms
step:318/2330 train_time:18174ms step_avg:57.15ms
step:319/2330 train_time:18229ms step_avg:57.14ms
step:320/2330 train_time:18288ms step_avg:57.15ms
step:321/2330 train_time:18344ms step_avg:57.15ms
step:322/2330 train_time:18403ms step_avg:57.15ms
step:323/2330 train_time:18459ms step_avg:57.15ms
step:324/2330 train_time:18519ms step_avg:57.16ms
step:325/2330 train_time:18574ms step_avg:57.15ms
step:326/2330 train_time:18634ms step_avg:57.16ms
step:327/2330 train_time:18689ms step_avg:57.15ms
step:328/2330 train_time:18748ms step_avg:57.16ms
step:329/2330 train_time:18804ms step_avg:57.16ms
step:330/2330 train_time:18863ms step_avg:57.16ms
step:331/2330 train_time:18919ms step_avg:57.16ms
step:332/2330 train_time:18978ms step_avg:57.16ms
step:333/2330 train_time:19033ms step_avg:57.16ms
step:334/2330 train_time:19092ms step_avg:57.16ms
step:335/2330 train_time:19147ms step_avg:57.16ms
step:336/2330 train_time:19207ms step_avg:57.16ms
step:337/2330 train_time:19263ms step_avg:57.16ms
step:338/2330 train_time:19321ms step_avg:57.16ms
step:339/2330 train_time:19377ms step_avg:57.16ms
step:340/2330 train_time:19435ms step_avg:57.16ms
step:341/2330 train_time:19491ms step_avg:57.16ms
step:342/2330 train_time:19550ms step_avg:57.16ms
step:343/2330 train_time:19606ms step_avg:57.16ms
step:344/2330 train_time:19665ms step_avg:57.17ms
step:345/2330 train_time:19721ms step_avg:57.16ms
step:346/2330 train_time:19780ms step_avg:57.17ms
step:347/2330 train_time:19836ms step_avg:57.16ms
step:348/2330 train_time:19895ms step_avg:57.17ms
step:349/2330 train_time:19951ms step_avg:57.17ms
step:350/2330 train_time:20010ms step_avg:57.17ms
step:351/2330 train_time:20065ms step_avg:57.17ms
step:352/2330 train_time:20124ms step_avg:57.17ms
step:353/2330 train_time:20180ms step_avg:57.17ms
step:354/2330 train_time:20239ms step_avg:57.17ms
step:355/2330 train_time:20295ms step_avg:57.17ms
step:356/2330 train_time:20353ms step_avg:57.17ms
step:357/2330 train_time:20409ms step_avg:57.17ms
step:358/2330 train_time:20468ms step_avg:57.17ms
step:359/2330 train_time:20524ms step_avg:57.17ms
step:360/2330 train_time:20582ms step_avg:57.17ms
step:361/2330 train_time:20638ms step_avg:57.17ms
step:362/2330 train_time:20697ms step_avg:57.17ms
step:363/2330 train_time:20752ms step_avg:57.17ms
step:364/2330 train_time:20811ms step_avg:57.17ms
step:365/2330 train_time:20867ms step_avg:57.17ms
step:366/2330 train_time:20926ms step_avg:57.17ms
step:367/2330 train_time:20982ms step_avg:57.17ms
step:368/2330 train_time:21040ms step_avg:57.18ms
step:369/2330 train_time:21096ms step_avg:57.17ms
step:370/2330 train_time:21155ms step_avg:57.17ms
step:371/2330 train_time:21210ms step_avg:57.17ms
step:372/2330 train_time:21269ms step_avg:57.17ms
step:373/2330 train_time:21324ms step_avg:57.17ms
step:374/2330 train_time:21384ms step_avg:57.18ms
step:375/2330 train_time:21440ms step_avg:57.17ms
step:376/2330 train_time:21499ms step_avg:57.18ms
step:377/2330 train_time:21555ms step_avg:57.17ms
step:378/2330 train_time:21613ms step_avg:57.18ms
step:379/2330 train_time:21669ms step_avg:57.17ms
step:380/2330 train_time:21728ms step_avg:57.18ms
step:381/2330 train_time:21784ms step_avg:57.18ms
step:382/2330 train_time:21843ms step_avg:57.18ms
step:383/2330 train_time:21899ms step_avg:57.18ms
step:384/2330 train_time:21957ms step_avg:57.18ms
step:385/2330 train_time:22013ms step_avg:57.18ms
step:386/2330 train_time:22071ms step_avg:57.18ms
step:387/2330 train_time:22127ms step_avg:57.17ms
step:388/2330 train_time:22185ms step_avg:57.18ms
step:389/2330 train_time:22241ms step_avg:57.17ms
step:390/2330 train_time:22299ms step_avg:57.18ms
step:391/2330 train_time:22355ms step_avg:57.17ms
step:392/2330 train_time:22413ms step_avg:57.18ms
step:393/2330 train_time:22469ms step_avg:57.17ms
step:394/2330 train_time:22529ms step_avg:57.18ms
step:395/2330 train_time:22585ms step_avg:57.18ms
step:396/2330 train_time:22644ms step_avg:57.18ms
step:397/2330 train_time:22701ms step_avg:57.18ms
step:398/2330 train_time:22759ms step_avg:57.18ms
step:399/2330 train_time:22815ms step_avg:57.18ms
step:400/2330 train_time:22873ms step_avg:57.18ms
step:401/2330 train_time:22930ms step_avg:57.18ms
step:402/2330 train_time:22988ms step_avg:57.18ms
step:403/2330 train_time:23044ms step_avg:57.18ms
step:404/2330 train_time:23103ms step_avg:57.19ms
step:405/2330 train_time:23159ms step_avg:57.18ms
step:406/2330 train_time:23217ms step_avg:57.19ms
step:407/2330 train_time:23273ms step_avg:57.18ms
step:408/2330 train_time:23332ms step_avg:57.19ms
step:409/2330 train_time:23387ms step_avg:57.18ms
step:410/2330 train_time:23446ms step_avg:57.19ms
step:411/2330 train_time:23503ms step_avg:57.19ms
step:412/2330 train_time:23562ms step_avg:57.19ms
step:413/2330 train_time:23618ms step_avg:57.19ms
step:414/2330 train_time:23677ms step_avg:57.19ms
step:415/2330 train_time:23733ms step_avg:57.19ms
step:416/2330 train_time:23790ms step_avg:57.19ms
step:417/2330 train_time:23846ms step_avg:57.18ms
step:418/2330 train_time:23906ms step_avg:57.19ms
step:419/2330 train_time:23962ms step_avg:57.19ms
step:420/2330 train_time:24020ms step_avg:57.19ms
step:421/2330 train_time:24076ms step_avg:57.19ms
step:422/2330 train_time:24134ms step_avg:57.19ms
step:423/2330 train_time:24190ms step_avg:57.19ms
step:424/2330 train_time:24249ms step_avg:57.19ms
step:425/2330 train_time:24304ms step_avg:57.19ms
step:426/2330 train_time:24363ms step_avg:57.19ms
step:427/2330 train_time:24419ms step_avg:57.19ms
step:428/2330 train_time:24478ms step_avg:57.19ms
step:429/2330 train_time:24534ms step_avg:57.19ms
step:430/2330 train_time:24593ms step_avg:57.19ms
step:431/2330 train_time:24648ms step_avg:57.19ms
step:432/2330 train_time:24708ms step_avg:57.19ms
step:433/2330 train_time:24764ms step_avg:57.19ms
step:434/2330 train_time:24823ms step_avg:57.20ms
step:435/2330 train_time:24879ms step_avg:57.19ms
step:436/2330 train_time:24938ms step_avg:57.20ms
step:437/2330 train_time:24994ms step_avg:57.19ms
step:438/2330 train_time:25052ms step_avg:57.20ms
step:439/2330 train_time:25107ms step_avg:57.19ms
step:440/2330 train_time:25167ms step_avg:57.20ms
step:441/2330 train_time:25223ms step_avg:57.20ms
step:442/2330 train_time:25282ms step_avg:57.20ms
step:443/2330 train_time:25338ms step_avg:57.20ms
step:444/2330 train_time:25396ms step_avg:57.20ms
step:445/2330 train_time:25452ms step_avg:57.20ms
step:446/2330 train_time:25510ms step_avg:57.20ms
step:447/2330 train_time:25566ms step_avg:57.19ms
step:448/2330 train_time:25626ms step_avg:57.20ms
step:449/2330 train_time:25682ms step_avg:57.20ms
step:450/2330 train_time:25741ms step_avg:57.20ms
step:451/2330 train_time:25797ms step_avg:57.20ms
step:452/2330 train_time:25855ms step_avg:57.20ms
step:453/2330 train_time:25912ms step_avg:57.20ms
step:454/2330 train_time:25970ms step_avg:57.20ms
step:455/2330 train_time:26026ms step_avg:57.20ms
step:456/2330 train_time:26085ms step_avg:57.20ms
step:457/2330 train_time:26141ms step_avg:57.20ms
step:458/2330 train_time:26199ms step_avg:57.20ms
step:459/2330 train_time:26255ms step_avg:57.20ms
step:460/2330 train_time:26313ms step_avg:57.20ms
step:461/2330 train_time:26370ms step_avg:57.20ms
step:462/2330 train_time:26428ms step_avg:57.20ms
step:463/2330 train_time:26484ms step_avg:57.20ms
step:464/2330 train_time:26542ms step_avg:57.20ms
step:465/2330 train_time:26598ms step_avg:57.20ms
step:466/2330 train_time:26657ms step_avg:57.20ms
step:467/2330 train_time:26713ms step_avg:57.20ms
step:468/2330 train_time:26772ms step_avg:57.20ms
step:469/2330 train_time:26828ms step_avg:57.20ms
step:470/2330 train_time:26887ms step_avg:57.21ms
step:471/2330 train_time:26944ms step_avg:57.21ms
step:472/2330 train_time:27002ms step_avg:57.21ms
step:473/2330 train_time:27058ms step_avg:57.21ms
step:474/2330 train_time:27117ms step_avg:57.21ms
step:475/2330 train_time:27173ms step_avg:57.21ms
step:476/2330 train_time:27232ms step_avg:57.21ms
step:477/2330 train_time:27287ms step_avg:57.21ms
step:478/2330 train_time:27346ms step_avg:57.21ms
step:479/2330 train_time:27402ms step_avg:57.21ms
step:480/2330 train_time:27460ms step_avg:57.21ms
step:481/2330 train_time:27516ms step_avg:57.21ms
step:482/2330 train_time:27575ms step_avg:57.21ms
step:483/2330 train_time:27631ms step_avg:57.21ms
step:484/2330 train_time:27690ms step_avg:57.21ms
step:485/2330 train_time:27745ms step_avg:57.21ms
step:486/2330 train_time:27805ms step_avg:57.21ms
step:487/2330 train_time:27862ms step_avg:57.21ms
step:488/2330 train_time:27921ms step_avg:57.22ms
step:489/2330 train_time:27977ms step_avg:57.21ms
step:490/2330 train_time:28035ms step_avg:57.21ms
step:491/2330 train_time:28091ms step_avg:57.21ms
step:492/2330 train_time:28149ms step_avg:57.21ms
step:493/2330 train_time:28206ms step_avg:57.21ms
step:494/2330 train_time:28265ms step_avg:57.22ms
step:495/2330 train_time:28321ms step_avg:57.21ms
step:496/2330 train_time:28379ms step_avg:57.22ms
step:497/2330 train_time:28435ms step_avg:57.21ms
step:498/2330 train_time:28494ms step_avg:57.22ms
step:499/2330 train_time:28550ms step_avg:57.21ms
step:500/2330 train_time:28609ms step_avg:57.22ms
step:500/2330 val_loss:4.8984 train_time:28688ms step_avg:57.38ms
step:501/2330 train_time:28706ms step_avg:57.30ms
step:502/2330 train_time:28726ms step_avg:57.22ms
step:503/2330 train_time:28781ms step_avg:57.22ms
step:504/2330 train_time:28846ms step_avg:57.23ms
step:505/2330 train_time:28903ms step_avg:57.23ms
step:506/2330 train_time:28962ms step_avg:57.24ms
step:507/2330 train_time:29018ms step_avg:57.24ms
step:508/2330 train_time:29077ms step_avg:57.24ms
step:509/2330 train_time:29133ms step_avg:57.24ms
step:510/2330 train_time:29191ms step_avg:57.24ms
step:511/2330 train_time:29246ms step_avg:57.23ms
step:512/2330 train_time:29304ms step_avg:57.24ms
step:513/2330 train_time:29360ms step_avg:57.23ms
step:514/2330 train_time:29418ms step_avg:57.23ms
step:515/2330 train_time:29473ms step_avg:57.23ms
step:516/2330 train_time:29531ms step_avg:57.23ms
step:517/2330 train_time:29587ms step_avg:57.23ms
step:518/2330 train_time:29645ms step_avg:57.23ms
step:519/2330 train_time:29701ms step_avg:57.23ms
step:520/2330 train_time:29761ms step_avg:57.23ms
step:521/2330 train_time:29817ms step_avg:57.23ms
step:522/2330 train_time:29877ms step_avg:57.24ms
step:523/2330 train_time:29933ms step_avg:57.23ms
step:524/2330 train_time:29994ms step_avg:57.24ms
step:525/2330 train_time:30050ms step_avg:57.24ms
step:526/2330 train_time:30109ms step_avg:57.24ms
step:527/2330 train_time:30165ms step_avg:57.24ms
step:528/2330 train_time:30224ms step_avg:57.24ms
step:529/2330 train_time:30279ms step_avg:57.24ms
step:530/2330 train_time:30338ms step_avg:57.24ms
step:531/2330 train_time:30393ms step_avg:57.24ms
step:532/2330 train_time:30452ms step_avg:57.24ms
step:533/2330 train_time:30508ms step_avg:57.24ms
step:534/2330 train_time:30567ms step_avg:57.24ms
step:535/2330 train_time:30623ms step_avg:57.24ms
step:536/2330 train_time:30682ms step_avg:57.24ms
step:537/2330 train_time:30738ms step_avg:57.24ms
step:538/2330 train_time:30797ms step_avg:57.24ms
step:539/2330 train_time:30853ms step_avg:57.24ms
step:540/2330 train_time:30914ms step_avg:57.25ms
step:541/2330 train_time:30970ms step_avg:57.25ms
step:542/2330 train_time:31029ms step_avg:57.25ms
step:543/2330 train_time:31085ms step_avg:57.25ms
step:544/2330 train_time:31144ms step_avg:57.25ms
step:545/2330 train_time:31200ms step_avg:57.25ms
step:546/2330 train_time:31259ms step_avg:57.25ms
step:547/2330 train_time:31315ms step_avg:57.25ms
step:548/2330 train_time:31373ms step_avg:57.25ms
step:549/2330 train_time:31429ms step_avg:57.25ms
step:550/2330 train_time:31487ms step_avg:57.25ms
step:551/2330 train_time:31542ms step_avg:57.25ms
step:552/2330 train_time:31601ms step_avg:57.25ms
step:553/2330 train_time:31657ms step_avg:57.25ms
step:554/2330 train_time:31715ms step_avg:57.25ms
step:555/2330 train_time:31771ms step_avg:57.24ms
step:556/2330 train_time:31830ms step_avg:57.25ms
step:557/2330 train_time:31886ms step_avg:57.25ms
step:558/2330 train_time:31947ms step_avg:57.25ms
step:559/2330 train_time:32003ms step_avg:57.25ms
step:560/2330 train_time:32062ms step_avg:57.25ms
step:561/2330 train_time:32119ms step_avg:57.25ms
step:562/2330 train_time:32177ms step_avg:57.26ms
step:563/2330 train_time:32233ms step_avg:57.25ms
step:564/2330 train_time:32292ms step_avg:57.26ms
step:565/2330 train_time:32348ms step_avg:57.25ms
step:566/2330 train_time:32406ms step_avg:57.25ms
step:567/2330 train_time:32462ms step_avg:57.25ms
step:568/2330 train_time:32521ms step_avg:57.25ms
step:569/2330 train_time:32577ms step_avg:57.25ms
step:570/2330 train_time:32635ms step_avg:57.25ms
step:571/2330 train_time:32691ms step_avg:57.25ms
step:572/2330 train_time:32750ms step_avg:57.26ms
step:573/2330 train_time:32806ms step_avg:57.25ms
step:574/2330 train_time:32865ms step_avg:57.26ms
step:575/2330 train_time:32920ms step_avg:57.25ms
step:576/2330 train_time:32981ms step_avg:57.26ms
step:577/2330 train_time:33037ms step_avg:57.26ms
step:578/2330 train_time:33095ms step_avg:57.26ms
step:579/2330 train_time:33151ms step_avg:57.26ms
step:580/2330 train_time:33211ms step_avg:57.26ms
step:581/2330 train_time:33267ms step_avg:57.26ms
step:582/2330 train_time:33326ms step_avg:57.26ms
step:583/2330 train_time:33382ms step_avg:57.26ms
step:584/2330 train_time:33440ms step_avg:57.26ms
step:585/2330 train_time:33496ms step_avg:57.26ms
step:586/2330 train_time:33554ms step_avg:57.26ms
step:587/2330 train_time:33610ms step_avg:57.26ms
step:588/2330 train_time:33668ms step_avg:57.26ms
step:589/2330 train_time:33724ms step_avg:57.26ms
step:590/2330 train_time:33783ms step_avg:57.26ms
step:591/2330 train_time:33838ms step_avg:57.26ms
step:592/2330 train_time:33897ms step_avg:57.26ms
step:593/2330 train_time:33953ms step_avg:57.26ms
step:594/2330 train_time:34013ms step_avg:57.26ms
step:595/2330 train_time:34069ms step_avg:57.26ms
step:596/2330 train_time:34128ms step_avg:57.26ms
step:597/2330 train_time:34184ms step_avg:57.26ms
step:598/2330 train_time:34243ms step_avg:57.26ms
step:599/2330 train_time:34299ms step_avg:57.26ms
step:600/2330 train_time:34357ms step_avg:57.26ms
step:601/2330 train_time:34413ms step_avg:57.26ms
step:602/2330 train_time:34473ms step_avg:57.26ms
step:603/2330 train_time:34528ms step_avg:57.26ms
step:604/2330 train_time:34588ms step_avg:57.26ms
step:605/2330 train_time:34644ms step_avg:57.26ms
step:606/2330 train_time:34702ms step_avg:57.26ms
step:607/2330 train_time:34759ms step_avg:57.26ms
step:608/2330 train_time:34817ms step_avg:57.26ms
step:609/2330 train_time:34873ms step_avg:57.26ms
step:610/2330 train_time:34932ms step_avg:57.27ms
step:611/2330 train_time:34988ms step_avg:57.26ms
step:612/2330 train_time:35048ms step_avg:57.27ms
step:613/2330 train_time:35104ms step_avg:57.27ms
step:614/2330 train_time:35163ms step_avg:57.27ms
step:615/2330 train_time:35220ms step_avg:57.27ms
step:616/2330 train_time:35278ms step_avg:57.27ms
step:617/2330 train_time:35335ms step_avg:57.27ms
step:618/2330 train_time:35393ms step_avg:57.27ms
step:619/2330 train_time:35449ms step_avg:57.27ms
step:620/2330 train_time:35508ms step_avg:57.27ms
step:621/2330 train_time:35564ms step_avg:57.27ms
step:622/2330 train_time:35622ms step_avg:57.27ms
step:623/2330 train_time:35678ms step_avg:57.27ms
step:624/2330 train_time:35738ms step_avg:57.27ms
step:625/2330 train_time:35793ms step_avg:57.27ms
step:626/2330 train_time:35853ms step_avg:57.27ms
step:627/2330 train_time:35909ms step_avg:57.27ms
step:628/2330 train_time:35968ms step_avg:57.27ms
step:629/2330 train_time:36024ms step_avg:57.27ms
step:630/2330 train_time:36082ms step_avg:57.27ms
step:631/2330 train_time:36138ms step_avg:57.27ms
step:632/2330 train_time:36197ms step_avg:57.27ms
step:633/2330 train_time:36253ms step_avg:57.27ms
step:634/2330 train_time:36312ms step_avg:57.27ms
step:635/2330 train_time:36368ms step_avg:57.27ms
step:636/2330 train_time:36428ms step_avg:57.28ms
step:637/2330 train_time:36484ms step_avg:57.27ms
step:638/2330 train_time:36543ms step_avg:57.28ms
step:639/2330 train_time:36600ms step_avg:57.28ms
step:640/2330 train_time:36658ms step_avg:57.28ms
step:641/2330 train_time:36714ms step_avg:57.28ms
step:642/2330 train_time:36773ms step_avg:57.28ms
step:643/2330 train_time:36829ms step_avg:57.28ms
step:644/2330 train_time:36888ms step_avg:57.28ms
step:645/2330 train_time:36944ms step_avg:57.28ms
step:646/2330 train_time:37003ms step_avg:57.28ms
step:647/2330 train_time:37059ms step_avg:57.28ms
step:648/2330 train_time:37118ms step_avg:57.28ms
step:649/2330 train_time:37174ms step_avg:57.28ms
step:650/2330 train_time:37232ms step_avg:57.28ms
step:651/2330 train_time:37289ms step_avg:57.28ms
step:652/2330 train_time:37347ms step_avg:57.28ms
step:653/2330 train_time:37403ms step_avg:57.28ms
step:654/2330 train_time:37463ms step_avg:57.28ms
step:655/2330 train_time:37520ms step_avg:57.28ms
step:656/2330 train_time:37578ms step_avg:57.28ms
step:657/2330 train_time:37634ms step_avg:57.28ms
step:658/2330 train_time:37693ms step_avg:57.28ms
step:659/2330 train_time:37749ms step_avg:57.28ms
step:660/2330 train_time:37807ms step_avg:57.28ms
step:661/2330 train_time:37864ms step_avg:57.28ms
step:662/2330 train_time:37923ms step_avg:57.29ms
step:663/2330 train_time:37979ms step_avg:57.28ms
step:664/2330 train_time:38037ms step_avg:57.29ms
step:665/2330 train_time:38093ms step_avg:57.28ms
step:666/2330 train_time:38152ms step_avg:57.29ms
step:667/2330 train_time:38208ms step_avg:57.28ms
step:668/2330 train_time:38267ms step_avg:57.29ms
step:669/2330 train_time:38324ms step_avg:57.28ms
step:670/2330 train_time:38383ms step_avg:57.29ms
step:671/2330 train_time:38439ms step_avg:57.29ms
step:672/2330 train_time:38498ms step_avg:57.29ms
step:673/2330 train_time:38554ms step_avg:57.29ms
step:674/2330 train_time:38613ms step_avg:57.29ms
step:675/2330 train_time:38668ms step_avg:57.29ms
step:676/2330 train_time:38727ms step_avg:57.29ms
step:677/2330 train_time:38783ms step_avg:57.29ms
step:678/2330 train_time:38842ms step_avg:57.29ms
step:679/2330 train_time:38898ms step_avg:57.29ms
step:680/2330 train_time:38957ms step_avg:57.29ms
step:681/2330 train_time:39013ms step_avg:57.29ms
step:682/2330 train_time:39073ms step_avg:57.29ms
step:683/2330 train_time:39129ms step_avg:57.29ms
step:684/2330 train_time:39188ms step_avg:57.29ms
step:685/2330 train_time:39244ms step_avg:57.29ms
step:686/2330 train_time:39302ms step_avg:57.29ms
step:687/2330 train_time:39358ms step_avg:57.29ms
step:688/2330 train_time:39417ms step_avg:57.29ms
step:689/2330 train_time:39473ms step_avg:57.29ms
step:690/2330 train_time:39532ms step_avg:57.29ms
step:691/2330 train_time:39588ms step_avg:57.29ms
step:692/2330 train_time:39647ms step_avg:57.29ms
step:693/2330 train_time:39703ms step_avg:57.29ms
step:694/2330 train_time:39762ms step_avg:57.29ms
step:695/2330 train_time:39818ms step_avg:57.29ms
step:696/2330 train_time:39876ms step_avg:57.29ms
step:697/2330 train_time:39932ms step_avg:57.29ms
step:698/2330 train_time:39991ms step_avg:57.29ms
step:699/2330 train_time:40047ms step_avg:57.29ms
step:700/2330 train_time:40106ms step_avg:57.29ms
step:701/2330 train_time:40162ms step_avg:57.29ms
step:702/2330 train_time:40221ms step_avg:57.30ms
step:703/2330 train_time:40278ms step_avg:57.29ms
step:704/2330 train_time:40337ms step_avg:57.30ms
step:705/2330 train_time:40392ms step_avg:57.29ms
step:706/2330 train_time:40451ms step_avg:57.30ms
step:707/2330 train_time:40507ms step_avg:57.29ms
step:708/2330 train_time:40567ms step_avg:57.30ms
step:709/2330 train_time:40623ms step_avg:57.30ms
step:710/2330 train_time:40682ms step_avg:57.30ms
step:711/2330 train_time:40739ms step_avg:57.30ms
step:712/2330 train_time:40797ms step_avg:57.30ms
step:713/2330 train_time:40853ms step_avg:57.30ms
step:714/2330 train_time:40912ms step_avg:57.30ms
step:715/2330 train_time:40968ms step_avg:57.30ms
step:716/2330 train_time:41028ms step_avg:57.30ms
step:717/2330 train_time:41084ms step_avg:57.30ms
step:718/2330 train_time:41142ms step_avg:57.30ms
step:719/2330 train_time:41198ms step_avg:57.30ms
step:720/2330 train_time:41257ms step_avg:57.30ms
step:721/2330 train_time:41313ms step_avg:57.30ms
step:722/2330 train_time:41372ms step_avg:57.30ms
step:723/2330 train_time:41428ms step_avg:57.30ms
step:724/2330 train_time:41487ms step_avg:57.30ms
step:725/2330 train_time:41543ms step_avg:57.30ms
step:726/2330 train_time:41602ms step_avg:57.30ms
step:727/2330 train_time:41658ms step_avg:57.30ms
step:728/2330 train_time:41717ms step_avg:57.30ms
step:729/2330 train_time:41773ms step_avg:57.30ms
step:730/2330 train_time:41832ms step_avg:57.30ms
step:731/2330 train_time:41889ms step_avg:57.30ms
step:732/2330 train_time:41947ms step_avg:57.31ms
step:733/2330 train_time:42003ms step_avg:57.30ms
step:734/2330 train_time:42063ms step_avg:57.31ms
step:735/2330 train_time:42119ms step_avg:57.30ms
step:736/2330 train_time:42178ms step_avg:57.31ms
step:737/2330 train_time:42233ms step_avg:57.30ms
step:738/2330 train_time:42292ms step_avg:57.31ms
step:739/2330 train_time:42347ms step_avg:57.30ms
step:740/2330 train_time:42406ms step_avg:57.31ms
step:741/2330 train_time:42463ms step_avg:57.31ms
step:742/2330 train_time:42523ms step_avg:57.31ms
step:743/2330 train_time:42579ms step_avg:57.31ms
step:744/2330 train_time:42637ms step_avg:57.31ms
step:745/2330 train_time:42692ms step_avg:57.31ms
step:746/2330 train_time:42752ms step_avg:57.31ms
step:747/2330 train_time:42808ms step_avg:57.31ms
step:748/2330 train_time:42868ms step_avg:57.31ms
step:749/2330 train_time:42924ms step_avg:57.31ms
step:750/2330 train_time:42984ms step_avg:57.31ms
step:750/2330 val_loss:4.5255 train_time:43063ms step_avg:57.42ms
step:751/2330 train_time:43082ms step_avg:57.37ms
step:752/2330 train_time:43102ms step_avg:57.32ms
step:753/2330 train_time:43159ms step_avg:57.32ms
step:754/2330 train_time:43222ms step_avg:57.32ms
step:755/2330 train_time:43279ms step_avg:57.32ms
step:756/2330 train_time:43340ms step_avg:57.33ms
step:757/2330 train_time:43395ms step_avg:57.33ms
step:758/2330 train_time:43454ms step_avg:57.33ms
step:759/2330 train_time:43509ms step_avg:57.32ms
step:760/2330 train_time:43568ms step_avg:57.33ms
step:761/2330 train_time:43624ms step_avg:57.32ms
step:762/2330 train_time:43682ms step_avg:57.33ms
step:763/2330 train_time:43738ms step_avg:57.32ms
step:764/2330 train_time:43796ms step_avg:57.32ms
step:765/2330 train_time:43853ms step_avg:57.32ms
step:766/2330 train_time:43911ms step_avg:57.33ms
step:767/2330 train_time:43968ms step_avg:57.32ms
step:768/2330 train_time:44027ms step_avg:57.33ms
step:769/2330 train_time:44085ms step_avg:57.33ms
step:770/2330 train_time:44145ms step_avg:57.33ms
step:771/2330 train_time:44204ms step_avg:57.33ms
step:772/2330 train_time:44266ms step_avg:57.34ms
step:773/2330 train_time:44324ms step_avg:57.34ms
step:774/2330 train_time:44384ms step_avg:57.34ms
step:775/2330 train_time:44442ms step_avg:57.34ms
step:776/2330 train_time:44501ms step_avg:57.35ms
step:777/2330 train_time:44557ms step_avg:57.35ms
step:778/2330 train_time:44617ms step_avg:57.35ms
step:779/2330 train_time:44674ms step_avg:57.35ms
step:780/2330 train_time:44732ms step_avg:57.35ms
step:781/2330 train_time:44789ms step_avg:57.35ms
step:782/2330 train_time:44848ms step_avg:57.35ms
step:783/2330 train_time:44904ms step_avg:57.35ms
step:784/2330 train_time:44964ms step_avg:57.35ms
step:785/2330 train_time:45020ms step_avg:57.35ms
step:786/2330 train_time:45081ms step_avg:57.35ms
step:787/2330 train_time:45137ms step_avg:57.35ms
step:788/2330 train_time:45198ms step_avg:57.36ms
step:789/2330 train_time:45255ms step_avg:57.36ms
step:790/2330 train_time:45314ms step_avg:57.36ms
step:791/2330 train_time:45371ms step_avg:57.36ms
step:792/2330 train_time:45432ms step_avg:57.36ms
step:793/2330 train_time:45489ms step_avg:57.36ms
step:794/2330 train_time:45549ms step_avg:57.37ms
step:795/2330 train_time:45607ms step_avg:57.37ms
step:796/2330 train_time:45666ms step_avg:57.37ms
step:797/2330 train_time:45723ms step_avg:57.37ms
step:798/2330 train_time:45782ms step_avg:57.37ms
step:799/2330 train_time:45838ms step_avg:57.37ms
step:800/2330 train_time:45897ms step_avg:57.37ms
step:801/2330 train_time:45953ms step_avg:57.37ms
step:802/2330 train_time:46013ms step_avg:57.37ms
step:803/2330 train_time:46071ms step_avg:57.37ms
step:804/2330 train_time:46131ms step_avg:57.38ms
step:805/2330 train_time:46188ms step_avg:57.38ms
step:806/2330 train_time:46248ms step_avg:57.38ms
step:807/2330 train_time:46305ms step_avg:57.38ms
step:808/2330 train_time:46365ms step_avg:57.38ms
step:809/2330 train_time:46423ms step_avg:57.38ms
step:810/2330 train_time:46483ms step_avg:57.39ms
step:811/2330 train_time:46540ms step_avg:57.39ms
step:812/2330 train_time:46600ms step_avg:57.39ms
step:813/2330 train_time:46656ms step_avg:57.39ms
step:814/2330 train_time:46715ms step_avg:57.39ms
step:815/2330 train_time:46771ms step_avg:57.39ms
step:816/2330 train_time:46831ms step_avg:57.39ms
step:817/2330 train_time:46887ms step_avg:57.39ms
step:818/2330 train_time:46946ms step_avg:57.39ms
step:819/2330 train_time:47003ms step_avg:57.39ms
step:820/2330 train_time:47064ms step_avg:57.39ms
step:821/2330 train_time:47121ms step_avg:57.40ms
step:822/2330 train_time:47181ms step_avg:57.40ms
step:823/2330 train_time:47238ms step_avg:57.40ms
step:824/2330 train_time:47298ms step_avg:57.40ms
step:825/2330 train_time:47355ms step_avg:57.40ms
step:826/2330 train_time:47415ms step_avg:57.40ms
step:827/2330 train_time:47471ms step_avg:57.40ms
step:828/2330 train_time:47532ms step_avg:57.41ms
step:829/2330 train_time:47589ms step_avg:57.41ms
step:830/2330 train_time:47649ms step_avg:57.41ms
step:831/2330 train_time:47706ms step_avg:57.41ms
step:832/2330 train_time:47767ms step_avg:57.41ms
step:833/2330 train_time:47824ms step_avg:57.41ms
step:834/2330 train_time:47883ms step_avg:57.41ms
step:835/2330 train_time:47940ms step_avg:57.41ms
step:836/2330 train_time:48000ms step_avg:57.42ms
step:837/2330 train_time:48056ms step_avg:57.42ms
step:838/2330 train_time:48116ms step_avg:57.42ms
step:839/2330 train_time:48173ms step_avg:57.42ms
step:840/2330 train_time:48234ms step_avg:57.42ms
step:841/2330 train_time:48290ms step_avg:57.42ms
step:842/2330 train_time:48350ms step_avg:57.42ms
step:843/2330 train_time:48407ms step_avg:57.42ms
step:844/2330 train_time:48467ms step_avg:57.43ms
step:845/2330 train_time:48525ms step_avg:57.43ms
step:846/2330 train_time:48585ms step_avg:57.43ms
step:847/2330 train_time:48642ms step_avg:57.43ms
step:848/2330 train_time:48701ms step_avg:57.43ms
step:849/2330 train_time:48757ms step_avg:57.43ms
step:850/2330 train_time:48817ms step_avg:57.43ms
step:851/2330 train_time:48873ms step_avg:57.43ms
step:852/2330 train_time:48932ms step_avg:57.43ms
step:853/2330 train_time:48990ms step_avg:57.43ms
step:854/2330 train_time:49049ms step_avg:57.43ms
step:855/2330 train_time:49106ms step_avg:57.43ms
step:856/2330 train_time:49165ms step_avg:57.44ms
step:857/2330 train_time:49222ms step_avg:57.44ms
step:858/2330 train_time:49282ms step_avg:57.44ms
step:859/2330 train_time:49339ms step_avg:57.44ms
step:860/2330 train_time:49399ms step_avg:57.44ms
step:861/2330 train_time:49456ms step_avg:57.44ms
step:862/2330 train_time:49516ms step_avg:57.44ms
step:863/2330 train_time:49573ms step_avg:57.44ms
step:864/2330 train_time:49634ms step_avg:57.45ms
step:865/2330 train_time:49691ms step_avg:57.45ms
step:866/2330 train_time:49750ms step_avg:57.45ms
step:867/2330 train_time:49807ms step_avg:57.45ms
step:868/2330 train_time:49867ms step_avg:57.45ms
step:869/2330 train_time:49923ms step_avg:57.45ms
step:870/2330 train_time:49983ms step_avg:57.45ms
step:871/2330 train_time:50040ms step_avg:57.45ms
step:872/2330 train_time:50101ms step_avg:57.45ms
step:873/2330 train_time:50157ms step_avg:57.45ms
step:874/2330 train_time:50215ms step_avg:57.45ms
step:875/2330 train_time:50272ms step_avg:57.45ms
step:876/2330 train_time:50332ms step_avg:57.46ms
step:877/2330 train_time:50389ms step_avg:57.46ms
step:878/2330 train_time:50450ms step_avg:57.46ms
step:879/2330 train_time:50507ms step_avg:57.46ms
step:880/2330 train_time:50567ms step_avg:57.46ms
step:881/2330 train_time:50624ms step_avg:57.46ms
step:882/2330 train_time:50684ms step_avg:57.47ms
step:883/2330 train_time:50742ms step_avg:57.47ms
step:884/2330 train_time:50801ms step_avg:57.47ms
step:885/2330 train_time:50857ms step_avg:57.47ms
step:886/2330 train_time:50917ms step_avg:57.47ms
step:887/2330 train_time:50973ms step_avg:57.47ms
step:888/2330 train_time:51033ms step_avg:57.47ms
step:889/2330 train_time:51090ms step_avg:57.47ms
step:890/2330 train_time:51149ms step_avg:57.47ms
step:891/2330 train_time:51205ms step_avg:57.47ms
step:892/2330 train_time:51265ms step_avg:57.47ms
step:893/2330 train_time:51323ms step_avg:57.47ms
step:894/2330 train_time:51382ms step_avg:57.47ms
step:895/2330 train_time:51439ms step_avg:57.47ms
step:896/2330 train_time:51498ms step_avg:57.48ms
step:897/2330 train_time:51554ms step_avg:57.47ms
step:898/2330 train_time:51616ms step_avg:57.48ms
step:899/2330 train_time:51673ms step_avg:57.48ms
step:900/2330 train_time:51733ms step_avg:57.48ms
step:901/2330 train_time:51790ms step_avg:57.48ms
step:902/2330 train_time:51849ms step_avg:57.48ms
step:903/2330 train_time:51907ms step_avg:57.48ms
step:904/2330 train_time:51967ms step_avg:57.49ms
step:905/2330 train_time:52023ms step_avg:57.48ms
step:906/2330 train_time:52083ms step_avg:57.49ms
step:907/2330 train_time:52140ms step_avg:57.49ms
step:908/2330 train_time:52199ms step_avg:57.49ms
step:909/2330 train_time:52256ms step_avg:57.49ms
step:910/2330 train_time:52315ms step_avg:57.49ms
step:911/2330 train_time:52372ms step_avg:57.49ms
step:912/2330 train_time:52432ms step_avg:57.49ms
step:913/2330 train_time:52488ms step_avg:57.49ms
step:914/2330 train_time:52549ms step_avg:57.49ms
step:915/2330 train_time:52606ms step_avg:57.49ms
step:916/2330 train_time:52665ms step_avg:57.49ms
step:917/2330 train_time:52722ms step_avg:57.49ms
step:918/2330 train_time:52781ms step_avg:57.50ms
step:919/2330 train_time:52838ms step_avg:57.49ms
step:920/2330 train_time:52897ms step_avg:57.50ms
step:921/2330 train_time:52955ms step_avg:57.50ms
step:922/2330 train_time:53014ms step_avg:57.50ms
step:923/2330 train_time:53071ms step_avg:57.50ms
step:924/2330 train_time:53131ms step_avg:57.50ms
step:925/2330 train_time:53188ms step_avg:57.50ms
step:926/2330 train_time:53248ms step_avg:57.50ms
step:927/2330 train_time:53304ms step_avg:57.50ms
step:928/2330 train_time:53364ms step_avg:57.50ms
step:929/2330 train_time:53421ms step_avg:57.50ms
step:930/2330 train_time:53482ms step_avg:57.51ms
step:931/2330 train_time:53538ms step_avg:57.51ms
step:932/2330 train_time:53598ms step_avg:57.51ms
step:933/2330 train_time:53655ms step_avg:57.51ms
step:934/2330 train_time:53715ms step_avg:57.51ms
step:935/2330 train_time:53771ms step_avg:57.51ms
step:936/2330 train_time:53831ms step_avg:57.51ms
step:937/2330 train_time:53888ms step_avg:57.51ms
step:938/2330 train_time:53947ms step_avg:57.51ms
step:939/2330 train_time:54004ms step_avg:57.51ms
step:940/2330 train_time:54063ms step_avg:57.51ms
step:941/2330 train_time:54121ms step_avg:57.51ms
step:942/2330 train_time:54180ms step_avg:57.52ms
step:943/2330 train_time:54235ms step_avg:57.51ms
step:944/2330 train_time:54296ms step_avg:57.52ms
step:945/2330 train_time:54352ms step_avg:57.52ms
step:946/2330 train_time:54413ms step_avg:57.52ms
step:947/2330 train_time:54470ms step_avg:57.52ms
step:948/2330 train_time:54530ms step_avg:57.52ms
step:949/2330 train_time:54587ms step_avg:57.52ms
step:950/2330 train_time:54646ms step_avg:57.52ms
step:951/2330 train_time:54702ms step_avg:57.52ms
step:952/2330 train_time:54763ms step_avg:57.52ms
step:953/2330 train_time:54820ms step_avg:57.52ms
step:954/2330 train_time:54879ms step_avg:57.53ms
step:955/2330 train_time:54935ms step_avg:57.52ms
step:956/2330 train_time:54995ms step_avg:57.53ms
step:957/2330 train_time:55051ms step_avg:57.52ms
step:958/2330 train_time:55112ms step_avg:57.53ms
step:959/2330 train_time:55169ms step_avg:57.53ms
step:960/2330 train_time:55228ms step_avg:57.53ms
step:961/2330 train_time:55285ms step_avg:57.53ms
step:962/2330 train_time:55345ms step_avg:57.53ms
step:963/2330 train_time:55402ms step_avg:57.53ms
step:964/2330 train_time:55462ms step_avg:57.53ms
step:965/2330 train_time:55518ms step_avg:57.53ms
step:966/2330 train_time:55579ms step_avg:57.54ms
step:967/2330 train_time:55635ms step_avg:57.53ms
step:968/2330 train_time:55695ms step_avg:57.54ms
step:969/2330 train_time:55751ms step_avg:57.53ms
step:970/2330 train_time:55811ms step_avg:57.54ms
step:971/2330 train_time:55868ms step_avg:57.54ms
step:972/2330 train_time:55928ms step_avg:57.54ms
step:973/2330 train_time:55985ms step_avg:57.54ms
step:974/2330 train_time:56045ms step_avg:57.54ms
step:975/2330 train_time:56102ms step_avg:57.54ms
step:976/2330 train_time:56161ms step_avg:57.54ms
step:977/2330 train_time:56217ms step_avg:57.54ms
step:978/2330 train_time:56277ms step_avg:57.54ms
step:979/2330 train_time:56334ms step_avg:57.54ms
step:980/2330 train_time:56393ms step_avg:57.54ms
step:981/2330 train_time:56450ms step_avg:57.54ms
step:982/2330 train_time:56511ms step_avg:57.55ms
step:983/2330 train_time:56568ms step_avg:57.55ms
step:984/2330 train_time:56628ms step_avg:57.55ms
step:985/2330 train_time:56685ms step_avg:57.55ms
step:986/2330 train_time:56745ms step_avg:57.55ms
step:987/2330 train_time:56802ms step_avg:57.55ms
step:988/2330 train_time:56863ms step_avg:57.55ms
step:989/2330 train_time:56920ms step_avg:57.55ms
step:990/2330 train_time:56979ms step_avg:57.55ms
step:991/2330 train_time:57035ms step_avg:57.55ms
step:992/2330 train_time:57095ms step_avg:57.56ms
step:993/2330 train_time:57152ms step_avg:57.55ms
step:994/2330 train_time:57212ms step_avg:57.56ms
step:995/2330 train_time:57269ms step_avg:57.56ms
step:996/2330 train_time:57328ms step_avg:57.56ms
step:997/2330 train_time:57384ms step_avg:57.56ms
step:998/2330 train_time:57444ms step_avg:57.56ms
step:999/2330 train_time:57501ms step_avg:57.56ms
step:1000/2330 train_time:57560ms step_avg:57.56ms
step:1000/2330 val_loss:4.2915 train_time:57641ms step_avg:57.64ms
step:1001/2330 train_time:57660ms step_avg:57.60ms
step:1002/2330 train_time:57679ms step_avg:57.56ms
step:1003/2330 train_time:57734ms step_avg:57.56ms
step:1004/2330 train_time:57801ms step_avg:57.57ms
step:1005/2330 train_time:57856ms step_avg:57.57ms
step:1006/2330 train_time:57918ms step_avg:57.57ms
step:1007/2330 train_time:57973ms step_avg:57.57ms
step:1008/2330 train_time:58034ms step_avg:57.57ms
step:1009/2330 train_time:58090ms step_avg:57.57ms
step:1010/2330 train_time:58150ms step_avg:57.57ms
step:1011/2330 train_time:58206ms step_avg:57.57ms
step:1012/2330 train_time:58266ms step_avg:57.57ms
step:1013/2330 train_time:58322ms step_avg:57.57ms
step:1014/2330 train_time:58380ms step_avg:57.57ms
step:1015/2330 train_time:58436ms step_avg:57.57ms
step:1016/2330 train_time:58495ms step_avg:57.57ms
step:1017/2330 train_time:58553ms step_avg:57.57ms
step:1018/2330 train_time:58614ms step_avg:57.58ms
step:1019/2330 train_time:58673ms step_avg:57.58ms
step:1020/2330 train_time:58734ms step_avg:57.58ms
step:1021/2330 train_time:58791ms step_avg:57.58ms
step:1022/2330 train_time:58851ms step_avg:57.58ms
step:1023/2330 train_time:58907ms step_avg:57.58ms
step:1024/2330 train_time:58968ms step_avg:57.59ms
step:1025/2330 train_time:59025ms step_avg:57.58ms
step:1026/2330 train_time:59084ms step_avg:57.59ms
step:1027/2330 train_time:59140ms step_avg:57.59ms
step:1028/2330 train_time:59199ms step_avg:57.59ms
step:1029/2330 train_time:59255ms step_avg:57.59ms
step:1030/2330 train_time:59314ms step_avg:57.59ms
step:1031/2330 train_time:59371ms step_avg:57.59ms
step:1032/2330 train_time:59430ms step_avg:57.59ms
step:1033/2330 train_time:59487ms step_avg:57.59ms
step:1034/2330 train_time:59547ms step_avg:57.59ms
step:1035/2330 train_time:59605ms step_avg:57.59ms
step:1036/2330 train_time:59666ms step_avg:57.59ms
step:1037/2330 train_time:59723ms step_avg:57.59ms
step:1038/2330 train_time:59782ms step_avg:57.59ms
step:1039/2330 train_time:59839ms step_avg:57.59ms
step:1040/2330 train_time:59898ms step_avg:57.59ms
step:1041/2330 train_time:59955ms step_avg:57.59ms
step:1042/2330 train_time:60016ms step_avg:57.60ms
step:1043/2330 train_time:60072ms step_avg:57.60ms
step:1044/2330 train_time:60131ms step_avg:57.60ms
step:1045/2330 train_time:60188ms step_avg:57.60ms
step:1046/2330 train_time:60248ms step_avg:57.60ms
step:1047/2330 train_time:60304ms step_avg:57.60ms
step:1048/2330 train_time:60363ms step_avg:57.60ms
step:1049/2330 train_time:60419ms step_avg:57.60ms
step:1050/2330 train_time:60478ms step_avg:57.60ms
step:1051/2330 train_time:60535ms step_avg:57.60ms
step:1052/2330 train_time:60595ms step_avg:57.60ms
step:1053/2330 train_time:60652ms step_avg:57.60ms
step:1054/2330 train_time:60712ms step_avg:57.60ms
step:1055/2330 train_time:60770ms step_avg:57.60ms
step:1056/2330 train_time:60829ms step_avg:57.60ms
step:1057/2330 train_time:60887ms step_avg:57.60ms
step:1058/2330 train_time:60947ms step_avg:57.61ms
step:1059/2330 train_time:61004ms step_avg:57.61ms
step:1060/2330 train_time:61064ms step_avg:57.61ms
step:1061/2330 train_time:61120ms step_avg:57.61ms
step:1062/2330 train_time:61179ms step_avg:57.61ms
step:1063/2330 train_time:61235ms step_avg:57.61ms
step:1064/2330 train_time:61294ms step_avg:57.61ms
step:1065/2330 train_time:61351ms step_avg:57.61ms
step:1066/2330 train_time:61410ms step_avg:57.61ms
step:1067/2330 train_time:61467ms step_avg:57.61ms
step:1068/2330 train_time:61527ms step_avg:57.61ms
step:1069/2330 train_time:61584ms step_avg:57.61ms
step:1070/2330 train_time:61643ms step_avg:57.61ms
step:1071/2330 train_time:61700ms step_avg:57.61ms
step:1072/2330 train_time:61760ms step_avg:57.61ms
step:1073/2330 train_time:61816ms step_avg:57.61ms
step:1074/2330 train_time:61878ms step_avg:57.61ms
step:1075/2330 train_time:61935ms step_avg:57.61ms
step:1076/2330 train_time:61995ms step_avg:57.62ms
step:1077/2330 train_time:62052ms step_avg:57.62ms
step:1078/2330 train_time:62111ms step_avg:57.62ms
step:1079/2330 train_time:62168ms step_avg:57.62ms
step:1080/2330 train_time:62228ms step_avg:57.62ms
step:1081/2330 train_time:62284ms step_avg:57.62ms
step:1082/2330 train_time:62344ms step_avg:57.62ms
step:1083/2330 train_time:62401ms step_avg:57.62ms
step:1084/2330 train_time:62460ms step_avg:57.62ms
step:1085/2330 train_time:62517ms step_avg:57.62ms
step:1086/2330 train_time:62577ms step_avg:57.62ms
step:1087/2330 train_time:62633ms step_avg:57.62ms
step:1088/2330 train_time:62693ms step_avg:57.62ms
step:1089/2330 train_time:62751ms step_avg:57.62ms
step:1090/2330 train_time:62811ms step_avg:57.62ms
step:1091/2330 train_time:62869ms step_avg:57.62ms
step:1092/2330 train_time:62928ms step_avg:57.63ms
step:1093/2330 train_time:62985ms step_avg:57.63ms
step:1094/2330 train_time:63045ms step_avg:57.63ms
step:1095/2330 train_time:63101ms step_avg:57.63ms
step:1096/2330 train_time:63160ms step_avg:57.63ms
step:1097/2330 train_time:63217ms step_avg:57.63ms
step:1098/2330 train_time:63277ms step_avg:57.63ms
step:1099/2330 train_time:63334ms step_avg:57.63ms
step:1100/2330 train_time:63393ms step_avg:57.63ms
step:1101/2330 train_time:63450ms step_avg:57.63ms
step:1102/2330 train_time:63510ms step_avg:57.63ms
step:1103/2330 train_time:63567ms step_avg:57.63ms
step:1104/2330 train_time:63627ms step_avg:57.63ms
step:1105/2330 train_time:63684ms step_avg:57.63ms
step:1106/2330 train_time:63743ms step_avg:57.63ms
step:1107/2330 train_time:63801ms step_avg:57.63ms
step:1108/2330 train_time:63860ms step_avg:57.64ms
step:1109/2330 train_time:63917ms step_avg:57.63ms
step:1110/2330 train_time:63977ms step_avg:57.64ms
step:1111/2330 train_time:64034ms step_avg:57.64ms
step:1112/2330 train_time:64094ms step_avg:57.64ms
step:1113/2330 train_time:64151ms step_avg:57.64ms
step:1114/2330 train_time:64211ms step_avg:57.64ms
step:1115/2330 train_time:64268ms step_avg:57.64ms
step:1116/2330 train_time:64327ms step_avg:57.64ms
step:1117/2330 train_time:64384ms step_avg:57.64ms
step:1118/2330 train_time:64443ms step_avg:57.64ms
step:1119/2330 train_time:64500ms step_avg:57.64ms
step:1120/2330 train_time:64559ms step_avg:57.64ms
step:1121/2330 train_time:64616ms step_avg:57.64ms
step:1122/2330 train_time:64676ms step_avg:57.64ms
step:1123/2330 train_time:64732ms step_avg:57.64ms
step:1124/2330 train_time:64792ms step_avg:57.64ms
step:1125/2330 train_time:64850ms step_avg:57.64ms
step:1126/2330 train_time:64909ms step_avg:57.65ms
step:1127/2330 train_time:64966ms step_avg:57.64ms
step:1128/2330 train_time:65025ms step_avg:57.65ms
step:1129/2330 train_time:65082ms step_avg:57.65ms
step:1130/2330 train_time:65141ms step_avg:57.65ms
step:1131/2330 train_time:65198ms step_avg:57.65ms
step:1132/2330 train_time:65258ms step_avg:57.65ms
step:1133/2330 train_time:65314ms step_avg:57.65ms
step:1134/2330 train_time:65374ms step_avg:57.65ms
step:1135/2330 train_time:65431ms step_avg:57.65ms
step:1136/2330 train_time:65490ms step_avg:57.65ms
step:1137/2330 train_time:65547ms step_avg:57.65ms
step:1138/2330 train_time:65607ms step_avg:57.65ms
step:1139/2330 train_time:65664ms step_avg:57.65ms
step:1140/2330 train_time:65723ms step_avg:57.65ms
step:1141/2330 train_time:65780ms step_avg:57.65ms
step:1142/2330 train_time:65839ms step_avg:57.65ms
step:1143/2330 train_time:65896ms step_avg:57.65ms
step:1144/2330 train_time:65958ms step_avg:57.66ms
step:1145/2330 train_time:66014ms step_avg:57.65ms
step:1146/2330 train_time:66074ms step_avg:57.66ms
step:1147/2330 train_time:66130ms step_avg:57.65ms
step:1148/2330 train_time:66190ms step_avg:57.66ms
step:1149/2330 train_time:66248ms step_avg:57.66ms
step:1150/2330 train_time:66308ms step_avg:57.66ms
step:1151/2330 train_time:66365ms step_avg:57.66ms
step:1152/2330 train_time:66424ms step_avg:57.66ms
step:1153/2330 train_time:66481ms step_avg:57.66ms
step:1154/2330 train_time:66540ms step_avg:57.66ms
step:1155/2330 train_time:66597ms step_avg:57.66ms
step:1156/2330 train_time:66656ms step_avg:57.66ms
step:1157/2330 train_time:66713ms step_avg:57.66ms
step:1158/2330 train_time:66773ms step_avg:57.66ms
step:1159/2330 train_time:66830ms step_avg:57.66ms
step:1160/2330 train_time:66890ms step_avg:57.66ms
step:1161/2330 train_time:66947ms step_avg:57.66ms
step:1162/2330 train_time:67007ms step_avg:57.67ms
step:1163/2330 train_time:67065ms step_avg:57.67ms
step:1164/2330 train_time:67124ms step_avg:57.67ms
step:1165/2330 train_time:67180ms step_avg:57.67ms
step:1166/2330 train_time:67240ms step_avg:57.67ms
step:1167/2330 train_time:67296ms step_avg:57.67ms
step:1168/2330 train_time:67356ms step_avg:57.67ms
step:1169/2330 train_time:67413ms step_avg:57.67ms
step:1170/2330 train_time:67473ms step_avg:57.67ms
step:1171/2330 train_time:67530ms step_avg:57.67ms
step:1172/2330 train_time:67590ms step_avg:57.67ms
step:1173/2330 train_time:67646ms step_avg:57.67ms
step:1174/2330 train_time:67706ms step_avg:57.67ms
step:1175/2330 train_time:67763ms step_avg:57.67ms
step:1176/2330 train_time:67822ms step_avg:57.67ms
step:1177/2330 train_time:67878ms step_avg:57.67ms
step:1178/2330 train_time:67939ms step_avg:57.67ms
step:1179/2330 train_time:67995ms step_avg:57.67ms
step:1180/2330 train_time:68055ms step_avg:57.67ms
step:1181/2330 train_time:68112ms step_avg:57.67ms
step:1182/2330 train_time:68172ms step_avg:57.67ms
step:1183/2330 train_time:68229ms step_avg:57.67ms
step:1184/2330 train_time:68288ms step_avg:57.68ms
step:1185/2330 train_time:68345ms step_avg:57.68ms
step:1186/2330 train_time:68405ms step_avg:57.68ms
step:1187/2330 train_time:68461ms step_avg:57.68ms
step:1188/2330 train_time:68520ms step_avg:57.68ms
step:1189/2330 train_time:68577ms step_avg:57.68ms
step:1190/2330 train_time:68637ms step_avg:57.68ms
step:1191/2330 train_time:68694ms step_avg:57.68ms
step:1192/2330 train_time:68754ms step_avg:57.68ms
step:1193/2330 train_time:68812ms step_avg:57.68ms
step:1194/2330 train_time:68871ms step_avg:57.68ms
step:1195/2330 train_time:68928ms step_avg:57.68ms
step:1196/2330 train_time:68988ms step_avg:57.68ms
step:1197/2330 train_time:69044ms step_avg:57.68ms
step:1198/2330 train_time:69105ms step_avg:57.68ms
step:1199/2330 train_time:69161ms step_avg:57.68ms
step:1200/2330 train_time:69221ms step_avg:57.68ms
step:1201/2330 train_time:69276ms step_avg:57.68ms
step:1202/2330 train_time:69337ms step_avg:57.68ms
step:1203/2330 train_time:69393ms step_avg:57.68ms
step:1204/2330 train_time:69454ms step_avg:57.69ms
step:1205/2330 train_time:69511ms step_avg:57.69ms
step:1206/2330 train_time:69570ms step_avg:57.69ms
step:1207/2330 train_time:69627ms step_avg:57.69ms
step:1208/2330 train_time:69687ms step_avg:57.69ms
step:1209/2330 train_time:69744ms step_avg:57.69ms
step:1210/2330 train_time:69803ms step_avg:57.69ms
step:1211/2330 train_time:69861ms step_avg:57.69ms
step:1212/2330 train_time:69920ms step_avg:57.69ms
step:1213/2330 train_time:69976ms step_avg:57.69ms
step:1214/2330 train_time:70036ms step_avg:57.69ms
step:1215/2330 train_time:70093ms step_avg:57.69ms
step:1216/2330 train_time:70153ms step_avg:57.69ms
step:1217/2330 train_time:70210ms step_avg:57.69ms
step:1218/2330 train_time:70270ms step_avg:57.69ms
step:1219/2330 train_time:70326ms step_avg:57.69ms
step:1220/2330 train_time:70386ms step_avg:57.69ms
step:1221/2330 train_time:70443ms step_avg:57.69ms
step:1222/2330 train_time:70502ms step_avg:57.69ms
step:1223/2330 train_time:70559ms step_avg:57.69ms
step:1224/2330 train_time:70618ms step_avg:57.69ms
step:1225/2330 train_time:70675ms step_avg:57.69ms
step:1226/2330 train_time:70736ms step_avg:57.70ms
step:1227/2330 train_time:70792ms step_avg:57.70ms
step:1228/2330 train_time:70853ms step_avg:57.70ms
step:1229/2330 train_time:70910ms step_avg:57.70ms
step:1230/2330 train_time:70969ms step_avg:57.70ms
step:1231/2330 train_time:71026ms step_avg:57.70ms
step:1232/2330 train_time:71085ms step_avg:57.70ms
step:1233/2330 train_time:71141ms step_avg:57.70ms
step:1234/2330 train_time:71201ms step_avg:57.70ms
step:1235/2330 train_time:71257ms step_avg:57.70ms
step:1236/2330 train_time:71317ms step_avg:57.70ms
step:1237/2330 train_time:71373ms step_avg:57.70ms
step:1238/2330 train_time:71434ms step_avg:57.70ms
step:1239/2330 train_time:71490ms step_avg:57.70ms
step:1240/2330 train_time:71550ms step_avg:57.70ms
step:1241/2330 train_time:71606ms step_avg:57.70ms
step:1242/2330 train_time:71666ms step_avg:57.70ms
step:1243/2330 train_time:71723ms step_avg:57.70ms
step:1244/2330 train_time:71782ms step_avg:57.70ms
step:1245/2330 train_time:71838ms step_avg:57.70ms
step:1246/2330 train_time:71898ms step_avg:57.70ms
step:1247/2330 train_time:71955ms step_avg:57.70ms
step:1248/2330 train_time:72014ms step_avg:57.70ms
step:1249/2330 train_time:72071ms step_avg:57.70ms
step:1250/2330 train_time:72131ms step_avg:57.70ms
step:1250/2330 val_loss:4.1808 train_time:72211ms step_avg:57.77ms
step:1251/2330 train_time:72230ms step_avg:57.74ms
step:1252/2330 train_time:72250ms step_avg:57.71ms
step:1253/2330 train_time:72307ms step_avg:57.71ms
step:1254/2330 train_time:72371ms step_avg:57.71ms
step:1255/2330 train_time:72427ms step_avg:57.71ms
step:1256/2330 train_time:72489ms step_avg:57.71ms
step:1257/2330 train_time:72545ms step_avg:57.71ms
step:1258/2330 train_time:72606ms step_avg:57.72ms
step:1259/2330 train_time:72661ms step_avg:57.71ms
step:1260/2330 train_time:72722ms step_avg:57.72ms
step:1261/2330 train_time:72778ms step_avg:57.71ms
step:1262/2330 train_time:72836ms step_avg:57.71ms
step:1263/2330 train_time:72892ms step_avg:57.71ms
step:1264/2330 train_time:72951ms step_avg:57.71ms
step:1265/2330 train_time:73007ms step_avg:57.71ms
step:1266/2330 train_time:73066ms step_avg:57.71ms
step:1267/2330 train_time:73122ms step_avg:57.71ms
step:1268/2330 train_time:73183ms step_avg:57.72ms
step:1269/2330 train_time:73241ms step_avg:57.72ms
step:1270/2330 train_time:73302ms step_avg:57.72ms
step:1271/2330 train_time:73360ms step_avg:57.72ms
step:1272/2330 train_time:73420ms step_avg:57.72ms
step:1273/2330 train_time:73477ms step_avg:57.72ms
step:1274/2330 train_time:73537ms step_avg:57.72ms
step:1275/2330 train_time:73593ms step_avg:57.72ms
step:1276/2330 train_time:73652ms step_avg:57.72ms
step:1277/2330 train_time:73708ms step_avg:57.72ms
step:1278/2330 train_time:73767ms step_avg:57.72ms
step:1279/2330 train_time:73823ms step_avg:57.72ms
step:1280/2330 train_time:73883ms step_avg:57.72ms
step:1281/2330 train_time:73939ms step_avg:57.72ms
step:1282/2330 train_time:73999ms step_avg:57.72ms
step:1283/2330 train_time:74055ms step_avg:57.72ms
step:1284/2330 train_time:74114ms step_avg:57.72ms
step:1285/2330 train_time:74171ms step_avg:57.72ms
step:1286/2330 train_time:74231ms step_avg:57.72ms
step:1287/2330 train_time:74287ms step_avg:57.72ms
step:1288/2330 train_time:74348ms step_avg:57.72ms
step:1289/2330 train_time:74405ms step_avg:57.72ms
step:1290/2330 train_time:74466ms step_avg:57.73ms
step:1291/2330 train_time:74523ms step_avg:57.73ms
step:1292/2330 train_time:74584ms step_avg:57.73ms
step:1293/2330 train_time:74640ms step_avg:57.73ms
step:1294/2330 train_time:74699ms step_avg:57.73ms
step:1295/2330 train_time:74756ms step_avg:57.73ms
step:1296/2330 train_time:74815ms step_avg:57.73ms
step:1297/2330 train_time:74871ms step_avg:57.73ms
step:1298/2330 train_time:74930ms step_avg:57.73ms
step:1299/2330 train_time:74986ms step_avg:57.73ms
step:1300/2330 train_time:75045ms step_avg:57.73ms
step:1301/2330 train_time:75101ms step_avg:57.73ms
step:1302/2330 train_time:75162ms step_avg:57.73ms
step:1303/2330 train_time:75219ms step_avg:57.73ms
step:1304/2330 train_time:75280ms step_avg:57.73ms
step:1305/2330 train_time:75337ms step_avg:57.73ms
step:1306/2330 train_time:75398ms step_avg:57.73ms
step:1307/2330 train_time:75454ms step_avg:57.73ms
step:1308/2330 train_time:75514ms step_avg:57.73ms
step:1309/2330 train_time:75571ms step_avg:57.73ms
step:1310/2330 train_time:75630ms step_avg:57.73ms
step:1311/2330 train_time:75686ms step_avg:57.73ms
step:1312/2330 train_time:75746ms step_avg:57.73ms
step:1313/2330 train_time:75802ms step_avg:57.73ms
step:1314/2330 train_time:75862ms step_avg:57.73ms
step:1315/2330 train_time:75918ms step_avg:57.73ms
step:1316/2330 train_time:75978ms step_avg:57.73ms
step:1317/2330 train_time:76034ms step_avg:57.73ms
step:1318/2330 train_time:76093ms step_avg:57.73ms
step:1319/2330 train_time:76150ms step_avg:57.73ms
step:1320/2330 train_time:76209ms step_avg:57.73ms
step:1321/2330 train_time:76265ms step_avg:57.73ms
step:1322/2330 train_time:76326ms step_avg:57.73ms
step:1323/2330 train_time:76382ms step_avg:57.73ms
step:1324/2330 train_time:76443ms step_avg:57.74ms
step:1325/2330 train_time:76500ms step_avg:57.74ms
step:1326/2330 train_time:76560ms step_avg:57.74ms
step:1327/2330 train_time:76617ms step_avg:57.74ms
step:1328/2330 train_time:76677ms step_avg:57.74ms
step:1329/2330 train_time:76733ms step_avg:57.74ms
step:1330/2330 train_time:76792ms step_avg:57.74ms
step:1331/2330 train_time:76848ms step_avg:57.74ms
step:1332/2330 train_time:76907ms step_avg:57.74ms
step:1333/2330 train_time:76963ms step_avg:57.74ms
step:1334/2330 train_time:77023ms step_avg:57.74ms
step:1335/2330 train_time:77080ms step_avg:57.74ms
step:1336/2330 train_time:77140ms step_avg:57.74ms
step:1337/2330 train_time:77197ms step_avg:57.74ms
step:1338/2330 train_time:77256ms step_avg:57.74ms
step:1339/2330 train_time:77313ms step_avg:57.74ms
step:1340/2330 train_time:77372ms step_avg:57.74ms
step:1341/2330 train_time:77428ms step_avg:57.74ms
step:1342/2330 train_time:77488ms step_avg:57.74ms
step:1343/2330 train_time:77545ms step_avg:57.74ms
step:1344/2330 train_time:77606ms step_avg:57.74ms
step:1345/2330 train_time:77662ms step_avg:57.74ms
step:1346/2330 train_time:77723ms step_avg:57.74ms
step:1347/2330 train_time:77780ms step_avg:57.74ms
step:1348/2330 train_time:77840ms step_avg:57.74ms
step:1349/2330 train_time:77896ms step_avg:57.74ms
step:1350/2330 train_time:77956ms step_avg:57.75ms
step:1351/2330 train_time:78012ms step_avg:57.74ms
step:1352/2330 train_time:78071ms step_avg:57.74ms
step:1353/2330 train_time:78127ms step_avg:57.74ms
step:1354/2330 train_time:78187ms step_avg:57.75ms
step:1355/2330 train_time:78244ms step_avg:57.74ms
step:1356/2330 train_time:78303ms step_avg:57.75ms
step:1357/2330 train_time:78360ms step_avg:57.75ms
step:1358/2330 train_time:78420ms step_avg:57.75ms
step:1359/2330 train_time:78477ms step_avg:57.75ms
step:1360/2330 train_time:78537ms step_avg:57.75ms
step:1361/2330 train_time:78593ms step_avg:57.75ms
step:1362/2330 train_time:78653ms step_avg:57.75ms
step:1363/2330 train_time:78709ms step_avg:57.75ms
step:1364/2330 train_time:78769ms step_avg:57.75ms
step:1365/2330 train_time:78825ms step_avg:57.75ms
step:1366/2330 train_time:78885ms step_avg:57.75ms
step:1367/2330 train_time:78942ms step_avg:57.75ms
step:1368/2330 train_time:79001ms step_avg:57.75ms
step:1369/2330 train_time:79058ms step_avg:57.75ms
step:1370/2330 train_time:79117ms step_avg:57.75ms
step:1371/2330 train_time:79173ms step_avg:57.75ms
step:1372/2330 train_time:79232ms step_avg:57.75ms
step:1373/2330 train_time:79288ms step_avg:57.75ms
step:1374/2330 train_time:79348ms step_avg:57.75ms
step:1375/2330 train_time:79404ms step_avg:57.75ms
step:1376/2330 train_time:79465ms step_avg:57.75ms
step:1377/2330 train_time:79522ms step_avg:57.75ms
step:1378/2330 train_time:79583ms step_avg:57.75ms
step:1379/2330 train_time:79640ms step_avg:57.75ms
step:1380/2330 train_time:79700ms step_avg:57.75ms
step:1381/2330 train_time:79757ms step_avg:57.75ms
step:1382/2330 train_time:79816ms step_avg:57.75ms
step:1383/2330 train_time:79873ms step_avg:57.75ms
step:1384/2330 train_time:79932ms step_avg:57.75ms
step:1385/2330 train_time:79989ms step_avg:57.75ms
step:1386/2330 train_time:80048ms step_avg:57.75ms
step:1387/2330 train_time:80104ms step_avg:57.75ms
step:1388/2330 train_time:80164ms step_avg:57.76ms
step:1389/2330 train_time:80221ms step_avg:57.75ms
step:1390/2330 train_time:80281ms step_avg:57.76ms
step:1391/2330 train_time:80337ms step_avg:57.76ms
step:1392/2330 train_time:80397ms step_avg:57.76ms
step:1393/2330 train_time:80453ms step_avg:57.76ms
step:1394/2330 train_time:80512ms step_avg:57.76ms
step:1395/2330 train_time:80569ms step_avg:57.76ms
step:1396/2330 train_time:80629ms step_avg:57.76ms
step:1397/2330 train_time:80686ms step_avg:57.76ms
step:1398/2330 train_time:80746ms step_avg:57.76ms
step:1399/2330 train_time:80803ms step_avg:57.76ms
step:1400/2330 train_time:80863ms step_avg:57.76ms
step:1401/2330 train_time:80919ms step_avg:57.76ms
step:1402/2330 train_time:80979ms step_avg:57.76ms
step:1403/2330 train_time:81036ms step_avg:57.76ms
step:1404/2330 train_time:81096ms step_avg:57.76ms
step:1405/2330 train_time:81152ms step_avg:57.76ms
step:1406/2330 train_time:81212ms step_avg:57.76ms
step:1407/2330 train_time:81268ms step_avg:57.76ms
step:1408/2330 train_time:81327ms step_avg:57.76ms
step:1409/2330 train_time:81383ms step_avg:57.76ms
step:1410/2330 train_time:81445ms step_avg:57.76ms
step:1411/2330 train_time:81501ms step_avg:57.76ms
step:1412/2330 train_time:81561ms step_avg:57.76ms
step:1413/2330 train_time:81618ms step_avg:57.76ms
step:1414/2330 train_time:81679ms step_avg:57.76ms
step:1415/2330 train_time:81736ms step_avg:57.76ms
step:1416/2330 train_time:81795ms step_avg:57.76ms
step:1417/2330 train_time:81852ms step_avg:57.76ms
step:1418/2330 train_time:81910ms step_avg:57.76ms
step:1419/2330 train_time:81967ms step_avg:57.76ms
step:1420/2330 train_time:82026ms step_avg:57.77ms
step:1421/2330 train_time:82083ms step_avg:57.76ms
step:1422/2330 train_time:82143ms step_avg:57.77ms
step:1423/2330 train_time:82200ms step_avg:57.77ms
step:1424/2330 train_time:82259ms step_avg:57.77ms
step:1425/2330 train_time:82316ms step_avg:57.77ms
step:1426/2330 train_time:82375ms step_avg:57.77ms
step:1427/2330 train_time:82431ms step_avg:57.77ms
step:1428/2330 train_time:82491ms step_avg:57.77ms
step:1429/2330 train_time:82547ms step_avg:57.77ms
step:1430/2330 train_time:82608ms step_avg:57.77ms
step:1431/2330 train_time:82664ms step_avg:57.77ms
step:1432/2330 train_time:82726ms step_avg:57.77ms
step:1433/2330 train_time:82782ms step_avg:57.77ms
step:1434/2330 train_time:82842ms step_avg:57.77ms
step:1435/2330 train_time:82899ms step_avg:57.77ms
step:1436/2330 train_time:82960ms step_avg:57.77ms
step:1437/2330 train_time:83017ms step_avg:57.77ms
step:1438/2330 train_time:83077ms step_avg:57.77ms
step:1439/2330 train_time:83134ms step_avg:57.77ms
step:1440/2330 train_time:83194ms step_avg:57.77ms
step:1441/2330 train_time:83250ms step_avg:57.77ms
step:1442/2330 train_time:83309ms step_avg:57.77ms
step:1443/2330 train_time:83365ms step_avg:57.77ms
step:1444/2330 train_time:83425ms step_avg:57.77ms
step:1445/2330 train_time:83482ms step_avg:57.77ms
step:1446/2330 train_time:83541ms step_avg:57.77ms
step:1447/2330 train_time:83598ms step_avg:57.77ms
step:1448/2330 train_time:83658ms step_avg:57.77ms
step:1449/2330 train_time:83714ms step_avg:57.77ms
step:1450/2330 train_time:83774ms step_avg:57.78ms
step:1451/2330 train_time:83831ms step_avg:57.77ms
step:1452/2330 train_time:83890ms step_avg:57.78ms
step:1453/2330 train_time:83946ms step_avg:57.77ms
step:1454/2330 train_time:84007ms step_avg:57.78ms
step:1455/2330 train_time:84063ms step_avg:57.78ms
step:1456/2330 train_time:84124ms step_avg:57.78ms
step:1457/2330 train_time:84180ms step_avg:57.78ms
step:1458/2330 train_time:84240ms step_avg:57.78ms
step:1459/2330 train_time:84297ms step_avg:57.78ms
step:1460/2330 train_time:84357ms step_avg:57.78ms
step:1461/2330 train_time:84414ms step_avg:57.78ms
step:1462/2330 train_time:84473ms step_avg:57.78ms
step:1463/2330 train_time:84530ms step_avg:57.78ms
step:1464/2330 train_time:84589ms step_avg:57.78ms
step:1465/2330 train_time:84646ms step_avg:57.78ms
step:1466/2330 train_time:84706ms step_avg:57.78ms
step:1467/2330 train_time:84762ms step_avg:57.78ms
step:1468/2330 train_time:84823ms step_avg:57.78ms
step:1469/2330 train_time:84879ms step_avg:57.78ms
step:1470/2330 train_time:84940ms step_avg:57.78ms
step:1471/2330 train_time:84997ms step_avg:57.78ms
step:1472/2330 train_time:85056ms step_avg:57.78ms
step:1473/2330 train_time:85112ms step_avg:57.78ms
step:1474/2330 train_time:85172ms step_avg:57.78ms
step:1475/2330 train_time:85228ms step_avg:57.78ms
step:1476/2330 train_time:85288ms step_avg:57.78ms
step:1477/2330 train_time:85344ms step_avg:57.78ms
step:1478/2330 train_time:85404ms step_avg:57.78ms
step:1479/2330 train_time:85461ms step_avg:57.78ms
step:1480/2330 train_time:85520ms step_avg:57.78ms
step:1481/2330 train_time:85577ms step_avg:57.78ms
step:1482/2330 train_time:85637ms step_avg:57.78ms
step:1483/2330 train_time:85694ms step_avg:57.78ms
step:1484/2330 train_time:85753ms step_avg:57.79ms
step:1485/2330 train_time:85810ms step_avg:57.78ms
step:1486/2330 train_time:85869ms step_avg:57.79ms
step:1487/2330 train_time:85925ms step_avg:57.78ms
step:1488/2330 train_time:85986ms step_avg:57.79ms
step:1489/2330 train_time:86043ms step_avg:57.79ms
step:1490/2330 train_time:86103ms step_avg:57.79ms
step:1491/2330 train_time:86159ms step_avg:57.79ms
step:1492/2330 train_time:86219ms step_avg:57.79ms
step:1493/2330 train_time:86276ms step_avg:57.79ms
step:1494/2330 train_time:86335ms step_avg:57.79ms
step:1495/2330 train_time:86391ms step_avg:57.79ms
step:1496/2330 train_time:86451ms step_avg:57.79ms
step:1497/2330 train_time:86507ms step_avg:57.79ms
step:1498/2330 train_time:86567ms step_avg:57.79ms
step:1499/2330 train_time:86624ms step_avg:57.79ms
step:1500/2330 train_time:86684ms step_avg:57.79ms
step:1500/2330 val_loss:4.0809 train_time:86763ms step_avg:57.84ms
step:1501/2330 train_time:86781ms step_avg:57.82ms
step:1502/2330 train_time:86802ms step_avg:57.79ms
step:1503/2330 train_time:86862ms step_avg:57.79ms
step:1504/2330 train_time:86924ms step_avg:57.80ms
step:1505/2330 train_time:86981ms step_avg:57.79ms
step:1506/2330 train_time:87042ms step_avg:57.80ms
step:1507/2330 train_time:87098ms step_avg:57.80ms
step:1508/2330 train_time:87158ms step_avg:57.80ms
step:1509/2330 train_time:87214ms step_avg:57.80ms
step:1510/2330 train_time:87274ms step_avg:57.80ms
step:1511/2330 train_time:87330ms step_avg:57.80ms
step:1512/2330 train_time:87388ms step_avg:57.80ms
step:1513/2330 train_time:87444ms step_avg:57.80ms
step:1514/2330 train_time:87502ms step_avg:57.80ms
step:1515/2330 train_time:87558ms step_avg:57.79ms
step:1516/2330 train_time:87617ms step_avg:57.80ms
step:1517/2330 train_time:87674ms step_avg:57.79ms
step:1518/2330 train_time:87733ms step_avg:57.80ms
step:1519/2330 train_time:87792ms step_avg:57.80ms
step:1520/2330 train_time:87853ms step_avg:57.80ms
step:1521/2330 train_time:87911ms step_avg:57.80ms
step:1522/2330 train_time:87972ms step_avg:57.80ms
step:1523/2330 train_time:88029ms step_avg:57.80ms
step:1524/2330 train_time:88089ms step_avg:57.80ms
step:1525/2330 train_time:88145ms step_avg:57.80ms
step:1526/2330 train_time:88204ms step_avg:57.80ms
step:1527/2330 train_time:88260ms step_avg:57.80ms
step:1528/2330 train_time:88320ms step_avg:57.80ms
step:1529/2330 train_time:88377ms step_avg:57.80ms
step:1530/2330 train_time:88436ms step_avg:57.80ms
step:1531/2330 train_time:88493ms step_avg:57.80ms
step:1532/2330 train_time:88552ms step_avg:57.80ms
step:1533/2330 train_time:88609ms step_avg:57.80ms
step:1534/2330 train_time:88669ms step_avg:57.80ms
step:1535/2330 train_time:88727ms step_avg:57.80ms
step:1536/2330 train_time:88786ms step_avg:57.80ms
step:1537/2330 train_time:88843ms step_avg:57.80ms
step:1538/2330 train_time:88905ms step_avg:57.81ms
step:1539/2330 train_time:88962ms step_avg:57.81ms
step:1540/2330 train_time:89024ms step_avg:57.81ms
step:1541/2330 train_time:89081ms step_avg:57.81ms
step:1542/2330 train_time:89142ms step_avg:57.81ms
step:1543/2330 train_time:89198ms step_avg:57.81ms
step:1544/2330 train_time:89259ms step_avg:57.81ms
step:1545/2330 train_time:89316ms step_avg:57.81ms
step:1546/2330 train_time:89376ms step_avg:57.81ms
step:1547/2330 train_time:89432ms step_avg:57.81ms
step:1548/2330 train_time:89492ms step_avg:57.81ms
step:1549/2330 train_time:89548ms step_avg:57.81ms
step:1550/2330 train_time:89608ms step_avg:57.81ms
step:1551/2330 train_time:89665ms step_avg:57.81ms
step:1552/2330 train_time:89724ms step_avg:57.81ms
step:1553/2330 train_time:89781ms step_avg:57.81ms
step:1554/2330 train_time:89841ms step_avg:57.81ms
step:1555/2330 train_time:89899ms step_avg:57.81ms
step:1556/2330 train_time:89961ms step_avg:57.82ms
step:1557/2330 train_time:90018ms step_avg:57.82ms
step:1558/2330 train_time:90079ms step_avg:57.82ms
step:1559/2330 train_time:90136ms step_avg:57.82ms
step:1560/2330 train_time:90197ms step_avg:57.82ms
step:1561/2330 train_time:90255ms step_avg:57.82ms
step:1562/2330 train_time:90314ms step_avg:57.82ms
step:1563/2330 train_time:90371ms step_avg:57.82ms
step:1564/2330 train_time:90430ms step_avg:57.82ms
step:1565/2330 train_time:90486ms step_avg:57.82ms
step:1566/2330 train_time:90546ms step_avg:57.82ms
step:1567/2330 train_time:90602ms step_avg:57.82ms
step:1568/2330 train_time:90662ms step_avg:57.82ms
step:1569/2330 train_time:90719ms step_avg:57.82ms
step:1570/2330 train_time:90779ms step_avg:57.82ms
step:1571/2330 train_time:90836ms step_avg:57.82ms
step:1572/2330 train_time:90897ms step_avg:57.82ms
step:1573/2330 train_time:90956ms step_avg:57.82ms
step:1574/2330 train_time:91016ms step_avg:57.82ms
step:1575/2330 train_time:91073ms step_avg:57.82ms
step:1576/2330 train_time:91134ms step_avg:57.83ms
step:1577/2330 train_time:91191ms step_avg:57.83ms
step:1578/2330 train_time:91251ms step_avg:57.83ms
step:1579/2330 train_time:91308ms step_avg:57.83ms
step:1580/2330 train_time:91367ms step_avg:57.83ms
step:1581/2330 train_time:91425ms step_avg:57.83ms
step:1582/2330 train_time:91484ms step_avg:57.83ms
step:1583/2330 train_time:91540ms step_avg:57.83ms
step:1584/2330 train_time:91601ms step_avg:57.83ms
step:1585/2330 train_time:91657ms step_avg:57.83ms
step:1586/2330 train_time:91717ms step_avg:57.83ms
step:1587/2330 train_time:91774ms step_avg:57.83ms
step:1588/2330 train_time:91834ms step_avg:57.83ms
step:1589/2330 train_time:91891ms step_avg:57.83ms
step:1590/2330 train_time:91951ms step_avg:57.83ms
step:1591/2330 train_time:92009ms step_avg:57.83ms
step:1592/2330 train_time:92069ms step_avg:57.83ms
step:1593/2330 train_time:92126ms step_avg:57.83ms
step:1594/2330 train_time:92186ms step_avg:57.83ms
step:1595/2330 train_time:92243ms step_avg:57.83ms
step:1596/2330 train_time:92303ms step_avg:57.83ms
step:1597/2330 train_time:92360ms step_avg:57.83ms
step:1598/2330 train_time:92421ms step_avg:57.84ms
step:1599/2330 train_time:92478ms step_avg:57.83ms
step:1600/2330 train_time:92538ms step_avg:57.84ms
step:1601/2330 train_time:92595ms step_avg:57.84ms
step:1602/2330 train_time:92655ms step_avg:57.84ms
step:1603/2330 train_time:92712ms step_avg:57.84ms
step:1604/2330 train_time:92772ms step_avg:57.84ms
step:1605/2330 train_time:92830ms step_avg:57.84ms
step:1606/2330 train_time:92889ms step_avg:57.84ms
step:1607/2330 train_time:92946ms step_avg:57.84ms
step:1608/2330 train_time:93005ms step_avg:57.84ms
step:1609/2330 train_time:93062ms step_avg:57.84ms
step:1610/2330 train_time:93123ms step_avg:57.84ms
step:1611/2330 train_time:93179ms step_avg:57.84ms
step:1612/2330 train_time:93240ms step_avg:57.84ms
step:1613/2330 train_time:93297ms step_avg:57.84ms
step:1614/2330 train_time:93359ms step_avg:57.84ms
step:1615/2330 train_time:93416ms step_avg:57.84ms
step:1616/2330 train_time:93476ms step_avg:57.84ms
step:1617/2330 train_time:93533ms step_avg:57.84ms
step:1618/2330 train_time:93593ms step_avg:57.85ms
step:1619/2330 train_time:93650ms step_avg:57.84ms
step:1620/2330 train_time:93710ms step_avg:57.85ms
step:1621/2330 train_time:93767ms step_avg:57.85ms
step:1622/2330 train_time:93827ms step_avg:57.85ms
step:1623/2330 train_time:93883ms step_avg:57.85ms
step:1624/2330 train_time:93943ms step_avg:57.85ms
step:1625/2330 train_time:94000ms step_avg:57.85ms
step:1626/2330 train_time:94060ms step_avg:57.85ms
step:1627/2330 train_time:94117ms step_avg:57.85ms
step:1628/2330 train_time:94177ms step_avg:57.85ms
step:1629/2330 train_time:94234ms step_avg:57.85ms
step:1630/2330 train_time:94296ms step_avg:57.85ms
step:1631/2330 train_time:94352ms step_avg:57.85ms
step:1632/2330 train_time:94413ms step_avg:57.85ms
step:1633/2330 train_time:94470ms step_avg:57.85ms
step:1634/2330 train_time:94530ms step_avg:57.85ms
step:1635/2330 train_time:94588ms step_avg:57.85ms
step:1636/2330 train_time:94647ms step_avg:57.85ms
step:1637/2330 train_time:94704ms step_avg:57.85ms
step:1638/2330 train_time:94763ms step_avg:57.85ms
step:1639/2330 train_time:94820ms step_avg:57.85ms
step:1640/2330 train_time:94880ms step_avg:57.85ms
step:1641/2330 train_time:94938ms step_avg:57.85ms
step:1642/2330 train_time:94998ms step_avg:57.85ms
step:1643/2330 train_time:95055ms step_avg:57.85ms
step:1644/2330 train_time:95115ms step_avg:57.86ms
step:1645/2330 train_time:95172ms step_avg:57.86ms
step:1646/2330 train_time:95233ms step_avg:57.86ms
step:1647/2330 train_time:95290ms step_avg:57.86ms
step:1648/2330 train_time:95349ms step_avg:57.86ms
step:1649/2330 train_time:95406ms step_avg:57.86ms
step:1650/2330 train_time:95466ms step_avg:57.86ms
step:1651/2330 train_time:95523ms step_avg:57.86ms
step:1652/2330 train_time:95583ms step_avg:57.86ms
step:1653/2330 train_time:95640ms step_avg:57.86ms
step:1654/2330 train_time:95701ms step_avg:57.86ms
step:1655/2330 train_time:95758ms step_avg:57.86ms
step:1656/2330 train_time:95818ms step_avg:57.86ms
step:1657/2330 train_time:95875ms step_avg:57.86ms
step:1658/2330 train_time:95936ms step_avg:57.86ms
step:1659/2330 train_time:95993ms step_avg:57.86ms
step:1660/2330 train_time:96052ms step_avg:57.86ms
step:1661/2330 train_time:96109ms step_avg:57.86ms
step:1662/2330 train_time:96169ms step_avg:57.86ms
step:1663/2330 train_time:96225ms step_avg:57.86ms
step:1664/2330 train_time:96285ms step_avg:57.86ms
step:1665/2330 train_time:96342ms step_avg:57.86ms
step:1666/2330 train_time:96403ms step_avg:57.86ms
step:1667/2330 train_time:96460ms step_avg:57.86ms
step:1668/2330 train_time:96521ms step_avg:57.87ms
step:1669/2330 train_time:96577ms step_avg:57.87ms
step:1670/2330 train_time:96638ms step_avg:57.87ms
step:1671/2330 train_time:96694ms step_avg:57.87ms
step:1672/2330 train_time:96756ms step_avg:57.87ms
step:1673/2330 train_time:96814ms step_avg:57.87ms
step:1674/2330 train_time:96874ms step_avg:57.87ms
step:1675/2330 train_time:96931ms step_avg:57.87ms
step:1676/2330 train_time:96991ms step_avg:57.87ms
step:1677/2330 train_time:97047ms step_avg:57.87ms
step:1678/2330 train_time:97107ms step_avg:57.87ms
step:1679/2330 train_time:97164ms step_avg:57.87ms
step:1680/2330 train_time:97224ms step_avg:57.87ms
step:1681/2330 train_time:97281ms step_avg:57.87ms
step:1682/2330 train_time:97341ms step_avg:57.87ms
step:1683/2330 train_time:97398ms step_avg:57.87ms
step:1684/2330 train_time:97459ms step_avg:57.87ms
step:1685/2330 train_time:97516ms step_avg:57.87ms
step:1686/2330 train_time:97576ms step_avg:57.87ms
step:1687/2330 train_time:97633ms step_avg:57.87ms
step:1688/2330 train_time:97693ms step_avg:57.88ms
step:1689/2330 train_time:97750ms step_avg:57.87ms
step:1690/2330 train_time:97810ms step_avg:57.88ms
step:1691/2330 train_time:97867ms step_avg:57.88ms
step:1692/2330 train_time:97927ms step_avg:57.88ms
step:1693/2330 train_time:97984ms step_avg:57.88ms
step:1694/2330 train_time:98043ms step_avg:57.88ms
step:1695/2330 train_time:98100ms step_avg:57.88ms
step:1696/2330 train_time:98161ms step_avg:57.88ms
step:1697/2330 train_time:98218ms step_avg:57.88ms
step:1698/2330 train_time:98278ms step_avg:57.88ms
step:1699/2330 train_time:98336ms step_avg:57.88ms
step:1700/2330 train_time:98396ms step_avg:57.88ms
step:1701/2330 train_time:98452ms step_avg:57.88ms
step:1702/2330 train_time:98512ms step_avg:57.88ms
step:1703/2330 train_time:98570ms step_avg:57.88ms
step:1704/2330 train_time:98630ms step_avg:57.88ms
step:1705/2330 train_time:98687ms step_avg:57.88ms
step:1706/2330 train_time:98747ms step_avg:57.88ms
step:1707/2330 train_time:98804ms step_avg:57.88ms
step:1708/2330 train_time:98865ms step_avg:57.88ms
step:1709/2330 train_time:98922ms step_avg:57.88ms
step:1710/2330 train_time:98981ms step_avg:57.88ms
step:1711/2330 train_time:99038ms step_avg:57.88ms
step:1712/2330 train_time:99098ms step_avg:57.88ms
step:1713/2330 train_time:99155ms step_avg:57.88ms
step:1714/2330 train_time:99216ms step_avg:57.89ms
step:1715/2330 train_time:99274ms step_avg:57.89ms
step:1716/2330 train_time:99334ms step_avg:57.89ms
step:1717/2330 train_time:99390ms step_avg:57.89ms
step:1718/2330 train_time:99450ms step_avg:57.89ms
step:1719/2330 train_time:99507ms step_avg:57.89ms
step:1720/2330 train_time:99566ms step_avg:57.89ms
step:1721/2330 train_time:99623ms step_avg:57.89ms
step:1722/2330 train_time:99684ms step_avg:57.89ms
step:1723/2330 train_time:99741ms step_avg:57.89ms
step:1724/2330 train_time:99801ms step_avg:57.89ms
step:1725/2330 train_time:99858ms step_avg:57.89ms
step:1726/2330 train_time:99918ms step_avg:57.89ms
step:1727/2330 train_time:99975ms step_avg:57.89ms
step:1728/2330 train_time:100035ms step_avg:57.89ms
step:1729/2330 train_time:100092ms step_avg:57.89ms
step:1730/2330 train_time:100153ms step_avg:57.89ms
step:1731/2330 train_time:100210ms step_avg:57.89ms
step:1732/2330 train_time:100269ms step_avg:57.89ms
step:1733/2330 train_time:100326ms step_avg:57.89ms
step:1734/2330 train_time:100386ms step_avg:57.89ms
step:1735/2330 train_time:100443ms step_avg:57.89ms
step:1736/2330 train_time:100504ms step_avg:57.89ms
step:1737/2330 train_time:100561ms step_avg:57.89ms
step:1738/2330 train_time:100621ms step_avg:57.89ms
step:1739/2330 train_time:100678ms step_avg:57.89ms
step:1740/2330 train_time:100738ms step_avg:57.90ms
step:1741/2330 train_time:100795ms step_avg:57.89ms
step:1742/2330 train_time:100856ms step_avg:57.90ms
step:1743/2330 train_time:100914ms step_avg:57.90ms
step:1744/2330 train_time:100974ms step_avg:57.90ms
step:1745/2330 train_time:101031ms step_avg:57.90ms
step:1746/2330 train_time:101091ms step_avg:57.90ms
step:1747/2330 train_time:101148ms step_avg:57.90ms
step:1748/2330 train_time:101208ms step_avg:57.90ms
step:1749/2330 train_time:101266ms step_avg:57.90ms
step:1750/2330 train_time:101325ms step_avg:57.90ms
step:1750/2330 val_loss:3.9890 train_time:101406ms step_avg:57.95ms
step:1751/2330 train_time:101425ms step_avg:57.92ms
step:1752/2330 train_time:101445ms step_avg:57.90ms
step:1753/2330 train_time:101500ms step_avg:57.90ms
step:1754/2330 train_time:101564ms step_avg:57.90ms
step:1755/2330 train_time:101620ms step_avg:57.90ms
step:1756/2330 train_time:101682ms step_avg:57.91ms
step:1757/2330 train_time:101739ms step_avg:57.90ms
step:1758/2330 train_time:101798ms step_avg:57.91ms
step:1759/2330 train_time:101854ms step_avg:57.90ms
step:1760/2330 train_time:101913ms step_avg:57.91ms
step:1761/2330 train_time:101969ms step_avg:57.90ms
step:1762/2330 train_time:102029ms step_avg:57.91ms
step:1763/2330 train_time:102085ms step_avg:57.90ms
step:1764/2330 train_time:102144ms step_avg:57.90ms
step:1765/2330 train_time:102201ms step_avg:57.90ms
step:1766/2330 train_time:102260ms step_avg:57.90ms
step:1767/2330 train_time:102321ms step_avg:57.91ms
step:1768/2330 train_time:102385ms step_avg:57.91ms
step:1769/2330 train_time:102443ms step_avg:57.91ms
step:1770/2330 train_time:102503ms step_avg:57.91ms
step:1771/2330 train_time:102560ms step_avg:57.91ms
step:1772/2330 train_time:102620ms step_avg:57.91ms
step:1773/2330 train_time:102676ms step_avg:57.91ms
step:1774/2330 train_time:102737ms step_avg:57.91ms
step:1775/2330 train_time:102793ms step_avg:57.91ms
step:1776/2330 train_time:102853ms step_avg:57.91ms
step:1777/2330 train_time:102909ms step_avg:57.91ms
step:1778/2330 train_time:102969ms step_avg:57.91ms
step:1779/2330 train_time:103026ms step_avg:57.91ms
step:1780/2330 train_time:103085ms step_avg:57.91ms
step:1781/2330 train_time:103142ms step_avg:57.91ms
step:1782/2330 train_time:103201ms step_avg:57.91ms
step:1783/2330 train_time:103259ms step_avg:57.91ms
step:1784/2330 train_time:103320ms step_avg:57.91ms
step:1785/2330 train_time:103379ms step_avg:57.92ms
step:1786/2330 train_time:103441ms step_avg:57.92ms
step:1787/2330 train_time:103498ms step_avg:57.92ms
step:1788/2330 train_time:103559ms step_avg:57.92ms
step:1789/2330 train_time:103616ms step_avg:57.92ms
step:1790/2330 train_time:103676ms step_avg:57.92ms
step:1791/2330 train_time:103734ms step_avg:57.92ms
step:1792/2330 train_time:103794ms step_avg:57.92ms
step:1793/2330 train_time:103851ms step_avg:57.92ms
step:1794/2330 train_time:103910ms step_avg:57.92ms
step:1795/2330 train_time:103967ms step_avg:57.92ms
step:1796/2330 train_time:104026ms step_avg:57.92ms
step:1797/2330 train_time:104082ms step_avg:57.92ms
step:1798/2330 train_time:104142ms step_avg:57.92ms
step:1799/2330 train_time:104199ms step_avg:57.92ms
step:1800/2330 train_time:104260ms step_avg:57.92ms
step:1801/2330 train_time:104318ms step_avg:57.92ms
step:1802/2330 train_time:104379ms step_avg:57.92ms
step:1803/2330 train_time:104437ms step_avg:57.92ms
step:1804/2330 train_time:104497ms step_avg:57.93ms
step:1805/2330 train_time:104554ms step_avg:57.92ms
step:1806/2330 train_time:104615ms step_avg:57.93ms
step:1807/2330 train_time:104673ms step_avg:57.93ms
step:1808/2330 train_time:104732ms step_avg:57.93ms
step:1809/2330 train_time:104788ms step_avg:57.93ms
step:1810/2330 train_time:104849ms step_avg:57.93ms
step:1811/2330 train_time:104906ms step_avg:57.93ms
step:1812/2330 train_time:104966ms step_avg:57.93ms
step:1813/2330 train_time:105022ms step_avg:57.93ms
step:1814/2330 train_time:105082ms step_avg:57.93ms
step:1815/2330 train_time:105139ms step_avg:57.93ms
step:1816/2330 train_time:105199ms step_avg:57.93ms
step:1817/2330 train_time:105258ms step_avg:57.93ms
step:1818/2330 train_time:105318ms step_avg:57.93ms
step:1819/2330 train_time:105375ms step_avg:57.93ms
step:1820/2330 train_time:105436ms step_avg:57.93ms
step:1821/2330 train_time:105493ms step_avg:57.93ms
step:1822/2330 train_time:105553ms step_avg:57.93ms
step:1823/2330 train_time:105610ms step_avg:57.93ms
step:1824/2330 train_time:105670ms step_avg:57.93ms
step:1825/2330 train_time:105727ms step_avg:57.93ms
step:1826/2330 train_time:105787ms step_avg:57.93ms
step:1827/2330 train_time:105844ms step_avg:57.93ms
step:1828/2330 train_time:105904ms step_avg:57.93ms
step:1829/2330 train_time:105961ms step_avg:57.93ms
step:1830/2330 train_time:106021ms step_avg:57.93ms
step:1831/2330 train_time:106078ms step_avg:57.93ms
step:1832/2330 train_time:106138ms step_avg:57.94ms
step:1833/2330 train_time:106195ms step_avg:57.94ms
step:1834/2330 train_time:106255ms step_avg:57.94ms
step:1835/2330 train_time:106312ms step_avg:57.94ms
step:1836/2330 train_time:106372ms step_avg:57.94ms
step:1837/2330 train_time:106430ms step_avg:57.94ms
step:1838/2330 train_time:106490ms step_avg:57.94ms
step:1839/2330 train_time:106547ms step_avg:57.94ms
step:1840/2330 train_time:106607ms step_avg:57.94ms
step:1841/2330 train_time:106664ms step_avg:57.94ms
step:1842/2330 train_time:106724ms step_avg:57.94ms
step:1843/2330 train_time:106783ms step_avg:57.94ms
step:1844/2330 train_time:106843ms step_avg:57.94ms
step:1845/2330 train_time:106900ms step_avg:57.94ms
step:1846/2330 train_time:106960ms step_avg:57.94ms
step:1847/2330 train_time:107017ms step_avg:57.94ms
step:1848/2330 train_time:107077ms step_avg:57.94ms
step:1849/2330 train_time:107134ms step_avg:57.94ms
step:1850/2330 train_time:107194ms step_avg:57.94ms
step:1851/2330 train_time:107251ms step_avg:57.94ms
step:1852/2330 train_time:107311ms step_avg:57.94ms
step:1853/2330 train_time:107367ms step_avg:57.94ms
step:1854/2330 train_time:107428ms step_avg:57.94ms
step:1855/2330 train_time:107485ms step_avg:57.94ms
step:1856/2330 train_time:107545ms step_avg:57.94ms
step:1857/2330 train_time:107603ms step_avg:57.94ms
step:1858/2330 train_time:107663ms step_avg:57.95ms
step:1859/2330 train_time:107720ms step_avg:57.94ms
step:1860/2330 train_time:107780ms step_avg:57.95ms
step:1861/2330 train_time:107838ms step_avg:57.95ms
step:1862/2330 train_time:107898ms step_avg:57.95ms
step:1863/2330 train_time:107955ms step_avg:57.95ms
step:1864/2330 train_time:108014ms step_avg:57.95ms
step:1865/2330 train_time:108071ms step_avg:57.95ms
step:1866/2330 train_time:108130ms step_avg:57.95ms
step:1867/2330 train_time:108188ms step_avg:57.95ms
step:1868/2330 train_time:108247ms step_avg:57.95ms
step:1869/2330 train_time:108304ms step_avg:57.95ms
step:1870/2330 train_time:108364ms step_avg:57.95ms
step:1871/2330 train_time:108420ms step_avg:57.95ms
step:1872/2330 train_time:108481ms step_avg:57.95ms
step:1873/2330 train_time:108539ms step_avg:57.95ms
step:1874/2330 train_time:108599ms step_avg:57.95ms
step:1875/2330 train_time:108656ms step_avg:57.95ms
step:1876/2330 train_time:108716ms step_avg:57.95ms
step:1877/2330 train_time:108774ms step_avg:57.95ms
step:1878/2330 train_time:108833ms step_avg:57.95ms
step:1879/2330 train_time:108890ms step_avg:57.95ms
step:1880/2330 train_time:108949ms step_avg:57.95ms
step:1881/2330 train_time:109006ms step_avg:57.95ms
step:1882/2330 train_time:109067ms step_avg:57.95ms
step:1883/2330 train_time:109123ms step_avg:57.95ms
step:1884/2330 train_time:109184ms step_avg:57.95ms
step:1885/2330 train_time:109241ms step_avg:57.95ms
step:1886/2330 train_time:109302ms step_avg:57.95ms
step:1887/2330 train_time:109359ms step_avg:57.95ms
step:1888/2330 train_time:109419ms step_avg:57.95ms
step:1889/2330 train_time:109477ms step_avg:57.95ms
step:1890/2330 train_time:109537ms step_avg:57.96ms
step:1891/2330 train_time:109594ms step_avg:57.96ms
step:1892/2330 train_time:109653ms step_avg:57.96ms
step:1893/2330 train_time:109710ms step_avg:57.96ms
step:1894/2330 train_time:109770ms step_avg:57.96ms
step:1895/2330 train_time:109827ms step_avg:57.96ms
step:1896/2330 train_time:109887ms step_avg:57.96ms
step:1897/2330 train_time:109944ms step_avg:57.96ms
step:1898/2330 train_time:110004ms step_avg:57.96ms
step:1899/2330 train_time:110061ms step_avg:57.96ms
step:1900/2330 train_time:110121ms step_avg:57.96ms
step:1901/2330 train_time:110179ms step_avg:57.96ms
step:1902/2330 train_time:110239ms step_avg:57.96ms
step:1903/2330 train_time:110295ms step_avg:57.96ms
step:1904/2330 train_time:110355ms step_avg:57.96ms
step:1905/2330 train_time:110412ms step_avg:57.96ms
step:1906/2330 train_time:110472ms step_avg:57.96ms
step:1907/2330 train_time:110529ms step_avg:57.96ms
step:1908/2330 train_time:110590ms step_avg:57.96ms
step:1909/2330 train_time:110646ms step_avg:57.96ms
step:1910/2330 train_time:110706ms step_avg:57.96ms
step:1911/2330 train_time:110763ms step_avg:57.96ms
step:1912/2330 train_time:110824ms step_avg:57.96ms
step:1913/2330 train_time:110881ms step_avg:57.96ms
step:1914/2330 train_time:110941ms step_avg:57.96ms
step:1915/2330 train_time:110998ms step_avg:57.96ms
step:1916/2330 train_time:111058ms step_avg:57.96ms
step:1917/2330 train_time:111116ms step_avg:57.96ms
step:1918/2330 train_time:111177ms step_avg:57.96ms
step:1919/2330 train_time:111234ms step_avg:57.96ms
step:1920/2330 train_time:111293ms step_avg:57.96ms
step:1921/2330 train_time:111350ms step_avg:57.96ms
step:1922/2330 train_time:111410ms step_avg:57.97ms
step:1923/2330 train_time:111467ms step_avg:57.97ms
step:1924/2330 train_time:111528ms step_avg:57.97ms
step:1925/2330 train_time:111585ms step_avg:57.97ms
step:1926/2330 train_time:111645ms step_avg:57.97ms
step:1927/2330 train_time:111702ms step_avg:57.97ms
step:1928/2330 train_time:111762ms step_avg:57.97ms
step:1929/2330 train_time:111820ms step_avg:57.97ms
step:1930/2330 train_time:111880ms step_avg:57.97ms
step:1931/2330 train_time:111937ms step_avg:57.97ms
step:1932/2330 train_time:111997ms step_avg:57.97ms
step:1933/2330 train_time:112055ms step_avg:57.97ms
step:1934/2330 train_time:112114ms step_avg:57.97ms
step:1935/2330 train_time:112172ms step_avg:57.97ms
step:1936/2330 train_time:112232ms step_avg:57.97ms
step:1937/2330 train_time:112289ms step_avg:57.97ms
step:1938/2330 train_time:112349ms step_avg:57.97ms
step:1939/2330 train_time:112405ms step_avg:57.97ms
step:1940/2330 train_time:112466ms step_avg:57.97ms
step:1941/2330 train_time:112523ms step_avg:57.97ms
step:1942/2330 train_time:112583ms step_avg:57.97ms
step:1943/2330 train_time:112640ms step_avg:57.97ms
step:1944/2330 train_time:112699ms step_avg:57.97ms
step:1945/2330 train_time:112757ms step_avg:57.97ms
step:1946/2330 train_time:112816ms step_avg:57.97ms
step:1947/2330 train_time:112874ms step_avg:57.97ms
step:1948/2330 train_time:112933ms step_avg:57.97ms
step:1949/2330 train_time:112991ms step_avg:57.97ms
step:1950/2330 train_time:113050ms step_avg:57.97ms
step:1951/2330 train_time:113107ms step_avg:57.97ms
step:1952/2330 train_time:113166ms step_avg:57.97ms
step:1953/2330 train_time:113224ms step_avg:57.97ms
step:1954/2330 train_time:113284ms step_avg:57.98ms
step:1955/2330 train_time:113340ms step_avg:57.97ms
step:1956/2330 train_time:113401ms step_avg:57.98ms
step:1957/2330 train_time:113458ms step_avg:57.98ms
step:1958/2330 train_time:113518ms step_avg:57.98ms
step:1959/2330 train_time:113575ms step_avg:57.98ms
step:1960/2330 train_time:113635ms step_avg:57.98ms
step:1961/2330 train_time:113691ms step_avg:57.98ms
step:1962/2330 train_time:113752ms step_avg:57.98ms
step:1963/2330 train_time:113808ms step_avg:57.98ms
step:1964/2330 train_time:113869ms step_avg:57.98ms
step:1965/2330 train_time:113926ms step_avg:57.98ms
step:1966/2330 train_time:113987ms step_avg:57.98ms
step:1967/2330 train_time:114044ms step_avg:57.98ms
step:1968/2330 train_time:114104ms step_avg:57.98ms
step:1969/2330 train_time:114161ms step_avg:57.98ms
step:1970/2330 train_time:114222ms step_avg:57.98ms
step:1971/2330 train_time:114279ms step_avg:57.98ms
step:1972/2330 train_time:114339ms step_avg:57.98ms
step:1973/2330 train_time:114397ms step_avg:57.98ms
step:1974/2330 train_time:114456ms step_avg:57.98ms
step:1975/2330 train_time:114513ms step_avg:57.98ms
step:1976/2330 train_time:114573ms step_avg:57.98ms
step:1977/2330 train_time:114630ms step_avg:57.98ms
step:1978/2330 train_time:114689ms step_avg:57.98ms
step:1979/2330 train_time:114745ms step_avg:57.98ms
step:1980/2330 train_time:114806ms step_avg:57.98ms
step:1981/2330 train_time:114863ms step_avg:57.98ms
step:1982/2330 train_time:114924ms step_avg:57.98ms
step:1983/2330 train_time:114981ms step_avg:57.98ms
step:1984/2330 train_time:115041ms step_avg:57.98ms
step:1985/2330 train_time:115099ms step_avg:57.98ms
step:1986/2330 train_time:115158ms step_avg:57.99ms
step:1987/2330 train_time:115215ms step_avg:57.98ms
step:1988/2330 train_time:115275ms step_avg:57.99ms
step:1989/2330 train_time:115332ms step_avg:57.98ms
step:1990/2330 train_time:115392ms step_avg:57.99ms
step:1991/2330 train_time:115448ms step_avg:57.99ms
step:1992/2330 train_time:115509ms step_avg:57.99ms
step:1993/2330 train_time:115566ms step_avg:57.99ms
step:1994/2330 train_time:115626ms step_avg:57.99ms
step:1995/2330 train_time:115683ms step_avg:57.99ms
step:1996/2330 train_time:115744ms step_avg:57.99ms
step:1997/2330 train_time:115802ms step_avg:57.99ms
step:1998/2330 train_time:115861ms step_avg:57.99ms
step:1999/2330 train_time:115918ms step_avg:57.99ms
step:2000/2330 train_time:115979ms step_avg:57.99ms
step:2000/2330 val_loss:3.9257 train_time:116060ms step_avg:58.03ms
step:2001/2330 train_time:116078ms step_avg:58.01ms
step:2002/2330 train_time:116099ms step_avg:57.99ms
step:2003/2330 train_time:116157ms step_avg:57.99ms
step:2004/2330 train_time:116223ms step_avg:58.00ms
step:2005/2330 train_time:116282ms step_avg:58.00ms
step:2006/2330 train_time:116343ms step_avg:58.00ms
step:2007/2330 train_time:116400ms step_avg:58.00ms
step:2008/2330 train_time:116459ms step_avg:58.00ms
step:2009/2330 train_time:116516ms step_avg:58.00ms
step:2010/2330 train_time:116577ms step_avg:58.00ms
step:2011/2330 train_time:116633ms step_avg:58.00ms
step:2012/2330 train_time:116692ms step_avg:58.00ms
step:2013/2330 train_time:116749ms step_avg:58.00ms
step:2014/2330 train_time:116808ms step_avg:58.00ms
step:2015/2330 train_time:116865ms step_avg:58.00ms
step:2016/2330 train_time:116924ms step_avg:58.00ms
step:2017/2330 train_time:116980ms step_avg:58.00ms
step:2018/2330 train_time:117040ms step_avg:58.00ms
step:2019/2330 train_time:117097ms step_avg:58.00ms
step:2020/2330 train_time:117160ms step_avg:58.00ms
step:2021/2330 train_time:117218ms step_avg:58.00ms
step:2022/2330 train_time:117280ms step_avg:58.00ms
step:2023/2330 train_time:117338ms step_avg:58.00ms
step:2024/2330 train_time:117399ms step_avg:58.00ms
step:2025/2330 train_time:117456ms step_avg:58.00ms
step:2026/2330 train_time:117516ms step_avg:58.00ms
step:2027/2330 train_time:117574ms step_avg:58.00ms
step:2028/2330 train_time:117634ms step_avg:58.00ms
step:2029/2330 train_time:117690ms step_avg:58.00ms
step:2030/2330 train_time:117749ms step_avg:58.00ms
step:2031/2330 train_time:117806ms step_avg:58.00ms
step:2032/2330 train_time:117865ms step_avg:58.00ms
step:2033/2330 train_time:117921ms step_avg:58.00ms
step:2034/2330 train_time:117981ms step_avg:58.00ms
step:2035/2330 train_time:118037ms step_avg:58.00ms
step:2036/2330 train_time:118098ms step_avg:58.01ms
step:2037/2330 train_time:118156ms step_avg:58.00ms
step:2038/2330 train_time:118216ms step_avg:58.01ms
step:2039/2330 train_time:118274ms step_avg:58.01ms
step:2040/2330 train_time:118335ms step_avg:58.01ms
step:2041/2330 train_time:118394ms step_avg:58.01ms
step:2042/2330 train_time:118454ms step_avg:58.01ms
step:2043/2330 train_time:118513ms step_avg:58.01ms
step:2044/2330 train_time:118573ms step_avg:58.01ms
step:2045/2330 train_time:118629ms step_avg:58.01ms
step:2046/2330 train_time:118689ms step_avg:58.01ms
step:2047/2330 train_time:118746ms step_avg:58.01ms
step:2048/2330 train_time:118805ms step_avg:58.01ms
step:2049/2330 train_time:118862ms step_avg:58.01ms
step:2050/2330 train_time:118922ms step_avg:58.01ms
step:2051/2330 train_time:118979ms step_avg:58.01ms
step:2052/2330 train_time:119040ms step_avg:58.01ms
step:2053/2330 train_time:119098ms step_avg:58.01ms
step:2054/2330 train_time:119158ms step_avg:58.01ms
step:2055/2330 train_time:119215ms step_avg:58.01ms
step:2056/2330 train_time:119276ms step_avg:58.01ms
step:2057/2330 train_time:119333ms step_avg:58.01ms
step:2058/2330 train_time:119393ms step_avg:58.01ms
step:2059/2330 train_time:119451ms step_avg:58.01ms
step:2060/2330 train_time:119512ms step_avg:58.02ms
step:2061/2330 train_time:119569ms step_avg:58.02ms
step:2062/2330 train_time:119629ms step_avg:58.02ms
step:2063/2330 train_time:119687ms step_avg:58.02ms
step:2064/2330 train_time:119746ms step_avg:58.02ms
step:2065/2330 train_time:119803ms step_avg:58.02ms
step:2066/2330 train_time:119863ms step_avg:58.02ms
step:2067/2330 train_time:119920ms step_avg:58.02ms
step:2068/2330 train_time:119979ms step_avg:58.02ms
step:2069/2330 train_time:120036ms step_avg:58.02ms
step:2070/2330 train_time:120097ms step_avg:58.02ms
step:2071/2330 train_time:120155ms step_avg:58.02ms
step:2072/2330 train_time:120215ms step_avg:58.02ms
step:2073/2330 train_time:120272ms step_avg:58.02ms
step:2074/2330 train_time:120332ms step_avg:58.02ms
step:2075/2330 train_time:120390ms step_avg:58.02ms
step:2076/2330 train_time:120450ms step_avg:58.02ms
step:2077/2330 train_time:120507ms step_avg:58.02ms
step:2078/2330 train_time:120567ms step_avg:58.02ms
step:2079/2330 train_time:120623ms step_avg:58.02ms
step:2080/2330 train_time:120684ms step_avg:58.02ms
step:2081/2330 train_time:120741ms step_avg:58.02ms
step:2082/2330 train_time:120801ms step_avg:58.02ms
step:2083/2330 train_time:120857ms step_avg:58.02ms
step:2084/2330 train_time:120917ms step_avg:58.02ms
step:2085/2330 train_time:120975ms step_avg:58.02ms
step:2086/2330 train_time:121034ms step_avg:58.02ms
step:2087/2330 train_time:121092ms step_avg:58.02ms
step:2088/2330 train_time:121152ms step_avg:58.02ms
step:2089/2330 train_time:121209ms step_avg:58.02ms
step:2090/2330 train_time:121268ms step_avg:58.02ms
step:2091/2330 train_time:121325ms step_avg:58.02ms
step:2092/2330 train_time:121386ms step_avg:58.02ms
step:2093/2330 train_time:121443ms step_avg:58.02ms
step:2094/2330 train_time:121504ms step_avg:58.02ms
step:2095/2330 train_time:121561ms step_avg:58.02ms
step:2096/2330 train_time:121621ms step_avg:58.03ms
step:2097/2330 train_time:121678ms step_avg:58.02ms
step:2098/2330 train_time:121738ms step_avg:58.03ms
step:2099/2330 train_time:121795ms step_avg:58.03ms
step:2100/2330 train_time:121855ms step_avg:58.03ms
step:2101/2330 train_time:121912ms step_avg:58.03ms
step:2102/2330 train_time:121972ms step_avg:58.03ms
step:2103/2330 train_time:122029ms step_avg:58.03ms
step:2104/2330 train_time:122089ms step_avg:58.03ms
step:2105/2330 train_time:122146ms step_avg:58.03ms
step:2106/2330 train_time:122206ms step_avg:58.03ms
step:2107/2330 train_time:122262ms step_avg:58.03ms
step:2108/2330 train_time:122323ms step_avg:58.03ms
step:2109/2330 train_time:122380ms step_avg:58.03ms
step:2110/2330 train_time:122441ms step_avg:58.03ms
step:2111/2330 train_time:122499ms step_avg:58.03ms
step:2112/2330 train_time:122558ms step_avg:58.03ms
step:2113/2330 train_time:122616ms step_avg:58.03ms
step:2114/2330 train_time:122676ms step_avg:58.03ms
step:2115/2330 train_time:122733ms step_avg:58.03ms
step:2116/2330 train_time:122793ms step_avg:58.03ms
step:2117/2330 train_time:122850ms step_avg:58.03ms
step:2118/2330 train_time:122910ms step_avg:58.03ms
step:2119/2330 train_time:122968ms step_avg:58.03ms
step:2120/2330 train_time:123027ms step_avg:58.03ms
step:2121/2330 train_time:123084ms step_avg:58.03ms
step:2122/2330 train_time:123144ms step_avg:58.03ms
step:2123/2330 train_time:123202ms step_avg:58.03ms
step:2124/2330 train_time:123262ms step_avg:58.03ms
step:2125/2330 train_time:123319ms step_avg:58.03ms
step:2126/2330 train_time:123379ms step_avg:58.03ms
step:2127/2330 train_time:123435ms step_avg:58.03ms
step:2128/2330 train_time:123497ms step_avg:58.03ms
step:2129/2330 train_time:123554ms step_avg:58.03ms
step:2130/2330 train_time:123614ms step_avg:58.03ms
step:2131/2330 train_time:123672ms step_avg:58.03ms
step:2132/2330 train_time:123731ms step_avg:58.04ms
step:2133/2330 train_time:123789ms step_avg:58.04ms
step:2134/2330 train_time:123849ms step_avg:58.04ms
step:2135/2330 train_time:123906ms step_avg:58.04ms
step:2136/2330 train_time:123965ms step_avg:58.04ms
step:2137/2330 train_time:124022ms step_avg:58.04ms
step:2138/2330 train_time:124083ms step_avg:58.04ms
step:2139/2330 train_time:124140ms step_avg:58.04ms
step:2140/2330 train_time:124201ms step_avg:58.04ms
step:2141/2330 train_time:124258ms step_avg:58.04ms
step:2142/2330 train_time:124318ms step_avg:58.04ms
step:2143/2330 train_time:124376ms step_avg:58.04ms
step:2144/2330 train_time:124436ms step_avg:58.04ms
step:2145/2330 train_time:124494ms step_avg:58.04ms
step:2146/2330 train_time:124553ms step_avg:58.04ms
step:2147/2330 train_time:124611ms step_avg:58.04ms
step:2148/2330 train_time:124671ms step_avg:58.04ms
step:2149/2330 train_time:124728ms step_avg:58.04ms
step:2150/2330 train_time:124787ms step_avg:58.04ms
step:2151/2330 train_time:124844ms step_avg:58.04ms
step:2152/2330 train_time:124904ms step_avg:58.04ms
step:2153/2330 train_time:124961ms step_avg:58.04ms
step:2154/2330 train_time:125021ms step_avg:58.04ms
step:2155/2330 train_time:125078ms step_avg:58.04ms
step:2156/2330 train_time:125138ms step_avg:58.04ms
step:2157/2330 train_time:125195ms step_avg:58.04ms
step:2158/2330 train_time:125256ms step_avg:58.04ms
step:2159/2330 train_time:125313ms step_avg:58.04ms
step:2160/2330 train_time:125373ms step_avg:58.04ms
step:2161/2330 train_time:125430ms step_avg:58.04ms
step:2162/2330 train_time:125490ms step_avg:58.04ms
step:2163/2330 train_time:125548ms step_avg:58.04ms
step:2164/2330 train_time:125607ms step_avg:58.04ms
step:2165/2330 train_time:125665ms step_avg:58.04ms
step:2166/2330 train_time:125725ms step_avg:58.04ms
step:2167/2330 train_time:125782ms step_avg:58.04ms
step:2168/2330 train_time:125841ms step_avg:58.04ms
step:2169/2330 train_time:125899ms step_avg:58.04ms
step:2170/2330 train_time:125958ms step_avg:58.05ms
step:2171/2330 train_time:126016ms step_avg:58.04ms
step:2172/2330 train_time:126076ms step_avg:58.05ms
step:2173/2330 train_time:126133ms step_avg:58.05ms
step:2174/2330 train_time:126193ms step_avg:58.05ms
step:2175/2330 train_time:126250ms step_avg:58.05ms
step:2176/2330 train_time:126311ms step_avg:58.05ms
step:2177/2330 train_time:126367ms step_avg:58.05ms
step:2178/2330 train_time:126428ms step_avg:58.05ms
step:2179/2330 train_time:126484ms step_avg:58.05ms
step:2180/2330 train_time:126546ms step_avg:58.05ms
step:2181/2330 train_time:126603ms step_avg:58.05ms
step:2182/2330 train_time:126664ms step_avg:58.05ms
step:2183/2330 train_time:126721ms step_avg:58.05ms
step:2184/2330 train_time:126781ms step_avg:58.05ms
step:2185/2330 train_time:126838ms step_avg:58.05ms
step:2186/2330 train_time:126899ms step_avg:58.05ms
step:2187/2330 train_time:126956ms step_avg:58.05ms
step:2188/2330 train_time:127016ms step_avg:58.05ms
step:2189/2330 train_time:127074ms step_avg:58.05ms
step:2190/2330 train_time:127133ms step_avg:58.05ms
step:2191/2330 train_time:127190ms step_avg:58.05ms
step:2192/2330 train_time:127251ms step_avg:58.05ms
step:2193/2330 train_time:127308ms step_avg:58.05ms
step:2194/2330 train_time:127368ms step_avg:58.05ms
step:2195/2330 train_time:127425ms step_avg:58.05ms
step:2196/2330 train_time:127484ms step_avg:58.05ms
step:2197/2330 train_time:127541ms step_avg:58.05ms
step:2198/2330 train_time:127602ms step_avg:58.05ms
step:2199/2330 train_time:127660ms step_avg:58.05ms
step:2200/2330 train_time:127720ms step_avg:58.05ms
step:2201/2330 train_time:127778ms step_avg:58.05ms
step:2202/2330 train_time:127838ms step_avg:58.06ms
step:2203/2330 train_time:127894ms step_avg:58.05ms
step:2204/2330 train_time:127954ms step_avg:58.06ms
step:2205/2330 train_time:128012ms step_avg:58.06ms
step:2206/2330 train_time:128071ms step_avg:58.06ms
step:2207/2330 train_time:128128ms step_avg:58.06ms
step:2208/2330 train_time:128188ms step_avg:58.06ms
step:2209/2330 train_time:128245ms step_avg:58.06ms
step:2210/2330 train_time:128305ms step_avg:58.06ms
step:2211/2330 train_time:128362ms step_avg:58.06ms
step:2212/2330 train_time:128422ms step_avg:58.06ms
step:2213/2330 train_time:128479ms step_avg:58.06ms
step:2214/2330 train_time:128540ms step_avg:58.06ms
step:2215/2330 train_time:128598ms step_avg:58.06ms
step:2216/2330 train_time:128658ms step_avg:58.06ms
step:2217/2330 train_time:128715ms step_avg:58.06ms
step:2218/2330 train_time:128775ms step_avg:58.06ms
step:2219/2330 train_time:128832ms step_avg:58.06ms
step:2220/2330 train_time:128892ms step_avg:58.06ms
step:2221/2330 train_time:128949ms step_avg:58.06ms
step:2222/2330 train_time:129008ms step_avg:58.06ms
step:2223/2330 train_time:129065ms step_avg:58.06ms
step:2224/2330 train_time:129124ms step_avg:58.06ms
step:2225/2330 train_time:129182ms step_avg:58.06ms
step:2226/2330 train_time:129242ms step_avg:58.06ms
step:2227/2330 train_time:129299ms step_avg:58.06ms
step:2228/2330 train_time:129360ms step_avg:58.06ms
step:2229/2330 train_time:129417ms step_avg:58.06ms
step:2230/2330 train_time:129476ms step_avg:58.06ms
step:2231/2330 train_time:129534ms step_avg:58.06ms
step:2232/2330 train_time:129593ms step_avg:58.06ms
step:2233/2330 train_time:129650ms step_avg:58.06ms
step:2234/2330 train_time:129709ms step_avg:58.06ms
step:2235/2330 train_time:129766ms step_avg:58.06ms
step:2236/2330 train_time:129827ms step_avg:58.06ms
step:2237/2330 train_time:129884ms step_avg:58.06ms
step:2238/2330 train_time:129944ms step_avg:58.06ms
step:2239/2330 train_time:130002ms step_avg:58.06ms
step:2240/2330 train_time:130061ms step_avg:58.06ms
step:2241/2330 train_time:130119ms step_avg:58.06ms
step:2242/2330 train_time:130179ms step_avg:58.06ms
step:2243/2330 train_time:130236ms step_avg:58.06ms
step:2244/2330 train_time:130296ms step_avg:58.06ms
step:2245/2330 train_time:130353ms step_avg:58.06ms
step:2246/2330 train_time:130413ms step_avg:58.06ms
step:2247/2330 train_time:130470ms step_avg:58.06ms
step:2248/2330 train_time:130529ms step_avg:58.06ms
step:2249/2330 train_time:130587ms step_avg:58.06ms
step:2250/2330 train_time:130646ms step_avg:58.07ms
step:2250/2330 val_loss:3.8775 train_time:130727ms step_avg:58.10ms
step:2251/2330 train_time:130746ms step_avg:58.08ms
step:2252/2330 train_time:130767ms step_avg:58.07ms
step:2253/2330 train_time:130830ms step_avg:58.07ms
step:2254/2330 train_time:130894ms step_avg:58.07ms
step:2255/2330 train_time:130952ms step_avg:58.07ms
step:2256/2330 train_time:131011ms step_avg:58.07ms
step:2257/2330 train_time:131068ms step_avg:58.07ms
step:2258/2330 train_time:131129ms step_avg:58.07ms
step:2259/2330 train_time:131186ms step_avg:58.07ms
step:2260/2330 train_time:131245ms step_avg:58.07ms
step:2261/2330 train_time:131302ms step_avg:58.07ms
step:2262/2330 train_time:131361ms step_avg:58.07ms
step:2263/2330 train_time:131417ms step_avg:58.07ms
step:2264/2330 train_time:131477ms step_avg:58.07ms
step:2265/2330 train_time:131533ms step_avg:58.07ms
step:2266/2330 train_time:131593ms step_avg:58.07ms
step:2267/2330 train_time:131649ms step_avg:58.07ms
step:2268/2330 train_time:131710ms step_avg:58.07ms
step:2269/2330 train_time:131767ms step_avg:58.07ms
step:2270/2330 train_time:131831ms step_avg:58.08ms
step:2271/2330 train_time:131889ms step_avg:58.08ms
step:2272/2330 train_time:131950ms step_avg:58.08ms
step:2273/2330 train_time:132007ms step_avg:58.08ms
step:2274/2330 train_time:132069ms step_avg:58.08ms
step:2275/2330 train_time:132126ms step_avg:58.08ms
step:2276/2330 train_time:132186ms step_avg:58.08ms
step:2277/2330 train_time:132243ms step_avg:58.08ms
step:2278/2330 train_time:132303ms step_avg:58.08ms
step:2279/2330 train_time:132359ms step_avg:58.08ms
step:2280/2330 train_time:132419ms step_avg:58.08ms
step:2281/2330 train_time:132475ms step_avg:58.08ms
step:2282/2330 train_time:132535ms step_avg:58.08ms
step:2283/2330 train_time:132592ms step_avg:58.08ms
step:2284/2330 train_time:132651ms step_avg:58.08ms
step:2285/2330 train_time:132708ms step_avg:58.08ms
step:2286/2330 train_time:132769ms step_avg:58.08ms
step:2287/2330 train_time:132827ms step_avg:58.08ms
step:2288/2330 train_time:132888ms step_avg:58.08ms
step:2289/2330 train_time:132947ms step_avg:58.08ms
step:2290/2330 train_time:133007ms step_avg:58.08ms
step:2291/2330 train_time:133064ms step_avg:58.08ms
step:2292/2330 train_time:133124ms step_avg:58.08ms
step:2293/2330 train_time:133181ms step_avg:58.08ms
step:2294/2330 train_time:133241ms step_avg:58.08ms
step:2295/2330 train_time:133299ms step_avg:58.08ms
step:2296/2330 train_time:133358ms step_avg:58.08ms
step:2297/2330 train_time:133415ms step_avg:58.08ms
step:2298/2330 train_time:133474ms step_avg:58.08ms
step:2299/2330 train_time:133532ms step_avg:58.08ms
step:2300/2330 train_time:133591ms step_avg:58.08ms
step:2301/2330 train_time:133649ms step_avg:58.08ms
step:2302/2330 train_time:133709ms step_avg:58.08ms
step:2303/2330 train_time:133766ms step_avg:58.08ms
step:2304/2330 train_time:133828ms step_avg:58.09ms
step:2305/2330 train_time:133886ms step_avg:58.09ms
step:2306/2330 train_time:133946ms step_avg:58.09ms
step:2307/2330 train_time:134004ms step_avg:58.09ms
step:2308/2330 train_time:134065ms step_avg:58.09ms
step:2309/2330 train_time:134123ms step_avg:58.09ms
step:2310/2330 train_time:134182ms step_avg:58.09ms
step:2311/2330 train_time:134239ms step_avg:58.09ms
step:2312/2330 train_time:134299ms step_avg:58.09ms
step:2313/2330 train_time:134355ms step_avg:58.09ms
step:2314/2330 train_time:134415ms step_avg:58.09ms
step:2315/2330 train_time:134472ms step_avg:58.09ms
step:2316/2330 train_time:134531ms step_avg:58.09ms
step:2317/2330 train_time:134588ms step_avg:58.09ms
step:2318/2330 train_time:134647ms step_avg:58.09ms
step:2319/2330 train_time:134704ms step_avg:58.09ms
step:2320/2330 train_time:134765ms step_avg:58.09ms
step:2321/2330 train_time:134822ms step_avg:58.09ms
step:2322/2330 train_time:134883ms step_avg:58.09ms
step:2323/2330 train_time:134940ms step_avg:58.09ms
step:2324/2330 train_time:135000ms step_avg:58.09ms
step:2325/2330 train_time:135058ms step_avg:58.09ms
step:2326/2330 train_time:135117ms step_avg:58.09ms
step:2327/2330 train_time:135174ms step_avg:58.09ms
step:2328/2330 train_time:135235ms step_avg:58.09ms
step:2329/2330 train_time:135292ms step_avg:58.09ms
step:2330/2330 train_time:135351ms step_avg:58.09ms
step:2330/2330 val_loss:3.8624 train_time:135432ms step_avg:58.13ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
