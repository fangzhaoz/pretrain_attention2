import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--adamw_lr', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_lr_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_lr_tuning/{args2.adamw_lr}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=float(args2.adamw_lr),  betas=(0.85, 0.85), eps=1e-10, weight_decay=0.0, fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_lr_tuning/{args2.adamw_lr}", exist_ok=True)
            torch.save(log, f"logs_adamw_small_lr_tuning/{args2.adamw_lr}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 01:38:13 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   28C    P0             114W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   27C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   25C    P0             115W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   24C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   27C    P0             113W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   25C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   26C    P0             109W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:88ms step_avg:87.86ms
step:2/2330 train_time:187ms step_avg:93.55ms
step:3/2330 train_time:207ms step_avg:68.98ms
step:4/2330 train_time:228ms step_avg:56.99ms
step:5/2330 train_time:278ms step_avg:55.54ms
step:6/2330 train_time:336ms step_avg:56.01ms
step:7/2330 train_time:391ms step_avg:55.93ms
step:8/2330 train_time:450ms step_avg:56.20ms
step:9/2330 train_time:504ms step_avg:56.02ms
step:10/2330 train_time:562ms step_avg:56.21ms
step:11/2330 train_time:617ms step_avg:56.11ms
step:12/2330 train_time:676ms step_avg:56.31ms
step:13/2330 train_time:731ms step_avg:56.23ms
step:14/2330 train_time:789ms step_avg:56.33ms
step:15/2330 train_time:844ms step_avg:56.27ms
step:16/2330 train_time:902ms step_avg:56.35ms
step:17/2330 train_time:957ms step_avg:56.27ms
step:18/2330 train_time:1015ms step_avg:56.40ms
step:19/2330 train_time:1072ms step_avg:56.44ms
step:20/2330 train_time:1134ms step_avg:56.69ms
step:21/2330 train_time:1192ms step_avg:56.75ms
step:22/2330 train_time:1252ms step_avg:56.90ms
step:23/2330 train_time:1307ms step_avg:56.84ms
step:24/2330 train_time:1367ms step_avg:56.96ms
step:25/2330 train_time:1423ms step_avg:56.91ms
step:26/2330 train_time:1481ms step_avg:56.98ms
step:27/2330 train_time:1537ms step_avg:56.91ms
step:28/2330 train_time:1595ms step_avg:56.97ms
step:29/2330 train_time:1650ms step_avg:56.91ms
step:30/2330 train_time:1709ms step_avg:56.95ms
step:31/2330 train_time:1764ms step_avg:56.89ms
step:32/2330 train_time:1822ms step_avg:56.95ms
step:33/2330 train_time:1877ms step_avg:56.89ms
step:34/2330 train_time:1936ms step_avg:56.93ms
step:35/2330 train_time:1991ms step_avg:56.88ms
step:36/2330 train_time:2050ms step_avg:56.93ms
step:37/2330 train_time:2105ms step_avg:56.89ms
step:38/2330 train_time:2167ms step_avg:57.03ms
step:39/2330 train_time:2223ms step_avg:57.00ms
step:40/2330 train_time:2284ms step_avg:57.09ms
step:41/2330 train_time:2339ms step_avg:57.06ms
step:42/2330 train_time:2399ms step_avg:57.12ms
step:43/2330 train_time:2455ms step_avg:57.08ms
step:44/2330 train_time:2513ms step_avg:57.11ms
step:45/2330 train_time:2569ms step_avg:57.08ms
step:46/2330 train_time:2627ms step_avg:57.11ms
step:47/2330 train_time:2683ms step_avg:57.08ms
step:48/2330 train_time:2741ms step_avg:57.10ms
step:49/2330 train_time:2796ms step_avg:57.07ms
step:50/2330 train_time:2854ms step_avg:57.08ms
step:51/2330 train_time:2910ms step_avg:57.06ms
step:52/2330 train_time:2968ms step_avg:57.08ms
step:53/2330 train_time:3024ms step_avg:57.05ms
step:54/2330 train_time:3082ms step_avg:57.07ms
step:55/2330 train_time:3137ms step_avg:57.04ms
step:56/2330 train_time:3198ms step_avg:57.10ms
step:57/2330 train_time:3253ms step_avg:57.07ms
step:58/2330 train_time:3313ms step_avg:57.12ms
step:59/2330 train_time:3368ms step_avg:57.09ms
step:60/2330 train_time:3428ms step_avg:57.14ms
step:61/2330 train_time:3483ms step_avg:57.10ms
step:62/2330 train_time:3542ms step_avg:57.14ms
step:63/2330 train_time:3597ms step_avg:57.10ms
step:64/2330 train_time:3657ms step_avg:57.14ms
step:65/2330 train_time:3713ms step_avg:57.12ms
step:66/2330 train_time:3771ms step_avg:57.14ms
step:67/2330 train_time:3826ms step_avg:57.11ms
step:68/2330 train_time:3885ms step_avg:57.13ms
step:69/2330 train_time:3941ms step_avg:57.12ms
step:70/2330 train_time:3999ms step_avg:57.13ms
step:71/2330 train_time:4055ms step_avg:57.12ms
step:72/2330 train_time:4114ms step_avg:57.14ms
step:73/2330 train_time:4170ms step_avg:57.12ms
step:74/2330 train_time:4229ms step_avg:57.15ms
step:75/2330 train_time:4285ms step_avg:57.14ms
step:76/2330 train_time:4344ms step_avg:57.16ms
step:77/2330 train_time:4400ms step_avg:57.14ms
step:78/2330 train_time:4458ms step_avg:57.16ms
step:79/2330 train_time:4514ms step_avg:57.14ms
step:80/2330 train_time:4573ms step_avg:57.16ms
step:81/2330 train_time:4628ms step_avg:57.14ms
step:82/2330 train_time:4687ms step_avg:57.15ms
step:83/2330 train_time:4742ms step_avg:57.14ms
step:84/2330 train_time:4801ms step_avg:57.15ms
step:85/2330 train_time:4856ms step_avg:57.13ms
step:86/2330 train_time:4915ms step_avg:57.15ms
step:87/2330 train_time:4971ms step_avg:57.14ms
step:88/2330 train_time:5030ms step_avg:57.16ms
step:89/2330 train_time:5085ms step_avg:57.14ms
step:90/2330 train_time:5144ms step_avg:57.16ms
step:91/2330 train_time:5200ms step_avg:57.14ms
step:92/2330 train_time:5260ms step_avg:57.17ms
step:93/2330 train_time:5316ms step_avg:57.16ms
step:94/2330 train_time:5374ms step_avg:57.17ms
step:95/2330 train_time:5431ms step_avg:57.17ms
step:96/2330 train_time:5490ms step_avg:57.19ms
step:97/2330 train_time:5546ms step_avg:57.17ms
step:98/2330 train_time:5604ms step_avg:57.18ms
step:99/2330 train_time:5659ms step_avg:57.16ms
step:100/2330 train_time:5719ms step_avg:57.19ms
step:101/2330 train_time:5775ms step_avg:57.17ms
step:102/2330 train_time:5833ms step_avg:57.19ms
step:103/2330 train_time:5889ms step_avg:57.17ms
step:104/2330 train_time:5948ms step_avg:57.19ms
step:105/2330 train_time:6003ms step_avg:57.17ms
step:106/2330 train_time:6062ms step_avg:57.19ms
step:107/2330 train_time:6117ms step_avg:57.17ms
step:108/2330 train_time:6177ms step_avg:57.19ms
step:109/2330 train_time:6234ms step_avg:57.19ms
step:110/2330 train_time:6293ms step_avg:57.21ms
step:111/2330 train_time:6349ms step_avg:57.20ms
step:112/2330 train_time:6408ms step_avg:57.22ms
step:113/2330 train_time:6464ms step_avg:57.20ms
step:114/2330 train_time:6523ms step_avg:57.22ms
step:115/2330 train_time:6578ms step_avg:57.20ms
step:116/2330 train_time:6637ms step_avg:57.21ms
step:117/2330 train_time:6693ms step_avg:57.20ms
step:118/2330 train_time:6751ms step_avg:57.21ms
step:119/2330 train_time:6806ms step_avg:57.20ms
step:120/2330 train_time:6865ms step_avg:57.21ms
step:121/2330 train_time:6921ms step_avg:57.19ms
step:122/2330 train_time:6979ms step_avg:57.20ms
step:123/2330 train_time:7035ms step_avg:57.19ms
step:124/2330 train_time:7093ms step_avg:57.20ms
step:125/2330 train_time:7149ms step_avg:57.19ms
step:126/2330 train_time:7209ms step_avg:57.21ms
step:127/2330 train_time:7264ms step_avg:57.20ms
step:128/2330 train_time:7324ms step_avg:57.22ms
step:129/2330 train_time:7380ms step_avg:57.21ms
step:130/2330 train_time:7440ms step_avg:57.23ms
step:131/2330 train_time:7496ms step_avg:57.22ms
step:132/2330 train_time:7554ms step_avg:57.22ms
step:133/2330 train_time:7610ms step_avg:57.21ms
step:134/2330 train_time:7669ms step_avg:57.23ms
step:135/2330 train_time:7724ms step_avg:57.22ms
step:136/2330 train_time:7783ms step_avg:57.23ms
step:137/2330 train_time:7838ms step_avg:57.21ms
step:138/2330 train_time:7897ms step_avg:57.22ms
step:139/2330 train_time:7952ms step_avg:57.21ms
step:140/2330 train_time:8011ms step_avg:57.22ms
step:141/2330 train_time:8066ms step_avg:57.21ms
step:142/2330 train_time:8125ms step_avg:57.22ms
step:143/2330 train_time:8181ms step_avg:57.21ms
step:144/2330 train_time:8240ms step_avg:57.22ms
step:145/2330 train_time:8296ms step_avg:57.21ms
step:146/2330 train_time:8355ms step_avg:57.23ms
step:147/2330 train_time:8412ms step_avg:57.22ms
step:148/2330 train_time:8471ms step_avg:57.23ms
step:149/2330 train_time:8527ms step_avg:57.23ms
step:150/2330 train_time:8585ms step_avg:57.23ms
step:151/2330 train_time:8642ms step_avg:57.23ms
step:152/2330 train_time:8701ms step_avg:57.24ms
step:153/2330 train_time:8756ms step_avg:57.23ms
step:154/2330 train_time:8814ms step_avg:57.24ms
step:155/2330 train_time:8870ms step_avg:57.22ms
step:156/2330 train_time:8928ms step_avg:57.23ms
step:157/2330 train_time:8984ms step_avg:57.22ms
step:158/2330 train_time:9042ms step_avg:57.23ms
step:159/2330 train_time:9098ms step_avg:57.22ms
step:160/2330 train_time:9157ms step_avg:57.23ms
step:161/2330 train_time:9213ms step_avg:57.22ms
step:162/2330 train_time:9271ms step_avg:57.23ms
step:163/2330 train_time:9327ms step_avg:57.22ms
step:164/2330 train_time:9387ms step_avg:57.24ms
step:165/2330 train_time:9442ms step_avg:57.23ms
step:166/2330 train_time:9501ms step_avg:57.24ms
step:167/2330 train_time:9557ms step_avg:57.22ms
step:168/2330 train_time:9617ms step_avg:57.24ms
step:169/2330 train_time:9672ms step_avg:57.23ms
step:170/2330 train_time:9731ms step_avg:57.24ms
step:171/2330 train_time:9787ms step_avg:57.23ms
step:172/2330 train_time:9846ms step_avg:57.24ms
step:173/2330 train_time:9902ms step_avg:57.24ms
step:174/2330 train_time:9960ms step_avg:57.24ms
step:175/2330 train_time:10016ms step_avg:57.23ms
step:176/2330 train_time:10074ms step_avg:57.24ms
step:177/2330 train_time:10130ms step_avg:57.23ms
step:178/2330 train_time:10189ms step_avg:57.24ms
step:179/2330 train_time:10245ms step_avg:57.24ms
step:180/2330 train_time:10303ms step_avg:57.24ms
step:181/2330 train_time:10359ms step_avg:57.23ms
step:182/2330 train_time:10419ms step_avg:57.25ms
step:183/2330 train_time:10475ms step_avg:57.24ms
step:184/2330 train_time:10533ms step_avg:57.25ms
step:185/2330 train_time:10589ms step_avg:57.24ms
step:186/2330 train_time:10648ms step_avg:57.25ms
step:187/2330 train_time:10704ms step_avg:57.24ms
step:188/2330 train_time:10763ms step_avg:57.25ms
step:189/2330 train_time:10818ms step_avg:57.24ms
step:190/2330 train_time:10878ms step_avg:57.25ms
step:191/2330 train_time:10934ms step_avg:57.24ms
step:192/2330 train_time:10993ms step_avg:57.25ms
step:193/2330 train_time:11048ms step_avg:57.24ms
step:194/2330 train_time:11107ms step_avg:57.25ms
step:195/2330 train_time:11163ms step_avg:57.24ms
step:196/2330 train_time:11221ms step_avg:57.25ms
step:197/2330 train_time:11277ms step_avg:57.24ms
step:198/2330 train_time:11337ms step_avg:57.26ms
step:199/2330 train_time:11393ms step_avg:57.25ms
step:200/2330 train_time:11452ms step_avg:57.26ms
step:201/2330 train_time:11508ms step_avg:57.25ms
step:202/2330 train_time:11567ms step_avg:57.26ms
step:203/2330 train_time:11623ms step_avg:57.25ms
step:204/2330 train_time:11682ms step_avg:57.26ms
step:205/2330 train_time:11737ms step_avg:57.26ms
step:206/2330 train_time:11796ms step_avg:57.26ms
step:207/2330 train_time:11852ms step_avg:57.26ms
step:208/2330 train_time:11911ms step_avg:57.26ms
step:209/2330 train_time:11966ms step_avg:57.26ms
step:210/2330 train_time:12025ms step_avg:57.26ms
step:211/2330 train_time:12081ms step_avg:57.26ms
step:212/2330 train_time:12140ms step_avg:57.26ms
step:213/2330 train_time:12195ms step_avg:57.26ms
step:214/2330 train_time:12254ms step_avg:57.26ms
step:215/2330 train_time:12310ms step_avg:57.26ms
step:216/2330 train_time:12368ms step_avg:57.26ms
step:217/2330 train_time:12424ms step_avg:57.25ms
step:218/2330 train_time:12483ms step_avg:57.26ms
step:219/2330 train_time:12539ms step_avg:57.26ms
step:220/2330 train_time:12598ms step_avg:57.26ms
step:221/2330 train_time:12654ms step_avg:57.26ms
step:222/2330 train_time:12713ms step_avg:57.26ms
step:223/2330 train_time:12769ms step_avg:57.26ms
step:224/2330 train_time:12827ms step_avg:57.27ms
step:225/2330 train_time:12884ms step_avg:57.26ms
step:226/2330 train_time:12942ms step_avg:57.27ms
step:227/2330 train_time:12998ms step_avg:57.26ms
step:228/2330 train_time:13056ms step_avg:57.26ms
step:229/2330 train_time:13112ms step_avg:57.26ms
step:230/2330 train_time:13171ms step_avg:57.26ms
step:231/2330 train_time:13226ms step_avg:57.26ms
step:232/2330 train_time:13285ms step_avg:57.26ms
step:233/2330 train_time:13340ms step_avg:57.25ms
step:234/2330 train_time:13400ms step_avg:57.26ms
step:235/2330 train_time:13456ms step_avg:57.26ms
step:236/2330 train_time:13515ms step_avg:57.27ms
step:237/2330 train_time:13571ms step_avg:57.26ms
step:238/2330 train_time:13630ms step_avg:57.27ms
step:239/2330 train_time:13686ms step_avg:57.26ms
step:240/2330 train_time:13745ms step_avg:57.27ms
step:241/2330 train_time:13801ms step_avg:57.27ms
step:242/2330 train_time:13859ms step_avg:57.27ms
step:243/2330 train_time:13915ms step_avg:57.26ms
step:244/2330 train_time:13973ms step_avg:57.27ms
step:245/2330 train_time:14029ms step_avg:57.26ms
step:246/2330 train_time:14090ms step_avg:57.28ms
step:247/2330 train_time:14146ms step_avg:57.27ms
step:248/2330 train_time:14205ms step_avg:57.28ms
step:249/2330 train_time:14260ms step_avg:57.27ms
step:250/2330 train_time:14319ms step_avg:57.28ms
step:250/2330 val_loss:4.8520 train_time:14398ms step_avg:57.59ms
step:251/2330 train_time:14417ms step_avg:57.44ms
step:252/2330 train_time:14437ms step_avg:57.29ms
step:253/2330 train_time:14491ms step_avg:57.28ms
step:254/2330 train_time:14554ms step_avg:57.30ms
step:255/2330 train_time:14608ms step_avg:57.29ms
step:256/2330 train_time:14675ms step_avg:57.32ms
step:257/2330 train_time:14730ms step_avg:57.31ms
step:258/2330 train_time:14791ms step_avg:57.33ms
step:259/2330 train_time:14846ms step_avg:57.32ms
step:260/2330 train_time:14903ms step_avg:57.32ms
step:261/2330 train_time:14959ms step_avg:57.31ms
step:262/2330 train_time:15017ms step_avg:57.32ms
step:263/2330 train_time:15072ms step_avg:57.31ms
step:264/2330 train_time:15131ms step_avg:57.31ms
step:265/2330 train_time:15186ms step_avg:57.31ms
step:266/2330 train_time:15244ms step_avg:57.31ms
step:267/2330 train_time:15299ms step_avg:57.30ms
step:268/2330 train_time:15360ms step_avg:57.31ms
step:269/2330 train_time:15417ms step_avg:57.31ms
step:270/2330 train_time:15477ms step_avg:57.32ms
step:271/2330 train_time:15533ms step_avg:57.32ms
step:272/2330 train_time:15593ms step_avg:57.33ms
step:273/2330 train_time:15649ms step_avg:57.32ms
step:274/2330 train_time:15709ms step_avg:57.33ms
step:275/2330 train_time:15765ms step_avg:57.33ms
step:276/2330 train_time:15823ms step_avg:57.33ms
step:277/2330 train_time:15879ms step_avg:57.32ms
step:278/2330 train_time:15938ms step_avg:57.33ms
step:279/2330 train_time:15993ms step_avg:57.32ms
step:280/2330 train_time:16052ms step_avg:57.33ms
step:281/2330 train_time:16107ms step_avg:57.32ms
step:282/2330 train_time:16165ms step_avg:57.32ms
step:283/2330 train_time:16220ms step_avg:57.31ms
step:284/2330 train_time:16280ms step_avg:57.32ms
step:285/2330 train_time:16335ms step_avg:57.32ms
step:286/2330 train_time:16395ms step_avg:57.32ms
step:287/2330 train_time:16451ms step_avg:57.32ms
step:288/2330 train_time:16510ms step_avg:57.33ms
step:289/2330 train_time:16566ms step_avg:57.32ms
step:290/2330 train_time:16625ms step_avg:57.33ms
step:291/2330 train_time:16682ms step_avg:57.33ms
step:292/2330 train_time:16740ms step_avg:57.33ms
step:293/2330 train_time:16796ms step_avg:57.32ms
step:294/2330 train_time:16855ms step_avg:57.33ms
step:295/2330 train_time:16910ms step_avg:57.32ms
step:296/2330 train_time:16970ms step_avg:57.33ms
step:297/2330 train_time:17025ms step_avg:57.32ms
step:298/2330 train_time:17084ms step_avg:57.33ms
step:299/2330 train_time:17139ms step_avg:57.32ms
step:300/2330 train_time:17198ms step_avg:57.33ms
step:301/2330 train_time:17254ms step_avg:57.32ms
step:302/2330 train_time:17312ms step_avg:57.32ms
step:303/2330 train_time:17368ms step_avg:57.32ms
step:304/2330 train_time:17426ms step_avg:57.32ms
step:305/2330 train_time:17483ms step_avg:57.32ms
step:306/2330 train_time:17541ms step_avg:57.32ms
step:307/2330 train_time:17597ms step_avg:57.32ms
step:308/2330 train_time:17657ms step_avg:57.33ms
step:309/2330 train_time:17712ms step_avg:57.32ms
step:310/2330 train_time:17772ms step_avg:57.33ms
step:311/2330 train_time:17828ms step_avg:57.32ms
step:312/2330 train_time:17887ms step_avg:57.33ms
step:313/2330 train_time:17943ms step_avg:57.33ms
step:314/2330 train_time:18002ms step_avg:57.33ms
step:315/2330 train_time:18057ms step_avg:57.33ms
step:316/2330 train_time:18116ms step_avg:57.33ms
step:317/2330 train_time:18172ms step_avg:57.33ms
step:318/2330 train_time:18231ms step_avg:57.33ms
step:319/2330 train_time:18286ms step_avg:57.32ms
step:320/2330 train_time:18345ms step_avg:57.33ms
step:321/2330 train_time:18401ms step_avg:57.32ms
step:322/2330 train_time:18460ms step_avg:57.33ms
step:323/2330 train_time:18515ms step_avg:57.32ms
step:324/2330 train_time:18574ms step_avg:57.33ms
step:325/2330 train_time:18630ms step_avg:57.32ms
step:326/2330 train_time:18690ms step_avg:57.33ms
step:327/2330 train_time:18746ms step_avg:57.33ms
step:328/2330 train_time:18805ms step_avg:57.33ms
step:329/2330 train_time:18860ms step_avg:57.33ms
step:330/2330 train_time:18920ms step_avg:57.33ms
step:331/2330 train_time:18975ms step_avg:57.33ms
step:332/2330 train_time:19034ms step_avg:57.33ms
step:333/2330 train_time:19090ms step_avg:57.33ms
step:334/2330 train_time:19149ms step_avg:57.33ms
step:335/2330 train_time:19206ms step_avg:57.33ms
step:336/2330 train_time:19265ms step_avg:57.33ms
step:337/2330 train_time:19320ms step_avg:57.33ms
step:338/2330 train_time:19379ms step_avg:57.33ms
step:339/2330 train_time:19434ms step_avg:57.33ms
step:340/2330 train_time:19494ms step_avg:57.33ms
step:341/2330 train_time:19550ms step_avg:57.33ms
step:342/2330 train_time:19609ms step_avg:57.34ms
step:343/2330 train_time:19666ms step_avg:57.33ms
step:344/2330 train_time:19724ms step_avg:57.34ms
step:345/2330 train_time:19780ms step_avg:57.33ms
step:346/2330 train_time:19839ms step_avg:57.34ms
step:347/2330 train_time:19896ms step_avg:57.34ms
step:348/2330 train_time:19954ms step_avg:57.34ms
step:349/2330 train_time:20010ms step_avg:57.33ms
step:350/2330 train_time:20069ms step_avg:57.34ms
step:351/2330 train_time:20125ms step_avg:57.34ms
step:352/2330 train_time:20184ms step_avg:57.34ms
step:353/2330 train_time:20239ms step_avg:57.34ms
step:354/2330 train_time:20298ms step_avg:57.34ms
step:355/2330 train_time:20354ms step_avg:57.34ms
step:356/2330 train_time:20413ms step_avg:57.34ms
step:357/2330 train_time:20469ms step_avg:57.34ms
step:358/2330 train_time:20528ms step_avg:57.34ms
step:359/2330 train_time:20583ms step_avg:57.34ms
step:360/2330 train_time:20644ms step_avg:57.34ms
step:361/2330 train_time:20700ms step_avg:57.34ms
step:362/2330 train_time:20759ms step_avg:57.34ms
step:363/2330 train_time:20815ms step_avg:57.34ms
step:364/2330 train_time:20873ms step_avg:57.34ms
step:365/2330 train_time:20929ms step_avg:57.34ms
step:366/2330 train_time:20989ms step_avg:57.35ms
step:367/2330 train_time:21045ms step_avg:57.34ms
step:368/2330 train_time:21104ms step_avg:57.35ms
step:369/2330 train_time:21159ms step_avg:57.34ms
step:370/2330 train_time:21218ms step_avg:57.34ms
step:371/2330 train_time:21274ms step_avg:57.34ms
step:372/2330 train_time:21332ms step_avg:57.34ms
step:373/2330 train_time:21389ms step_avg:57.34ms
step:374/2330 train_time:21447ms step_avg:57.35ms
step:375/2330 train_time:21504ms step_avg:57.34ms
step:376/2330 train_time:21562ms step_avg:57.35ms
step:377/2330 train_time:21618ms step_avg:57.34ms
step:378/2330 train_time:21677ms step_avg:57.35ms
step:379/2330 train_time:21733ms step_avg:57.34ms
step:380/2330 train_time:21792ms step_avg:57.35ms
step:381/2330 train_time:21848ms step_avg:57.34ms
step:382/2330 train_time:21907ms step_avg:57.35ms
step:383/2330 train_time:21964ms step_avg:57.35ms
step:384/2330 train_time:22022ms step_avg:57.35ms
step:385/2330 train_time:22078ms step_avg:57.35ms
step:386/2330 train_time:22136ms step_avg:57.35ms
step:387/2330 train_time:22192ms step_avg:57.34ms
step:388/2330 train_time:22251ms step_avg:57.35ms
step:389/2330 train_time:22307ms step_avg:57.34ms
step:390/2330 train_time:22366ms step_avg:57.35ms
step:391/2330 train_time:22421ms step_avg:57.34ms
step:392/2330 train_time:22480ms step_avg:57.35ms
step:393/2330 train_time:22537ms step_avg:57.34ms
step:394/2330 train_time:22595ms step_avg:57.35ms
step:395/2330 train_time:22651ms step_avg:57.34ms
step:396/2330 train_time:22710ms step_avg:57.35ms
step:397/2330 train_time:22766ms step_avg:57.35ms
step:398/2330 train_time:22824ms step_avg:57.35ms
step:399/2330 train_time:22880ms step_avg:57.34ms
step:400/2330 train_time:22939ms step_avg:57.35ms
step:401/2330 train_time:22995ms step_avg:57.34ms
step:402/2330 train_time:23055ms step_avg:57.35ms
step:403/2330 train_time:23111ms step_avg:57.35ms
step:404/2330 train_time:23169ms step_avg:57.35ms
step:405/2330 train_time:23225ms step_avg:57.35ms
step:406/2330 train_time:23284ms step_avg:57.35ms
step:407/2330 train_time:23340ms step_avg:57.35ms
step:408/2330 train_time:23399ms step_avg:57.35ms
step:409/2330 train_time:23455ms step_avg:57.35ms
step:410/2330 train_time:23514ms step_avg:57.35ms
step:411/2330 train_time:23570ms step_avg:57.35ms
step:412/2330 train_time:23628ms step_avg:57.35ms
step:413/2330 train_time:23684ms step_avg:57.35ms
step:414/2330 train_time:23744ms step_avg:57.35ms
step:415/2330 train_time:23800ms step_avg:57.35ms
step:416/2330 train_time:23858ms step_avg:57.35ms
step:417/2330 train_time:23915ms step_avg:57.35ms
step:418/2330 train_time:23973ms step_avg:57.35ms
step:419/2330 train_time:24029ms step_avg:57.35ms
step:420/2330 train_time:24089ms step_avg:57.35ms
step:421/2330 train_time:24146ms step_avg:57.35ms
step:422/2330 train_time:24204ms step_avg:57.36ms
step:423/2330 train_time:24260ms step_avg:57.35ms
step:424/2330 train_time:24321ms step_avg:57.36ms
step:425/2330 train_time:24377ms step_avg:57.36ms
step:426/2330 train_time:24435ms step_avg:57.36ms
step:427/2330 train_time:24491ms step_avg:57.36ms
step:428/2330 train_time:24549ms step_avg:57.36ms
step:429/2330 train_time:24605ms step_avg:57.35ms
step:430/2330 train_time:24664ms step_avg:57.36ms
step:431/2330 train_time:24719ms step_avg:57.35ms
step:432/2330 train_time:24779ms step_avg:57.36ms
step:433/2330 train_time:24835ms step_avg:57.36ms
step:434/2330 train_time:24894ms step_avg:57.36ms
step:435/2330 train_time:24950ms step_avg:57.36ms
step:436/2330 train_time:25008ms step_avg:57.36ms
step:437/2330 train_time:25065ms step_avg:57.36ms
step:438/2330 train_time:25123ms step_avg:57.36ms
step:439/2330 train_time:25179ms step_avg:57.36ms
step:440/2330 train_time:25238ms step_avg:57.36ms
step:441/2330 train_time:25294ms step_avg:57.36ms
step:442/2330 train_time:25353ms step_avg:57.36ms
step:443/2330 train_time:25408ms step_avg:57.35ms
step:444/2330 train_time:25469ms step_avg:57.36ms
step:445/2330 train_time:25525ms step_avg:57.36ms
step:446/2330 train_time:25584ms step_avg:57.36ms
step:447/2330 train_time:25639ms step_avg:57.36ms
step:448/2330 train_time:25699ms step_avg:57.36ms
step:449/2330 train_time:25755ms step_avg:57.36ms
step:450/2330 train_time:25814ms step_avg:57.36ms
step:451/2330 train_time:25869ms step_avg:57.36ms
step:452/2330 train_time:25930ms step_avg:57.37ms
step:453/2330 train_time:25987ms step_avg:57.37ms
step:454/2330 train_time:26046ms step_avg:57.37ms
step:455/2330 train_time:26102ms step_avg:57.37ms
step:456/2330 train_time:26161ms step_avg:57.37ms
step:457/2330 train_time:26217ms step_avg:57.37ms
step:458/2330 train_time:26276ms step_avg:57.37ms
step:459/2330 train_time:26332ms step_avg:57.37ms
step:460/2330 train_time:26391ms step_avg:57.37ms
step:461/2330 train_time:26447ms step_avg:57.37ms
step:462/2330 train_time:26506ms step_avg:57.37ms
step:463/2330 train_time:26562ms step_avg:57.37ms
step:464/2330 train_time:26620ms step_avg:57.37ms
step:465/2330 train_time:26677ms step_avg:57.37ms
step:466/2330 train_time:26735ms step_avg:57.37ms
step:467/2330 train_time:26791ms step_avg:57.37ms
step:468/2330 train_time:26851ms step_avg:57.37ms
step:469/2330 train_time:26907ms step_avg:57.37ms
step:470/2330 train_time:26966ms step_avg:57.38ms
step:471/2330 train_time:27022ms step_avg:57.37ms
step:472/2330 train_time:27081ms step_avg:57.37ms
step:473/2330 train_time:27136ms step_avg:57.37ms
step:474/2330 train_time:27196ms step_avg:57.37ms
step:475/2330 train_time:27252ms step_avg:57.37ms
step:476/2330 train_time:27311ms step_avg:57.38ms
step:477/2330 train_time:27367ms step_avg:57.37ms
step:478/2330 train_time:27425ms step_avg:57.38ms
step:479/2330 train_time:27482ms step_avg:57.37ms
step:480/2330 train_time:27541ms step_avg:57.38ms
step:481/2330 train_time:27596ms step_avg:57.37ms
step:482/2330 train_time:27656ms step_avg:57.38ms
step:483/2330 train_time:27711ms step_avg:57.37ms
step:484/2330 train_time:27770ms step_avg:57.38ms
step:485/2330 train_time:27826ms step_avg:57.37ms
step:486/2330 train_time:27885ms step_avg:57.38ms
step:487/2330 train_time:27941ms step_avg:57.37ms
step:488/2330 train_time:28001ms step_avg:57.38ms
step:489/2330 train_time:28057ms step_avg:57.38ms
step:490/2330 train_time:28116ms step_avg:57.38ms
step:491/2330 train_time:28172ms step_avg:57.38ms
step:492/2330 train_time:28231ms step_avg:57.38ms
step:493/2330 train_time:28288ms step_avg:57.38ms
step:494/2330 train_time:28346ms step_avg:57.38ms
step:495/2330 train_time:28402ms step_avg:57.38ms
step:496/2330 train_time:28461ms step_avg:57.38ms
step:497/2330 train_time:28517ms step_avg:57.38ms
step:498/2330 train_time:28576ms step_avg:57.38ms
step:499/2330 train_time:28632ms step_avg:57.38ms
step:500/2330 train_time:28692ms step_avg:57.38ms
step:500/2330 val_loss:4.3825 train_time:28770ms step_avg:57.54ms
step:501/2330 train_time:28790ms step_avg:57.47ms
step:502/2330 train_time:28811ms step_avg:57.39ms
step:503/2330 train_time:28866ms step_avg:57.39ms
step:504/2330 train_time:28932ms step_avg:57.41ms
step:505/2330 train_time:28989ms step_avg:57.40ms
step:506/2330 train_time:29048ms step_avg:57.41ms
step:507/2330 train_time:29103ms step_avg:57.40ms
step:508/2330 train_time:29163ms step_avg:57.41ms
step:509/2330 train_time:29219ms step_avg:57.40ms
step:510/2330 train_time:29277ms step_avg:57.41ms
step:511/2330 train_time:29332ms step_avg:57.40ms
step:512/2330 train_time:29391ms step_avg:57.40ms
step:513/2330 train_time:29446ms step_avg:57.40ms
step:514/2330 train_time:29505ms step_avg:57.40ms
step:515/2330 train_time:29560ms step_avg:57.40ms
step:516/2330 train_time:29618ms step_avg:57.40ms
step:517/2330 train_time:29674ms step_avg:57.40ms
step:518/2330 train_time:29732ms step_avg:57.40ms
step:519/2330 train_time:29789ms step_avg:57.40ms
step:520/2330 train_time:29850ms step_avg:57.40ms
step:521/2330 train_time:29907ms step_avg:57.40ms
step:522/2330 train_time:29967ms step_avg:57.41ms
step:523/2330 train_time:30023ms step_avg:57.41ms
step:524/2330 train_time:30084ms step_avg:57.41ms
step:525/2330 train_time:30140ms step_avg:57.41ms
step:526/2330 train_time:30199ms step_avg:57.41ms
step:527/2330 train_time:30255ms step_avg:57.41ms
step:528/2330 train_time:30314ms step_avg:57.41ms
step:529/2330 train_time:30369ms step_avg:57.41ms
step:530/2330 train_time:30428ms step_avg:57.41ms
step:531/2330 train_time:30483ms step_avg:57.41ms
step:532/2330 train_time:30541ms step_avg:57.41ms
step:533/2330 train_time:30597ms step_avg:57.41ms
step:534/2330 train_time:30655ms step_avg:57.41ms
step:535/2330 train_time:30712ms step_avg:57.40ms
step:536/2330 train_time:30771ms step_avg:57.41ms
step:537/2330 train_time:30827ms step_avg:57.41ms
step:538/2330 train_time:30888ms step_avg:57.41ms
step:539/2330 train_time:30944ms step_avg:57.41ms
step:540/2330 train_time:31004ms step_avg:57.42ms
step:541/2330 train_time:31060ms step_avg:57.41ms
step:542/2330 train_time:31120ms step_avg:57.42ms
step:543/2330 train_time:31176ms step_avg:57.41ms
step:544/2330 train_time:31236ms step_avg:57.42ms
step:545/2330 train_time:31291ms step_avg:57.41ms
step:546/2330 train_time:31350ms step_avg:57.42ms
step:547/2330 train_time:31405ms step_avg:57.41ms
step:548/2330 train_time:31464ms step_avg:57.42ms
step:549/2330 train_time:31520ms step_avg:57.41ms
step:550/2330 train_time:31578ms step_avg:57.42ms
step:551/2330 train_time:31634ms step_avg:57.41ms
step:552/2330 train_time:31693ms step_avg:57.41ms
step:553/2330 train_time:31749ms step_avg:57.41ms
step:554/2330 train_time:31809ms step_avg:57.42ms
step:555/2330 train_time:31865ms step_avg:57.41ms
step:556/2330 train_time:31924ms step_avg:57.42ms
step:557/2330 train_time:31980ms step_avg:57.41ms
step:558/2330 train_time:32040ms step_avg:57.42ms
step:559/2330 train_time:32096ms step_avg:57.42ms
step:560/2330 train_time:32155ms step_avg:57.42ms
step:561/2330 train_time:32212ms step_avg:57.42ms
step:562/2330 train_time:32270ms step_avg:57.42ms
step:563/2330 train_time:32326ms step_avg:57.42ms
step:564/2330 train_time:32385ms step_avg:57.42ms
step:565/2330 train_time:32440ms step_avg:57.42ms
step:566/2330 train_time:32499ms step_avg:57.42ms
step:567/2330 train_time:32555ms step_avg:57.42ms
step:568/2330 train_time:32614ms step_avg:57.42ms
step:569/2330 train_time:32670ms step_avg:57.42ms
step:570/2330 train_time:32728ms step_avg:57.42ms
step:571/2330 train_time:32784ms step_avg:57.42ms
step:572/2330 train_time:32843ms step_avg:57.42ms
step:573/2330 train_time:32900ms step_avg:57.42ms
step:574/2330 train_time:32959ms step_avg:57.42ms
step:575/2330 train_time:33015ms step_avg:57.42ms
step:576/2330 train_time:33075ms step_avg:57.42ms
step:577/2330 train_time:33131ms step_avg:57.42ms
step:578/2330 train_time:33190ms step_avg:57.42ms
step:579/2330 train_time:33246ms step_avg:57.42ms
step:580/2330 train_time:33305ms step_avg:57.42ms
step:581/2330 train_time:33360ms step_avg:57.42ms
step:582/2330 train_time:33420ms step_avg:57.42ms
step:583/2330 train_time:33475ms step_avg:57.42ms
step:584/2330 train_time:33534ms step_avg:57.42ms
step:585/2330 train_time:33590ms step_avg:57.42ms
step:586/2330 train_time:33648ms step_avg:57.42ms
step:587/2330 train_time:33703ms step_avg:57.42ms
step:588/2330 train_time:33764ms step_avg:57.42ms
step:589/2330 train_time:33820ms step_avg:57.42ms
step:590/2330 train_time:33879ms step_avg:57.42ms
step:591/2330 train_time:33934ms step_avg:57.42ms
step:592/2330 train_time:33995ms step_avg:57.42ms
step:593/2330 train_time:34051ms step_avg:57.42ms
step:594/2330 train_time:34109ms step_avg:57.42ms
step:595/2330 train_time:34165ms step_avg:57.42ms
step:596/2330 train_time:34225ms step_avg:57.42ms
step:597/2330 train_time:34281ms step_avg:57.42ms
step:598/2330 train_time:34340ms step_avg:57.42ms
step:599/2330 train_time:34396ms step_avg:57.42ms
step:600/2330 train_time:34455ms step_avg:57.43ms
step:601/2330 train_time:34511ms step_avg:57.42ms
step:602/2330 train_time:34570ms step_avg:57.43ms
step:603/2330 train_time:34626ms step_avg:57.42ms
step:604/2330 train_time:34684ms step_avg:57.42ms
step:605/2330 train_time:34741ms step_avg:57.42ms
step:606/2330 train_time:34800ms step_avg:57.43ms
step:607/2330 train_time:34856ms step_avg:57.42ms
step:608/2330 train_time:34915ms step_avg:57.43ms
step:609/2330 train_time:34971ms step_avg:57.42ms
step:610/2330 train_time:35030ms step_avg:57.43ms
step:611/2330 train_time:35086ms step_avg:57.42ms
step:612/2330 train_time:35145ms step_avg:57.43ms
step:613/2330 train_time:35201ms step_avg:57.42ms
step:614/2330 train_time:35260ms step_avg:57.43ms
step:615/2330 train_time:35316ms step_avg:57.43ms
step:616/2330 train_time:35375ms step_avg:57.43ms
step:617/2330 train_time:35431ms step_avg:57.43ms
step:618/2330 train_time:35490ms step_avg:57.43ms
step:619/2330 train_time:35546ms step_avg:57.42ms
step:620/2330 train_time:35604ms step_avg:57.43ms
step:621/2330 train_time:35660ms step_avg:57.42ms
step:622/2330 train_time:35720ms step_avg:57.43ms
step:623/2330 train_time:35776ms step_avg:57.43ms
step:624/2330 train_time:35835ms step_avg:57.43ms
step:625/2330 train_time:35891ms step_avg:57.43ms
step:626/2330 train_time:35950ms step_avg:57.43ms
step:627/2330 train_time:36006ms step_avg:57.43ms
step:628/2330 train_time:36065ms step_avg:57.43ms
step:629/2330 train_time:36121ms step_avg:57.43ms
step:630/2330 train_time:36181ms step_avg:57.43ms
step:631/2330 train_time:36237ms step_avg:57.43ms
step:632/2330 train_time:36296ms step_avg:57.43ms
step:633/2330 train_time:36351ms step_avg:57.43ms
step:634/2330 train_time:36411ms step_avg:57.43ms
step:635/2330 train_time:36466ms step_avg:57.43ms
step:636/2330 train_time:36525ms step_avg:57.43ms
step:637/2330 train_time:36580ms step_avg:57.43ms
step:638/2330 train_time:36639ms step_avg:57.43ms
step:639/2330 train_time:36695ms step_avg:57.43ms
step:640/2330 train_time:36754ms step_avg:57.43ms
step:641/2330 train_time:36810ms step_avg:57.43ms
step:642/2330 train_time:36869ms step_avg:57.43ms
step:643/2330 train_time:36925ms step_avg:57.43ms
step:644/2330 train_time:36985ms step_avg:57.43ms
step:645/2330 train_time:37041ms step_avg:57.43ms
step:646/2330 train_time:37100ms step_avg:57.43ms
step:647/2330 train_time:37157ms step_avg:57.43ms
step:648/2330 train_time:37216ms step_avg:57.43ms
step:649/2330 train_time:37272ms step_avg:57.43ms
step:650/2330 train_time:37331ms step_avg:57.43ms
step:651/2330 train_time:37387ms step_avg:57.43ms
step:652/2330 train_time:37445ms step_avg:57.43ms
step:653/2330 train_time:37501ms step_avg:57.43ms
step:654/2330 train_time:37560ms step_avg:57.43ms
step:655/2330 train_time:37615ms step_avg:57.43ms
step:656/2330 train_time:37675ms step_avg:57.43ms
step:657/2330 train_time:37732ms step_avg:57.43ms
step:658/2330 train_time:37790ms step_avg:57.43ms
step:659/2330 train_time:37846ms step_avg:57.43ms
step:660/2330 train_time:37906ms step_avg:57.43ms
step:661/2330 train_time:37962ms step_avg:57.43ms
step:662/2330 train_time:38022ms step_avg:57.44ms
step:663/2330 train_time:38079ms step_avg:57.43ms
step:664/2330 train_time:38138ms step_avg:57.44ms
step:665/2330 train_time:38194ms step_avg:57.43ms
step:666/2330 train_time:38252ms step_avg:57.44ms
step:667/2330 train_time:38308ms step_avg:57.43ms
step:668/2330 train_time:38367ms step_avg:57.44ms
step:669/2330 train_time:38423ms step_avg:57.43ms
step:670/2330 train_time:38483ms step_avg:57.44ms
step:671/2330 train_time:38539ms step_avg:57.44ms
step:672/2330 train_time:38598ms step_avg:57.44ms
step:673/2330 train_time:38655ms step_avg:57.44ms
step:674/2330 train_time:38714ms step_avg:57.44ms
step:675/2330 train_time:38769ms step_avg:57.44ms
step:676/2330 train_time:38829ms step_avg:57.44ms
step:677/2330 train_time:38885ms step_avg:57.44ms
step:678/2330 train_time:38944ms step_avg:57.44ms
step:679/2330 train_time:39000ms step_avg:57.44ms
step:680/2330 train_time:39060ms step_avg:57.44ms
step:681/2330 train_time:39116ms step_avg:57.44ms
step:682/2330 train_time:39175ms step_avg:57.44ms
step:683/2330 train_time:39231ms step_avg:57.44ms
step:684/2330 train_time:39290ms step_avg:57.44ms
step:685/2330 train_time:39346ms step_avg:57.44ms
step:686/2330 train_time:39405ms step_avg:57.44ms
step:687/2330 train_time:39461ms step_avg:57.44ms
step:688/2330 train_time:39520ms step_avg:57.44ms
step:689/2330 train_time:39576ms step_avg:57.44ms
step:690/2330 train_time:39635ms step_avg:57.44ms
step:691/2330 train_time:39691ms step_avg:57.44ms
step:692/2330 train_time:39751ms step_avg:57.44ms
step:693/2330 train_time:39807ms step_avg:57.44ms
step:694/2330 train_time:39865ms step_avg:57.44ms
step:695/2330 train_time:39922ms step_avg:57.44ms
step:696/2330 train_time:39981ms step_avg:57.44ms
step:697/2330 train_time:40037ms step_avg:57.44ms
step:698/2330 train_time:40097ms step_avg:57.45ms
step:699/2330 train_time:40153ms step_avg:57.44ms
step:700/2330 train_time:40212ms step_avg:57.45ms
step:701/2330 train_time:40268ms step_avg:57.44ms
step:702/2330 train_time:40326ms step_avg:57.45ms
step:703/2330 train_time:40382ms step_avg:57.44ms
step:704/2330 train_time:40442ms step_avg:57.45ms
step:705/2330 train_time:40498ms step_avg:57.44ms
step:706/2330 train_time:40557ms step_avg:57.45ms
step:707/2330 train_time:40613ms step_avg:57.44ms
step:708/2330 train_time:40672ms step_avg:57.45ms
step:709/2330 train_time:40728ms step_avg:57.44ms
step:710/2330 train_time:40787ms step_avg:57.45ms
step:711/2330 train_time:40842ms step_avg:57.44ms
step:712/2330 train_time:40901ms step_avg:57.45ms
step:713/2330 train_time:40957ms step_avg:57.44ms
step:714/2330 train_time:41018ms step_avg:57.45ms
step:715/2330 train_time:41073ms step_avg:57.45ms
step:716/2330 train_time:41133ms step_avg:57.45ms
step:717/2330 train_time:41189ms step_avg:57.45ms
step:718/2330 train_time:41248ms step_avg:57.45ms
step:719/2330 train_time:41304ms step_avg:57.45ms
step:720/2330 train_time:41365ms step_avg:57.45ms
step:721/2330 train_time:41421ms step_avg:57.45ms
step:722/2330 train_time:41480ms step_avg:57.45ms
step:723/2330 train_time:41536ms step_avg:57.45ms
step:724/2330 train_time:41595ms step_avg:57.45ms
step:725/2330 train_time:41651ms step_avg:57.45ms
step:726/2330 train_time:41710ms step_avg:57.45ms
step:727/2330 train_time:41766ms step_avg:57.45ms
step:728/2330 train_time:41825ms step_avg:57.45ms
step:729/2330 train_time:41882ms step_avg:57.45ms
step:730/2330 train_time:41940ms step_avg:57.45ms
step:731/2330 train_time:41997ms step_avg:57.45ms
step:732/2330 train_time:42056ms step_avg:57.45ms
step:733/2330 train_time:42113ms step_avg:57.45ms
step:734/2330 train_time:42171ms step_avg:57.45ms
step:735/2330 train_time:42227ms step_avg:57.45ms
step:736/2330 train_time:42286ms step_avg:57.45ms
step:737/2330 train_time:42342ms step_avg:57.45ms
step:738/2330 train_time:42401ms step_avg:57.45ms
step:739/2330 train_time:42457ms step_avg:57.45ms
step:740/2330 train_time:42517ms step_avg:57.46ms
step:741/2330 train_time:42574ms step_avg:57.45ms
step:742/2330 train_time:42633ms step_avg:57.46ms
step:743/2330 train_time:42689ms step_avg:57.45ms
step:744/2330 train_time:42747ms step_avg:57.46ms
step:745/2330 train_time:42803ms step_avg:57.45ms
step:746/2330 train_time:42862ms step_avg:57.46ms
step:747/2330 train_time:42918ms step_avg:57.45ms
step:748/2330 train_time:42978ms step_avg:57.46ms
step:749/2330 train_time:43034ms step_avg:57.46ms
step:750/2330 train_time:43094ms step_avg:57.46ms
step:750/2330 val_loss:4.2023 train_time:43173ms step_avg:57.56ms
step:751/2330 train_time:43193ms step_avg:57.51ms
step:752/2330 train_time:43213ms step_avg:57.46ms
step:753/2330 train_time:43268ms step_avg:57.46ms
step:754/2330 train_time:43335ms step_avg:57.47ms
step:755/2330 train_time:43392ms step_avg:57.47ms
step:756/2330 train_time:43453ms step_avg:57.48ms
step:757/2330 train_time:43509ms step_avg:57.48ms
step:758/2330 train_time:43568ms step_avg:57.48ms
step:759/2330 train_time:43624ms step_avg:57.48ms
step:760/2330 train_time:43682ms step_avg:57.48ms
step:761/2330 train_time:43737ms step_avg:57.47ms
step:762/2330 train_time:43796ms step_avg:57.47ms
step:763/2330 train_time:43851ms step_avg:57.47ms
step:764/2330 train_time:43910ms step_avg:57.47ms
step:765/2330 train_time:43967ms step_avg:57.47ms
step:766/2330 train_time:44025ms step_avg:57.47ms
step:767/2330 train_time:44082ms step_avg:57.47ms
step:768/2330 train_time:44142ms step_avg:57.48ms
step:769/2330 train_time:44199ms step_avg:57.48ms
step:770/2330 train_time:44263ms step_avg:57.48ms
step:771/2330 train_time:44321ms step_avg:57.48ms
step:772/2330 train_time:44383ms step_avg:57.49ms
step:773/2330 train_time:44439ms step_avg:57.49ms
step:774/2330 train_time:44500ms step_avg:57.49ms
step:775/2330 train_time:44558ms step_avg:57.49ms
step:776/2330 train_time:44616ms step_avg:57.50ms
step:777/2330 train_time:44673ms step_avg:57.49ms
step:778/2330 train_time:44732ms step_avg:57.50ms
step:779/2330 train_time:44789ms step_avg:57.50ms
step:780/2330 train_time:44848ms step_avg:57.50ms
step:781/2330 train_time:44904ms step_avg:57.50ms
step:782/2330 train_time:44964ms step_avg:57.50ms
step:783/2330 train_time:45021ms step_avg:57.50ms
step:784/2330 train_time:45080ms step_avg:57.50ms
step:785/2330 train_time:45136ms step_avg:57.50ms
step:786/2330 train_time:45197ms step_avg:57.50ms
step:787/2330 train_time:45254ms step_avg:57.50ms
step:788/2330 train_time:45316ms step_avg:57.51ms
step:789/2330 train_time:45374ms step_avg:57.51ms
step:790/2330 train_time:45435ms step_avg:57.51ms
step:791/2330 train_time:45492ms step_avg:57.51ms
step:792/2330 train_time:45552ms step_avg:57.52ms
step:793/2330 train_time:45609ms step_avg:57.51ms
step:794/2330 train_time:45669ms step_avg:57.52ms
step:795/2330 train_time:45727ms step_avg:57.52ms
step:796/2330 train_time:45786ms step_avg:57.52ms
step:797/2330 train_time:45843ms step_avg:57.52ms
step:798/2330 train_time:45902ms step_avg:57.52ms
step:799/2330 train_time:45959ms step_avg:57.52ms
step:800/2330 train_time:46018ms step_avg:57.52ms
step:801/2330 train_time:46076ms step_avg:57.52ms
step:802/2330 train_time:46135ms step_avg:57.52ms
step:803/2330 train_time:46192ms step_avg:57.52ms
step:804/2330 train_time:46252ms step_avg:57.53ms
step:805/2330 train_time:46309ms step_avg:57.53ms
step:806/2330 train_time:46372ms step_avg:57.53ms
step:807/2330 train_time:46429ms step_avg:57.53ms
step:808/2330 train_time:46490ms step_avg:57.54ms
step:809/2330 train_time:46547ms step_avg:57.54ms
step:810/2330 train_time:46607ms step_avg:57.54ms
step:811/2330 train_time:46664ms step_avg:57.54ms
step:812/2330 train_time:46724ms step_avg:57.54ms
step:813/2330 train_time:46781ms step_avg:57.54ms
step:814/2330 train_time:46840ms step_avg:57.54ms
step:815/2330 train_time:46897ms step_avg:57.54ms
step:816/2330 train_time:46955ms step_avg:57.54ms
step:817/2330 train_time:47012ms step_avg:57.54ms
step:818/2330 train_time:47073ms step_avg:57.55ms
step:819/2330 train_time:47129ms step_avg:57.54ms
step:820/2330 train_time:47189ms step_avg:57.55ms
step:821/2330 train_time:47246ms step_avg:57.55ms
step:822/2330 train_time:47307ms step_avg:57.55ms
step:823/2330 train_time:47363ms step_avg:57.55ms
step:824/2330 train_time:47424ms step_avg:57.55ms
step:825/2330 train_time:47482ms step_avg:57.55ms
step:826/2330 train_time:47542ms step_avg:57.56ms
step:827/2330 train_time:47599ms step_avg:57.56ms
step:828/2330 train_time:47659ms step_avg:57.56ms
step:829/2330 train_time:47715ms step_avg:57.56ms
step:830/2330 train_time:47776ms step_avg:57.56ms
step:831/2330 train_time:47833ms step_avg:57.56ms
step:832/2330 train_time:47891ms step_avg:57.56ms
step:833/2330 train_time:47948ms step_avg:57.56ms
step:834/2330 train_time:48008ms step_avg:57.56ms
step:835/2330 train_time:48065ms step_avg:57.56ms
step:836/2330 train_time:48126ms step_avg:57.57ms
step:837/2330 train_time:48183ms step_avg:57.57ms
step:838/2330 train_time:48243ms step_avg:57.57ms
step:839/2330 train_time:48301ms step_avg:57.57ms
step:840/2330 train_time:48361ms step_avg:57.57ms
step:841/2330 train_time:48418ms step_avg:57.57ms
step:842/2330 train_time:48478ms step_avg:57.58ms
step:843/2330 train_time:48535ms step_avg:57.57ms
step:844/2330 train_time:48595ms step_avg:57.58ms
step:845/2330 train_time:48652ms step_avg:57.58ms
step:846/2330 train_time:48712ms step_avg:57.58ms
step:847/2330 train_time:48769ms step_avg:57.58ms
step:848/2330 train_time:48829ms step_avg:57.58ms
step:849/2330 train_time:48886ms step_avg:57.58ms
step:850/2330 train_time:48946ms step_avg:57.58ms
step:851/2330 train_time:49003ms step_avg:57.58ms
step:852/2330 train_time:49063ms step_avg:57.59ms
step:853/2330 train_time:49119ms step_avg:57.58ms
step:854/2330 train_time:49180ms step_avg:57.59ms
step:855/2330 train_time:49237ms step_avg:57.59ms
step:856/2330 train_time:49296ms step_avg:57.59ms
step:857/2330 train_time:49354ms step_avg:57.59ms
step:858/2330 train_time:49413ms step_avg:57.59ms
step:859/2330 train_time:49470ms step_avg:57.59ms
step:860/2330 train_time:49531ms step_avg:57.59ms
step:861/2330 train_time:49588ms step_avg:57.59ms
step:862/2330 train_time:49648ms step_avg:57.60ms
step:863/2330 train_time:49706ms step_avg:57.60ms
step:864/2330 train_time:49766ms step_avg:57.60ms
step:865/2330 train_time:49823ms step_avg:57.60ms
step:866/2330 train_time:49883ms step_avg:57.60ms
step:867/2330 train_time:49940ms step_avg:57.60ms
step:868/2330 train_time:49999ms step_avg:57.60ms
step:869/2330 train_time:50055ms step_avg:57.60ms
step:870/2330 train_time:50115ms step_avg:57.60ms
step:871/2330 train_time:50172ms step_avg:57.60ms
step:872/2330 train_time:50232ms step_avg:57.61ms
step:873/2330 train_time:50289ms step_avg:57.61ms
step:874/2330 train_time:50350ms step_avg:57.61ms
step:875/2330 train_time:50407ms step_avg:57.61ms
step:876/2330 train_time:50468ms step_avg:57.61ms
step:877/2330 train_time:50525ms step_avg:57.61ms
step:878/2330 train_time:50586ms step_avg:57.61ms
step:879/2330 train_time:50642ms step_avg:57.61ms
step:880/2330 train_time:50702ms step_avg:57.62ms
step:881/2330 train_time:50759ms step_avg:57.62ms
step:882/2330 train_time:50819ms step_avg:57.62ms
step:883/2330 train_time:50877ms step_avg:57.62ms
step:884/2330 train_time:50936ms step_avg:57.62ms
step:885/2330 train_time:50992ms step_avg:57.62ms
step:886/2330 train_time:51052ms step_avg:57.62ms
step:887/2330 train_time:51109ms step_avg:57.62ms
step:888/2330 train_time:51169ms step_avg:57.62ms
step:889/2330 train_time:51226ms step_avg:57.62ms
step:890/2330 train_time:51286ms step_avg:57.62ms
step:891/2330 train_time:51343ms step_avg:57.62ms
step:892/2330 train_time:51402ms step_avg:57.63ms
step:893/2330 train_time:51459ms step_avg:57.62ms
step:894/2330 train_time:51519ms step_avg:57.63ms
step:895/2330 train_time:51576ms step_avg:57.63ms
step:896/2330 train_time:51636ms step_avg:57.63ms
step:897/2330 train_time:51692ms step_avg:57.63ms
step:898/2330 train_time:51754ms step_avg:57.63ms
step:899/2330 train_time:51811ms step_avg:57.63ms
step:900/2330 train_time:51871ms step_avg:57.63ms
step:901/2330 train_time:51928ms step_avg:57.63ms
step:902/2330 train_time:51988ms step_avg:57.64ms
step:903/2330 train_time:52045ms step_avg:57.64ms
step:904/2330 train_time:52104ms step_avg:57.64ms
step:905/2330 train_time:52161ms step_avg:57.64ms
step:906/2330 train_time:52220ms step_avg:57.64ms
step:907/2330 train_time:52277ms step_avg:57.64ms
step:908/2330 train_time:52336ms step_avg:57.64ms
step:909/2330 train_time:52393ms step_avg:57.64ms
step:910/2330 train_time:52453ms step_avg:57.64ms
step:911/2330 train_time:52510ms step_avg:57.64ms
step:912/2330 train_time:52570ms step_avg:57.64ms
step:913/2330 train_time:52628ms step_avg:57.64ms
step:914/2330 train_time:52688ms step_avg:57.65ms
step:915/2330 train_time:52746ms step_avg:57.65ms
step:916/2330 train_time:52805ms step_avg:57.65ms
step:917/2330 train_time:52863ms step_avg:57.65ms
step:918/2330 train_time:52922ms step_avg:57.65ms
step:919/2330 train_time:52979ms step_avg:57.65ms
step:920/2330 train_time:53038ms step_avg:57.65ms
step:921/2330 train_time:53095ms step_avg:57.65ms
step:922/2330 train_time:53154ms step_avg:57.65ms
step:923/2330 train_time:53211ms step_avg:57.65ms
step:924/2330 train_time:53271ms step_avg:57.65ms
step:925/2330 train_time:53328ms step_avg:57.65ms
step:926/2330 train_time:53388ms step_avg:57.65ms
step:927/2330 train_time:53445ms step_avg:57.65ms
step:928/2330 train_time:53505ms step_avg:57.66ms
step:929/2330 train_time:53562ms step_avg:57.66ms
step:930/2330 train_time:53620ms step_avg:57.66ms
step:931/2330 train_time:53677ms step_avg:57.66ms
step:932/2330 train_time:53737ms step_avg:57.66ms
step:933/2330 train_time:53793ms step_avg:57.66ms
step:934/2330 train_time:53855ms step_avg:57.66ms
step:935/2330 train_time:53912ms step_avg:57.66ms
step:936/2330 train_time:53972ms step_avg:57.66ms
step:937/2330 train_time:54029ms step_avg:57.66ms
step:938/2330 train_time:54088ms step_avg:57.66ms
step:939/2330 train_time:54144ms step_avg:57.66ms
step:940/2330 train_time:54205ms step_avg:57.66ms
step:941/2330 train_time:54262ms step_avg:57.66ms
step:942/2330 train_time:54322ms step_avg:57.67ms
step:943/2330 train_time:54378ms step_avg:57.66ms
step:944/2330 train_time:54438ms step_avg:57.67ms
step:945/2330 train_time:54495ms step_avg:57.67ms
step:946/2330 train_time:54555ms step_avg:57.67ms
step:947/2330 train_time:54612ms step_avg:57.67ms
step:948/2330 train_time:54672ms step_avg:57.67ms
step:949/2330 train_time:54729ms step_avg:57.67ms
step:950/2330 train_time:54788ms step_avg:57.67ms
step:951/2330 train_time:54846ms step_avg:57.67ms
step:952/2330 train_time:54905ms step_avg:57.67ms
step:953/2330 train_time:54962ms step_avg:57.67ms
step:954/2330 train_time:55023ms step_avg:57.68ms
step:955/2330 train_time:55079ms step_avg:57.67ms
step:956/2330 train_time:55139ms step_avg:57.68ms
step:957/2330 train_time:55196ms step_avg:57.68ms
step:958/2330 train_time:55255ms step_avg:57.68ms
step:959/2330 train_time:55312ms step_avg:57.68ms
step:960/2330 train_time:55373ms step_avg:57.68ms
step:961/2330 train_time:55430ms step_avg:57.68ms
step:962/2330 train_time:55489ms step_avg:57.68ms
step:963/2330 train_time:55547ms step_avg:57.68ms
step:964/2330 train_time:55606ms step_avg:57.68ms
step:965/2330 train_time:55663ms step_avg:57.68ms
step:966/2330 train_time:55723ms step_avg:57.68ms
step:967/2330 train_time:55780ms step_avg:57.68ms
step:968/2330 train_time:55840ms step_avg:57.69ms
step:969/2330 train_time:55897ms step_avg:57.69ms
step:970/2330 train_time:55957ms step_avg:57.69ms
step:971/2330 train_time:56014ms step_avg:57.69ms
step:972/2330 train_time:56074ms step_avg:57.69ms
step:973/2330 train_time:56131ms step_avg:57.69ms
step:974/2330 train_time:56190ms step_avg:57.69ms
step:975/2330 train_time:56248ms step_avg:57.69ms
step:976/2330 train_time:56307ms step_avg:57.69ms
step:977/2330 train_time:56364ms step_avg:57.69ms
step:978/2330 train_time:56424ms step_avg:57.69ms
step:979/2330 train_time:56481ms step_avg:57.69ms
step:980/2330 train_time:56540ms step_avg:57.69ms
step:981/2330 train_time:56597ms step_avg:57.69ms
step:982/2330 train_time:56656ms step_avg:57.69ms
step:983/2330 train_time:56713ms step_avg:57.69ms
step:984/2330 train_time:56773ms step_avg:57.70ms
step:985/2330 train_time:56830ms step_avg:57.70ms
step:986/2330 train_time:56890ms step_avg:57.70ms
step:987/2330 train_time:56947ms step_avg:57.70ms
step:988/2330 train_time:57008ms step_avg:57.70ms
step:989/2330 train_time:57065ms step_avg:57.70ms
step:990/2330 train_time:57125ms step_avg:57.70ms
step:991/2330 train_time:57182ms step_avg:57.70ms
step:992/2330 train_time:57242ms step_avg:57.70ms
step:993/2330 train_time:57299ms step_avg:57.70ms
step:994/2330 train_time:57358ms step_avg:57.70ms
step:995/2330 train_time:57415ms step_avg:57.70ms
step:996/2330 train_time:57474ms step_avg:57.70ms
step:997/2330 train_time:57531ms step_avg:57.70ms
step:998/2330 train_time:57591ms step_avg:57.71ms
step:999/2330 train_time:57648ms step_avg:57.71ms
step:1000/2330 train_time:57708ms step_avg:57.71ms
step:1000/2330 val_loss:4.0640 train_time:57788ms step_avg:57.79ms
step:1001/2330 train_time:57807ms step_avg:57.75ms
step:1002/2330 train_time:57827ms step_avg:57.71ms
step:1003/2330 train_time:57884ms step_avg:57.71ms
step:1004/2330 train_time:57948ms step_avg:57.72ms
step:1005/2330 train_time:58006ms step_avg:57.72ms
step:1006/2330 train_time:58068ms step_avg:57.72ms
step:1007/2330 train_time:58124ms step_avg:57.72ms
step:1008/2330 train_time:58184ms step_avg:57.72ms
step:1009/2330 train_time:58240ms step_avg:57.72ms
step:1010/2330 train_time:58301ms step_avg:57.72ms
step:1011/2330 train_time:58357ms step_avg:57.72ms
step:1012/2330 train_time:58416ms step_avg:57.72ms
step:1013/2330 train_time:58472ms step_avg:57.72ms
step:1014/2330 train_time:58532ms step_avg:57.72ms
step:1015/2330 train_time:58588ms step_avg:57.72ms
step:1016/2330 train_time:58646ms step_avg:57.72ms
step:1017/2330 train_time:58703ms step_avg:57.72ms
step:1018/2330 train_time:58764ms step_avg:57.73ms
step:1019/2330 train_time:58822ms step_avg:57.73ms
step:1020/2330 train_time:58884ms step_avg:57.73ms
step:1021/2330 train_time:58942ms step_avg:57.73ms
step:1022/2330 train_time:59004ms step_avg:57.73ms
step:1023/2330 train_time:59062ms step_avg:57.73ms
step:1024/2330 train_time:59123ms step_avg:57.74ms
step:1025/2330 train_time:59179ms step_avg:57.74ms
step:1026/2330 train_time:59239ms step_avg:57.74ms
step:1027/2330 train_time:59296ms step_avg:57.74ms
step:1028/2330 train_time:59355ms step_avg:57.74ms
step:1029/2330 train_time:59412ms step_avg:57.74ms
step:1030/2330 train_time:59471ms step_avg:57.74ms
step:1031/2330 train_time:59528ms step_avg:57.74ms
step:1032/2330 train_time:59587ms step_avg:57.74ms
step:1033/2330 train_time:59643ms step_avg:57.74ms
step:1034/2330 train_time:59704ms step_avg:57.74ms
step:1035/2330 train_time:59760ms step_avg:57.74ms
step:1036/2330 train_time:59822ms step_avg:57.74ms
step:1037/2330 train_time:59880ms step_avg:57.74ms
step:1038/2330 train_time:59940ms step_avg:57.75ms
step:1039/2330 train_time:59998ms step_avg:57.75ms
step:1040/2330 train_time:60060ms step_avg:57.75ms
step:1041/2330 train_time:60118ms step_avg:57.75ms
step:1042/2330 train_time:60178ms step_avg:57.75ms
step:1043/2330 train_time:60235ms step_avg:57.75ms
step:1044/2330 train_time:60294ms step_avg:57.75ms
step:1045/2330 train_time:60350ms step_avg:57.75ms
step:1046/2330 train_time:60411ms step_avg:57.75ms
step:1047/2330 train_time:60468ms step_avg:57.75ms
step:1048/2330 train_time:60527ms step_avg:57.75ms
step:1049/2330 train_time:60584ms step_avg:57.75ms
step:1050/2330 train_time:60643ms step_avg:57.76ms
step:1051/2330 train_time:60700ms step_avg:57.75ms
step:1052/2330 train_time:60760ms step_avg:57.76ms
step:1053/2330 train_time:60817ms step_avg:57.76ms
step:1054/2330 train_time:60878ms step_avg:57.76ms
step:1055/2330 train_time:60935ms step_avg:57.76ms
step:1056/2330 train_time:60997ms step_avg:57.76ms
step:1057/2330 train_time:61054ms step_avg:57.76ms
step:1058/2330 train_time:61115ms step_avg:57.76ms
step:1059/2330 train_time:61173ms step_avg:57.77ms
step:1060/2330 train_time:61233ms step_avg:57.77ms
step:1061/2330 train_time:61289ms step_avg:57.77ms
step:1062/2330 train_time:61349ms step_avg:57.77ms
step:1063/2330 train_time:61406ms step_avg:57.77ms
step:1064/2330 train_time:61464ms step_avg:57.77ms
step:1065/2330 train_time:61521ms step_avg:57.77ms
step:1066/2330 train_time:61581ms step_avg:57.77ms
step:1067/2330 train_time:61638ms step_avg:57.77ms
step:1068/2330 train_time:61698ms step_avg:57.77ms
step:1069/2330 train_time:61755ms step_avg:57.77ms
step:1070/2330 train_time:61815ms step_avg:57.77ms
step:1071/2330 train_time:61873ms step_avg:57.77ms
step:1072/2330 train_time:61934ms step_avg:57.77ms
step:1073/2330 train_time:61991ms step_avg:57.77ms
step:1074/2330 train_time:62050ms step_avg:57.77ms
step:1075/2330 train_time:62107ms step_avg:57.77ms
step:1076/2330 train_time:62167ms step_avg:57.78ms
step:1077/2330 train_time:62223ms step_avg:57.77ms
step:1078/2330 train_time:62284ms step_avg:57.78ms
step:1079/2330 train_time:62340ms step_avg:57.78ms
step:1080/2330 train_time:62401ms step_avg:57.78ms
step:1081/2330 train_time:62457ms step_avg:57.78ms
step:1082/2330 train_time:62517ms step_avg:57.78ms
step:1083/2330 train_time:62575ms step_avg:57.78ms
step:1084/2330 train_time:62634ms step_avg:57.78ms
step:1085/2330 train_time:62691ms step_avg:57.78ms
step:1086/2330 train_time:62752ms step_avg:57.78ms
step:1087/2330 train_time:62808ms step_avg:57.78ms
step:1088/2330 train_time:62869ms step_avg:57.78ms
step:1089/2330 train_time:62927ms step_avg:57.78ms
step:1090/2330 train_time:62986ms step_avg:57.79ms
step:1091/2330 train_time:63043ms step_avg:57.78ms
step:1092/2330 train_time:63104ms step_avg:57.79ms
step:1093/2330 train_time:63161ms step_avg:57.79ms
step:1094/2330 train_time:63222ms step_avg:57.79ms
step:1095/2330 train_time:63278ms step_avg:57.79ms
step:1096/2330 train_time:63338ms step_avg:57.79ms
step:1097/2330 train_time:63396ms step_avg:57.79ms
step:1098/2330 train_time:63455ms step_avg:57.79ms
step:1099/2330 train_time:63512ms step_avg:57.79ms
step:1100/2330 train_time:63572ms step_avg:57.79ms
step:1101/2330 train_time:63629ms step_avg:57.79ms
step:1102/2330 train_time:63688ms step_avg:57.79ms
step:1103/2330 train_time:63745ms step_avg:57.79ms
step:1104/2330 train_time:63807ms step_avg:57.80ms
step:1105/2330 train_time:63864ms step_avg:57.80ms
step:1106/2330 train_time:63924ms step_avg:57.80ms
step:1107/2330 train_time:63980ms step_avg:57.80ms
step:1108/2330 train_time:64041ms step_avg:57.80ms
step:1109/2330 train_time:64098ms step_avg:57.80ms
step:1110/2330 train_time:64159ms step_avg:57.80ms
step:1111/2330 train_time:64216ms step_avg:57.80ms
step:1112/2330 train_time:64276ms step_avg:57.80ms
step:1113/2330 train_time:64333ms step_avg:57.80ms
step:1114/2330 train_time:64392ms step_avg:57.80ms
step:1115/2330 train_time:64449ms step_avg:57.80ms
step:1116/2330 train_time:64509ms step_avg:57.80ms
step:1117/2330 train_time:64566ms step_avg:57.80ms
step:1118/2330 train_time:64626ms step_avg:57.81ms
step:1119/2330 train_time:64683ms step_avg:57.80ms
step:1120/2330 train_time:64743ms step_avg:57.81ms
step:1121/2330 train_time:64800ms step_avg:57.81ms
step:1122/2330 train_time:64861ms step_avg:57.81ms
step:1123/2330 train_time:64918ms step_avg:57.81ms
step:1124/2330 train_time:64979ms step_avg:57.81ms
step:1125/2330 train_time:65037ms step_avg:57.81ms
step:1126/2330 train_time:65096ms step_avg:57.81ms
step:1127/2330 train_time:65154ms step_avg:57.81ms
step:1128/2330 train_time:65214ms step_avg:57.81ms
step:1129/2330 train_time:65270ms step_avg:57.81ms
step:1130/2330 train_time:65330ms step_avg:57.81ms
step:1131/2330 train_time:65387ms step_avg:57.81ms
step:1132/2330 train_time:65447ms step_avg:57.82ms
step:1133/2330 train_time:65505ms step_avg:57.82ms
step:1134/2330 train_time:65564ms step_avg:57.82ms
step:1135/2330 train_time:65621ms step_avg:57.82ms
step:1136/2330 train_time:65681ms step_avg:57.82ms
step:1137/2330 train_time:65738ms step_avg:57.82ms
step:1138/2330 train_time:65798ms step_avg:57.82ms
step:1139/2330 train_time:65854ms step_avg:57.82ms
step:1140/2330 train_time:65915ms step_avg:57.82ms
step:1141/2330 train_time:65972ms step_avg:57.82ms
step:1142/2330 train_time:66033ms step_avg:57.82ms
step:1143/2330 train_time:66089ms step_avg:57.82ms
step:1144/2330 train_time:66149ms step_avg:57.82ms
step:1145/2330 train_time:66206ms step_avg:57.82ms
step:1146/2330 train_time:66266ms step_avg:57.82ms
step:1147/2330 train_time:66323ms step_avg:57.82ms
step:1148/2330 train_time:66383ms step_avg:57.82ms
step:1149/2330 train_time:66440ms step_avg:57.82ms
step:1150/2330 train_time:66501ms step_avg:57.83ms
step:1151/2330 train_time:66557ms step_avg:57.83ms
step:1152/2330 train_time:66618ms step_avg:57.83ms
step:1153/2330 train_time:66675ms step_avg:57.83ms
step:1154/2330 train_time:66735ms step_avg:57.83ms
step:1155/2330 train_time:66792ms step_avg:57.83ms
step:1156/2330 train_time:66852ms step_avg:57.83ms
step:1157/2330 train_time:66909ms step_avg:57.83ms
step:1158/2330 train_time:66969ms step_avg:57.83ms
step:1159/2330 train_time:67026ms step_avg:57.83ms
step:1160/2330 train_time:67086ms step_avg:57.83ms
step:1161/2330 train_time:67142ms step_avg:57.83ms
step:1162/2330 train_time:67202ms step_avg:57.83ms
step:1163/2330 train_time:67259ms step_avg:57.83ms
step:1164/2330 train_time:67320ms step_avg:57.84ms
step:1165/2330 train_time:67378ms step_avg:57.84ms
step:1166/2330 train_time:67437ms step_avg:57.84ms
step:1167/2330 train_time:67494ms step_avg:57.84ms
step:1168/2330 train_time:67555ms step_avg:57.84ms
step:1169/2330 train_time:67612ms step_avg:57.84ms
step:1170/2330 train_time:67672ms step_avg:57.84ms
step:1171/2330 train_time:67729ms step_avg:57.84ms
step:1172/2330 train_time:67789ms step_avg:57.84ms
step:1173/2330 train_time:67846ms step_avg:57.84ms
step:1174/2330 train_time:67907ms step_avg:57.84ms
step:1175/2330 train_time:67964ms step_avg:57.84ms
step:1176/2330 train_time:68024ms step_avg:57.84ms
step:1177/2330 train_time:68081ms step_avg:57.84ms
step:1178/2330 train_time:68141ms step_avg:57.84ms
step:1179/2330 train_time:68198ms step_avg:57.84ms
step:1180/2330 train_time:68258ms step_avg:57.85ms
step:1181/2330 train_time:68316ms step_avg:57.85ms
step:1182/2330 train_time:68375ms step_avg:57.85ms
step:1183/2330 train_time:68432ms step_avg:57.85ms
step:1184/2330 train_time:68492ms step_avg:57.85ms
step:1185/2330 train_time:68549ms step_avg:57.85ms
step:1186/2330 train_time:68609ms step_avg:57.85ms
step:1187/2330 train_time:68666ms step_avg:57.85ms
step:1188/2330 train_time:68726ms step_avg:57.85ms
step:1189/2330 train_time:68783ms step_avg:57.85ms
step:1190/2330 train_time:68843ms step_avg:57.85ms
step:1191/2330 train_time:68900ms step_avg:57.85ms
step:1192/2330 train_time:68961ms step_avg:57.85ms
step:1193/2330 train_time:69019ms step_avg:57.85ms
step:1194/2330 train_time:69078ms step_avg:57.85ms
step:1195/2330 train_time:69135ms step_avg:57.85ms
step:1196/2330 train_time:69194ms step_avg:57.85ms
step:1197/2330 train_time:69251ms step_avg:57.85ms
step:1198/2330 train_time:69311ms step_avg:57.86ms
step:1199/2330 train_time:69368ms step_avg:57.86ms
step:1200/2330 train_time:69427ms step_avg:57.86ms
step:1201/2330 train_time:69484ms step_avg:57.86ms
step:1202/2330 train_time:69544ms step_avg:57.86ms
step:1203/2330 train_time:69602ms step_avg:57.86ms
step:1204/2330 train_time:69662ms step_avg:57.86ms
step:1205/2330 train_time:69719ms step_avg:57.86ms
step:1206/2330 train_time:69779ms step_avg:57.86ms
step:1207/2330 train_time:69836ms step_avg:57.86ms
step:1208/2330 train_time:69897ms step_avg:57.86ms
step:1209/2330 train_time:69953ms step_avg:57.86ms
step:1210/2330 train_time:70014ms step_avg:57.86ms
step:1211/2330 train_time:70071ms step_avg:57.86ms
step:1212/2330 train_time:70131ms step_avg:57.86ms
step:1213/2330 train_time:70188ms step_avg:57.86ms
step:1214/2330 train_time:70248ms step_avg:57.87ms
step:1215/2330 train_time:70305ms step_avg:57.86ms
step:1216/2330 train_time:70365ms step_avg:57.87ms
step:1217/2330 train_time:70422ms step_avg:57.87ms
step:1218/2330 train_time:70481ms step_avg:57.87ms
step:1219/2330 train_time:70539ms step_avg:57.87ms
step:1220/2330 train_time:70599ms step_avg:57.87ms
step:1221/2330 train_time:70657ms step_avg:57.87ms
step:1222/2330 train_time:70718ms step_avg:57.87ms
step:1223/2330 train_time:70774ms step_avg:57.87ms
step:1224/2330 train_time:70835ms step_avg:57.87ms
step:1225/2330 train_time:70892ms step_avg:57.87ms
step:1226/2330 train_time:70951ms step_avg:57.87ms
step:1227/2330 train_time:71007ms step_avg:57.87ms
step:1228/2330 train_time:71069ms step_avg:57.87ms
step:1229/2330 train_time:71126ms step_avg:57.87ms
step:1230/2330 train_time:71186ms step_avg:57.87ms
step:1231/2330 train_time:71242ms step_avg:57.87ms
step:1232/2330 train_time:71304ms step_avg:57.88ms
step:1233/2330 train_time:71360ms step_avg:57.88ms
step:1234/2330 train_time:71421ms step_avg:57.88ms
step:1235/2330 train_time:71477ms step_avg:57.88ms
step:1236/2330 train_time:71537ms step_avg:57.88ms
step:1237/2330 train_time:71595ms step_avg:57.88ms
step:1238/2330 train_time:71655ms step_avg:57.88ms
step:1239/2330 train_time:71712ms step_avg:57.88ms
step:1240/2330 train_time:71772ms step_avg:57.88ms
step:1241/2330 train_time:71828ms step_avg:57.88ms
step:1242/2330 train_time:71889ms step_avg:57.88ms
step:1243/2330 train_time:71945ms step_avg:57.88ms
step:1244/2330 train_time:72005ms step_avg:57.88ms
step:1245/2330 train_time:72061ms step_avg:57.88ms
step:1246/2330 train_time:72123ms step_avg:57.88ms
step:1247/2330 train_time:72180ms step_avg:57.88ms
step:1248/2330 train_time:72241ms step_avg:57.89ms
step:1249/2330 train_time:72297ms step_avg:57.88ms
step:1250/2330 train_time:72358ms step_avg:57.89ms
step:1250/2330 val_loss:3.9878 train_time:72439ms step_avg:57.95ms
step:1251/2330 train_time:72459ms step_avg:57.92ms
step:1252/2330 train_time:72479ms step_avg:57.89ms
step:1253/2330 train_time:72538ms step_avg:57.89ms
step:1254/2330 train_time:72603ms step_avg:57.90ms
step:1255/2330 train_time:72660ms step_avg:57.90ms
step:1256/2330 train_time:72722ms step_avg:57.90ms
step:1257/2330 train_time:72779ms step_avg:57.90ms
step:1258/2330 train_time:72839ms step_avg:57.90ms
step:1259/2330 train_time:72895ms step_avg:57.90ms
step:1260/2330 train_time:72956ms step_avg:57.90ms
step:1261/2330 train_time:73012ms step_avg:57.90ms
step:1262/2330 train_time:73071ms step_avg:57.90ms
step:1263/2330 train_time:73127ms step_avg:57.90ms
step:1264/2330 train_time:73188ms step_avg:57.90ms
step:1265/2330 train_time:73244ms step_avg:57.90ms
step:1266/2330 train_time:73304ms step_avg:57.90ms
step:1267/2330 train_time:73361ms step_avg:57.90ms
step:1268/2330 train_time:73421ms step_avg:57.90ms
step:1269/2330 train_time:73479ms step_avg:57.90ms
step:1270/2330 train_time:73542ms step_avg:57.91ms
step:1271/2330 train_time:73601ms step_avg:57.91ms
step:1272/2330 train_time:73663ms step_avg:57.91ms
step:1273/2330 train_time:73720ms step_avg:57.91ms
step:1274/2330 train_time:73781ms step_avg:57.91ms
step:1275/2330 train_time:73838ms step_avg:57.91ms
step:1276/2330 train_time:73899ms step_avg:57.91ms
step:1277/2330 train_time:73955ms step_avg:57.91ms
step:1278/2330 train_time:74015ms step_avg:57.91ms
step:1279/2330 train_time:74071ms step_avg:57.91ms
step:1280/2330 train_time:74130ms step_avg:57.91ms
step:1281/2330 train_time:74187ms step_avg:57.91ms
step:1282/2330 train_time:74247ms step_avg:57.91ms
step:1283/2330 train_time:74303ms step_avg:57.91ms
step:1284/2330 train_time:74363ms step_avg:57.92ms
step:1285/2330 train_time:74420ms step_avg:57.91ms
step:1286/2330 train_time:74482ms step_avg:57.92ms
step:1287/2330 train_time:74539ms step_avg:57.92ms
step:1288/2330 train_time:74601ms step_avg:57.92ms
step:1289/2330 train_time:74657ms step_avg:57.92ms
step:1290/2330 train_time:74719ms step_avg:57.92ms
step:1291/2330 train_time:74776ms step_avg:57.92ms
step:1292/2330 train_time:74837ms step_avg:57.92ms
step:1293/2330 train_time:74893ms step_avg:57.92ms
step:1294/2330 train_time:74954ms step_avg:57.92ms
step:1295/2330 train_time:75010ms step_avg:57.92ms
step:1296/2330 train_time:75070ms step_avg:57.92ms
step:1297/2330 train_time:75125ms step_avg:57.92ms
step:1298/2330 train_time:75186ms step_avg:57.92ms
step:1299/2330 train_time:75242ms step_avg:57.92ms
step:1300/2330 train_time:75302ms step_avg:57.92ms
step:1301/2330 train_time:75359ms step_avg:57.92ms
step:1302/2330 train_time:75419ms step_avg:57.93ms
step:1303/2330 train_time:75476ms step_avg:57.92ms
step:1304/2330 train_time:75537ms step_avg:57.93ms
step:1305/2330 train_time:75594ms step_avg:57.93ms
step:1306/2330 train_time:75655ms step_avg:57.93ms
step:1307/2330 train_time:75712ms step_avg:57.93ms
step:1308/2330 train_time:75774ms step_avg:57.93ms
step:1309/2330 train_time:75831ms step_avg:57.93ms
step:1310/2330 train_time:75891ms step_avg:57.93ms
step:1311/2330 train_time:75948ms step_avg:57.93ms
step:1312/2330 train_time:76007ms step_avg:57.93ms
step:1313/2330 train_time:76064ms step_avg:57.93ms
step:1314/2330 train_time:76123ms step_avg:57.93ms
step:1315/2330 train_time:76179ms step_avg:57.93ms
step:1316/2330 train_time:76239ms step_avg:57.93ms
step:1317/2330 train_time:76295ms step_avg:57.93ms
step:1318/2330 train_time:76356ms step_avg:57.93ms
step:1319/2330 train_time:76413ms step_avg:57.93ms
step:1320/2330 train_time:76473ms step_avg:57.93ms
step:1321/2330 train_time:76531ms step_avg:57.93ms
step:1322/2330 train_time:76591ms step_avg:57.94ms
step:1323/2330 train_time:76648ms step_avg:57.93ms
step:1324/2330 train_time:76709ms step_avg:57.94ms
step:1325/2330 train_time:76766ms step_avg:57.94ms
step:1326/2330 train_time:76828ms step_avg:57.94ms
step:1327/2330 train_time:76884ms step_avg:57.94ms
step:1328/2330 train_time:76946ms step_avg:57.94ms
step:1329/2330 train_time:77004ms step_avg:57.94ms
step:1330/2330 train_time:77062ms step_avg:57.94ms
step:1331/2330 train_time:77120ms step_avg:57.94ms
step:1332/2330 train_time:77179ms step_avg:57.94ms
step:1333/2330 train_time:77235ms step_avg:57.94ms
step:1334/2330 train_time:77296ms step_avg:57.94ms
step:1335/2330 train_time:77353ms step_avg:57.94ms
step:1336/2330 train_time:77413ms step_avg:57.94ms
step:1337/2330 train_time:77469ms step_avg:57.94ms
step:1338/2330 train_time:77529ms step_avg:57.94ms
step:1339/2330 train_time:77587ms step_avg:57.94ms
step:1340/2330 train_time:77648ms step_avg:57.95ms
step:1341/2330 train_time:77704ms step_avg:57.95ms
step:1342/2330 train_time:77765ms step_avg:57.95ms
step:1343/2330 train_time:77822ms step_avg:57.95ms
step:1344/2330 train_time:77882ms step_avg:57.95ms
step:1345/2330 train_time:77939ms step_avg:57.95ms
step:1346/2330 train_time:78000ms step_avg:57.95ms
step:1347/2330 train_time:78056ms step_avg:57.95ms
step:1348/2330 train_time:78116ms step_avg:57.95ms
step:1349/2330 train_time:78172ms step_avg:57.95ms
step:1350/2330 train_time:78233ms step_avg:57.95ms
step:1351/2330 train_time:78290ms step_avg:57.95ms
step:1352/2330 train_time:78350ms step_avg:57.95ms
step:1353/2330 train_time:78407ms step_avg:57.95ms
step:1354/2330 train_time:78466ms step_avg:57.95ms
step:1355/2330 train_time:78524ms step_avg:57.95ms
step:1356/2330 train_time:78584ms step_avg:57.95ms
step:1357/2330 train_time:78641ms step_avg:57.95ms
step:1358/2330 train_time:78702ms step_avg:57.95ms
step:1359/2330 train_time:78758ms step_avg:57.95ms
step:1360/2330 train_time:78820ms step_avg:57.96ms
step:1361/2330 train_time:78876ms step_avg:57.95ms
step:1362/2330 train_time:78938ms step_avg:57.96ms
step:1363/2330 train_time:78994ms step_avg:57.96ms
step:1364/2330 train_time:79054ms step_avg:57.96ms
step:1365/2330 train_time:79111ms step_avg:57.96ms
step:1366/2330 train_time:79171ms step_avg:57.96ms
step:1367/2330 train_time:79227ms step_avg:57.96ms
step:1368/2330 train_time:79287ms step_avg:57.96ms
step:1369/2330 train_time:79344ms step_avg:57.96ms
step:1370/2330 train_time:79405ms step_avg:57.96ms
step:1371/2330 train_time:79462ms step_avg:57.96ms
step:1372/2330 train_time:79521ms step_avg:57.96ms
step:1373/2330 train_time:79578ms step_avg:57.96ms
step:1374/2330 train_time:79638ms step_avg:57.96ms
step:1375/2330 train_time:79694ms step_avg:57.96ms
step:1376/2330 train_time:79755ms step_avg:57.96ms
step:1377/2330 train_time:79811ms step_avg:57.96ms
step:1378/2330 train_time:79873ms step_avg:57.96ms
step:1379/2330 train_time:79930ms step_avg:57.96ms
step:1380/2330 train_time:79991ms step_avg:57.96ms
step:1381/2330 train_time:80047ms step_avg:57.96ms
step:1382/2330 train_time:80108ms step_avg:57.97ms
step:1383/2330 train_time:80165ms step_avg:57.96ms
step:1384/2330 train_time:80225ms step_avg:57.97ms
step:1385/2330 train_time:80281ms step_avg:57.96ms
step:1386/2330 train_time:80343ms step_avg:57.97ms
step:1387/2330 train_time:80400ms step_avg:57.97ms
step:1388/2330 train_time:80461ms step_avg:57.97ms
step:1389/2330 train_time:80517ms step_avg:57.97ms
step:1390/2330 train_time:80577ms step_avg:57.97ms
step:1391/2330 train_time:80634ms step_avg:57.97ms
step:1392/2330 train_time:80694ms step_avg:57.97ms
step:1393/2330 train_time:80751ms step_avg:57.97ms
step:1394/2330 train_time:80811ms step_avg:57.97ms
step:1395/2330 train_time:80867ms step_avg:57.97ms
step:1396/2330 train_time:80928ms step_avg:57.97ms
step:1397/2330 train_time:80985ms step_avg:57.97ms
step:1398/2330 train_time:81046ms step_avg:57.97ms
step:1399/2330 train_time:81103ms step_avg:57.97ms
step:1400/2330 train_time:81164ms step_avg:57.97ms
step:1401/2330 train_time:81221ms step_avg:57.97ms
step:1402/2330 train_time:81281ms step_avg:57.97ms
step:1403/2330 train_time:81337ms step_avg:57.97ms
step:1404/2330 train_time:81398ms step_avg:57.98ms
step:1405/2330 train_time:81455ms step_avg:57.98ms
step:1406/2330 train_time:81515ms step_avg:57.98ms
step:1407/2330 train_time:81571ms step_avg:57.98ms
step:1408/2330 train_time:81632ms step_avg:57.98ms
step:1409/2330 train_time:81688ms step_avg:57.98ms
step:1410/2330 train_time:81748ms step_avg:57.98ms
step:1411/2330 train_time:81805ms step_avg:57.98ms
step:1412/2330 train_time:81866ms step_avg:57.98ms
step:1413/2330 train_time:81922ms step_avg:57.98ms
step:1414/2330 train_time:81983ms step_avg:57.98ms
step:1415/2330 train_time:82040ms step_avg:57.98ms
step:1416/2330 train_time:82102ms step_avg:57.98ms
step:1417/2330 train_time:82159ms step_avg:57.98ms
step:1418/2330 train_time:82219ms step_avg:57.98ms
step:1419/2330 train_time:82276ms step_avg:57.98ms
step:1420/2330 train_time:82336ms step_avg:57.98ms
step:1421/2330 train_time:82393ms step_avg:57.98ms
step:1422/2330 train_time:82453ms step_avg:57.98ms
step:1423/2330 train_time:82510ms step_avg:57.98ms
step:1424/2330 train_time:82570ms step_avg:57.98ms
step:1425/2330 train_time:82626ms step_avg:57.98ms
step:1426/2330 train_time:82687ms step_avg:57.99ms
step:1427/2330 train_time:82744ms step_avg:57.98ms
step:1428/2330 train_time:82805ms step_avg:57.99ms
step:1429/2330 train_time:82862ms step_avg:57.99ms
step:1430/2330 train_time:82922ms step_avg:57.99ms
step:1431/2330 train_time:82979ms step_avg:57.99ms
step:1432/2330 train_time:83039ms step_avg:57.99ms
step:1433/2330 train_time:83096ms step_avg:57.99ms
step:1434/2330 train_time:83157ms step_avg:57.99ms
step:1435/2330 train_time:83213ms step_avg:57.99ms
step:1436/2330 train_time:83273ms step_avg:57.99ms
step:1437/2330 train_time:83330ms step_avg:57.99ms
step:1438/2330 train_time:83391ms step_avg:57.99ms
step:1439/2330 train_time:83447ms step_avg:57.99ms
step:1440/2330 train_time:83508ms step_avg:57.99ms
step:1441/2330 train_time:83565ms step_avg:57.99ms
step:1442/2330 train_time:83625ms step_avg:57.99ms
step:1443/2330 train_time:83682ms step_avg:57.99ms
step:1444/2330 train_time:83743ms step_avg:57.99ms
step:1445/2330 train_time:83799ms step_avg:57.99ms
step:1446/2330 train_time:83860ms step_avg:57.99ms
step:1447/2330 train_time:83917ms step_avg:57.99ms
step:1448/2330 train_time:83977ms step_avg:58.00ms
step:1449/2330 train_time:84033ms step_avg:57.99ms
step:1450/2330 train_time:84095ms step_avg:58.00ms
step:1451/2330 train_time:84152ms step_avg:58.00ms
step:1452/2330 train_time:84211ms step_avg:58.00ms
step:1453/2330 train_time:84268ms step_avg:58.00ms
step:1454/2330 train_time:84329ms step_avg:58.00ms
step:1455/2330 train_time:84386ms step_avg:58.00ms
step:1456/2330 train_time:84447ms step_avg:58.00ms
step:1457/2330 train_time:84504ms step_avg:58.00ms
step:1458/2330 train_time:84564ms step_avg:58.00ms
step:1459/2330 train_time:84620ms step_avg:58.00ms
step:1460/2330 train_time:84680ms step_avg:58.00ms
step:1461/2330 train_time:84738ms step_avg:58.00ms
step:1462/2330 train_time:84797ms step_avg:58.00ms
step:1463/2330 train_time:84854ms step_avg:58.00ms
step:1464/2330 train_time:84914ms step_avg:58.00ms
step:1465/2330 train_time:84970ms step_avg:58.00ms
step:1466/2330 train_time:85032ms step_avg:58.00ms
step:1467/2330 train_time:85088ms step_avg:58.00ms
step:1468/2330 train_time:85149ms step_avg:58.00ms
step:1469/2330 train_time:85205ms step_avg:58.00ms
step:1470/2330 train_time:85266ms step_avg:58.00ms
step:1471/2330 train_time:85322ms step_avg:58.00ms
step:1472/2330 train_time:85383ms step_avg:58.00ms
step:1473/2330 train_time:85440ms step_avg:58.00ms
step:1474/2330 train_time:85501ms step_avg:58.01ms
step:1475/2330 train_time:85557ms step_avg:58.00ms
step:1476/2330 train_time:85617ms step_avg:58.01ms
step:1477/2330 train_time:85674ms step_avg:58.01ms
step:1478/2330 train_time:85734ms step_avg:58.01ms
step:1479/2330 train_time:85791ms step_avg:58.01ms
step:1480/2330 train_time:85851ms step_avg:58.01ms
step:1481/2330 train_time:85908ms step_avg:58.01ms
step:1482/2330 train_time:85968ms step_avg:58.01ms
step:1483/2330 train_time:86025ms step_avg:58.01ms
step:1484/2330 train_time:86085ms step_avg:58.01ms
step:1485/2330 train_time:86142ms step_avg:58.01ms
step:1486/2330 train_time:86203ms step_avg:58.01ms
step:1487/2330 train_time:86260ms step_avg:58.01ms
step:1488/2330 train_time:86320ms step_avg:58.01ms
step:1489/2330 train_time:86376ms step_avg:58.01ms
step:1490/2330 train_time:86436ms step_avg:58.01ms
step:1491/2330 train_time:86493ms step_avg:58.01ms
step:1492/2330 train_time:86553ms step_avg:58.01ms
step:1493/2330 train_time:86610ms step_avg:58.01ms
step:1494/2330 train_time:86670ms step_avg:58.01ms
step:1495/2330 train_time:86726ms step_avg:58.01ms
step:1496/2330 train_time:86788ms step_avg:58.01ms
step:1497/2330 train_time:86845ms step_avg:58.01ms
step:1498/2330 train_time:86904ms step_avg:58.01ms
step:1499/2330 train_time:86961ms step_avg:58.01ms
step:1500/2330 train_time:87022ms step_avg:58.01ms
step:1500/2330 val_loss:3.9078 train_time:87103ms step_avg:58.07ms
step:1501/2330 train_time:87122ms step_avg:58.04ms
step:1502/2330 train_time:87142ms step_avg:58.02ms
step:1503/2330 train_time:87199ms step_avg:58.02ms
step:1504/2330 train_time:87268ms step_avg:58.02ms
step:1505/2330 train_time:87325ms step_avg:58.02ms
step:1506/2330 train_time:87386ms step_avg:58.03ms
step:1507/2330 train_time:87442ms step_avg:58.02ms
step:1508/2330 train_time:87503ms step_avg:58.03ms
step:1509/2330 train_time:87559ms step_avg:58.02ms
step:1510/2330 train_time:87618ms step_avg:58.03ms
step:1511/2330 train_time:87674ms step_avg:58.02ms
step:1512/2330 train_time:87735ms step_avg:58.03ms
step:1513/2330 train_time:87791ms step_avg:58.02ms
step:1514/2330 train_time:87850ms step_avg:58.03ms
step:1515/2330 train_time:87906ms step_avg:58.02ms
step:1516/2330 train_time:87965ms step_avg:58.02ms
step:1517/2330 train_time:88021ms step_avg:58.02ms
step:1518/2330 train_time:88083ms step_avg:58.03ms
step:1519/2330 train_time:88140ms step_avg:58.02ms
step:1520/2330 train_time:88203ms step_avg:58.03ms
step:1521/2330 train_time:88259ms step_avg:58.03ms
step:1522/2330 train_time:88322ms step_avg:58.03ms
step:1523/2330 train_time:88379ms step_avg:58.03ms
step:1524/2330 train_time:88440ms step_avg:58.03ms
step:1525/2330 train_time:88496ms step_avg:58.03ms
step:1526/2330 train_time:88556ms step_avg:58.03ms
step:1527/2330 train_time:88612ms step_avg:58.03ms
step:1528/2330 train_time:88673ms step_avg:58.03ms
step:1529/2330 train_time:88731ms step_avg:58.03ms
step:1530/2330 train_time:88789ms step_avg:58.03ms
step:1531/2330 train_time:88846ms step_avg:58.03ms
step:1532/2330 train_time:88906ms step_avg:58.03ms
step:1533/2330 train_time:88962ms step_avg:58.03ms
step:1534/2330 train_time:89023ms step_avg:58.03ms
step:1535/2330 train_time:89081ms step_avg:58.03ms
step:1536/2330 train_time:89141ms step_avg:58.03ms
step:1537/2330 train_time:89198ms step_avg:58.03ms
step:1538/2330 train_time:89261ms step_avg:58.04ms
step:1539/2330 train_time:89319ms step_avg:58.04ms
step:1540/2330 train_time:89381ms step_avg:58.04ms
step:1541/2330 train_time:89438ms step_avg:58.04ms
step:1542/2330 train_time:89498ms step_avg:58.04ms
step:1543/2330 train_time:89555ms step_avg:58.04ms
step:1544/2330 train_time:89616ms step_avg:58.04ms
step:1545/2330 train_time:89673ms step_avg:58.04ms
step:1546/2330 train_time:89734ms step_avg:58.04ms
step:1547/2330 train_time:89791ms step_avg:58.04ms
step:1548/2330 train_time:89850ms step_avg:58.04ms
step:1549/2330 train_time:89907ms step_avg:58.04ms
step:1550/2330 train_time:89968ms step_avg:58.04ms
step:1551/2330 train_time:90026ms step_avg:58.04ms
step:1552/2330 train_time:90087ms step_avg:58.05ms
step:1553/2330 train_time:90146ms step_avg:58.05ms
step:1554/2330 train_time:90206ms step_avg:58.05ms
step:1555/2330 train_time:90265ms step_avg:58.05ms
step:1556/2330 train_time:90326ms step_avg:58.05ms
step:1557/2330 train_time:90383ms step_avg:58.05ms
step:1558/2330 train_time:90446ms step_avg:58.05ms
step:1559/2330 train_time:90503ms step_avg:58.05ms
step:1560/2330 train_time:90564ms step_avg:58.05ms
step:1561/2330 train_time:90621ms step_avg:58.05ms
step:1562/2330 train_time:90682ms step_avg:58.06ms
step:1563/2330 train_time:90739ms step_avg:58.05ms
step:1564/2330 train_time:90800ms step_avg:58.06ms
step:1565/2330 train_time:90856ms step_avg:58.06ms
step:1566/2330 train_time:90918ms step_avg:58.06ms
step:1567/2330 train_time:90975ms step_avg:58.06ms
step:1568/2330 train_time:91036ms step_avg:58.06ms
step:1569/2330 train_time:91093ms step_avg:58.06ms
step:1570/2330 train_time:91155ms step_avg:58.06ms
step:1571/2330 train_time:91213ms step_avg:58.06ms
step:1572/2330 train_time:91276ms step_avg:58.06ms
step:1573/2330 train_time:91334ms step_avg:58.06ms
step:1574/2330 train_time:91395ms step_avg:58.07ms
step:1575/2330 train_time:91454ms step_avg:58.07ms
step:1576/2330 train_time:91515ms step_avg:58.07ms
step:1577/2330 train_time:91572ms step_avg:58.07ms
step:1578/2330 train_time:91633ms step_avg:58.07ms
step:1579/2330 train_time:91691ms step_avg:58.07ms
step:1580/2330 train_time:91751ms step_avg:58.07ms
step:1581/2330 train_time:91808ms step_avg:58.07ms
step:1582/2330 train_time:91868ms step_avg:58.07ms
step:1583/2330 train_time:91925ms step_avg:58.07ms
step:1584/2330 train_time:91987ms step_avg:58.07ms
step:1585/2330 train_time:92043ms step_avg:58.07ms
step:1586/2330 train_time:92104ms step_avg:58.07ms
step:1587/2330 train_time:92160ms step_avg:58.07ms
step:1588/2330 train_time:92221ms step_avg:58.07ms
step:1589/2330 train_time:92278ms step_avg:58.07ms
step:1590/2330 train_time:92340ms step_avg:58.08ms
step:1591/2330 train_time:92397ms step_avg:58.07ms
step:1592/2330 train_time:92459ms step_avg:58.08ms
step:1593/2330 train_time:92516ms step_avg:58.08ms
step:1594/2330 train_time:92578ms step_avg:58.08ms
step:1595/2330 train_time:92635ms step_avg:58.08ms
step:1596/2330 train_time:92696ms step_avg:58.08ms
step:1597/2330 train_time:92752ms step_avg:58.08ms
step:1598/2330 train_time:92813ms step_avg:58.08ms
step:1599/2330 train_time:92869ms step_avg:58.08ms
step:1600/2330 train_time:92931ms step_avg:58.08ms
step:1601/2330 train_time:92988ms step_avg:58.08ms
step:1602/2330 train_time:93050ms step_avg:58.08ms
step:1603/2330 train_time:93108ms step_avg:58.08ms
step:1604/2330 train_time:93169ms step_avg:58.09ms
step:1605/2330 train_time:93228ms step_avg:58.09ms
step:1606/2330 train_time:93288ms step_avg:58.09ms
step:1607/2330 train_time:93346ms step_avg:58.09ms
step:1608/2330 train_time:93406ms step_avg:58.09ms
step:1609/2330 train_time:93463ms step_avg:58.09ms
step:1610/2330 train_time:93525ms step_avg:58.09ms
step:1611/2330 train_time:93582ms step_avg:58.09ms
step:1612/2330 train_time:93643ms step_avg:58.09ms
step:1613/2330 train_time:93700ms step_avg:58.09ms
step:1614/2330 train_time:93762ms step_avg:58.09ms
step:1615/2330 train_time:93819ms step_avg:58.09ms
step:1616/2330 train_time:93881ms step_avg:58.09ms
step:1617/2330 train_time:93937ms step_avg:58.09ms
step:1618/2330 train_time:93998ms step_avg:58.10ms
step:1619/2330 train_time:94055ms step_avg:58.09ms
step:1620/2330 train_time:94116ms step_avg:58.10ms
step:1621/2330 train_time:94174ms step_avg:58.10ms
step:1622/2330 train_time:94236ms step_avg:58.10ms
step:1623/2330 train_time:94293ms step_avg:58.10ms
step:1624/2330 train_time:94354ms step_avg:58.10ms
step:1625/2330 train_time:94412ms step_avg:58.10ms
step:1626/2330 train_time:94474ms step_avg:58.10ms
step:1627/2330 train_time:94532ms step_avg:58.10ms
step:1628/2330 train_time:94593ms step_avg:58.10ms
step:1629/2330 train_time:94651ms step_avg:58.10ms
step:1630/2330 train_time:94711ms step_avg:58.11ms
step:1631/2330 train_time:94768ms step_avg:58.10ms
step:1632/2330 train_time:94831ms step_avg:58.11ms
step:1633/2330 train_time:94889ms step_avg:58.11ms
step:1634/2330 train_time:94949ms step_avg:58.11ms
step:1635/2330 train_time:95006ms step_avg:58.11ms
step:1636/2330 train_time:95066ms step_avg:58.11ms
step:1637/2330 train_time:95122ms step_avg:58.11ms
step:1638/2330 train_time:95185ms step_avg:58.11ms
step:1639/2330 train_time:95241ms step_avg:58.11ms
step:1640/2330 train_time:95303ms step_avg:58.11ms
step:1641/2330 train_time:95360ms step_avg:58.11ms
step:1642/2330 train_time:95422ms step_avg:58.11ms
step:1643/2330 train_time:95479ms step_avg:58.11ms
step:1644/2330 train_time:95540ms step_avg:58.11ms
step:1645/2330 train_time:95596ms step_avg:58.11ms
step:1646/2330 train_time:95659ms step_avg:58.12ms
step:1647/2330 train_time:95715ms step_avg:58.11ms
step:1648/2330 train_time:95777ms step_avg:58.12ms
step:1649/2330 train_time:95834ms step_avg:58.12ms
step:1650/2330 train_time:95895ms step_avg:58.12ms
step:1651/2330 train_time:95952ms step_avg:58.12ms
step:1652/2330 train_time:96014ms step_avg:58.12ms
step:1653/2330 train_time:96072ms step_avg:58.12ms
step:1654/2330 train_time:96132ms step_avg:58.12ms
step:1655/2330 train_time:96190ms step_avg:58.12ms
step:1656/2330 train_time:96251ms step_avg:58.12ms
step:1657/2330 train_time:96308ms step_avg:58.12ms
step:1658/2330 train_time:96371ms step_avg:58.12ms
step:1659/2330 train_time:96428ms step_avg:58.12ms
step:1660/2330 train_time:96488ms step_avg:58.13ms
step:1661/2330 train_time:96546ms step_avg:58.13ms
step:1662/2330 train_time:96606ms step_avg:58.13ms
step:1663/2330 train_time:96664ms step_avg:58.13ms
step:1664/2330 train_time:96724ms step_avg:58.13ms
step:1665/2330 train_time:96781ms step_avg:58.13ms
step:1666/2330 train_time:96843ms step_avg:58.13ms
step:1667/2330 train_time:96899ms step_avg:58.13ms
step:1668/2330 train_time:96961ms step_avg:58.13ms
step:1669/2330 train_time:97017ms step_avg:58.13ms
step:1670/2330 train_time:97079ms step_avg:58.13ms
step:1671/2330 train_time:97136ms step_avg:58.13ms
step:1672/2330 train_time:97198ms step_avg:58.13ms
step:1673/2330 train_time:97255ms step_avg:58.13ms
step:1674/2330 train_time:97318ms step_avg:58.13ms
step:1675/2330 train_time:97375ms step_avg:58.13ms
step:1676/2330 train_time:97437ms step_avg:58.14ms
step:1677/2330 train_time:97494ms step_avg:58.14ms
step:1678/2330 train_time:97556ms step_avg:58.14ms
step:1679/2330 train_time:97613ms step_avg:58.14ms
step:1680/2330 train_time:97675ms step_avg:58.14ms
step:1681/2330 train_time:97733ms step_avg:58.14ms
step:1682/2330 train_time:97794ms step_avg:58.14ms
step:1683/2330 train_time:97851ms step_avg:58.14ms
step:1684/2330 train_time:97912ms step_avg:58.14ms
step:1685/2330 train_time:97969ms step_avg:58.14ms
step:1686/2330 train_time:98030ms step_avg:58.14ms
step:1687/2330 train_time:98088ms step_avg:58.14ms
step:1688/2330 train_time:98148ms step_avg:58.14ms
step:1689/2330 train_time:98205ms step_avg:58.14ms
step:1690/2330 train_time:98266ms step_avg:58.15ms
step:1691/2330 train_time:98323ms step_avg:58.14ms
step:1692/2330 train_time:98385ms step_avg:58.15ms
step:1693/2330 train_time:98442ms step_avg:58.15ms
step:1694/2330 train_time:98502ms step_avg:58.15ms
step:1695/2330 train_time:98559ms step_avg:58.15ms
step:1696/2330 train_time:98621ms step_avg:58.15ms
step:1697/2330 train_time:98678ms step_avg:58.15ms
step:1698/2330 train_time:98740ms step_avg:58.15ms
step:1699/2330 train_time:98796ms step_avg:58.15ms
step:1700/2330 train_time:98860ms step_avg:58.15ms
step:1701/2330 train_time:98916ms step_avg:58.15ms
step:1702/2330 train_time:98978ms step_avg:58.15ms
step:1703/2330 train_time:99035ms step_avg:58.15ms
step:1704/2330 train_time:99096ms step_avg:58.15ms
step:1705/2330 train_time:99152ms step_avg:58.15ms
step:1706/2330 train_time:99215ms step_avg:58.16ms
step:1707/2330 train_time:99273ms step_avg:58.16ms
step:1708/2330 train_time:99334ms step_avg:58.16ms
step:1709/2330 train_time:99392ms step_avg:58.16ms
step:1710/2330 train_time:99452ms step_avg:58.16ms
step:1711/2330 train_time:99509ms step_avg:58.16ms
step:1712/2330 train_time:99571ms step_avg:58.16ms
step:1713/2330 train_time:99629ms step_avg:58.16ms
step:1714/2330 train_time:99690ms step_avg:58.16ms
step:1715/2330 train_time:99748ms step_avg:58.16ms
step:1716/2330 train_time:99809ms step_avg:58.16ms
step:1717/2330 train_time:99867ms step_avg:58.16ms
step:1718/2330 train_time:99927ms step_avg:58.16ms
step:1719/2330 train_time:99984ms step_avg:58.16ms
step:1720/2330 train_time:100045ms step_avg:58.17ms
step:1721/2330 train_time:100101ms step_avg:58.16ms
step:1722/2330 train_time:100162ms step_avg:58.17ms
step:1723/2330 train_time:100219ms step_avg:58.17ms
step:1724/2330 train_time:100280ms step_avg:58.17ms
step:1725/2330 train_time:100337ms step_avg:58.17ms
step:1726/2330 train_time:100399ms step_avg:58.17ms
step:1727/2330 train_time:100455ms step_avg:58.17ms
step:1728/2330 train_time:100517ms step_avg:58.17ms
step:1729/2330 train_time:100574ms step_avg:58.17ms
step:1730/2330 train_time:100637ms step_avg:58.17ms
step:1731/2330 train_time:100694ms step_avg:58.17ms
step:1732/2330 train_time:100755ms step_avg:58.17ms
step:1733/2330 train_time:100812ms step_avg:58.17ms
step:1734/2330 train_time:100875ms step_avg:58.17ms
step:1735/2330 train_time:100933ms step_avg:58.17ms
step:1736/2330 train_time:100993ms step_avg:58.18ms
step:1737/2330 train_time:101051ms step_avg:58.18ms
step:1738/2330 train_time:101111ms step_avg:58.18ms
step:1739/2330 train_time:101168ms step_avg:58.18ms
step:1740/2330 train_time:101230ms step_avg:58.18ms
step:1741/2330 train_time:101287ms step_avg:58.18ms
step:1742/2330 train_time:101348ms step_avg:58.18ms
step:1743/2330 train_time:101405ms step_avg:58.18ms
step:1744/2330 train_time:101466ms step_avg:58.18ms
step:1745/2330 train_time:101523ms step_avg:58.18ms
step:1746/2330 train_time:101585ms step_avg:58.18ms
step:1747/2330 train_time:101642ms step_avg:58.18ms
step:1748/2330 train_time:101703ms step_avg:58.18ms
step:1749/2330 train_time:101760ms step_avg:58.18ms
step:1750/2330 train_time:101822ms step_avg:58.18ms
step:1750/2330 val_loss:3.8257 train_time:101904ms step_avg:58.23ms
step:1751/2330 train_time:101923ms step_avg:58.21ms
step:1752/2330 train_time:101942ms step_avg:58.19ms
step:1753/2330 train_time:101998ms step_avg:58.18ms
step:1754/2330 train_time:102065ms step_avg:58.19ms
step:1755/2330 train_time:102121ms step_avg:58.19ms
step:1756/2330 train_time:102186ms step_avg:58.19ms
step:1757/2330 train_time:102243ms step_avg:58.19ms
step:1758/2330 train_time:102304ms step_avg:58.19ms
step:1759/2330 train_time:102360ms step_avg:58.19ms
step:1760/2330 train_time:102422ms step_avg:58.19ms
step:1761/2330 train_time:102478ms step_avg:58.19ms
step:1762/2330 train_time:102538ms step_avg:58.19ms
step:1763/2330 train_time:102595ms step_avg:58.19ms
step:1764/2330 train_time:102655ms step_avg:58.19ms
step:1765/2330 train_time:102711ms step_avg:58.19ms
step:1766/2330 train_time:102770ms step_avg:58.19ms
step:1767/2330 train_time:102830ms step_avg:58.19ms
step:1768/2330 train_time:102891ms step_avg:58.20ms
step:1769/2330 train_time:102950ms step_avg:58.20ms
step:1770/2330 train_time:103011ms step_avg:58.20ms
step:1771/2330 train_time:103068ms step_avg:58.20ms
step:1772/2330 train_time:103128ms step_avg:58.20ms
step:1773/2330 train_time:103185ms step_avg:58.20ms
step:1774/2330 train_time:103247ms step_avg:58.20ms
step:1775/2330 train_time:103303ms step_avg:58.20ms
step:1776/2330 train_time:103366ms step_avg:58.20ms
step:1777/2330 train_time:103422ms step_avg:58.20ms
step:1778/2330 train_time:103484ms step_avg:58.20ms
step:1779/2330 train_time:103542ms step_avg:58.20ms
step:1780/2330 train_time:103602ms step_avg:58.20ms
step:1781/2330 train_time:103659ms step_avg:58.20ms
step:1782/2330 train_time:103719ms step_avg:58.20ms
step:1783/2330 train_time:103778ms step_avg:58.20ms
step:1784/2330 train_time:103839ms step_avg:58.21ms
step:1785/2330 train_time:103899ms step_avg:58.21ms
step:1786/2330 train_time:103960ms step_avg:58.21ms
step:1787/2330 train_time:104018ms step_avg:58.21ms
step:1788/2330 train_time:104079ms step_avg:58.21ms
step:1789/2330 train_time:104137ms step_avg:58.21ms
step:1790/2330 train_time:104197ms step_avg:58.21ms
step:1791/2330 train_time:104255ms step_avg:58.21ms
step:1792/2330 train_time:104315ms step_avg:58.21ms
step:1793/2330 train_time:104371ms step_avg:58.21ms
step:1794/2330 train_time:104432ms step_avg:58.21ms
step:1795/2330 train_time:104489ms step_avg:58.21ms
step:1796/2330 train_time:104551ms step_avg:58.21ms
step:1797/2330 train_time:104607ms step_avg:58.21ms
step:1798/2330 train_time:104668ms step_avg:58.21ms
step:1799/2330 train_time:104724ms step_avg:58.21ms
step:1800/2330 train_time:104786ms step_avg:58.21ms
step:1801/2330 train_time:104844ms step_avg:58.21ms
step:1802/2330 train_time:104906ms step_avg:58.22ms
step:1803/2330 train_time:104963ms step_avg:58.22ms
step:1804/2330 train_time:105024ms step_avg:58.22ms
step:1805/2330 train_time:105083ms step_avg:58.22ms
step:1806/2330 train_time:105143ms step_avg:58.22ms
step:1807/2330 train_time:105201ms step_avg:58.22ms
step:1808/2330 train_time:105261ms step_avg:58.22ms
step:1809/2330 train_time:105319ms step_avg:58.22ms
step:1810/2330 train_time:105380ms step_avg:58.22ms
step:1811/2330 train_time:105437ms step_avg:58.22ms
step:1812/2330 train_time:105498ms step_avg:58.22ms
step:1813/2330 train_time:105556ms step_avg:58.22ms
step:1814/2330 train_time:105616ms step_avg:58.22ms
step:1815/2330 train_time:105672ms step_avg:58.22ms
step:1816/2330 train_time:105732ms step_avg:58.22ms
step:1817/2330 train_time:105790ms step_avg:58.22ms
step:1818/2330 train_time:105850ms step_avg:58.22ms
step:1819/2330 train_time:105908ms step_avg:58.22ms
step:1820/2330 train_time:105968ms step_avg:58.22ms
step:1821/2330 train_time:106025ms step_avg:58.22ms
step:1822/2330 train_time:106087ms step_avg:58.23ms
step:1823/2330 train_time:106145ms step_avg:58.23ms
step:1824/2330 train_time:106206ms step_avg:58.23ms
step:1825/2330 train_time:106264ms step_avg:58.23ms
step:1826/2330 train_time:106325ms step_avg:58.23ms
step:1827/2330 train_time:106382ms step_avg:58.23ms
step:1828/2330 train_time:106443ms step_avg:58.23ms
step:1829/2330 train_time:106501ms step_avg:58.23ms
step:1830/2330 train_time:106562ms step_avg:58.23ms
step:1831/2330 train_time:106619ms step_avg:58.23ms
step:1832/2330 train_time:106679ms step_avg:58.23ms
step:1833/2330 train_time:106737ms step_avg:58.23ms
step:1834/2330 train_time:106798ms step_avg:58.23ms
step:1835/2330 train_time:106855ms step_avg:58.23ms
step:1836/2330 train_time:106916ms step_avg:58.23ms
step:1837/2330 train_time:106974ms step_avg:58.23ms
step:1838/2330 train_time:107033ms step_avg:58.23ms
step:1839/2330 train_time:107090ms step_avg:58.23ms
step:1840/2330 train_time:107152ms step_avg:58.23ms
step:1841/2330 train_time:107209ms step_avg:58.23ms
step:1842/2330 train_time:107270ms step_avg:58.24ms
step:1843/2330 train_time:107327ms step_avg:58.23ms
step:1844/2330 train_time:107388ms step_avg:58.24ms
step:1845/2330 train_time:107445ms step_avg:58.24ms
step:1846/2330 train_time:107507ms step_avg:58.24ms
step:1847/2330 train_time:107564ms step_avg:58.24ms
step:1848/2330 train_time:107625ms step_avg:58.24ms
step:1849/2330 train_time:107682ms step_avg:58.24ms
step:1850/2330 train_time:107743ms step_avg:58.24ms
step:1851/2330 train_time:107801ms step_avg:58.24ms
step:1852/2330 train_time:107862ms step_avg:58.24ms
step:1853/2330 train_time:107920ms step_avg:58.24ms
step:1854/2330 train_time:107979ms step_avg:58.24ms
step:1855/2330 train_time:108037ms step_avg:58.24ms
step:1856/2330 train_time:108098ms step_avg:58.24ms
step:1857/2330 train_time:108155ms step_avg:58.24ms
step:1858/2330 train_time:108216ms step_avg:58.24ms
step:1859/2330 train_time:108274ms step_avg:58.24ms
step:1860/2330 train_time:108334ms step_avg:58.24ms
step:1861/2330 train_time:108391ms step_avg:58.24ms
step:1862/2330 train_time:108452ms step_avg:58.24ms
step:1863/2330 train_time:108508ms step_avg:58.24ms
step:1864/2330 train_time:108569ms step_avg:58.25ms
step:1865/2330 train_time:108625ms step_avg:58.24ms
step:1866/2330 train_time:108689ms step_avg:58.25ms
step:1867/2330 train_time:108746ms step_avg:58.25ms
step:1868/2330 train_time:108807ms step_avg:58.25ms
step:1869/2330 train_time:108863ms step_avg:58.25ms
step:1870/2330 train_time:108925ms step_avg:58.25ms
step:1871/2330 train_time:108981ms step_avg:58.25ms
step:1872/2330 train_time:109043ms step_avg:58.25ms
step:1873/2330 train_time:109102ms step_avg:58.25ms
step:1874/2330 train_time:109162ms step_avg:58.25ms
step:1875/2330 train_time:109220ms step_avg:58.25ms
step:1876/2330 train_time:109281ms step_avg:58.25ms
step:1877/2330 train_time:109339ms step_avg:58.25ms
step:1878/2330 train_time:109400ms step_avg:58.25ms
step:1879/2330 train_time:109457ms step_avg:58.25ms
step:1880/2330 train_time:109517ms step_avg:58.25ms
step:1881/2330 train_time:109575ms step_avg:58.25ms
step:1882/2330 train_time:109636ms step_avg:58.25ms
step:1883/2330 train_time:109693ms step_avg:58.25ms
step:1884/2330 train_time:109754ms step_avg:58.26ms
step:1885/2330 train_time:109811ms step_avg:58.26ms
step:1886/2330 train_time:109872ms step_avg:58.26ms
step:1887/2330 train_time:109928ms step_avg:58.26ms
step:1888/2330 train_time:109989ms step_avg:58.26ms
step:1889/2330 train_time:110046ms step_avg:58.26ms
step:1890/2330 train_time:110108ms step_avg:58.26ms
step:1891/2330 train_time:110165ms step_avg:58.26ms
step:1892/2330 train_time:110227ms step_avg:58.26ms
step:1893/2330 train_time:110284ms step_avg:58.26ms
step:1894/2330 train_time:110346ms step_avg:58.26ms
step:1895/2330 train_time:110404ms step_avg:58.26ms
step:1896/2330 train_time:110464ms step_avg:58.26ms
step:1897/2330 train_time:110521ms step_avg:58.26ms
step:1898/2330 train_time:110582ms step_avg:58.26ms
step:1899/2330 train_time:110639ms step_avg:58.26ms
step:1900/2330 train_time:110699ms step_avg:58.26ms
step:1901/2330 train_time:110759ms step_avg:58.26ms
step:1902/2330 train_time:110819ms step_avg:58.26ms
step:1903/2330 train_time:110876ms step_avg:58.26ms
step:1904/2330 train_time:110936ms step_avg:58.26ms
step:1905/2330 train_time:110993ms step_avg:58.26ms
step:1906/2330 train_time:111053ms step_avg:58.27ms
step:1907/2330 train_time:111110ms step_avg:58.26ms
step:1908/2330 train_time:111172ms step_avg:58.27ms
step:1909/2330 train_time:111227ms step_avg:58.26ms
step:1910/2330 train_time:111289ms step_avg:58.27ms
step:1911/2330 train_time:111346ms step_avg:58.27ms
step:1912/2330 train_time:111408ms step_avg:58.27ms
step:1913/2330 train_time:111465ms step_avg:58.27ms
step:1914/2330 train_time:111527ms step_avg:58.27ms
step:1915/2330 train_time:111583ms step_avg:58.27ms
step:1916/2330 train_time:111645ms step_avg:58.27ms
step:1917/2330 train_time:111703ms step_avg:58.27ms
step:1918/2330 train_time:111764ms step_avg:58.27ms
step:1919/2330 train_time:111822ms step_avg:58.27ms
step:1920/2330 train_time:111883ms step_avg:58.27ms
step:1921/2330 train_time:111940ms step_avg:58.27ms
step:1922/2330 train_time:112000ms step_avg:58.27ms
step:1923/2330 train_time:112059ms step_avg:58.27ms
step:1924/2330 train_time:112119ms step_avg:58.27ms
step:1925/2330 train_time:112178ms step_avg:58.27ms
step:1926/2330 train_time:112238ms step_avg:58.28ms
step:1927/2330 train_time:112296ms step_avg:58.28ms
step:1928/2330 train_time:112356ms step_avg:58.28ms
step:1929/2330 train_time:112414ms step_avg:58.28ms
step:1930/2330 train_time:112474ms step_avg:58.28ms
step:1931/2330 train_time:112531ms step_avg:58.28ms
step:1932/2330 train_time:112592ms step_avg:58.28ms
step:1933/2330 train_time:112649ms step_avg:58.28ms
step:1934/2330 train_time:112710ms step_avg:58.28ms
step:1935/2330 train_time:112767ms step_avg:58.28ms
step:1936/2330 train_time:112829ms step_avg:58.28ms
step:1937/2330 train_time:112886ms step_avg:58.28ms
step:1938/2330 train_time:112947ms step_avg:58.28ms
step:1939/2330 train_time:113005ms step_avg:58.28ms
step:1940/2330 train_time:113065ms step_avg:58.28ms
step:1941/2330 train_time:113122ms step_avg:58.28ms
step:1942/2330 train_time:113184ms step_avg:58.28ms
step:1943/2330 train_time:113241ms step_avg:58.28ms
step:1944/2330 train_time:113304ms step_avg:58.28ms
step:1945/2330 train_time:113362ms step_avg:58.28ms
step:1946/2330 train_time:113422ms step_avg:58.28ms
step:1947/2330 train_time:113480ms step_avg:58.28ms
step:1948/2330 train_time:113541ms step_avg:58.29ms
step:1949/2330 train_time:113601ms step_avg:58.29ms
step:1950/2330 train_time:113661ms step_avg:58.29ms
step:1951/2330 train_time:113717ms step_avg:58.29ms
step:1952/2330 train_time:113778ms step_avg:58.29ms
step:1953/2330 train_time:113835ms step_avg:58.29ms
step:1954/2330 train_time:113895ms step_avg:58.29ms
step:1955/2330 train_time:113952ms step_avg:58.29ms
step:1956/2330 train_time:114013ms step_avg:58.29ms
step:1957/2330 train_time:114069ms step_avg:58.29ms
step:1958/2330 train_time:114130ms step_avg:58.29ms
step:1959/2330 train_time:114187ms step_avg:58.29ms
step:1960/2330 train_time:114251ms step_avg:58.29ms
step:1961/2330 train_time:114307ms step_avg:58.29ms
step:1962/2330 train_time:114369ms step_avg:58.29ms
step:1963/2330 train_time:114425ms step_avg:58.29ms
step:1964/2330 train_time:114487ms step_avg:58.29ms
step:1965/2330 train_time:114544ms step_avg:58.29ms
step:1966/2330 train_time:114606ms step_avg:58.29ms
step:1967/2330 train_time:114664ms step_avg:58.29ms
step:1968/2330 train_time:114724ms step_avg:58.29ms
step:1969/2330 train_time:114781ms step_avg:58.29ms
step:1970/2330 train_time:114842ms step_avg:58.30ms
step:1971/2330 train_time:114900ms step_avg:58.30ms
step:1972/2330 train_time:114960ms step_avg:58.30ms
step:1973/2330 train_time:115019ms step_avg:58.30ms
step:1974/2330 train_time:115079ms step_avg:58.30ms
step:1975/2330 train_time:115137ms step_avg:58.30ms
step:1976/2330 train_time:115197ms step_avg:58.30ms
step:1977/2330 train_time:115254ms step_avg:58.30ms
step:1978/2330 train_time:115314ms step_avg:58.30ms
step:1979/2330 train_time:115371ms step_avg:58.30ms
step:1980/2330 train_time:115433ms step_avg:58.30ms
step:1981/2330 train_time:115490ms step_avg:58.30ms
step:1982/2330 train_time:115551ms step_avg:58.30ms
step:1983/2330 train_time:115608ms step_avg:58.30ms
step:1984/2330 train_time:115670ms step_avg:58.30ms
step:1985/2330 train_time:115726ms step_avg:58.30ms
step:1986/2330 train_time:115789ms step_avg:58.30ms
step:1987/2330 train_time:115846ms step_avg:58.30ms
step:1988/2330 train_time:115908ms step_avg:58.30ms
step:1989/2330 train_time:115964ms step_avg:58.30ms
step:1990/2330 train_time:116026ms step_avg:58.30ms
step:1991/2330 train_time:116083ms step_avg:58.30ms
step:1992/2330 train_time:116146ms step_avg:58.31ms
step:1993/2330 train_time:116204ms step_avg:58.31ms
step:1994/2330 train_time:116265ms step_avg:58.31ms
step:1995/2330 train_time:116323ms step_avg:58.31ms
step:1996/2330 train_time:116383ms step_avg:58.31ms
step:1997/2330 train_time:116441ms step_avg:58.31ms
step:1998/2330 train_time:116501ms step_avg:58.31ms
step:1999/2330 train_time:116559ms step_avg:58.31ms
step:2000/2330 train_time:116619ms step_avg:58.31ms
step:2000/2330 val_loss:3.7640 train_time:116700ms step_avg:58.35ms
step:2001/2330 train_time:116720ms step_avg:58.33ms
step:2002/2330 train_time:116742ms step_avg:58.31ms
step:2003/2330 train_time:116803ms step_avg:58.31ms
step:2004/2330 train_time:116866ms step_avg:58.32ms
step:2005/2330 train_time:116923ms step_avg:58.32ms
step:2006/2330 train_time:116984ms step_avg:58.32ms
step:2007/2330 train_time:117041ms step_avg:58.32ms
step:2008/2330 train_time:117102ms step_avg:58.32ms
step:2009/2330 train_time:117158ms step_avg:58.32ms
step:2010/2330 train_time:117219ms step_avg:58.32ms
step:2011/2330 train_time:117275ms step_avg:58.32ms
step:2012/2330 train_time:117335ms step_avg:58.32ms
step:2013/2330 train_time:117392ms step_avg:58.32ms
step:2014/2330 train_time:117452ms step_avg:58.32ms
step:2015/2330 train_time:117508ms step_avg:58.32ms
step:2016/2330 train_time:117568ms step_avg:58.32ms
step:2017/2330 train_time:117625ms step_avg:58.32ms
step:2018/2330 train_time:117686ms step_avg:58.32ms
step:2019/2330 train_time:117745ms step_avg:58.32ms
step:2020/2330 train_time:117807ms step_avg:58.32ms
step:2021/2330 train_time:117865ms step_avg:58.32ms
step:2022/2330 train_time:117929ms step_avg:58.32ms
step:2023/2330 train_time:117986ms step_avg:58.32ms
step:2024/2330 train_time:118046ms step_avg:58.32ms
step:2025/2330 train_time:118103ms step_avg:58.32ms
step:2026/2330 train_time:118164ms step_avg:58.32ms
step:2027/2330 train_time:118221ms step_avg:58.32ms
step:2028/2330 train_time:118282ms step_avg:58.32ms
step:2029/2330 train_time:118339ms step_avg:58.32ms
step:2030/2330 train_time:118399ms step_avg:58.32ms
step:2031/2330 train_time:118455ms step_avg:58.32ms
step:2032/2330 train_time:118515ms step_avg:58.32ms
step:2033/2330 train_time:118572ms step_avg:58.32ms
step:2034/2330 train_time:118631ms step_avg:58.32ms
step:2035/2330 train_time:118688ms step_avg:58.32ms
step:2036/2330 train_time:118750ms step_avg:58.33ms
step:2037/2330 train_time:118807ms step_avg:58.32ms
step:2038/2330 train_time:118870ms step_avg:58.33ms
step:2039/2330 train_time:118928ms step_avg:58.33ms
step:2040/2330 train_time:118991ms step_avg:58.33ms
step:2041/2330 train_time:119047ms step_avg:58.33ms
step:2042/2330 train_time:119110ms step_avg:58.33ms
step:2043/2330 train_time:119167ms step_avg:58.33ms
step:2044/2330 train_time:119229ms step_avg:58.33ms
step:2045/2330 train_time:119285ms step_avg:58.33ms
step:2046/2330 train_time:119347ms step_avg:58.33ms
step:2047/2330 train_time:119404ms step_avg:58.33ms
step:2048/2330 train_time:119464ms step_avg:58.33ms
step:2049/2330 train_time:119521ms step_avg:58.33ms
step:2050/2330 train_time:119581ms step_avg:58.33ms
step:2051/2330 train_time:119639ms step_avg:58.33ms
step:2052/2330 train_time:119699ms step_avg:58.33ms
step:2053/2330 train_time:119756ms step_avg:58.33ms
step:2054/2330 train_time:119817ms step_avg:58.33ms
step:2055/2330 train_time:119875ms step_avg:58.33ms
step:2056/2330 train_time:119938ms step_avg:58.34ms
step:2057/2330 train_time:119995ms step_avg:58.33ms
step:2058/2330 train_time:120056ms step_avg:58.34ms
step:2059/2330 train_time:120114ms step_avg:58.34ms
step:2060/2330 train_time:120175ms step_avg:58.34ms
step:2061/2330 train_time:120230ms step_avg:58.34ms
step:2062/2330 train_time:120293ms step_avg:58.34ms
step:2063/2330 train_time:120349ms step_avg:58.34ms
step:2064/2330 train_time:120411ms step_avg:58.34ms
step:2065/2330 train_time:120467ms step_avg:58.34ms
step:2066/2330 train_time:120529ms step_avg:58.34ms
step:2067/2330 train_time:120585ms step_avg:58.34ms
step:2068/2330 train_time:120647ms step_avg:58.34ms
step:2069/2330 train_time:120704ms step_avg:58.34ms
step:2070/2330 train_time:120765ms step_avg:58.34ms
step:2071/2330 train_time:120823ms step_avg:58.34ms
step:2072/2330 train_time:120885ms step_avg:58.34ms
step:2073/2330 train_time:120944ms step_avg:58.34ms
step:2074/2330 train_time:121004ms step_avg:58.34ms
step:2075/2330 train_time:121062ms step_avg:58.34ms
step:2076/2330 train_time:121122ms step_avg:58.34ms
step:2077/2330 train_time:121181ms step_avg:58.34ms
step:2078/2330 train_time:121241ms step_avg:58.35ms
step:2079/2330 train_time:121298ms step_avg:58.34ms
step:2080/2330 train_time:121358ms step_avg:58.35ms
step:2081/2330 train_time:121415ms step_avg:58.34ms
step:2082/2330 train_time:121476ms step_avg:58.35ms
step:2083/2330 train_time:121532ms step_avg:58.34ms
step:2084/2330 train_time:121593ms step_avg:58.35ms
step:2085/2330 train_time:121649ms step_avg:58.34ms
step:2086/2330 train_time:121710ms step_avg:58.35ms
step:2087/2330 train_time:121767ms step_avg:58.35ms
step:2088/2330 train_time:121830ms step_avg:58.35ms
step:2089/2330 train_time:121887ms step_avg:58.35ms
step:2090/2330 train_time:121948ms step_avg:58.35ms
step:2091/2330 train_time:122005ms step_avg:58.35ms
step:2092/2330 train_time:122066ms step_avg:58.35ms
step:2093/2330 train_time:122124ms step_avg:58.35ms
step:2094/2330 train_time:122186ms step_avg:58.35ms
step:2095/2330 train_time:122243ms step_avg:58.35ms
step:2096/2330 train_time:122304ms step_avg:58.35ms
step:2097/2330 train_time:122361ms step_avg:58.35ms
step:2098/2330 train_time:122422ms step_avg:58.35ms
step:2099/2330 train_time:122480ms step_avg:58.35ms
step:2100/2330 train_time:122540ms step_avg:58.35ms
step:2101/2330 train_time:122597ms step_avg:58.35ms
step:2102/2330 train_time:122658ms step_avg:58.35ms
step:2103/2330 train_time:122714ms step_avg:58.35ms
step:2104/2330 train_time:122776ms step_avg:58.35ms
step:2105/2330 train_time:122832ms step_avg:58.35ms
step:2106/2330 train_time:122894ms step_avg:58.35ms
step:2107/2330 train_time:122950ms step_avg:58.35ms
step:2108/2330 train_time:123013ms step_avg:58.36ms
step:2109/2330 train_time:123069ms step_avg:58.35ms
step:2110/2330 train_time:123132ms step_avg:58.36ms
step:2111/2330 train_time:123189ms step_avg:58.36ms
step:2112/2330 train_time:123250ms step_avg:58.36ms
step:2113/2330 train_time:123307ms step_avg:58.36ms
step:2114/2330 train_time:123368ms step_avg:58.36ms
step:2115/2330 train_time:123425ms step_avg:58.36ms
step:2116/2330 train_time:123486ms step_avg:58.36ms
step:2117/2330 train_time:123543ms step_avg:58.36ms
step:2118/2330 train_time:123603ms step_avg:58.36ms
step:2119/2330 train_time:123660ms step_avg:58.36ms
step:2120/2330 train_time:123721ms step_avg:58.36ms
step:2121/2330 train_time:123778ms step_avg:58.36ms
step:2122/2330 train_time:123840ms step_avg:58.36ms
step:2123/2330 train_time:123897ms step_avg:58.36ms
step:2124/2330 train_time:123959ms step_avg:58.36ms
step:2125/2330 train_time:124015ms step_avg:58.36ms
step:2126/2330 train_time:124077ms step_avg:58.36ms
step:2127/2330 train_time:124133ms step_avg:58.36ms
step:2128/2330 train_time:124195ms step_avg:58.36ms
step:2129/2330 train_time:124251ms step_avg:58.36ms
step:2130/2330 train_time:124313ms step_avg:58.36ms
step:2131/2330 train_time:124370ms step_avg:58.36ms
step:2132/2330 train_time:124431ms step_avg:58.36ms
step:2133/2330 train_time:124488ms step_avg:58.36ms
step:2134/2330 train_time:124549ms step_avg:58.36ms
step:2135/2330 train_time:124605ms step_avg:58.36ms
step:2136/2330 train_time:124667ms step_avg:58.36ms
step:2137/2330 train_time:124725ms step_avg:58.36ms
step:2138/2330 train_time:124786ms step_avg:58.37ms
step:2139/2330 train_time:124842ms step_avg:58.36ms
step:2140/2330 train_time:124904ms step_avg:58.37ms
step:2141/2330 train_time:124961ms step_avg:58.37ms
step:2142/2330 train_time:125023ms step_avg:58.37ms
step:2143/2330 train_time:125082ms step_avg:58.37ms
step:2144/2330 train_time:125142ms step_avg:58.37ms
step:2145/2330 train_time:125201ms step_avg:58.37ms
step:2146/2330 train_time:125261ms step_avg:58.37ms
step:2147/2330 train_time:125318ms step_avg:58.37ms
step:2148/2330 train_time:125379ms step_avg:58.37ms
step:2149/2330 train_time:125436ms step_avg:58.37ms
step:2150/2330 train_time:125497ms step_avg:58.37ms
step:2151/2330 train_time:125553ms step_avg:58.37ms
step:2152/2330 train_time:125614ms step_avg:58.37ms
step:2153/2330 train_time:125671ms step_avg:58.37ms
step:2154/2330 train_time:125732ms step_avg:58.37ms
step:2155/2330 train_time:125788ms step_avg:58.37ms
step:2156/2330 train_time:125851ms step_avg:58.37ms
step:2157/2330 train_time:125907ms step_avg:58.37ms
step:2158/2330 train_time:125971ms step_avg:58.37ms
step:2159/2330 train_time:126028ms step_avg:58.37ms
step:2160/2330 train_time:126089ms step_avg:58.37ms
step:2161/2330 train_time:126146ms step_avg:58.37ms
step:2162/2330 train_time:126208ms step_avg:58.38ms
step:2163/2330 train_time:126265ms step_avg:58.37ms
step:2164/2330 train_time:126326ms step_avg:58.38ms
step:2165/2330 train_time:126384ms step_avg:58.38ms
step:2166/2330 train_time:126444ms step_avg:58.38ms
step:2167/2330 train_time:126501ms step_avg:58.38ms
step:2168/2330 train_time:126562ms step_avg:58.38ms
step:2169/2330 train_time:126619ms step_avg:58.38ms
step:2170/2330 train_time:126680ms step_avg:58.38ms
step:2171/2330 train_time:126737ms step_avg:58.38ms
step:2172/2330 train_time:126798ms step_avg:58.38ms
step:2173/2330 train_time:126855ms step_avg:58.38ms
step:2174/2330 train_time:126916ms step_avg:58.38ms
step:2175/2330 train_time:126973ms step_avg:58.38ms
step:2176/2330 train_time:127035ms step_avg:58.38ms
step:2177/2330 train_time:127091ms step_avg:58.38ms
step:2178/2330 train_time:127153ms step_avg:58.38ms
step:2179/2330 train_time:127209ms step_avg:58.38ms
step:2180/2330 train_time:127272ms step_avg:58.38ms
step:2181/2330 train_time:127329ms step_avg:58.38ms
step:2182/2330 train_time:127390ms step_avg:58.38ms
step:2183/2330 train_time:127446ms step_avg:58.38ms
step:2184/2330 train_time:127508ms step_avg:58.38ms
step:2185/2330 train_time:127565ms step_avg:58.38ms
step:2186/2330 train_time:127627ms step_avg:58.38ms
step:2187/2330 train_time:127684ms step_avg:58.38ms
step:2188/2330 train_time:127745ms step_avg:58.38ms
step:2189/2330 train_time:127803ms step_avg:58.38ms
step:2190/2330 train_time:127863ms step_avg:58.38ms
step:2191/2330 train_time:127921ms step_avg:58.38ms
step:2192/2330 train_time:127982ms step_avg:58.39ms
step:2193/2330 train_time:128040ms step_avg:58.39ms
step:2194/2330 train_time:128100ms step_avg:58.39ms
step:2195/2330 train_time:128157ms step_avg:58.39ms
step:2196/2330 train_time:128217ms step_avg:58.39ms
step:2197/2330 train_time:128274ms step_avg:58.39ms
step:2198/2330 train_time:128336ms step_avg:58.39ms
step:2199/2330 train_time:128392ms step_avg:58.39ms
step:2200/2330 train_time:128453ms step_avg:58.39ms
step:2201/2330 train_time:128510ms step_avg:58.39ms
step:2202/2330 train_time:128572ms step_avg:58.39ms
step:2203/2330 train_time:128629ms step_avg:58.39ms
step:2204/2330 train_time:128691ms step_avg:58.39ms
step:2205/2330 train_time:128747ms step_avg:58.39ms
step:2206/2330 train_time:128808ms step_avg:58.39ms
step:2207/2330 train_time:128865ms step_avg:58.39ms
step:2208/2330 train_time:128928ms step_avg:58.39ms
step:2209/2330 train_time:128985ms step_avg:58.39ms
step:2210/2330 train_time:129046ms step_avg:58.39ms
step:2211/2330 train_time:129104ms step_avg:58.39ms
step:2212/2330 train_time:129164ms step_avg:58.39ms
step:2213/2330 train_time:129222ms step_avg:58.39ms
step:2214/2330 train_time:129282ms step_avg:58.39ms
step:2215/2330 train_time:129339ms step_avg:58.39ms
step:2216/2330 train_time:129401ms step_avg:58.39ms
step:2217/2330 train_time:129458ms step_avg:58.39ms
step:2218/2330 train_time:129519ms step_avg:58.39ms
step:2219/2330 train_time:129576ms step_avg:58.39ms
step:2220/2330 train_time:129638ms step_avg:58.40ms
step:2221/2330 train_time:129694ms step_avg:58.39ms
step:2222/2330 train_time:129755ms step_avg:58.40ms
step:2223/2330 train_time:129811ms step_avg:58.39ms
step:2224/2330 train_time:129873ms step_avg:58.40ms
step:2225/2330 train_time:129930ms step_avg:58.40ms
step:2226/2330 train_time:129991ms step_avg:58.40ms
step:2227/2330 train_time:130047ms step_avg:58.40ms
step:2228/2330 train_time:130110ms step_avg:58.40ms
step:2229/2330 train_time:130168ms step_avg:58.40ms
step:2230/2330 train_time:130229ms step_avg:58.40ms
step:2231/2330 train_time:130286ms step_avg:58.40ms
step:2232/2330 train_time:130347ms step_avg:58.40ms
step:2233/2330 train_time:130404ms step_avg:58.40ms
step:2234/2330 train_time:130465ms step_avg:58.40ms
step:2235/2330 train_time:130523ms step_avg:58.40ms
step:2236/2330 train_time:130584ms step_avg:58.40ms
step:2237/2330 train_time:130641ms step_avg:58.40ms
step:2238/2330 train_time:130702ms step_avg:58.40ms
step:2239/2330 train_time:130759ms step_avg:58.40ms
step:2240/2330 train_time:130820ms step_avg:58.40ms
step:2241/2330 train_time:130878ms step_avg:58.40ms
step:2242/2330 train_time:130939ms step_avg:58.40ms
step:2243/2330 train_time:130996ms step_avg:58.40ms
step:2244/2330 train_time:131057ms step_avg:58.40ms
step:2245/2330 train_time:131114ms step_avg:58.40ms
step:2246/2330 train_time:131176ms step_avg:58.40ms
step:2247/2330 train_time:131232ms step_avg:58.40ms
step:2248/2330 train_time:131294ms step_avg:58.40ms
step:2249/2330 train_time:131350ms step_avg:58.40ms
step:2250/2330 train_time:131414ms step_avg:58.41ms
step:2250/2330 val_loss:3.7166 train_time:131495ms step_avg:58.44ms
step:2251/2330 train_time:131515ms step_avg:58.43ms
step:2252/2330 train_time:131534ms step_avg:58.41ms
step:2253/2330 train_time:131592ms step_avg:58.41ms
step:2254/2330 train_time:131656ms step_avg:58.41ms
step:2255/2330 train_time:131713ms step_avg:58.41ms
step:2256/2330 train_time:131777ms step_avg:58.41ms
step:2257/2330 train_time:131833ms step_avg:58.41ms
step:2258/2330 train_time:131894ms step_avg:58.41ms
step:2259/2330 train_time:131950ms step_avg:58.41ms
step:2260/2330 train_time:132010ms step_avg:58.41ms
step:2261/2330 train_time:132067ms step_avg:58.41ms
step:2262/2330 train_time:132126ms step_avg:58.41ms
step:2263/2330 train_time:132182ms step_avg:58.41ms
step:2264/2330 train_time:132244ms step_avg:58.41ms
step:2265/2330 train_time:132300ms step_avg:58.41ms
step:2266/2330 train_time:132360ms step_avg:58.41ms
step:2267/2330 train_time:132417ms step_avg:58.41ms
step:2268/2330 train_time:132480ms step_avg:58.41ms
step:2269/2330 train_time:132539ms step_avg:58.41ms
step:2270/2330 train_time:132600ms step_avg:58.41ms
step:2271/2330 train_time:132658ms step_avg:58.41ms
step:2272/2330 train_time:132721ms step_avg:58.42ms
step:2273/2330 train_time:132778ms step_avg:58.42ms
step:2274/2330 train_time:132840ms step_avg:58.42ms
step:2275/2330 train_time:132898ms step_avg:58.42ms
step:2276/2330 train_time:132958ms step_avg:58.42ms
step:2277/2330 train_time:133015ms step_avg:58.42ms
step:2278/2330 train_time:133074ms step_avg:58.42ms
step:2279/2330 train_time:133131ms step_avg:58.42ms
step:2280/2330 train_time:133192ms step_avg:58.42ms
step:2281/2330 train_time:133247ms step_avg:58.42ms
step:2282/2330 train_time:133307ms step_avg:58.42ms
step:2283/2330 train_time:133364ms step_avg:58.42ms
step:2284/2330 train_time:133425ms step_avg:58.42ms
step:2285/2330 train_time:133482ms step_avg:58.42ms
step:2286/2330 train_time:133545ms step_avg:58.42ms
step:2287/2330 train_time:133602ms step_avg:58.42ms
step:2288/2330 train_time:133663ms step_avg:58.42ms
step:2289/2330 train_time:133721ms step_avg:58.42ms
step:2290/2330 train_time:133782ms step_avg:58.42ms
step:2291/2330 train_time:133841ms step_avg:58.42ms
step:2292/2330 train_time:133901ms step_avg:58.42ms
step:2293/2330 train_time:133958ms step_avg:58.42ms
step:2294/2330 train_time:134019ms step_avg:58.42ms
step:2295/2330 train_time:134077ms step_avg:58.42ms
step:2296/2330 train_time:134138ms step_avg:58.42ms
step:2297/2330 train_time:134196ms step_avg:58.42ms
step:2298/2330 train_time:134256ms step_avg:58.42ms
step:2299/2330 train_time:134313ms step_avg:58.42ms
step:2300/2330 train_time:134373ms step_avg:58.42ms
step:2301/2330 train_time:134430ms step_avg:58.42ms
step:2302/2330 train_time:134491ms step_avg:58.42ms
step:2303/2330 train_time:134548ms step_avg:58.42ms
step:2304/2330 train_time:134608ms step_avg:58.42ms
step:2305/2330 train_time:134665ms step_avg:58.42ms
step:2306/2330 train_time:134728ms step_avg:58.42ms
step:2307/2330 train_time:134784ms step_avg:58.42ms
step:2308/2330 train_time:134848ms step_avg:58.43ms
step:2309/2330 train_time:134905ms step_avg:58.43ms
step:2310/2330 train_time:134965ms step_avg:58.43ms
step:2311/2330 train_time:135022ms step_avg:58.43ms
step:2312/2330 train_time:135084ms step_avg:58.43ms
step:2313/2330 train_time:135141ms step_avg:58.43ms
step:2314/2330 train_time:135203ms step_avg:58.43ms
step:2315/2330 train_time:135261ms step_avg:58.43ms
step:2316/2330 train_time:135321ms step_avg:58.43ms
step:2317/2330 train_time:135379ms step_avg:58.43ms
step:2318/2330 train_time:135439ms step_avg:58.43ms
step:2319/2330 train_time:135497ms step_avg:58.43ms
step:2320/2330 train_time:135557ms step_avg:58.43ms
step:2321/2330 train_time:135615ms step_avg:58.43ms
step:2322/2330 train_time:135675ms step_avg:58.43ms
step:2323/2330 train_time:135733ms step_avg:58.43ms
step:2324/2330 train_time:135795ms step_avg:58.43ms
step:2325/2330 train_time:135853ms step_avg:58.43ms
step:2326/2330 train_time:135913ms step_avg:58.43ms
step:2327/2330 train_time:135969ms step_avg:58.43ms
step:2328/2330 train_time:136031ms step_avg:58.43ms
step:2329/2330 train_time:136087ms step_avg:58.43ms
step:2330/2330 train_time:136148ms step_avg:58.43ms
step:2330/2330 val_loss:3.7012 train_time:136230ms step_avg:58.47ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
